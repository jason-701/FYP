{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Input, LeakyReLU, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gender', 'age', 'hypertension', 'heart_disease', 'smoking_history',\n",
      "       'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('diabetes_prediction_dataset.csv')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  hypertension  heart_disease    bmi  HbA1c_level  blood_glucose_level  \\\n",
      "0  80.0             0              1  25.19          6.6                  140   \n",
      "1  54.0             0              0  27.32          6.6                   80   \n",
      "2  28.0             0              0  27.32          5.7                  158   \n",
      "3  36.0             0              0  23.45          5.0                  155   \n",
      "4  76.0             1              1  20.14          4.8                  155   \n",
      "\n",
      "   diabetes  gender_Male  gender_Other  smoking_history_current  \\\n",
      "0         0        False         False                    False   \n",
      "1         0        False         False                    False   \n",
      "2         0         True         False                    False   \n",
      "3         0        False         False                     True   \n",
      "4         0         True         False                     True   \n",
      "\n",
      "   smoking_history_ever  smoking_history_former  smoking_history_never  \\\n",
      "0                 False                   False                   True   \n",
      "1                 False                   False                  False   \n",
      "2                 False                   False                   True   \n",
      "3                 False                   False                  False   \n",
      "4                 False                   False                  False   \n",
      "\n",
      "   smoking_history_not current  \n",
      "0                        False  \n",
      "1                        False  \n",
      "2                        False  \n",
      "3                        False  \n",
      "4                        False  \n"
     ]
    }
   ],
   "source": [
    "df_encoded = pd.get_dummies(df, columns=['gender', 'smoking_history'], drop_first=True)\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age group 0-29: 32435 records\n",
      "Age group 30-59: 42510 records\n",
      "Age group 60-89: 25055 records\n"
     ]
    }
   ],
   "source": [
    "age_groups = [(0, 29), (30, 59), (60, 89)]\n",
    "\n",
    "datasets = {}\n",
    "for start, end in age_groups:\n",
    "    group_name = f\"{start}-{end}\"\n",
    "    datasets[group_name] = df_encoded[(df_encoded['age'] >= start) & (df_encoded['age'] <= end)]\n",
    "\n",
    "# Display the number of records in each dataset\n",
    "for group_name, dataset in datasets.items():\n",
    "    print(f\"Age group {group_name}: {len(dataset)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = datasets['60-89']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level',\n",
      "       'blood_glucose_level', 'gender_Male', 'gender_Other',\n",
      "       'smoking_history_current', 'smoking_history_ever',\n",
      "       'smoking_history_former', 'smoking_history_never',\n",
      "       'smoking_history_not current'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Separate age 79-80 group (overrepresented)\n",
    "df_80 = testDF[(testDF['age'] == 80)]\n",
    "\n",
    "# Separate other age groups (underrepresented)\n",
    "df_underrepresented = testDF[(testDF['age'] != 80)]\n",
    "\n",
    "# Downsample the 79-80 group to match the underrepresented groups\n",
    "df_80_downsampled = resample(df_80, \n",
    "                                replace=False,    # sample without replacement\n",
    "                                n_samples=1100,   # define desired sample size\n",
    "                                random_state=42)  # reproducibility\n",
    "\n",
    "# Optionally, you can oversample underrepresented groups\n",
    "df_underrepresented_oversampled = resample(df_underrepresented, \n",
    "                                           replace=True,     # sample with replacement\n",
    "                                           n_samples=20000,   # match overrepresented group size\n",
    "                                           random_state=42)\n",
    "\n",
    "# Combine the resampled datasets\n",
    "df_resampled = pd.concat([df_80_downsampled, df_underrepresented_oversampled])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "df_resampled = df_resampled.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split into features and labels\n",
    "X_unscaled = df_resampled.drop(['diabetes'], axis=1)  # Assuming 'diabetes' is the target\n",
    "y = df_resampled['diabetes']\n",
    "\n",
    "# Normalize the features\n",
    "numerical_columns = X_unscaled.select_dtypes(include=np.number).columns\n",
    "boolean_columns = X_unscaled.select_dtypes(include=bool).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_numerical_scaled = pd.DataFrame(scaler.fit_transform(X_unscaled[numerical_columns]), columns=numerical_columns)\n",
    "\n",
    "X_scaled = pd.concat([df_numerical_scaled, X_unscaled[boolean_columns]], axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file exists. Loading the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ROG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 58 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Mean Reconstruction Error Percentage: 24.71%\n"
     ]
    }
   ],
   "source": [
    "# Filename for the autoencoder model\n",
    "model_file = 'autoencoder_resampled.keras'\n",
    "\n",
    "# Check if the model file exists, load it if available, otherwise train a new model\n",
    "if os.path.exists(model_file):\n",
    "    print(\"Model file exists. Loading the model...\")\n",
    "    autoencoder = load_model(model_file)\n",
    "else:\n",
    "    print(\"Model file does not exist. Training a new model...\")\n",
    "\n",
    "    # Define input dimensions and encoding dimensions\n",
    "    input_dim = x_train.shape[1]  # Assuming 12 features\n",
    "    encoding_dim = 13  # Bottleneck layer dimension\n",
    "\n",
    "    # Define the Encoder\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(256)(input_layer)\n",
    "    encoded = LeakyReLU()(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dropout(0.3)(encoded)\n",
    "    encoded = Dense(128)(encoded)\n",
    "    encoded = LeakyReLU()(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dropout(0.3)(encoded)\n",
    "    encoded = Dense(64)(encoded)\n",
    "    encoded = LeakyReLU()(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dropout(0.3)(encoded)\n",
    "    encoded = Dense(encoding_dim, activation='linear')(encoded)  # Bottleneck layer\n",
    "    \n",
    "    # Create the Encoder model\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    encoder.compile(optimizer=Adam(learning_rate=0.00005), loss='mse')\n",
    "\n",
    "    # Train the Encoder\n",
    "    history_encoder = encoder.fit(x_train, x_train, epochs=200, validation_split=0.2, verbose=1,\n",
    "                                  callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)])\n",
    "\n",
    "    # Define the Decoder\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    decoded = Dense(64)(encoded_input)\n",
    "    decoded = LeakyReLU()(decoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dense(128)(decoded)\n",
    "    decoded = LeakyReLU()(decoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dense(256)(decoded)\n",
    "    decoded = LeakyReLU()(decoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    \n",
    "    # Create the Decoder model\n",
    "    decoder = Model(encoded_input, decoded)\n",
    "    decoder.compile(optimizer=Adam(learning_rate=0.00005), loss='mse')\n",
    "\n",
    "    # Train the Decoder\n",
    "    encoded_train = encoder.predict(x_train)\n",
    "    history_decoder = decoder.fit(encoded_train, x_train, epochs=200, validation_split=0.2, verbose=1,\n",
    "                                  callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)])\n",
    "\n",
    "    # Combine Encoder and Decoder to form the Autoencoder\n",
    "    autoencoder_input = Input(shape=(input_dim,))\n",
    "    encoded_repr = encoder(autoencoder_input)\n",
    "    reconstructed = decoder(encoded_repr)\n",
    "    autoencoder = Model(autoencoder_input, reconstructed)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.00005), loss='mse')\n",
    "\n",
    "    # Save the model as autoencoder.keras\n",
    "    autoencoder.save(model_file)\n",
    "    print(\"Model trained and saved.\")\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history_encoder.history['loss'], label='Encoder Training Loss')\n",
    "    plt.plot(history_encoder.history['val_loss'], label='Encoder Validation Loss')\n",
    "    plt.plot(history_decoder.history['loss'], label='Decoder Training Loss')\n",
    "    plt.plot(history_decoder.history['val_loss'], label='Decoder Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Calculate reconstruction error on the test set\n",
    "reconstructed_test = autoencoder.predict(x_test)\n",
    "reconstruction_error = np.mean(np.square(x_test - reconstructed_test), axis=1)\n",
    "\n",
    "# Convert reconstruction error to percentage\n",
    "reconstruction_error_percentage = np.mean(reconstruction_error) * 100\n",
    "\n",
    "# Print mean reconstruction error percentage\n",
    "print(f'Mean Reconstruction Error Percentage: {reconstruction_error_percentage:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Test data for age range 0-29 and 30-59 created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ROG\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 58 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Load the autoencoder model\n",
    "model_file = 'autoencoder_resampled.keras'\n",
    "autoencoder = load_model(model_file)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "\n",
    "# Create test data for other age ranges\n",
    "testDF_0_29 = datasets['0-29']\n",
    "testDF_30_59 = datasets['30-59']\n",
    "\n",
    "# Prepare the test data\n",
    "x_test_0_29 = testDF_0_29.drop(['diabetes'], axis=1)\n",
    "y_test_0_29 = testDF_0_29['diabetes']\n",
    "\n",
    "x_test_30_59 = testDF_30_59.drop(['diabetes'], axis=1)\n",
    "y_test_30_59 = testDF_30_59['diabetes']\n",
    "\n",
    "print(\"Test data for age range 0-29 and 30-59 created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the test data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "temp = pd.DataFrame(scaler.fit_transform(x_test_0_29[numerical_columns]), columns=numerical_columns)\n",
    "x_test_0_29_scaled = pd.concat([temp, x_test_0_29[boolean_columns]], axis=1)\n",
    "\n",
    "temp = pd.DataFrame(scaler.fit_transform(x_test_30_59[numerical_columns]), columns=numerical_columns)\n",
    "x_test_30_59_scaled = pd.concat([temp, x_test_30_59[boolean_columns]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1699/1699\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "Reconstruction Error Percentage for 0-29 age group: 36.23308966636414\n"
     ]
    }
   ],
   "source": [
    "# Predict the reconstruction error for the 0-29 age group\n",
    "reconstructed_test_0_29 = autoencoder.predict(x_test_0_29_scaled)\n",
    "reconstruction_error_0_29 = np.mean(np.square(x_test_0_29_scaled - reconstructed_test_0_29), axis=1)\n",
    "\n",
    "# Convert reconstruction error to percentage\n",
    "reconstruction_error_percentage_0_29 = np.mean(reconstruction_error_0_29) * 100\n",
    "\n",
    "# Print reconstruction error percentage for 0-29 age group\n",
    "print(f'Reconstruction Error Percentage for 0-29 age group: {reconstruction_error_percentage_0_29}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2093/2093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "Reconstruction Error Percentage for 30-59 age group: 28.13130527173132\n"
     ]
    }
   ],
   "source": [
    "# Predict the reconstruction error for the 60-89 age group\n",
    "reconstructed_test_30_59 = autoencoder.predict(x_test_30_59_scaled)\n",
    "reconstruction_error_30_59 = np.mean(np.square(x_test_30_59_scaled - reconstructed_test_30_59), axis=1)\n",
    "\n",
    "# Convert reconstruction error to percentage\n",
    "reconstruction_error_percentage_30_59 = np.mean(reconstruction_error_30_59) * 100\n",
    "\n",
    "# Print reconstruction error percentage for 30-59 age group\n",
    "print(f'Reconstruction Error Percentage for 30-59 age group: {reconstruction_error_percentage_30_59}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
