{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('diabetes_prediction_dataset.csv')\n",
    "df_encoded = pd.get_dummies(df, columns=['gender', 'smoking_history'], drop_first=True)\n",
    "testDF = df_encoded.sample(frac=1).reset_index(drop=True)\n",
    "x_unscaled = testDF.drop(['diabetes'], axis=1)\n",
    "y = testDF['diabetes']\n",
    "\n",
    "# Normalize the data\n",
    "numerical_columns = x_unscaled.select_dtypes(include=np.number).columns\n",
    "boolean_columns = x_unscaled.select_dtypes(include=bool).columns\n",
    "scaler = StandardScaler()\n",
    "temp = pd.DataFrame(scaler.fit_transform(x_unscaled[numerical_columns]), columns=numerical_columns)\n",
    "x_scaled = pd.concat([temp, x_unscaled[boolean_columns]], axis=1)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_scaled, y, test_size=0.3, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_val_tensor = torch.tensor(x_val.values, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, encoding_dim, dropout):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_23304/800231515.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  autoencoder.load_state_dict(torch.load(autoencoder_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=13, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.0, inplace=False)\n",
       "    (6): Linear(in_features=16, out_features=8, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.0, inplace=False)\n",
       "    (6): Linear(in_features=32, out_features=13, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define input dimensions\n",
    "input_dim = x_train.shape[1]\n",
    "encoding_dim = 8\n",
    "\n",
    "autoencoder_path = './models/autoencoder.pth'\n",
    "\n",
    "# Initialize the model\n",
    "autoencoder = Autoencoder(input_dim, 32, encoding_dim, 0.0).to(device)\n",
    "\n",
    "# Load the model\n",
    "autoencoder.load_state_dict(torch.load(autoencoder_path))\n",
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_23304/2661790366.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  original_mlp.load_state_dict(torch.load(mlp_model_file))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.41%\n",
      "TP: 848\n",
      "TN: 13764\n",
      "FP: 6\n",
      "FN: 382\n",
      "Recall: 0.689431\n",
      "Precision: 0.992974\n",
      "F1 Score: 0.813820\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Load the original MLP model\n",
    "mlp_model_file = './models/mlp_model.pth'\n",
    "input_dim = x_train.shape[1]\n",
    "original_mlp = MLP(input_dim, 128, 0.3).to(device)\n",
    "original_mlp.load_state_dict(torch.load(mlp_model_file))\n",
    "original_mlp.eval()\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "y_true_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "# Evaluate the original MLP model\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        outputs = original_mlp(X_batch)  # Get predictions (probabilities from sigmoid)\n",
    "        predictions = (outputs > 0.5).float()  # Convert probabilities to binary (0 or 1)\n",
    "\n",
    "        y_true_list.extend(y_batch.cpu().numpy())  # Store true labels\n",
    "        y_pred_list.extend(predictions.cpu().numpy())  # Store predictions\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "y_true = np.array(y_true_list)\n",
    "y_pred = np.array(y_pred_list)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print('TP:', conf_matrix[1, 1])\n",
    "print('TN:', conf_matrix[0, 0])\n",
    "print('FP:', conf_matrix[0, 1])\n",
    "print('FN:', conf_matrix[1, 0])\n",
    "\n",
    "# Calculate recall, precision, and F1 score\n",
    "recall = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 0])\n",
    "precision = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[0, 1])\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(f'Recall: {recall:.6f}')\n",
    "print(f'Precision: {precision:.6f}')\n",
    "print(f'F1 Score: {f1_score:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRYING OUT ERROR-CORRECTING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bias Predictor Model\n",
    "# class BiasPredictor(nn.Module):\n",
    "#     def __init__(self, input_dim=1, hidden_size=128, dropout=0.2):\n",
    "#         super(BiasPredictor, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "#         self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "#         self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "#         self.fc3 = nn.Linear(hidden_size // 2, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout1(x)\n",
    "        \n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "        \n",
    "#         return self.fc3(x)\n",
    "\n",
    "class BiasPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_size=128, dropout=0.2):\n",
    "        super(BiasPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size // 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope=0.01)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the training data for the bias predictor\n",
    "def prepare_bias_data(reconstruction_errors, x_test, original_probs, ground_truth):\n",
    "    # Combine reconstruction errors with the existing normalized features\n",
    "    features = np.hstack((reconstruction_errors.reshape(-1, 1), x_test.values))\n",
    "    \n",
    "    # Compute target biases\n",
    "    target_bias = ground_truth - original_probs\n",
    "    print(\"Minimum bias: \", np.min(target_bias))\n",
    "    print(\"Maximum bias: \", np.max(target_bias))\n",
    "    \n",
    "    return torch.tensor(features, dtype=torch.float32), torch.tensor(target_bias, dtype=torch.float32)\n",
    "\n",
    "def prepare_bias_data_scaled(reconstruction_errors, x_data, original_probs, ground_truth, reconstruction_scale):\n",
    "    \"\"\"\n",
    "    Prepares data for the bias predictor by combining features and scaling reconstruction errors.\n",
    "    \"\"\"\n",
    "    # Scale reconstruction errors\n",
    "    scaled_reconstruction_errors = reconstruction_errors * reconstruction_scale\n",
    "    \n",
    "    # Combine scaled reconstruction errors with the existing features\n",
    "    # features = np.hstack((scaled_reconstruction_errors.reshape(-1, 1), x_data.values))\n",
    "    \n",
    "    #######################################################################################################\n",
    "    # use this for only reconstruction errors as the input feature\n",
    "    \n",
    "    features = scaled_reconstruction_errors.reshape(-1, 1)\n",
    "    # print(\"Minimum scaled reconstruction error: \", np.min(scaled_reconstruction_errors))\n",
    "    # print(\"Maximum scaled reconstruction error: \", np.max(scaled_reconstruction_errors))\n",
    "    \n",
    "    #######################################################################################################\n",
    "    \n",
    "    # Compute target biases\n",
    "    target_bias = ground_truth - original_probs\n",
    "    print(\"Minimum bias: \", np.min(target_bias))\n",
    "    print(\"Maximum bias: \", np.max(target_bias))\n",
    "    \n",
    "    return torch.tensor(features, dtype=torch.float32), torch.tensor(target_bias, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "def train_bias_predictor(features, target_bias, val_features, val_target_bias, epochs=1000, learning_rate=0.0001, patience=10, input_dim=1, hidden_size=128, dropout=0.2):\n",
    "    model = BiasPredictor(input_dim = input_dim, hidden_size=hidden_size, dropout = dropout).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "    patience_counter = 0  # Counter for early stopping\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features).squeeze()\n",
    "        loss = criterion(predictions, target_bias)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(val_features).squeeze()\n",
    "            val_loss = criterion(val_predictions, val_target_bias)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()  # Save the best model state\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "        # Stop training if patience is exceeded\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # Load the best model state before returning\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "# Train the Bias Predictor with Class Weights\n",
    "# def train_bias_predictor_with_weights(features, target_bias, ground_truth, epochs=100, learning_rate=0.001):\n",
    "# def train_bias_predictor_with_weights(features, target_bias, ground_truth, epochs=100, learning_rate=0.001, weight_positive_scale=1.0, weight_negative_scale=1.0):\n",
    "#     model = BiasPredictor().to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "#     # Define class weights\n",
    "#     num_positive = np.sum(ground_truth == 1)\n",
    "#     num_negative = np.sum(ground_truth == 0)\n",
    "#     total = len(ground_truth)\n",
    "\n",
    "#     weight_positive = (total / (2 * num_positive)) * weight_positive_scale\n",
    "#     weight_negative = (total / (2 * num_negative)) * weight_negative_scale \n",
    "\n",
    "#     # Convert weights to tensors\n",
    "#     class_weights = torch.tensor([weight_negative, weight_positive], dtype=torch.float32).to(device)\n",
    "#     criterion = nn.MSELoss(reduction='none')  # Use 'none' to apply weights manually\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         predictions = model(features).squeeze()\n",
    "        \n",
    "#         # Calculate weighted loss\n",
    "#         loss = criterion(predictions, target_bias)\n",
    "#         weights = torch.where(\n",
    "#             target_bias > 0,  # Assign weights based on ground truth class\n",
    "#             class_weights[1],  # Positive class weight\n",
    "#             class_weights[0]   # Negative class weight\n",
    "#         )\n",
    "#         weighted_loss = torch.mean(loss * weights)\n",
    "        \n",
    "#         weighted_loss.backward()\n",
    "#         optimizer.step()\n",
    "#     return model\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "def apply_bias_correction(bias_predictor, reconstruction_errors, x_test, original_probs):\n",
    "    # Combine normalized reconstruction errors with other features\n",
    "    # combined_features = np.hstack((reconstruction_errors.reshape(-1, 1), x_test.values))\n",
    "    # features = torch.tensor(combined_features, dtype=torch.float32).to(next(bias_predictor.parameters()).device)\n",
    "    \n",
    "    #######################################################################################################\n",
    "    # use this for only reconstruction errors as the input feature\n",
    "    \n",
    "    combined_features = reconstruction_errors.reshape(-1, 1)\n",
    "    features = torch.tensor(combined_features, dtype=torch.float32).to(next(bias_predictor.parameters()).device)\n",
    "    \n",
    "    #######################################################################################################\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bias_correction = bias_predictor(features).squeeze().cpu().numpy()\n",
    "    \n",
    "    print(np.min(bias_correction))\n",
    "    print(np.max(bias_correction))\n",
    "    \n",
    "    # Multiply with a scaling factor\n",
    "    bias_correction *= 1.2\n",
    "    \n",
    "    # Apply the correction and clip probabilities to [0, 1]\n",
    "    corrected_probs = np.clip(original_probs + bias_correction, 0, 1)\n",
    "    \n",
    "    # Number of predictions that changed\n",
    "    changed_predictions = np.sum((original_probs > 0.5).astype(int) != (corrected_probs > 0.5).astype(int))\n",
    "    print(f\"Number of changed predictions: {changed_predictions}\")\n",
    "    \n",
    "    return corrected_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reconstruction error:  2.3131484528514887\n",
      "Minimum bias:  -0.9999867677688599\n",
      "Maximum bias:  0.9999999999856334\n",
      "Minimum bias:  -0.7549043893814087\n",
      "Maximum bias:  0.9999999989679031\n",
      "Early stopping triggered at epoch 25. Best Val Loss: 0.0286\n",
      "0.031692833\n",
      "0.9871582\n",
      "Number of changed predictions: 82\n",
      "\n",
      "Corrected Accuracy: 97.08%\n",
      "True Positives:  3972\n",
      "True Negatives:  63981\n",
      "False Positives:  93\n",
      "False Negatives:  1954\n",
      "\n",
      "Original MLP Accuracy: 97.08%\n",
      "True Positives:  3934\n",
      "True Negatives:  64025\n",
      "False Positives:  49\n",
      "False Negatives:  1992\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_train = autoencoder(X_train_tensor).cpu().numpy()\n",
    "reconstructed_errors_train = np.mean(np.square(x_train.values - reconstructed_train), axis=1)\n",
    "mean_error_train = np.mean(reconstructed_errors_train)\n",
    "std_error_train = np.std(reconstructed_errors_train)\n",
    "reconstruction_errors_train_normalized = (reconstructed_errors_train - mean_error_train) / std_error_train\n",
    "print(\"Max reconstruction error: \", np.max(reconstructed_errors_train))\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_val = autoencoder(X_val_tensor).cpu().numpy()\n",
    "reconstructed_errors_val = np.mean(np.square(x_val.values - reconstructed_val), axis=1)\n",
    "mean_error_val = np.mean(reconstructed_errors_val)\n",
    "std_error_val = np.std(reconstructed_errors_val)\n",
    "reconstruction_errors_val_normalized = (reconstructed_errors_val - mean_error_val) / std_error_val\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_train = original_mlp(X_train_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    y_pred_val = original_mlp(X_val_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    \n",
    "original_probs_train = y_pred_train\n",
    "ground_truth_train = y_train.values\n",
    "\n",
    "original_probs_val = y_pred_val\n",
    "ground_truth_val = y_val.values\n",
    "\n",
    "\n",
    "# Prepare the data for training\n",
    "# features, target_bias = prepare_bias_data(reconstruction_errors_normalized, x_train, original_probs, ground_truth)\n",
    "\n",
    "# if model does not exist\n",
    "if not os.path.exists('./models/bias_predictor.pth'):\n",
    "    features_train, target_bias_train = prepare_bias_data_scaled(reconstruction_errors_train_normalized, x_train, original_probs_train, ground_truth_train, reconstruction_scale=50)\n",
    "    features_val, target_bias_val = prepare_bias_data_scaled(reconstruction_errors_val_normalized, x_val, original_probs_val, ground_truth_val, reconstruction_scale=50)\n",
    "\n",
    "    # Move tensors to the same device as the model\n",
    "    features_train, target_bias_train = features_train.to(device), target_bias_train.to(device)\n",
    "    features_val, target_bias_val = features_val.to(device), target_bias_val.to(device)\n",
    "\n",
    "    # Train the bias predictor\n",
    "    bias_predictor = train_bias_predictor(features_train, target_bias_train, features_val, target_bias_val, epochs=5000, learning_rate=0.0001, patience=10)\n",
    "                        \n",
    "    # Apply bias correction during inference\n",
    "    corrected_probs = apply_bias_correction(bias_predictor, reconstruction_errors_train_normalized, x_train, original_probs_train)\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    corrected_predictions = (corrected_probs > 0.5).astype(int)\n",
    "    # Calculate accuracy\n",
    "    corrected_accuracy = accuracy_score(y_train, corrected_predictions)\n",
    "    print(f\"\\nCorrected Accuracy: {corrected_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix_corrected = confusion_matrix(y_train, corrected_predictions)\n",
    "\n",
    "    tn, fp, fn, tp = conf_matrix_corrected.ravel()\n",
    "    print(\"True Positives: \", tp)\n",
    "    print(\"True Negatives: \", tn)\n",
    "    print(\"False Positives: \", fp)\n",
    "    print(\"False Negatives: \", fn)\n",
    "\n",
    "    # Evaluate the original MLP model\n",
    "    with torch.no_grad():\n",
    "        x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "        y_pred_original = original_mlp(x_train_tensor).cpu().numpy().flatten()\n",
    "        y_pred_original = (y_pred_original > 0.5).astype(int)  # Convert to binary\n",
    "\n",
    "    original_accuracy = accuracy_score(y_train, y_pred_original)\n",
    "    print(f\"\\nOriginal MLP Accuracy: {original_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix_original = confusion_matrix(y_train, y_pred_original)\n",
    "\n",
    "    original_tn, original_fp, original_fn, original_tp = conf_matrix_original.ravel()\n",
    "    print(\"True Positives: \", original_tp)\n",
    "    print(\"True Negatives: \", original_tn)\n",
    "    print(\"False Positives: \", original_fp)\n",
    "    print(\"False Negatives: \", original_fn)\n",
    "\n",
    "else:\n",
    "    # Load the trained bias predictor model\n",
    "    bias_predictor = BiasPredictor().to(device)\n",
    "    bias_predictor.load_state_dict(torch.load('./models/bias_predictor.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03751152\n",
      "0.9601753\n",
      "Number of changed predictions: 13\n",
      "\n",
      "Corrected Accuracy on Test Set: 97.42%\n",
      "True Positives:  855\n",
      "True Negatives:  13758\n",
      "False Positives:  12\n",
      "False Negatives:  375\n",
      "\n",
      "Original Accuracy on Test Set: 97.41%\n",
      "True Positives:  848\n",
      "True Negatives:  13764\n",
      "False Positives:  6\n",
      "False Negatives:  382\n",
      "Improvement:  0.2577319587628578\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    reconstructed_test = autoencoder(X_test_tensor).cpu().numpy()\n",
    "reconstructed_errors_test = np.mean(np.square(x_test.values - reconstructed_test), axis=1)\n",
    "mean_error_test = np.mean(reconstructed_errors_test)\n",
    "std_error_test = np.std(reconstructed_errors_test)\n",
    "reconstruction_errors_test_normalized = (reconstructed_errors_test - mean_error_test) / std_error_test\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "    y_pred_test = original_mlp(x_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "original_probs_test = y_pred_test\n",
    "\n",
    "original_accuracy_test = accuracy_score(y_test, (original_probs_test > 0.5).astype(int))\n",
    "\n",
    "\n",
    "corrected_probs_test = apply_bias_correction(bias_predictor, reconstruction_errors_test_normalized, x_test, original_probs_test)\n",
    "\n",
    "corrected_predictions_test = (corrected_probs_test > 0.5).astype(int)\n",
    "\n",
    "corrected_accuracy_test = accuracy_score(y_test, corrected_predictions_test)\n",
    "print(f\"\\nCorrected Accuracy on Test Set: {corrected_accuracy_test * 100:.2f}%\")\n",
    "\n",
    "conf_matrix_corrected_test = confusion_matrix(y_test, corrected_predictions_test)\n",
    "\n",
    "tn, fp, fn, tp = conf_matrix_corrected_test.ravel()\n",
    "print(\"True Positives: \", tp)\n",
    "print(\"True Negatives: \", tn)\n",
    "print(\"False Positives: \", fp)\n",
    "print(\"False Negatives: \", fn)\n",
    "\n",
    "print(f\"\\nOriginal Accuracy on Test Set: {original_accuracy_test * 100:.2f}%\")\n",
    "\n",
    "conf_matrix_original_test = confusion_matrix(y_test, (original_probs_test > 0.5).astype(int))\n",
    "\n",
    "tn, fp, fn, tp = conf_matrix_original_test.ravel()\n",
    "print(\"True Positives: \", tp)\n",
    "print(\"True Negatives: \", tn)\n",
    "print(\"False Positives: \", fp)\n",
    "print(\"False Negatives: \", fn)\n",
    "\n",
    "difference = corrected_accuracy_test - original_accuracy_test\n",
    "difference_percent = difference/(1-original_accuracy_test)\n",
    "print(\"Improvement: \", difference_percent*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if (corrected_accuracy_test > original_accuracy_test + 0.0010) and not os.path.exists('./models/bias_predictor.pth'):\n",
    "#     # save the model\n",
    "#     torch.save(bias_predictor.state_dict(), './models/bias_predictor.pth')\n",
    "#     print(\"Model saved\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify the indices where the predictions changed\n",
    "# changed_indices = np.where((original_probs_test > 0.5).astype(int) != corrected_predictions_test)[0]\n",
    "\n",
    "# # Print the indices of the changed data points\n",
    "# # print(\"Indices of changed data points:\", changed_indices)\n",
    "\n",
    "# print(\"Number of changed indices:\", len(changed_indices))\n",
    "# # Get the reconstruction error of the changed data points\n",
    "# changed_reconstruction_errors = reconstruction_errors_test_normalized[changed_indices]\n",
    "\n",
    "# # Define the ranges for the reconstruction errors\n",
    "# ranges = [-1.0, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0, 2.0, 3.0, 4.0, 5.0, 20]\n",
    "\n",
    "# # Count the number of changed indices within each range\n",
    "# counts, bin_edges = np.histogram(changed_reconstruction_errors, bins=ranges)\n",
    "\n",
    "# # Print the counts for each range\n",
    "# for i in range(len(ranges) - 1):\n",
    "#     print(f\"Range {ranges[i]} to {ranges[i+1]}: {counts[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying bias correction with individual reconstruction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importance(model, X, y_true, metric=accuracy_score, n_repeats=5):\n",
    "    model.eval()  # Ensure model is in evaluation mode\n",
    "    X_base = X.clone()  # Keep a copy of the original data\n",
    "    \n",
    "    # Get baseline predictions and accuracy\n",
    "    with torch.no_grad():\n",
    "        y_pred_base = (model(X).detach().cpu().numpy().flatten() > 0.5).astype(int)\n",
    "        baseline_score = metric(y_true.cpu().numpy().flatten(), y_pred_base)\n",
    "\n",
    "    feature_importance = np.zeros(X.shape[1])  # Array to store importance for each feature\n",
    "\n",
    "    for i in range(X.shape[1]):  # Loop through features\n",
    "        scores = []\n",
    "        for _ in range(n_repeats):  # Repeat for robustness\n",
    "            X_permuted = X_base.clone()  # Clone original dataset\n",
    "            permuted_feature = X_permuted[:, i].clone()  # Extract feature\n",
    "            X_permuted[:, i] = permuted_feature[torch.randperm(X.shape[0])]  # Shuffle the feature\n",
    "\n",
    "            # Get predictions with shuffled feature\n",
    "            with torch.no_grad():\n",
    "                y_pred_permuted = (model(X_permuted).detach().cpu().numpy().flatten() > 0.5).astype(int)\n",
    "                score = metric(y_true.cpu().numpy().flatten(), y_pred_permuted)\n",
    "            scores.append(score)\n",
    "\n",
    "        feature_importance[i] = baseline_score - np.mean(scores)  # Drop in performance\n",
    "\n",
    "    # Normalize so that importance values sum to 1\n",
    "    feature_importance /= feature_importance.sum()\n",
    "\n",
    "    return feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: 0.0078\n",
      "hypertension: -0.0006\n",
      "heart_disease: 0.0058\n",
      "bmi: 0.0116\n",
      "HbA1c_level: 0.5952\n",
      "blood_glucose_level: 0.3714\n",
      "gender_Male: 0.0017\n",
      "gender_Other: 0.0001\n",
      "smoking_history_current: -0.0005\n",
      "smoking_history_ever: 0.0023\n",
      "smoking_history_former: 0.0021\n",
      "smoking_history_never: 0.0023\n",
      "smoking_history_not current: 0.0009\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute feature importance on validation data\n",
    "feature_importance = compute_feature_importance(original_mlp, X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Get feature names if using pandas DataFrame\n",
    "feature_names = x_test.columns.tolist()\n",
    "\n",
    "# Plot normalized feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, feature_importance, color=\"royalblue\")\n",
    "plt.xlabel(\"Normalized Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"MLP Feature Importance (Normalized)\")\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"{feature_names[i]}: {feature_importance[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reconstruction error by column\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_train = autoencoder(X_train_tensor).cpu().numpy()\n",
    "    reconstructed_val = autoencoder(X_val_tensor).cpu().numpy()\n",
    "    reconstructed_test = autoencoder(X_test_tensor).cpu().numpy()\n",
    "    \n",
    "    \n",
    "reconstructed_errors_train = np.abs((x_train.values - reconstructed_train) / (x_train.values+1e-8)) * 100\n",
    "reconstructed_errors_val = np.abs((x_val.values - reconstructed_val) / (x_val.values+1e-8)) * 100\n",
    "reconstructed_errors_test = np.abs((x_test.values - reconstructed_test) / (x_test.values+1e-8)) * 100\n",
    "\n",
    "# Normalize based on column\n",
    "# x_scaler = StandardScaler()\n",
    "# x_scaler = RobustScaler()\n",
    "x_scaler = MinMaxScaler()\n",
    "\n",
    "reconstructed_train_normalized = x_scaler.fit_transform(reconstructed_errors_train)\n",
    "reconstructed_val_normalized = x_scaler.transform(reconstructed_errors_val)\n",
    "reconstructed_test_normalized = x_scaler.transform(reconstructed_errors_test)\n",
    "\n",
    "# only keep the 4th and 5th columns of the normalized reconstruction errors\n",
    "reconstructed_train_normalized = reconstructed_train_normalized[:, 4:6]\n",
    "reconstructed_val_normalized = reconstructed_val_normalized[:, 4:6]\n",
    "reconstructed_test_normalized = reconstructed_test_normalized[:, 4:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MLP prediction\n",
    "with torch.no_grad():\n",
    "    y_pred_train = original_mlp(X_train_tensor).cpu().numpy().flatten()\n",
    "    y_pred_val = original_mlp(X_val_tensor).cpu().numpy().flatten()\n",
    "    y_pred_test = original_mlp(X_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "ground_truth_train = y_train.values\n",
    "ground_truth_val = y_val.values\n",
    "ground_truth_test = y_test.values\n",
    "\n",
    "bias_train = ground_truth_train - y_pred_train\n",
    "bias_val = ground_truth_val - y_pred_val\n",
    "bias_test = ground_truth_test - y_pred_test\n",
    "\n",
    "# y_scaler = StandardScaler()\n",
    "# y_scaler = RobustScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "bias_train_normalized = y_scaler.fit_transform(bias_train.reshape(-1, 1)).flatten()\n",
    "bias_val_normalized = y_scaler.transform(bias_val.reshape(-1, 1)).flatten()\n",
    "bias_test_normalized = y_scaler.transform(bias_test.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_bias_predictor(features, target_bias, val_features, val_target_bias, \n",
    "#                          epochs=1000, learning_rate=0.0001, patience=10, \n",
    "#                          input_dim=2, hidden_size=128, dropout=0.2, batch_size=32):\n",
    "\n",
    "#     model = BiasPredictor(input_dim=input_dim, hidden_size=hidden_size, dropout=dropout).to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     criterion = nn.MSELoss()\n",
    "\n",
    "#     best_val_loss = float('inf')\n",
    "#     patience_counter = 0\n",
    "#     best_model_state = None\n",
    "\n",
    "#     # Reshape target_bias to (batch_size, 1)\n",
    "#     target_bias = target_bias.view(-1, 1)\n",
    "#     val_target_bias = val_target_bias.view(-1, 1)\n",
    "\n",
    "#     # Create DataLoaders\n",
    "#     train_dataset = TensorDataset(features, target_bias)\n",
    "#     val_dataset = TensorDataset(val_features, val_target_bias)\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         total_train_loss = 0.0\n",
    "\n",
    "#         # Training loop\n",
    "#         for batch_features, batch_targets in train_loader:\n",
    "#             batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             predictions = model(batch_features).squeeze()\n",
    "#             loss = criterion(predictions, batch_targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             total_train_loss += loss.item()\n",
    "\n",
    "#         # Validation loop\n",
    "#         model.eval()\n",
    "#         total_val_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for val_batch_features, val_batch_targets in val_loader:\n",
    "#                 val_batch_features, val_batch_targets = val_batch_features.to(device), val_batch_targets.to(device)\n",
    "\n",
    "#                 val_predictions = model(val_batch_features).squeeze()\n",
    "#                 val_loss = criterion(val_predictions, val_batch_targets)\n",
    "                \n",
    "#                 total_val_loss += val_loss.item()\n",
    "\n",
    "#         avg_train_loss = total_train_loss / len(train_loader)\n",
    "#         avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "#         # Early stopping check\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             patience_counter = 0\n",
    "#             best_model_state = model.state_dict()  # Save best model\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         # Print progress\n",
    "#         print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "#             break\n",
    "\n",
    "#     # Load best model before returning\n",
    "#     if best_model_state is not None:\n",
    "#         model.load_state_dict(best_model_state)\n",
    "\n",
    "#     return model\n",
    "\n",
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self, scale_factor=2.0):\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        weights = 1 + self.scale_factor * torch.abs(targets)  # Higher weight for large biases\n",
    "        return torch.mean(weights * (predictions - targets) ** 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_bias_predictor(features, target_bias, val_features, val_target_bias, \n",
    "                         epochs=1000, learning_rate=0.0001, patience=10, \n",
    "                         input_dim=2, hidden_size=128, dropout=0.2, batch_size=32, scale_factor=1.25):\n",
    "\n",
    "    model = BiasPredictor(input_dim=input_dim, hidden_size=hidden_size, dropout=dropout).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # criterion = nn.MSELoss()\n",
    "    criterion = WeightedMSELoss(scale_factor=scale_factor)\n",
    "\n",
    "    # Reshape target_bias to (batch_size, 1)\n",
    "    target_bias = target_bias.view(-1, 1)\n",
    "    val_target_bias = val_target_bias.view(-1, 1)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(features, target_bias)\n",
    "    val_dataset = TensorDataset(val_features, val_target_bias)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for batch_features, batch_targets in train_loader:\n",
    "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_features)  # Ensure output shape is (batch_size, 1)\n",
    "            loss = criterion(predictions, batch_targets.view(-1, 1))  # Match shapes\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_batch_features, val_batch_targets in val_loader:\n",
    "                val_batch_features, val_batch_targets = val_batch_features.to(device), val_batch_targets.to(device)\n",
    "\n",
    "                val_predictions = model(val_batch_features)  # Ensure correct shape\n",
    "                val_loss = criterion(val_predictions, val_batch_targets.view(-1, 1))  # Match shapes\n",
    "                \n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()  # Save best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # Load best model before returning\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_train_tensor = torch.tensor(reconstructed_train_normalized, dtype=torch.float32).to(device)\n",
    "bias_train_tensor = torch.tensor(bias_train_normalized, dtype=torch.float32).to(device)\n",
    "bias_train_tensor = bias_train_tensor.view(-1,1)\n",
    "\n",
    "reconstructed_val_tensor = torch.tensor(reconstructed_val_normalized, dtype=torch.float32).to(device)\n",
    "bias_val_tensor = torch.tensor(bias_val_normalized, dtype=torch.float32).to(device)\n",
    "bias_val_tensor = bias_val_tensor.view(-1,1)\n",
    "\n",
    "# model = train_bias_predictor(\n",
    "#     reconstructed_train_tensor, bias_train_tensor, \n",
    "#     reconstructed_val_tensor, bias_val_tensor,\n",
    "#     epochs=1000, learning_rate=0.0005, patience=10, \n",
    "#     input_dim=2, hidden_size=64, dropout=0.2, batch_size=32\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# # try on test set\n",
    "# reconstructed_test_tensor = torch.tensor(reconstructed_test_normalized, dtype=torch.float32).to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     bias_test_pred = model(reconstructed_test_tensor).cpu().numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias_test_pred_unscaled = y_scaler.inverse_transform(bias_test_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "# bias_test_pred_unscaled_scaled = bias_test_pred_unscaled\n",
    "# # bias_test_pred_unscaled_scaled = np.clip(bias_test_pred_unscaled_scaled, -0.5, 0.5)\n",
    "# corrected_probs_test = y_pred_test + bias_test_pred_unscaled_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert corrected prob test to binary\n",
    "# corrected_predictions_test = (corrected_probs_test > 0.5).astype(int)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# corrected_accuracy_test = accuracy_score(ground_truth_test, corrected_predictions_test)\n",
    "# print(f\"Corrected Accuracy on Test Set: {corrected_accuracy_test * 100:.2f}%\")\n",
    "\n",
    "# original_accuracy_test = accuracy_score(ground_truth_test, (y_pred_test > 0.5).astype(int))\n",
    "# print(f\"Original Accuracy on Test Set: {original_accuracy_test * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf_matrix_original_test = confusion_matrix(y_test, (original_probs_test > 0.5).astype(int))\n",
    "\n",
    "# tn, fp, fn, tp = conf_matrix_original_test.ravel()\n",
    "# recall_original = tp / (tp + fn)\n",
    "# precision_original = tp / (tp + fp)\n",
    "# f1_score_original = 2 * (precision_original * recall_original) / (precision_original + recall_original)\n",
    "# print(\"Original:\")\n",
    "# print(\"True Positives: \", tp)\n",
    "# print(\"True Negatives: \", tn)\n",
    "# print(\"False Positives: \", fp)\n",
    "# print(\"False Negatives: \", fn)\n",
    "# print(\"Recall: \", recall_original)\n",
    "# print(\"Precision: \", precision_original)\n",
    "# print(\"F1 Score: \", f1_score_original)\n",
    "\n",
    "\n",
    "# conf_matrix_corrected_test = confusion_matrix(y_test, corrected_predictions_test)\n",
    "\n",
    "# tn, fp, fn, tp = conf_matrix_corrected_test.ravel()\n",
    "# recall_corrected = tp / (tp + fn)\n",
    "# precision_corrected = tp / (tp + fp)\n",
    "# f1_score_corrected = 2 * (precision_corrected * recall_corrected) / (precision_corrected + recall_corrected)\n",
    "# print(\"\\n\\nCorrected:\")\n",
    "# print(\"True Positives: \", tp)\n",
    "# print(\"True Negatives: \", tn)\n",
    "# print(\"False Positives: \", fp)\n",
    "# print(\"False Negatives: \", fn)\n",
    "# print(\"Recall: \", recall_corrected)\n",
    "# print(\"Precision: \", precision_corrected)\n",
    "# print(\"F1 Score: \", f1_score_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying bias correction with combined reconstruction errors (percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_temp, y_train, y_temp = train_test_split(x_scaled, y, test_size=0.3, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_val_tensor = torch.tensor(x_val.values, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    reconstructed_train = autoencoder(X_train_tensor).cpu().numpy()\n",
    "    reconstructed_val = autoencoder(X_val_tensor).cpu().numpy()\n",
    "    reconstructed_test = autoencoder(X_test_tensor).cpu().numpy()\n",
    "    \n",
    "temp_train = np.abs((x_train.values - reconstructed_train) / (x_train.values+1e-8)) * 100\n",
    "temp_val = np.abs((x_val.values - reconstructed_val) / (x_val.values+1e-8)) * 100\n",
    "temp_test = np.abs((x_test.values - reconstructed_test) / (x_test.values+1e-8)) * 100\n",
    "\n",
    "\n",
    "# get mean per for each row of temp\n",
    "reconstruction_percent_train = np.mean(temp_train, axis=1)\n",
    "reconstruction_percent_val = np.mean(temp_val, axis=1)\n",
    "reconstruction_percent_test = np.mean(temp_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = StandardScaler()\n",
    "# x_scaler = RobustScaler()\n",
    "# x_scaler = MinMaxScaler()\n",
    "\n",
    "reconstruction_percent_train_normalized = x_scaler.fit_transform(reconstruction_percent_train.reshape(-1, 1)).flatten()\n",
    "reconstruction_percent_val_normalized = x_scaler.transform(reconstruction_percent_val.reshape(-1, 1)).flatten()\n",
    "reconstruction_percent_test_normalized = x_scaler.transform(reconstruction_percent_test.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MLP prediction\n",
    "with torch.no_grad():\n",
    "    y_pred_train = original_mlp(X_train_tensor).cpu().numpy().flatten()\n",
    "    y_pred_val = original_mlp(X_val_tensor).cpu().numpy().flatten()\n",
    "    y_pred_test = original_mlp(X_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "ground_truth_train = y_train.values\n",
    "ground_truth_val = y_val.values\n",
    "ground_truth_test = y_test.values\n",
    "\n",
    "bias_train = ground_truth_train - y_pred_train\n",
    "bias_val = ground_truth_val - y_pred_val\n",
    "bias_test = ground_truth_test - y_pred_test\n",
    "\n",
    "# y_scaler = StandardScaler()\n",
    "# y_scaler = RobustScaler()\n",
    "y_scaler = StandardScaler()\n",
    "\n",
    "bias_train_normalized = y_scaler.fit_transform(bias_train.reshape(-1, 1)).flatten()\n",
    "bias_val_normalized = y_scaler.transform(bias_val.reshape(-1, 1)).flatten()\n",
    "bias_test_normalized = y_scaler.transform(bias_test.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_train_tensor = torch.tensor(reconstruction_percent_train_normalized, dtype=torch.float32).view(-1,1).to(device)\n",
    "bias_train_tensor = torch.tensor(bias_train_normalized, dtype=torch.float32).to(device)\n",
    "bias_train_tensor = bias_train_tensor.view(-1,1)\n",
    "\n",
    "reconstructed_val_tensor = torch.tensor(reconstruction_percent_val_normalized, dtype=torch.float32).view(-1,1).to(device)\n",
    "bias_val_tensor = torch.tensor(bias_val_normalized, dtype=torch.float32).to(device)\n",
    "bias_val_tensor = bias_val_tensor.view(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = train_bias_predictor(\n",
    "#     reconstructed_train_tensor, bias_train_tensor, \n",
    "#     reconstructed_val_tensor, bias_val_tensor,\n",
    "#     epochs=1000, learning_rate=0.01, patience=10, \n",
    "#     input_dim=1, hidden_size=64, dropout=0.1, batch_size=128, scale_factor=1.25\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# # try on test set\n",
    "reconstructed_test_tensor = torch.tensor(reconstruction_percent_test_normalized, dtype=torch.float32).view(-1,1).to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     bias_test_pred = model(reconstructed_test_tensor).cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias_test_pred_unscaled = y_scaler.inverse_transform(bias_test_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "# corrected_probs_test = y_pred_test + bias_test_pred_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert corrected prob test to binary\n",
    "# corrected_predictions_test = (corrected_probs_test > 0.5).astype(int)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# corrected_accuracy_test = accuracy_score(ground_truth_test, corrected_predictions_test)\n",
    "# print(f\"Corrected Accuracy on Test Set: {corrected_accuracy_test * 100:.2f}%\")\n",
    "\n",
    "# original_accuracy_test = accuracy_score(ground_truth_test, (y_pred_test > 0.5).astype(int))\n",
    "# print(f\"Original Accuracy on Test Set: {original_accuracy_test * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf_matrix_original_test = confusion_matrix(y_test, (original_probs_test > 0.5).astype(int))\n",
    "\n",
    "# tn, fp, fn, tp = conf_matrix_original_test.ravel()\n",
    "# recall_original = tp / (tp + fn)\n",
    "# precision_original = tp / (tp + fp)\n",
    "# f1_score_original = 2 * (precision_original * recall_original) / (precision_original + recall_original)\n",
    "# print(\"Original:\")\n",
    "# print(\"True Positives: \", tp)\n",
    "# print(\"True Negatives: \", tn)\n",
    "# print(\"False Positives: \", fp)\n",
    "# print(\"False Negatives: \", fn)\n",
    "# print(\"Recall: \", recall_original)\n",
    "# print(\"Precision: \", precision_original)\n",
    "# print(\"F1 Score: \", f1_score_original)\n",
    "\n",
    "\n",
    "# conf_matrix_corrected_test = confusion_matrix(y_test, corrected_predictions_test)\n",
    "\n",
    "# tn, fp, fn, tp = conf_matrix_corrected_test.ravel()\n",
    "# recall_corrected = tp / (tp + fn)\n",
    "# precision_corrected = tp / (tp + fp)\n",
    "# f1_score_corrected = 2 * (precision_corrected * recall_corrected) / (precision_corrected + recall_corrected)\n",
    "# print(\"\\n\\nCorrected:\")\n",
    "# print(\"True Positives: \", tp)\n",
    "# print(\"True Negatives: \", tn)\n",
    "# print(\"False Positives: \", fp)\n",
    "# print(\"False Negatives: \", fn)\n",
    "# print(\"Recall: \", recall_corrected)\n",
    "# print(\"Precision: \", precision_corrected)\n",
    "# print(\"F1 Score: \", f1_score_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# # Histogram\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.hist(y_pred_train, bins=50, edgecolor='black', alpha=0.7)\n",
    "# plt.xlabel(\"Bias Values\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Histogram of bias_train\")\n",
    "\n",
    "# # Box Plot\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.boxplot(y_pred_train, vert=False)\n",
    "# plt.xlabel(\"Bias Values\")\n",
    "# plt.title(\"Box Plot of bias_train\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, reconstructed_test_tensor, y_pred_test, ground_truth_test):\n",
    "    \"\"\"Runs evaluation on the test set and returns accuracy, recall, precision, F1 score\"\"\"\n",
    "    with torch.no_grad():\n",
    "        bias_test_pred = model(reconstructed_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    bias_test_pred_unscaled = y_scaler.inverse_transform(bias_test_pred.reshape(-1, 1)).flatten()\n",
    "    corrected_probs_test = y_pred_test + bias_test_pred_unscaled\n",
    "    corrected_predictions_test = (corrected_probs_test > 0.5).astype(int)\n",
    "    \n",
    "    # Compute metrics\n",
    "    original_accuracy = accuracy_score(ground_truth_test, (y_pred_test > 0.5).astype(int))\n",
    "    corrected_accuracy = accuracy_score(ground_truth_test, corrected_predictions_test)\n",
    "    \n",
    "    conf_matrix_original = confusion_matrix(ground_truth_test, (y_pred_test > 0.5).astype(int))\n",
    "    tn, fp, fn, tp = conf_matrix_original.ravel()\n",
    "    recall_original = tp / (tp + fn)\n",
    "    precision_original = tp / (tp + fp)\n",
    "    f1_score_original = 2 * (precision_original * recall_original) / (precision_original + recall_original)\n",
    "    \n",
    "    conf_matrix_corrected = confusion_matrix(ground_truth_test, corrected_predictions_test)\n",
    "    tn, fp, fn, tp = conf_matrix_corrected.ravel()\n",
    "    recall_corrected = tp / (tp + fn)\n",
    "    precision_corrected = tp / (tp + fp)\n",
    "    f1_score_corrected = 2 * (precision_corrected * recall_corrected) / (precision_corrected + recall_corrected)\n",
    "    \n",
    "    return (original_accuracy, corrected_accuracy, recall_original, precision_original, f1_score_original,\n",
    "            recall_corrected, precision_corrected, f1_score_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Define hyperparameter grid\n",
    "learning_rates = [0.001, 0.005, 0.01]\n",
    "hidden_sizes = [16, 32, 64]\n",
    "dropouts = [0.0,0.1, 0.2]\n",
    "scale_factors = [1.0, 1.25, 1.5]\n",
    "\n",
    "# Number of runs per configuration\n",
    "num_runs = 5\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "total_combinations = len(learning_rates) * len(hidden_sizes) * len(dropouts) * len(scale_factors)\n",
    "count = 0\n",
    "# Iterate over hyperparameter combinations\n",
    "for lr, hidden_size, dropout, scale_factor in itertools.product(learning_rates, hidden_sizes, dropouts, scale_factors):\n",
    "    print(f\"Training with LR={lr}, Hidden={hidden_size}, Dropout={dropout}, Scale Factor={scale_factor}\")\n",
    "    print(f\"Combination {count+1}/{total_combinations}\")\n",
    "    count += 1\n",
    "    \n",
    "    # Store results for multiple runs\n",
    "    run_results = []\n",
    "    for _ in range(num_runs):\n",
    "        model = train_bias_predictor(\n",
    "            reconstructed_train_tensor, bias_train_tensor, \n",
    "            reconstructed_val_tensor, bias_val_tensor,\n",
    "            epochs=1000, learning_rate=lr, patience=10, \n",
    "            input_dim=1, hidden_size=hidden_size, dropout=dropout, scale_factor=scale_factor\n",
    "        )\n",
    "        model.eval()\n",
    "        run_results.append(evaluate_model(model, reconstructed_test_tensor, y_pred_test, ground_truth_test))\n",
    "    \n",
    "    # Compute averages across runs\n",
    "    avg_results = [sum(x) / num_runs for x in zip(*run_results)]\n",
    "    \n",
    "    # Store in results list\n",
    "    results.append([lr, hidden_size, dropout, scale_factor] + avg_results)\n",
    "\n",
    "# Convert to DataFrame\n",
    "columns = [\"Learning Rate\", \"Hidden Size\", \"Dropout\", \"Scale Factor\",\n",
    "           \"Original Accuracy\", \"Corrected Accuracy\", \n",
    "           \"Original Recall\", \"Original Precision\", \"Original F1 Score\",\n",
    "           \"Corrected Recall\", \"Corrected Precision\", \"Corrected F1 Score\"]\n",
    "df_results = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Save to CSV\n",
    "df_results.to_csv(\"hyperparameter_tuning_results.csv\", index=False)\n",
    "print(\"Hyperparameter tuning complete. Results saved to hyperparameter_tuning_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by corrected F1 score\n",
    "df_results = df_results.sort_values(by='Corrected F1 Score', ascending=False)\n",
    "df_results.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
