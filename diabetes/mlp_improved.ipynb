{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('diabetes_prediction_dataset.csv')\n",
    "df_encoded = pd.get_dummies(df, columns=['gender', 'smoking_history'], drop_first=True)\n",
    "testDF = df_encoded.sample(frac=1).reset_index(drop=True)\n",
    "x_unscaled = testDF.drop(['diabetes'], axis=1)\n",
    "y = testDF['diabetes']\n",
    "\n",
    "# Normalize the data\n",
    "numerical_columns = x_unscaled.select_dtypes(include=np.number).columns\n",
    "boolean_columns = x_unscaled.select_dtypes(include=bool).columns\n",
    "scaler = StandardScaler()\n",
    "temp = pd.DataFrame(scaler.fit_transform(x_unscaled[numerical_columns]), columns=numerical_columns)\n",
    "x_scaled = pd.concat([temp, x_unscaled[boolean_columns]], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder model with reduced complexity and dropout\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, encoding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Define the Decoder model with reduced complexity and dropout\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoding_dim, input_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_6000/3703475411.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(encoder_model_file))\n",
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_6000/3703475411.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load(decoder_model_file))\n"
     ]
    }
   ],
   "source": [
    "# Define input dimensions\n",
    "input_dim = x_train.shape[1]\n",
    "encoding_dim = 8\n",
    "\n",
    "# Load the trained autoencoder models\n",
    "encoder_model_file = './models/encoder2.pth'\n",
    "decoder_model_file = './models/decoder2.pth'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(input_dim, encoding_dim).to(device)\n",
    "decoder = Decoder(encoding_dim, input_dim).to(device)\n",
    "encoder.load_state_dict(torch.load(encoder_model_file))\n",
    "decoder.load_state_dict(torch.load(decoder_model_file))\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "autoencoder = Autoencoder(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values below 0.5: 74724\n",
      "Values above 0.5: 5276\n",
      "Original MLP Accuracy: 96.88%\n",
      "Total correct predictions: 77501\n",
      "Total wrong predictions: 2499\n",
      "\n",
      "True Negatives: 72703\n",
      "False Positives: 478\n",
      "False Negatives: 2021\n",
      "True Positives: 4798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_6000/3446797987.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  original_mlp.load_state_dict(torch.load(mlp_model_file))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Define the original MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Sigmoid for binary classification\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Load the original MLP model\n",
    "mlp_model_file = './models/mlp_model.pth'\n",
    "input_dim = x_train.shape[1]\n",
    "original_mlp = MLP(input_dim).to(device)\n",
    "original_mlp.load_state_dict(torch.load(mlp_model_file))\n",
    "original_mlp.eval()\n",
    "\n",
    "# Evaluate the original MLP model\n",
    "with torch.no_grad():\n",
    "    x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "    y_pred_original = original_mlp(x_train_tensor).cpu().numpy().flatten()\n",
    "    below_0_5 = np.sum(y_pred_original < 0.5)\n",
    "    above_0_5 = np.sum(y_pred_original >= 0.5)\n",
    "    print(f\"Values below 0.5: {below_0_5}\")\n",
    "    print(f\"Values above 0.5: {above_0_5}\")\n",
    "    y_pred_original = (y_pred_original > 0.5).astype(int)  # Convert to binary predictions\n",
    "\n",
    "# Calculate accuracy\n",
    "original_accuracy = accuracy_score(y_train, y_pred_original)\n",
    "print(f\"Original MLP Accuracy: {original_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix_original = confusion_matrix(y_train, y_pred_original)\n",
    "\n",
    "# Extract values from confusion matrix\n",
    "tn, fp, fn, tp = conf_matrix_original.ravel()\n",
    "\n",
    "print(f\"Total correct predictions: {tn + tp}\")\n",
    "print(f\"Total wrong predictions: {fp + fn}\\n\")\n",
    "print(f\"True Negatives: {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Positives: {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple error correction function by adding a bias (no learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn.functional as F  # For activation functions\n",
    "\n",
    "# def error_correction_function(mlp, autoencoder, x, bias_factor):\n",
    "#     # Ensure all computations happen on the correct device\n",
    "#     with torch.no_grad():\n",
    "#         # Calculate reconstruction error using the autoencoder\n",
    "#         x_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "#         reconstructed = autoencoder(x_tensor).cpu().numpy()\n",
    "#     reconstruction_error = np.mean(np.square(x - reconstructed), axis=1)\n",
    "#     threshold = np.mean(reconstruction_error)\n",
    "    \n",
    "#     # Make predictions with the MLP model\n",
    "#     with torch.no_grad():\n",
    "#         x_test_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "#         y_pred_proba = mlp(x_test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "#     # Adjust predictions based on reconstruction error with adaptive bias factor\n",
    "#     y_pred_proba_corrected = y_pred_proba.copy()\n",
    "#     high_error_indices = np.where(reconstruction_error > threshold)[0]\n",
    "#     print(\"Number of high error indices: \", len(high_error_indices))\n",
    "#     for idx in high_error_indices:\n",
    "#         adaptive_bias = bias_factor * (reconstruction_error[idx] / threshold)\n",
    "#         if y_pred_proba[idx] < 0.5:\n",
    "#             y_pred_proba_corrected[idx] += adaptive_bias\n",
    "#         else:\n",
    "#             y_pred_proba_corrected[idx] -= adaptive_bias\n",
    "#     # y_pred_proba_corrected[high_error_indices] += bias_factor  # Apply bias factor\n",
    "    \n",
    "#     initial_zeros = np.sum(y_pred_proba[high_error_indices] < 0.5)\n",
    "#     initial_ones = np.sum(y_pred_proba[high_error_indices] >= 0.5)\n",
    "#     print(f\"Initial predictions of 0: {initial_zeros}\")\n",
    "#     print(f\"Initial predictions of 1: {initial_ones}\")\n",
    "#     changed_predictions = np.sum((y_pred_proba[high_error_indices] > 0.5).astype(int) != (y_pred_proba_corrected[high_error_indices] > 0.5).astype(int))\n",
    "#     print(f\"Number of changed predictions: {changed_predictions}\")\n",
    "    \n",
    "#     y_pred_proba_corrected = np.clip(y_pred_proba_corrected, 0, 1)  # Ensure probabilities are in [0, 1]\n",
    "\n",
    "#     # Convert corrected probabilities to binary predictions\n",
    "#     y_pred_corrected = (y_pred_proba_corrected > 0.5).astype(int)\n",
    "    \n",
    "#     return y_pred_corrected, reconstruction_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_corrected, reconstruction_error = error_correction_function(original_mlp, autoencoder, x_test.values, bias_factor=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# # Calculate accuracy\n",
    "# corrected_accuracy = accuracy_score(y_test, y_pred_corrected)\n",
    "# print(f\"Corrected Accuracy: {corrected_accuracy * 100:.2f}%\")\n",
    "\n",
    "# # Calculate confusion matrix\n",
    "# conf_matrix_corrected = confusion_matrix(y_test, y_pred_corrected)\n",
    "\n",
    "# # Print confusion matrix results\n",
    "# tn, fp, fn, tp = conf_matrix_corrected.ravel()\n",
    "# print(f\"Corrected - True Negatives: {tn}\")\n",
    "# print(f\"Corrected - False Positives: {fp}\")\n",
    "# print(f\"Corrected - False Negatives: {fn}\")\n",
    "# print(f\"Corrected - True Positives: {tp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRYING OUT ERROR-CORRECTING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum bias:  -0.9969898462295532\n",
      "Maximum bias:  0.9999229263994494\n",
      "Epoch 10/1000, Loss: 0.02879871055483818\n",
      "Epoch 20/1000, Loss: 0.026784252375364304\n",
      "Epoch 30/1000, Loss: 0.02588556706905365\n",
      "Epoch 40/1000, Loss: 0.025621093809604645\n",
      "Epoch 50/1000, Loss: 0.02554793283343315\n",
      "Epoch 60/1000, Loss: 0.02549203485250473\n",
      "Epoch 70/1000, Loss: 0.02543548494577408\n",
      "Epoch 80/1000, Loss: 0.02541855350136757\n",
      "Epoch 90/1000, Loss: 0.025386448949575424\n",
      "Epoch 100/1000, Loss: 0.02538951113820076\n",
      "Epoch 110/1000, Loss: 0.02535116672515869\n",
      "Epoch 120/1000, Loss: 0.025339284911751747\n",
      "Epoch 130/1000, Loss: 0.025326382368803024\n",
      "Epoch 140/1000, Loss: 0.025301791727542877\n",
      "Epoch 150/1000, Loss: 0.02528621442615986\n",
      "Epoch 160/1000, Loss: 0.025278452783823013\n",
      "Epoch 170/1000, Loss: 0.025255326181650162\n",
      "Epoch 180/1000, Loss: 0.02524597942829132\n",
      "Epoch 190/1000, Loss: 0.025237051770091057\n",
      "Epoch 200/1000, Loss: 0.025226958096027374\n",
      "Epoch 210/1000, Loss: 0.025196675211191177\n",
      "Epoch 220/1000, Loss: 0.025169802829623222\n",
      "Epoch 230/1000, Loss: 0.02516736090183258\n",
      "Epoch 240/1000, Loss: 0.025165334343910217\n",
      "Epoch 250/1000, Loss: 0.02513512596487999\n",
      "Epoch 260/1000, Loss: 0.025094278156757355\n",
      "Epoch 270/1000, Loss: 0.02504556067287922\n",
      "Epoch 280/1000, Loss: 0.02505207061767578\n",
      "Epoch 290/1000, Loss: 0.024995621293783188\n",
      "Epoch 300/1000, Loss: 0.024998867884278297\n",
      "Epoch 310/1000, Loss: 0.02490571327507496\n",
      "Epoch 320/1000, Loss: 0.024904822930693626\n",
      "Epoch 330/1000, Loss: 0.024881862103939056\n",
      "Epoch 340/1000, Loss: 0.024829670786857605\n",
      "Epoch 350/1000, Loss: 0.024720925837755203\n",
      "Epoch 360/1000, Loss: 0.02479558438062668\n",
      "Epoch 370/1000, Loss: 0.02467866986989975\n",
      "Epoch 380/1000, Loss: 0.024682633578777313\n",
      "Epoch 390/1000, Loss: 0.024640081450343132\n",
      "Epoch 400/1000, Loss: 0.024552486836910248\n",
      "Epoch 410/1000, Loss: 0.024528924375772476\n",
      "Epoch 420/1000, Loss: 0.024426104500889778\n",
      "Epoch 430/1000, Loss: 0.024457374587655067\n",
      "Epoch 440/1000, Loss: 0.02440735325217247\n",
      "Epoch 450/1000, Loss: 0.02433314360678196\n",
      "Epoch 460/1000, Loss: 0.02423097752034664\n",
      "Epoch 470/1000, Loss: 0.024220889434218407\n",
      "Epoch 480/1000, Loss: 0.02412768453359604\n",
      "Epoch 490/1000, Loss: 0.02407955564558506\n",
      "Epoch 500/1000, Loss: 0.02414911799132824\n",
      "Epoch 510/1000, Loss: 0.024011339992284775\n",
      "Epoch 520/1000, Loss: 0.023998500779271126\n",
      "Epoch 530/1000, Loss: 0.024012651294469833\n",
      "Epoch 540/1000, Loss: 0.023902367800474167\n",
      "Epoch 550/1000, Loss: 0.023938888683915138\n",
      "Epoch 560/1000, Loss: 0.023870907723903656\n",
      "Epoch 570/1000, Loss: 0.02383207343518734\n",
      "Epoch 580/1000, Loss: 0.02376294881105423\n",
      "Epoch 590/1000, Loss: 0.0237465500831604\n",
      "Epoch 600/1000, Loss: 0.02369658462703228\n",
      "Epoch 610/1000, Loss: 0.02372000925242901\n",
      "Epoch 620/1000, Loss: 0.023658649995923042\n",
      "Epoch 630/1000, Loss: 0.023524435237050056\n",
      "Epoch 640/1000, Loss: 0.023623546585440636\n",
      "Epoch 650/1000, Loss: 0.023618245497345924\n",
      "Epoch 660/1000, Loss: 0.023524394258856773\n",
      "Epoch 670/1000, Loss: 0.023431992158293724\n",
      "Epoch 680/1000, Loss: 0.023455515503883362\n",
      "Epoch 690/1000, Loss: 0.023367566987872124\n",
      "Epoch 700/1000, Loss: 0.02331666648387909\n",
      "Epoch 710/1000, Loss: 0.02333098091185093\n",
      "Epoch 720/1000, Loss: 0.02330121211707592\n",
      "Epoch 730/1000, Loss: 0.023234836757183075\n",
      "Epoch 740/1000, Loss: 0.02317693643271923\n",
      "Epoch 750/1000, Loss: 0.02316868305206299\n",
      "Epoch 760/1000, Loss: 0.02312668040394783\n",
      "Epoch 770/1000, Loss: 0.02304944023489952\n",
      "Epoch 780/1000, Loss: 0.023132579401135445\n",
      "Epoch 790/1000, Loss: 0.02304304763674736\n",
      "Epoch 800/1000, Loss: 0.023045329377055168\n",
      "Epoch 810/1000, Loss: 0.023096831515431404\n",
      "Epoch 820/1000, Loss: 0.022938914597034454\n",
      "Epoch 830/1000, Loss: 0.022994304075837135\n",
      "Epoch 840/1000, Loss: 0.022891627624630928\n",
      "Epoch 850/1000, Loss: 0.022886307910084724\n",
      "Epoch 860/1000, Loss: 0.022911585867404938\n",
      "Epoch 870/1000, Loss: 0.022732198238372803\n",
      "Epoch 880/1000, Loss: 0.02282498963177204\n",
      "Epoch 890/1000, Loss: 0.022778136655688286\n",
      "Epoch 900/1000, Loss: 0.022660015150904655\n",
      "Epoch 910/1000, Loss: 0.02281089685857296\n",
      "Epoch 920/1000, Loss: 0.022716717794537544\n",
      "Epoch 930/1000, Loss: 0.022727634757757187\n",
      "Epoch 940/1000, Loss: 0.022646889090538025\n",
      "Epoch 950/1000, Loss: 0.022740693762898445\n",
      "Epoch 960/1000, Loss: 0.022666260600090027\n",
      "Epoch 970/1000, Loss: 0.022652167826890945\n",
      "Epoch 980/1000, Loss: 0.022571224719285965\n",
      "Epoch 990/1000, Loss: 0.022475065663456917\n",
      "Epoch 1000/1000, Loss: 0.022529400885105133\n",
      "Corrected Accuracy: 97.22%\n",
      "True Positives:  4767\n",
      "True Negatives:  73006\n",
      "False Positives:  175\n",
      "False Negatives:  2052\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the Bias Predictor Model\n",
    "class BiasPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiasPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(14, 128)  # Increased neurons\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Increased dropout rate\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)  # Increased neurons\n",
    "        self.dropout2 = nn.Dropout(0.3)  # Increased dropout rate\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.dropout3 = nn.Dropout(0.3)  # Increased dropout rate\n",
    "        \n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.fc6 = nn.Linear(8, 1)  # Output layer for bias correction\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        return self.fc6(x)\n",
    "\n",
    "# Prepare the training data for the bias predictor\n",
    "# def prepare_bias_data(reconstruction_errors, original_probs, ground_truth):\n",
    "#     # Use reconstruction errors as the only input feature\n",
    "#     features = reconstruction_errors.reshape(-1, 1)\n",
    "#     # print(features)\n",
    "#     print(\"Minimum reconstruction error: \", np.min(features))\n",
    "#     print(\"Maximum reconstruction error: \", np.max(features))\n",
    "#     # Compute target biases\n",
    "#     target_bias = ground_truth - original_probs\n",
    "#     print(\"Minimum bias: \", np.min(target_bias))\n",
    "#     print(\"Maximum bias: \", np.max(target_bias))\n",
    "#     return torch.tensor(features, dtype=torch.float32), torch.tensor(target_bias, dtype=torch.float32)\n",
    "def prepare_bias_data(reconstruction_errors, x_test, original_probs, ground_truth):\n",
    "    # Combine reconstruction errors with the existing normalized features\n",
    "    features = np.hstack((reconstruction_errors.reshape(-1, 1), x_test.values))\n",
    "    \n",
    "    # Compute target biases\n",
    "    target_bias = ground_truth - original_probs\n",
    "    print(\"Minimum bias: \", np.min(target_bias))\n",
    "    print(\"Maximum bias: \", np.max(target_bias))\n",
    "    \n",
    "    return torch.tensor(features, dtype=torch.float32), torch.tensor(target_bias, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the Bias Predictor\n",
    "def train_bias_predictor(features, target_bias, epochs=1000, learning_rate=0.001):\n",
    "    model = BiasPredictor().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features).squeeze()\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, target_bias)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Debugging information\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "# Train the Bias Predictor with Class Weights\n",
    "# def train_bias_predictor_with_weights(features, target_bias, ground_truth, epochs=100, learning_rate=0.001):\n",
    "# def train_bias_predictor_with_weights(features, target_bias, ground_truth, epochs=100, learning_rate=0.001, weight_positive_scale=1.0, weight_negative_scale=1.0):\n",
    "#     model = BiasPredictor().to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "#     # Define class weights\n",
    "#     num_positive = np.sum(ground_truth == 1)\n",
    "#     num_negative = np.sum(ground_truth == 0)\n",
    "#     total = len(ground_truth)\n",
    "\n",
    "#     weight_positive = (total / (2 * num_positive)) * weight_positive_scale\n",
    "#     weight_negative = (total / (2 * num_negative)) * weight_negative_scale \n",
    "\n",
    "#     # Convert weights to tensors\n",
    "#     class_weights = torch.tensor([weight_negative, weight_positive], dtype=torch.float32).to(device)\n",
    "#     criterion = nn.MSELoss(reduction='none')  # Use 'none' to apply weights manually\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         predictions = model(features).squeeze()\n",
    "        \n",
    "#         # Calculate weighted loss\n",
    "#         loss = criterion(predictions, target_bias)\n",
    "#         weights = torch.where(\n",
    "#             target_bias > 0,  # Assign weights based on ground truth class\n",
    "#             class_weights[1],  # Positive class weight\n",
    "#             class_weights[0]   # Negative class weight\n",
    "#         )\n",
    "#         weighted_loss = torch.mean(loss * weights)\n",
    "        \n",
    "#         weighted_loss.backward()\n",
    "#         optimizer.step()\n",
    "#     return model\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "# Apply bias correction during inference\n",
    "# def apply_bias_correction(bias_predictor, reconstruction_errors, original_probs):\n",
    "#     # Create feature tensor and move it to the correct device\n",
    "#     features = torch.tensor(reconstruction_errors.reshape(-1, 1), dtype=torch.float32).to(next(bias_predictor.parameters()).device)\n",
    "#     with torch.no_grad():\n",
    "#         bias_correction = bias_predictor(features).squeeze().cpu().numpy()\n",
    "#     # Apply the correction and clip probabilities to [0, 1]\n",
    "#     corrected_probs = np.clip(original_probs + bias_correction, 0, 1)\n",
    "#     return corrected_probs\n",
    "\n",
    "def apply_bias_correction(bias_predictor, reconstruction_errors, x_test, original_probs):\n",
    "    # Combine normalized reconstruction errors with other features\n",
    "    combined_features = np.hstack((reconstruction_errors.reshape(-1, 1), x_test.values))\n",
    "    features = torch.tensor(combined_features, dtype=torch.float32).to(next(bias_predictor.parameters()).device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bias_correction = bias_predictor(features).squeeze().cpu().numpy()\n",
    "    \n",
    "    # Apply the correction and clip probabilities to [0, 1]\n",
    "    corrected_probs = np.clip(original_probs + bias_correction, 0, 1)\n",
    "    return corrected_probs\n",
    "\n",
    "\n",
    "\n",
    "x_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    reconstructed = autoencoder(x_tensor).cpu().numpy()\n",
    "reconstruction_errors = np.mean(np.square(x_train.values - reconstructed), axis=1)\n",
    "\n",
    "# Normalizing the reconstruction errors\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_error = np.std(reconstruction_errors)\n",
    "reconstruction_errors_normalized = (reconstruction_errors - mean_error) / std_error\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "    # logits = original_mlp(x_test_tensor)\n",
    "    # y_pred_original = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "    y_pred_original = original_mlp(x_train_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    \n",
    "original_probs = y_pred_original\n",
    "ground_truth = y_train.values\n",
    "\n",
    "\n",
    "# Prepare the data for training\n",
    "features, target_bias = prepare_bias_data(reconstruction_errors_normalized, x_train, original_probs, ground_truth)\n",
    "\n",
    "# Move tensors to the same device as the model\n",
    "features, target_bias = features.to(device), target_bias.to(device)\n",
    "\n",
    "# Train the bias predictor\n",
    "# bias_predictor = train_bias_predictor(features, target_bias)\n",
    "\n",
    "# best_accuracy = 0\n",
    "# best_conf_matrix = None\n",
    "# for pos in np.arange(0.1, 2.0, 0.05):\n",
    "#     for neg in np.arange(1, 2.0, 0.05):\n",
    "        \n",
    "#         # Train the bias predictor with weights\n",
    "#         bias_predictor = train_bias_predictor_with_weights(features, target_bias, ground_truth, weight_positive_scale=pos, weight_negative_scale=neg)\n",
    "\n",
    "#         # Apply bias correction during inference\n",
    "#         corrected_probs = apply_bias_correction(bias_predictor, reconstruction_errors_normalized, x_test, original_probs)\n",
    "\n",
    "#         # Convert probabilities to binary predictions\n",
    "#         corrected_predictions = (corrected_probs > 0.5).astype(int)\n",
    "#         # Calculate accuracy\n",
    "#         corrected_accuracy = accuracy_score(y_test, corrected_predictions)\n",
    "#         print(f\"Corrected Accuracy for positive weight {pos:.2f} and negative weight {neg:.2f}: {corrected_accuracy * 100:.2f}%\")\n",
    "\n",
    "#         # Calculate confusion matrix\n",
    "#         conf_matrix_corrected = confusion_matrix(y_test, corrected_predictions)\n",
    "\n",
    "#         if corrected_accuracy > best_accuracy:\n",
    "#             print(\"new best accuracy: \", corrected_accuracy)\n",
    "#             best_accuracy = corrected_accuracy\n",
    "#             best_conf_matrix = conf_matrix_corrected\n",
    "            \n",
    "        \n",
    "# Train the bias predictor with weights\n",
    "bias_predictor = train_bias_predictor(features, target_bias)\n",
    "\n",
    "# Apply bias correction during inference\n",
    "corrected_probs = apply_bias_correction(bias_predictor, reconstruction_errors_normalized, x_train, original_probs)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "corrected_predictions = (corrected_probs > 0.5).astype(int)\n",
    "# Calculate accuracy\n",
    "corrected_accuracy = accuracy_score(y_train, corrected_predictions)\n",
    "print(f\"Corrected Accuracy: {corrected_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix_corrected = confusion_matrix(y_train, corrected_predictions)\n",
    "\n",
    "tn, fp, fn, tp = conf_matrix_corrected.ravel()\n",
    "print(\"True Positives: \", tp)\n",
    "print(\"True Negatives: \", tn)\n",
    "print(\"False Positives: \", fp)\n",
    "print(\"False Negatives: \", fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ground truth values equal to one: 6819\n",
      "Number of ground truth values equal to zero: 73181\n"
     ]
    }
   ],
   "source": [
    "num_ground_truth_ones = np.sum(ground_truth == 1)\n",
    "print(f\"Number of ground truth values equal to one: {num_ground_truth_ones}\")\n",
    "\n",
    "num_ground_truth_zeroes = np.sum(ground_truth == 0)\n",
    "print(f\"Number of ground truth values equal to zero: {num_ground_truth_zeroes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
