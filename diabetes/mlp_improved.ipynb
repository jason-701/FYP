{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('diabetes_prediction_dataset.csv')\n",
    "df_encoded = pd.get_dummies(df, columns=['gender', 'smoking_history'], drop_first=True)\n",
    "testDF = df_encoded.sample(frac=1).reset_index(drop=True)\n",
    "x_unscaled = testDF.drop(['diabetes'], axis=1)\n",
    "y = testDF['diabetes']\n",
    "\n",
    "# Normalize the data\n",
    "numerical_columns = x_unscaled.select_dtypes(include=np.number).columns\n",
    "boolean_columns = x_unscaled.select_dtypes(include=bool).columns\n",
    "scaler = StandardScaler()\n",
    "temp = pd.DataFrame(scaler.fit_transform(x_unscaled[numerical_columns]), columns=numerical_columns)\n",
    "x_scaled = pd.concat([temp, x_unscaled[boolean_columns]], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder model with reduced complexity and dropout\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, encoding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Define the Decoder model with reduced complexity and dropout\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoding_dim, input_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_23516/3703475411.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(encoder_model_file))\n",
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_23516/3703475411.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load(decoder_model_file))\n"
     ]
    }
   ],
   "source": [
    "# Define input dimensions\n",
    "input_dim = x_train.shape[1]\n",
    "encoding_dim = 8\n",
    "\n",
    "# Load the trained autoencoder models\n",
    "encoder_model_file = './models/encoder2.pth'\n",
    "decoder_model_file = './models/decoder2.pth'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(input_dim, encoding_dim).to(device)\n",
    "decoder = Decoder(encoding_dim, input_dim).to(device)\n",
    "encoder.load_state_dict(torch.load(encoder_model_file))\n",
    "decoder.load_state_dict(torch.load(decoder_model_file))\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "autoencoder = Autoencoder(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_23516/22405568.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  original_mlp.load_state_dict(torch.load(mlp_model_file))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values below 0.5: 74723\n",
      "Values above 0.5: 5277\n",
      "Original MLP Accuracy: 96.87%\n",
      "Total correct predictions: 77494\n",
      "Total wrong predictions: 2506\n",
      "\n",
      "True Negatives: 72692\n",
      "False Positives: 475\n",
      "False Negatives: 2031\n",
      "True Positives: 4802\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Define the original MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Sigmoid for binary classification\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Load the original MLP model\n",
    "mlp_model_file = './models/mlp_model.pth'\n",
    "input_dim = x_train.shape[1]\n",
    "original_mlp = MLP(input_dim).to(device)\n",
    "original_mlp.load_state_dict(torch.load(mlp_model_file))\n",
    "original_mlp.eval()\n",
    "\n",
    "# Evaluate the original MLP model\n",
    "with torch.no_grad():\n",
    "    x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "    y_pred_original = original_mlp(x_train_tensor).cpu().numpy().flatten()\n",
    "    below_0_5 = np.sum(y_pred_original < 0.5)\n",
    "    above_0_5 = np.sum(y_pred_original >= 0.5)\n",
    "    print(f\"Values below 0.5: {below_0_5}\")\n",
    "    print(f\"Values above 0.5: {above_0_5}\")\n",
    "    y_pred_original = (y_pred_original > 0.5).astype(int)  # Convert to binary predictions\n",
    "\n",
    "# Calculate accuracy\n",
    "original_accuracy = accuracy_score(y_train, y_pred_original)\n",
    "print(f\"Original MLP Accuracy: {original_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix_original = confusion_matrix(y_train, y_pred_original)\n",
    "\n",
    "# Extract values from confusion matrix\n",
    "original_tn, original_fp, original_fn, original_tp = conf_matrix_original.ravel()\n",
    "\n",
    "print(f\"Total correct predictions: {original_tn + original_tp}\")\n",
    "print(f\"Total wrong predictions: {original_fp + original_fn}\\n\")\n",
    "print(f\"True Negatives: {original_tn}\")\n",
    "print(f\"False Positives: {original_fp}\")\n",
    "print(f\"False Negatives: {original_fn}\")\n",
    "print(f\"True Positives: {original_tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple error correction function by adding a bias (no learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn.functional as F  # For activation functions\n",
    "\n",
    "# def error_correction_function(mlp, autoencoder, x, bias_factor):\n",
    "#     # Ensure all computations happen on the correct device\n",
    "#     with torch.no_grad():\n",
    "#         # Calculate reconstruction error using the autoencoder\n",
    "#         x_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "#         reconstructed = autoencoder(x_tensor).cpu().numpy()\n",
    "#     reconstruction_error = np.mean(np.square(x - reconstructed), axis=1)\n",
    "#     threshold = np.mean(reconstruction_error)\n",
    "    \n",
    "#     # Make predictions with the MLP model\n",
    "#     with torch.no_grad():\n",
    "#         x_test_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "#         y_pred_proba = mlp(x_test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "#     # Adjust predictions based on reconstruction error with adaptive bias factor\n",
    "#     y_pred_proba_corrected = y_pred_proba.copy()\n",
    "#     high_error_indices = np.where(reconstruction_error > threshold)[0]\n",
    "#     print(\"Number of high error indices: \", len(high_error_indices))\n",
    "#     for idx in high_error_indices:\n",
    "#         adaptive_bias = bias_factor * (reconstruction_error[idx] / threshold)\n",
    "#         if y_pred_proba[idx] < 0.5:\n",
    "#             y_pred_proba_corrected[idx] += adaptive_bias\n",
    "#         else:\n",
    "#             y_pred_proba_corrected[idx] -= adaptive_bias\n",
    "#     # y_pred_proba_corrected[high_error_indices] += bias_factor  # Apply bias factor\n",
    "    \n",
    "#     initial_zeros = np.sum(y_pred_proba[high_error_indices] < 0.5)\n",
    "#     initial_ones = np.sum(y_pred_proba[high_error_indices] >= 0.5)\n",
    "#     print(f\"Initial predictions of 0: {initial_zeros}\")\n",
    "#     print(f\"Initial predictions of 1: {initial_ones}\")\n",
    "#     changed_predictions = np.sum((y_pred_proba[high_error_indices] > 0.5).astype(int) != (y_pred_proba_corrected[high_error_indices] > 0.5).astype(int))\n",
    "#     print(f\"Number of changed predictions: {changed_predictions}\")\n",
    "    \n",
    "#     y_pred_proba_corrected = np.clip(y_pred_proba_corrected, 0, 1)  # Ensure probabilities are in [0, 1]\n",
    "\n",
    "#     # Convert corrected probabilities to binary predictions\n",
    "#     y_pred_corrected = (y_pred_proba_corrected > 0.5).astype(int)\n",
    "    \n",
    "#     return y_pred_corrected, reconstruction_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_corrected, reconstruction_error = error_correction_function(original_mlp, autoencoder, x_test.values, bias_factor=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# # Calculate accuracy\n",
    "# corrected_accuracy = accuracy_score(y_test, y_pred_corrected)\n",
    "# print(f\"Corrected Accuracy: {corrected_accuracy * 100:.2f}%\")\n",
    "\n",
    "# # Calculate confusion matrix\n",
    "# conf_matrix_corrected = confusion_matrix(y_test, y_pred_corrected)\n",
    "\n",
    "# # Print confusion matrix results\n",
    "# tn, fp, fn, tp = conf_matrix_corrected.ravel()\n",
    "# print(f\"Corrected - True Negatives: {tn}\")\n",
    "# print(f\"Corrected - False Positives: {fp}\")\n",
    "# print(f\"Corrected - False Negatives: {fn}\")\n",
    "# print(f\"Corrected - True Positives: {tp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRYING OUT ERROR-CORRECTING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bias Predictor Model\n",
    "class BiasPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiasPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(14, 128)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 8)\n",
    "        self.fc6 = nn.Linear(8, 1)  # Output layer for bias correction\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        return self.fc6(x)\n",
    "\n",
    "# Prepare the training data for the bias predictor\n",
    "def prepare_bias_data(reconstruction_errors, x_test, original_probs, ground_truth):\n",
    "    # Combine reconstruction errors with the existing normalized features\n",
    "    features = np.hstack((reconstruction_errors.reshape(-1, 1), x_test.values))\n",
    "    \n",
    "    # Compute target biases\n",
    "    target_bias = ground_truth - original_probs\n",
    "    print(\"Minimum bias: \", np.min(target_bias))\n",
    "    print(\"Maximum bias: \", np.max(target_bias))\n",
    "    \n",
    "    return torch.tensor(features, dtype=torch.float32), torch.tensor(target_bias, dtype=torch.float32)\n",
    "\n",
    "def prepare_bias_data_scaled(reconstruction_errors, x_data, original_probs, ground_truth, reconstruction_scale):\n",
    "    \"\"\"\n",
    "    Prepares data for the bias predictor by combining features and scaling reconstruction errors.\n",
    "    \"\"\"\n",
    "    # Scale reconstruction errors\n",
    "    scaled_reconstruction_errors = reconstruction_errors * reconstruction_scale\n",
    "    \n",
    "    # Combine scaled reconstruction errors with the existing features\n",
    "    features = np.hstack((scaled_reconstruction_errors.reshape(-1, 1), x_data.values))\n",
    "    \n",
    "    #######################################################################################################\n",
    "    # use this for only reconstruction errors as the input feature\n",
    "    \n",
    "    # features = scaled_reconstruction_errors.reshape(-1, 1)\n",
    "    # print(\"Minimum scaled reconstruction error: \", np.min(scaled_reconstruction_errors))\n",
    "    # print(\"Maximum scaled reconstruction error: \", np.max(scaled_reconstruction_errors))\n",
    "    \n",
    "    #######################################################################################################\n",
    "    \n",
    "    # Compute target biases\n",
    "    target_bias = ground_truth - original_probs\n",
    "    print(\"Minimum bias: \", np.min(target_bias))\n",
    "    print(\"Maximum bias: \", np.max(target_bias))\n",
    "    \n",
    "    return torch.tensor(features, dtype=torch.float32), torch.tensor(target_bias, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_bias_predictor(features, target_bias, val_features, val_target_bias, epochs=1000, learning_rate=0.0001, patience=10):\n",
    "    model = BiasPredictor().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "    patience_counter = 0  # Counter for early stopping\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features).squeeze()\n",
    "        loss = criterion(predictions, target_bias)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(val_features).squeeze()\n",
    "            val_loss = criterion(val_predictions, val_target_bias)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()  # Save the best model state\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "        # Stop training if patience is exceeded\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # Load the best model state before returning\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "# Train the Bias Predictor with Class Weights\n",
    "# def train_bias_predictor_with_weights(features, target_bias, ground_truth, epochs=100, learning_rate=0.001):\n",
    "# def train_bias_predictor_with_weights(features, target_bias, ground_truth, epochs=100, learning_rate=0.001, weight_positive_scale=1.0, weight_negative_scale=1.0):\n",
    "#     model = BiasPredictor().to(device)\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "#     # Define class weights\n",
    "#     num_positive = np.sum(ground_truth == 1)\n",
    "#     num_negative = np.sum(ground_truth == 0)\n",
    "#     total = len(ground_truth)\n",
    "\n",
    "#     weight_positive = (total / (2 * num_positive)) * weight_positive_scale\n",
    "#     weight_negative = (total / (2 * num_negative)) * weight_negative_scale \n",
    "\n",
    "#     # Convert weights to tensors\n",
    "#     class_weights = torch.tensor([weight_negative, weight_positive], dtype=torch.float32).to(device)\n",
    "#     criterion = nn.MSELoss(reduction='none')  # Use 'none' to apply weights manually\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         predictions = model(features).squeeze()\n",
    "        \n",
    "#         # Calculate weighted loss\n",
    "#         loss = criterion(predictions, target_bias)\n",
    "#         weights = torch.where(\n",
    "#             target_bias > 0,  # Assign weights based on ground truth class\n",
    "#             class_weights[1],  # Positive class weight\n",
    "#             class_weights[0]   # Negative class weight\n",
    "#         )\n",
    "#         weighted_loss = torch.mean(loss * weights)\n",
    "        \n",
    "#         weighted_loss.backward()\n",
    "#         optimizer.step()\n",
    "#     return model\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "def apply_bias_correction(bias_predictor, reconstruction_errors, x_test, original_probs):\n",
    "    # Combine normalized reconstruction errors with other features\n",
    "    combined_features = np.hstack((reconstruction_errors.reshape(-1, 1), x_test.values))\n",
    "    features = torch.tensor(combined_features, dtype=torch.float32).to(next(bias_predictor.parameters()).device)\n",
    "    \n",
    "    #######################################################################################################\n",
    "    # use this for only reconstruction errors as the input feature\n",
    "    \n",
    "    # combined_features = reconstruction_errors.reshape(-1, 1)\n",
    "    # features = torch.tensor(combined_features, dtype=torch.float32).to(next(bias_predictor.parameters()).device)\n",
    "    \n",
    "    #######################################################################################################\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bias_correction = bias_predictor(features).squeeze().cpu().numpy()\n",
    "    \n",
    "    # Multiply with a scaling factor\n",
    "    bias_correction *= 1.2\n",
    "    \n",
    "    # Apply the correction and clip probabilities to [0, 1]\n",
    "    corrected_probs = np.clip(original_probs + bias_correction, 0, 1)\n",
    "    \n",
    "    # Number of predictions that changed\n",
    "    changed_predictions = np.sum((original_probs > 0.5).astype(int) != (corrected_probs > 0.5).astype(int))\n",
    "    print(f\"Number of changed predictions: {changed_predictions}\")\n",
    "    \n",
    "    return corrected_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reconstruction error:  7.528515562734883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_23516/885771367.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bias_predictor.load_state_dict(torch.load('./models/bias_predictor.pth'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train_split.values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_split.values, dtype=torch.float32).to(device)\n",
    "x_val_tensor = torch.tensor(x_val_split.values, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val_split.values, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_train = autoencoder(x_train_tensor).cpu().numpy()\n",
    "reconstructed_errors_train = np.mean(np.square(x_train_split.values - reconstructed_train), axis=1)\n",
    "mean_error_train = np.mean(reconstructed_errors_train)\n",
    "std_error_train = np.std(reconstructed_errors_train)\n",
    "reconstruction_errors_train_normalized = (reconstructed_errors_train - mean_error_train) / std_error_train\n",
    "print(\"Max reconstruction error: \", np.max(reconstructed_errors_train))\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_val = autoencoder(x_val_tensor).cpu().numpy()\n",
    "reconstructed_errors_val = np.mean(np.square(x_val_split.values - reconstructed_val), axis=1)\n",
    "mean_error_val = np.mean(reconstructed_errors_val)\n",
    "std_error_val = np.std(reconstructed_errors_val)\n",
    "reconstruction_errors_val_normalized = (reconstructed_errors_val - mean_error_val) / std_error_val\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_train_tensor = torch.tensor(x_train_split.values, dtype=torch.float32).to(device)\n",
    "    # logits = original_mlp(x_test_tensor)\n",
    "    # y_pred_original = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "    y_pred_train = original_mlp(x_train_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    x_val_tensor = torch.tensor(x_val_split.values, dtype=torch.float32).to(device)\n",
    "    y_pred_val = original_mlp(x_val_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "    \n",
    "original_probs_train = y_pred_train\n",
    "ground_truth_train = y_train_split.values\n",
    "\n",
    "original_probs_val = y_pred_val\n",
    "ground_truth_val = y_val_split.values\n",
    "\n",
    "\n",
    "# Prepare the data for training\n",
    "# features, target_bias = prepare_bias_data(reconstruction_errors_normalized, x_train, original_probs, ground_truth)\n",
    "\n",
    "# if model does not exist\n",
    "if not os.path.exists('./models/bias_predictor.pth'):\n",
    "    features_train, target_bias_train = prepare_bias_data_scaled(reconstruction_errors_train_normalized, x_train_split, original_probs_train, ground_truth_train, reconstruction_scale=20)\n",
    "    features_val, target_bias_val = prepare_bias_data_scaled(reconstruction_errors_val_normalized, x_val_split, original_probs_val, ground_truth_val, reconstruction_scale=20)\n",
    "\n",
    "    # Move tensors to the same device as the model\n",
    "    features_train, target_bias_train = features_train.to(device), target_bias_train.to(device)\n",
    "    features_val, target_bias_val = features_val.to(device), target_bias_val.to(device)\n",
    "\n",
    "    # Train the bias predictor\n",
    "    bias_predictor = train_bias_predictor(features_train, target_bias_train, features_val, target_bias_val, epochs=5000, learning_rate=0.0001, patience=10)\n",
    "                        \n",
    "    # Apply bias correction during inference\n",
    "    corrected_probs = apply_bias_correction(bias_predictor, reconstruction_errors_train_normalized, x_train_split, original_probs_train)\n",
    "\n",
    "    # Convert probabilities to binary predictions\n",
    "    corrected_predictions = (corrected_probs > 0.5).astype(int)\n",
    "    # Calculate accuracy\n",
    "    corrected_accuracy = accuracy_score(y_train_split, corrected_predictions)\n",
    "    print(f\"\\nCorrected Accuracy: {corrected_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix_corrected = confusion_matrix(y_train_split, corrected_predictions)\n",
    "\n",
    "    tn, fp, fn, tp = conf_matrix_corrected.ravel()\n",
    "    print(\"True Positives: \", tp)\n",
    "    print(\"True Negatives: \", tn)\n",
    "    print(\"False Positives: \", fp)\n",
    "    print(\"False Negatives: \", fn)\n",
    "\n",
    "    # Evaluate the original MLP model\n",
    "    with torch.no_grad():\n",
    "        x_train_tensor = torch.tensor(x_train_split.values, dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train_split.values, dtype=torch.float32).to(device)\n",
    "        y_pred_original = original_mlp(x_train_tensor).cpu().numpy().flatten()\n",
    "        y_pred_original = (y_pred_original > 0.5).astype(int)  # Convert to binary\n",
    "\n",
    "    original_accuracy = accuracy_score(y_train_split, y_pred_original)\n",
    "    print(f\"\\nOriginal MLP Accuracy: {original_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix_original = confusion_matrix(y_train_split, y_pred_original)\n",
    "\n",
    "    original_tn, original_fp, original_fn, original_tp = conf_matrix_original.ravel()\n",
    "    print(\"True Positives: \", original_tp)\n",
    "    print(\"True Negatives: \", original_tn)\n",
    "    print(\"False Positives: \", original_fp)\n",
    "    print(\"False Negatives: \", original_fn)\n",
    "\n",
    "else:\n",
    "    # Load the trained bias predictor model\n",
    "    bias_predictor = BiasPredictor().to(device)\n",
    "    bias_predictor.load_state_dict(torch.load('./models/bias_predictor.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of changed predictions: 106\n",
      "\n",
      "Corrected Accuracy on Test Set: 97.03%\n",
      "True Positives:  1127\n",
      "True Negatives:  18279\n",
      "False Positives:  54\n",
      "False Negatives:  540\n",
      "\n",
      "Original Accuracy on Test Set: 96.82%\n",
      "True Positives:  1156\n",
      "True Negatives:  18208\n",
      "False Positives:  125\n",
      "False Negatives:  511\n",
      "Improvement:  6.603773584905969\n"
     ]
    }
   ],
   "source": [
    "x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_test = autoencoder(x_test_tensor).cpu().numpy()\n",
    "reconstructed_errors_test = np.mean(np.square(x_test.values - reconstructed_test), axis=1)\n",
    "mean_error_test = np.mean(reconstructed_errors_test)\n",
    "std_error_test = np.std(reconstructed_errors_test)\n",
    "reconstruction_errors_test_normalized = (reconstructed_errors_test - mean_error_test) / std_error_test\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "    y_pred_test = original_mlp(x_test_tensor).cpu().numpy().flatten()\n",
    "    \n",
    "original_probs_test = y_pred_test\n",
    "\n",
    "original_accuracy_test = accuracy_score(y_test, (original_probs_test > 0.5).astype(int))\n",
    "\n",
    "\n",
    "corrected_probs_test = apply_bias_correction(bias_predictor, reconstruction_errors_test_normalized, x_test, original_probs_test)\n",
    "\n",
    "corrected_predictions_test = (corrected_probs_test > 0.5).astype(int)\n",
    "\n",
    "corrected_accuracy_test = accuracy_score(y_test, corrected_predictions_test)\n",
    "print(f\"\\nCorrected Accuracy on Test Set: {corrected_accuracy_test * 100:.2f}%\")\n",
    "\n",
    "conf_matrix_corrected_test = confusion_matrix(y_test, corrected_predictions_test)\n",
    "\n",
    "tn, fp, fn, tp = conf_matrix_corrected_test.ravel()\n",
    "print(\"True Positives: \", tp)\n",
    "print(\"True Negatives: \", tn)\n",
    "print(\"False Positives: \", fp)\n",
    "print(\"False Negatives: \", fn)\n",
    "\n",
    "print(f\"\\nOriginal Accuracy on Test Set: {original_accuracy_test * 100:.2f}%\")\n",
    "\n",
    "conf_matrix_original_test = confusion_matrix(y_test, (original_probs_test > 0.5).astype(int))\n",
    "\n",
    "tn, fp, fn, tp = conf_matrix_original_test.ravel()\n",
    "print(\"True Positives: \", tp)\n",
    "print(\"True Negatives: \", tn)\n",
    "print(\"False Positives: \", fp)\n",
    "print(\"False Negatives: \", fn)\n",
    "\n",
    "difference = corrected_accuracy_test - original_accuracy_test\n",
    "difference_percent = difference/(1-original_accuracy_test)\n",
    "print(\"Improvement: \", difference_percent*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (corrected_accuracy_test > original_accuracy_test + 0.0010) and not os.path.exists('./models/bias_predictor.pth'):\n",
    "    # save the model\n",
    "    torch.save(bias_predictor.state_dict(), './models/bias_predictor.pth')\n",
    "    print(\"Model saved\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
