{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da68MaKdRTFR",
        "outputId": "bef54ea8-2e24-4cfa-a6d8-91a973a26414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('diabetes_prediction_dataset.csv')\n",
        "df_encoded = pd.get_dummies(df, columns=['gender', 'smoking_history'], drop_first=True)\n",
        "testDF = df_encoded.sample(frac=1).reset_index(drop=True)\n",
        "x_unscaled = testDF.drop(['diabetes'], axis=1)\n",
        "y = testDF['diabetes']\n",
        "\n",
        "# Normalize the data\n",
        "numerical_columns = x_unscaled.select_dtypes(include=np.number).columns\n",
        "boolean_columns = x_unscaled.select_dtypes(include=bool).columns\n",
        "scaler = StandardScaler()\n",
        "temp = pd.DataFrame(scaler.fit_transform(x_unscaled[numerical_columns]), columns=numerical_columns)\n",
        "x_scaled = pd.concat([temp, x_unscaled[boolean_columns]], axis=1)\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(x_scaled, y, test_size=0.3, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOKaWwwUSJn7",
        "outputId": "e02fb53a-216d-4b5b-b6c5-4a2034c7ff0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-296-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 0 1 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 0 1 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 1 0 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 1 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 1 0 ... 0 0 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 0 1 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-296-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n"
          ]
        }
      ],
      "source": [
        "x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
        "x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
        "x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "5JAtg4mkRTFU"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "X_val_tensor = torch.tensor(x_val.values, dtype=torch.float32).to(device)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "X_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "id": "y0fZOoJcRTFV"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size, encoding_dim, dropout):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 2, encoding_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZbq7Ol_RTFX",
        "outputId": "fd8fb0db-989b-4774-af70-bb18f8f53a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-299-8fad8731368b>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  autoencoder.load_state_dict(torch.load(autoencoder_path, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Autoencoder(\n",
              "  (encoder): Sequential(\n",
              "    (0): Linear(in_features=13, out_features=32, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.0, inplace=False)\n",
              "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.0, inplace=False)\n",
              "    (6): Linear(in_features=16, out_features=8, bias=True)\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=8, out_features=16, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.0, inplace=False)\n",
              "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.0, inplace=False)\n",
              "    (6): Linear(in_features=32, out_features=13, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 299
        }
      ],
      "source": [
        "# Define input dimensions\n",
        "input_dim = x_train.shape[1]\n",
        "encoding_dim = 8\n",
        "\n",
        "autoencoder_path = './models/autoencoder.pth'\n",
        "\n",
        "# Initialize the model\n",
        "autoencoder = Autoencoder(input_dim, 32, encoding_dim, 0.0).to(device)\n",
        "\n",
        "# Load the model\n",
        "autoencoder.load_state_dict(torch.load(autoencoder_path, map_location=torch.device('cpu')))\n",
        "autoencoder.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS37HZflRTFX"
      },
      "source": [
        "## Original MLP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dthvENP7RTFZ",
        "outputId": "af85f6d3-1ac3-44c5-b4fc-e3d1e142068f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-300-825b1d53ce33>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  original_mlp.load_state_dict(torch.load(mlp_model_file, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 97.18%\n",
            "TP: 847\n",
            "TN: 13730\n",
            "FP: 4\n",
            "FN: 419\n",
            "Recall: 0.669036\n",
            "Precision: 0.995300\n",
            "F1 Score: 0.800189\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size, dropout):\n",
        "        super(MLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Load the original MLP model\n",
        "mlp_model_file = './models/mlp_model.pth'\n",
        "input_dim = x_train.shape[1]\n",
        "original_mlp = MLP(input_dim, 128, 0.3).to(device)\n",
        "original_mlp.load_state_dict(torch.load(mlp_model_file, map_location=torch.device('cpu')))\n",
        "original_mlp.eval()\n",
        "\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize lists to store predictions and true labels\n",
        "y_true_list = []\n",
        "y_pred_list = []\n",
        "\n",
        "# Evaluate the original MLP model\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        outputs = original_mlp(X_batch)  # Get predictions (probabilities from sigmoid)\n",
        "        predictions = (outputs > 0.5).float()  # Convert probabilities to binary (0 or 1)\n",
        "\n",
        "        y_true_list.extend(y_batch.cpu().numpy())  # Store true labels\n",
        "        y_pred_list.extend(predictions.cpu().numpy())  # Store predictions\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "y_true = np.array(y_true_list)\n",
        "y_pred = np.array(y_pred_list)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print('TP:', conf_matrix[1, 1])\n",
        "print('TN:', conf_matrix[0, 0])\n",
        "print('FP:', conf_matrix[0, 1])\n",
        "print('FN:', conf_matrix[1, 0])\n",
        "\n",
        "# Calculate recall, precision, and F1 score\n",
        "recall = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 0])\n",
        "precision = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[0, 1])\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "print(f'Recall: {recall:.6f}')\n",
        "print(f'Precision: {precision:.6f}')\n",
        "print(f'F1 Score: {f1_score:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrkWThhKRTFa"
      },
      "source": [
        "### TRYING OUT ERROR-CORRECTING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "id": "i-CReA7eRTFb"
      },
      "outputs": [],
      "source": [
        "# Define the Bias Predictor Model\n",
        "# class BiasPredictor(nn.Module):\n",
        "#     def __init__(self, input_dim=1, hidden_size=128, dropout=0.2):\n",
        "#         super(BiasPredictor, self).__init__()\n",
        "#         self.fc1 = nn.Linear(input_dim, hidden_size)\n",
        "#         self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "#         self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "#         self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "#         self.fc3 = nn.Linear(hidden_size // 2, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.dropout1(x)\n",
        "\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = self.dropout2(x)\n",
        "\n",
        "#         return self.fc3(x)\n",
        "\n",
        "class BiasPredictor(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_size=128, dropout=0.2):\n",
        "        super(BiasPredictor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_size // 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.leaky_relu(self.fc2(x), negative_slope=0.01)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        return torch.tanh(self.fc3(x))\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the training data for the bias predictor\n",
        "def prepare_bias_data(reconstruction_errors, x_test, original_probs, ground_truth):\n",
        "    # Combine reconstruction errors with the existing normalized features\n",
        "    features = np.hstack((reconstruction_errors.reshape(-1, 1), x_test.values))\n",
        "\n",
        "    # Compute target biases\n",
        "    target_bias = ground_truth - original_probs\n",
        "    print(\"Minimum bias: \", np.min(target_bias))\n",
        "    print(\"Maximum bias: \", np.max(target_bias))\n",
        "\n",
        "    return torch.tensor(features, dtype=torch.float32), torch.tensor(target_bias, dtype=torch.float32)\n",
        "\n",
        "def prepare_bias_data_scaled(reconstruction_errors, x_data, original_probs, ground_truth, reconstruction_scale):\n",
        "    \"\"\"\n",
        "    Prepares data for the bias predictor by combining features and scaling reconstruction errors.\n",
        "    \"\"\"\n",
        "    # Scale reconstruction errors\n",
        "    scaled_reconstruction_errors = reconstruction_errors * reconstruction_scale\n",
        "\n",
        "    # Combine scaled reconstruction errors with the existing features\n",
        "    # features = np.hstack((scaled_reconstruction_errors.reshape(-1, 1), x_data.values))\n",
        "\n",
        "    #######################################################################################################\n",
        "    # use this for only reconstruction errors as the input feature\n",
        "\n",
        "    features = scaled_reconstruction_errors.reshape(-1, 1)\n",
        "    # print(\"Minimum scaled reconstruction error: \", np.min(scaled_reconstruction_errors))\n",
        "    # print(\"Maximum scaled reconstruction error: \", np.max(scaled_reconstruction_errors))\n",
        "\n",
        "    #######################################################################################################\n",
        "\n",
        "    # Compute target biases\n",
        "    target_bias = ground_truth - original_probs\n",
        "    print(\"Minimum bias: \", np.min(target_bias))\n",
        "    print(\"Maximum bias: \", np.max(target_bias))\n",
        "\n",
        "    return torch.tensor(features, dtype=torch.float32), torch.tensor(target_bias, dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "def train_bias_predictor(features, target_bias, val_features, val_target_bias, epochs=1000, learning_rate=0.0001, patience=10, input_dim=1, hidden_size=128, dropout=0.2):\n",
        "    model = BiasPredictor(input_dim = input_dim, hidden_size=hidden_size, dropout = dropout).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
        "    patience_counter = 0  # Counter for early stopping\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(features).squeeze()\n",
        "        loss = criterion(predictions, target_bias)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_predictions = model(val_features).squeeze()\n",
        "            val_loss = criterion(val_predictions, val_target_bias)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict()  # Save the best model state\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "        # Stop training if patience is exceeded\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
        "            break\n",
        "\n",
        "    # Load the best model state before returning\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "#####################################################################################################\n",
        "\n",
        "# Train the Bias Predictor with Class Weights\n",
        "# def train_bias_predictor_with_weights(features, target_bias, ground_truth, epochs=100, learning_rate=0.001):\n",
        "# def train_bias_predictor_with_weights(features, target_bias, ground_truth, epochs=100, learning_rate=0.001, weight_positive_scale=1.0, weight_negative_scale=1.0):\n",
        "#     model = BiasPredictor().to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#     # Define class weights\n",
        "#     num_positive = np.sum(ground_truth == 1)\n",
        "#     num_negative = np.sum(ground_truth == 0)\n",
        "#     total = len(ground_truth)\n",
        "\n",
        "#     weight_positive = (total / (2 * num_positive)) * weight_positive_scale\n",
        "#     weight_negative = (total / (2 * num_negative)) * weight_negative_scale\n",
        "\n",
        "#     # Convert weights to tensors\n",
        "#     class_weights = torch.tensor([weight_negative, weight_positive], dtype=torch.float32).to(device)\n",
        "#     criterion = nn.MSELoss(reduction='none')  # Use 'none' to apply weights manually\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         predictions = model(features).squeeze()\n",
        "\n",
        "#         # Calculate weighted loss\n",
        "#         loss = criterion(predictions, target_bias)\n",
        "#         weights = torch.where(\n",
        "#             target_bias > 0,  # Assign weights based on ground truth class\n",
        "#             class_weights[1],  # Positive class weight\n",
        "#             class_weights[0]   # Negative class weight\n",
        "#         )\n",
        "#         weighted_loss = torch.mean(loss * weights)\n",
        "\n",
        "#         weighted_loss.backward()\n",
        "#         optimizer.step()\n",
        "#     return model\n",
        "\n",
        "#####################################################################################################\n",
        "\n",
        "def apply_bias_correction(bias_predictor, reconstruction_errors, x_test, original_probs):\n",
        "    # Combine normalized reconstruction errors with other features\n",
        "    # combined_features = np.hstack((reconstruction_errors.reshape(-1, 1), x_test.values))\n",
        "    # features = torch.tensor(combined_features, dtype=torch.float32).to(next(bias_predictor.parameters()).device)\n",
        "\n",
        "    #######################################################################################################\n",
        "    # use this for only reconstruction errors as the input feature\n",
        "\n",
        "    combined_features = reconstruction_errors.reshape(-1, 1)\n",
        "    features = torch.tensor(combined_features, dtype=torch.float32).to(next(bias_predictor.parameters()).device)\n",
        "\n",
        "    #######################################################################################################\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bias_correction = bias_predictor(features).squeeze().cpu().numpy()\n",
        "\n",
        "    print(np.min(bias_correction))\n",
        "    print(np.max(bias_correction))\n",
        "\n",
        "    # Multiply with a scaling factor\n",
        "    bias_correction *= 1.2\n",
        "\n",
        "    # Apply the correction and clip probabilities to [0, 1]\n",
        "    corrected_probs = np.clip(original_probs + bias_correction, 0, 1)\n",
        "\n",
        "    # Number of predictions that changed\n",
        "    changed_predictions = np.sum((original_probs > 0.5).astype(int) != (corrected_probs > 0.5).astype(int))\n",
        "    print(f\"Number of changed predictions: {changed_predictions}\")\n",
        "\n",
        "    return corrected_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMariswORTFc",
        "outputId": "cea2fa70-2765-44ed-fac8-2cfa22a7b6e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max reconstruction error:  3.0516762627423932\n",
            "Minimum bias:  -0.7354510426521301\n",
            "Maximum bias:  0.999342622992117\n",
            "Minimum bias:  -0.4962349832057953\n",
            "Maximum bias:  0.9972428127657622\n",
            "Early stopping triggered at epoch 28. Best Val Loss: 0.0908\n",
            "-0.105383754\n",
            "0.23426042\n",
            "Number of changed predictions: 21\n",
            "\n",
            "Corrected Accuracy: 97.19%\n",
            "True Positives:  3941\n",
            "True Negatives:  64091\n",
            "False Positives:  20\n",
            "False Negatives:  1948\n",
            "\n",
            "Original MLP Accuracy: 97.20%\n",
            "True Positives:  3955\n",
            "True Negatives:  64088\n",
            "False Positives:  23\n",
            "False Negatives:  1934\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "with torch.no_grad():\n",
        "    reconstructed_train = autoencoder(X_train_tensor).cpu().numpy()\n",
        "reconstructed_errors_train = np.mean(np.square(x_train.values - reconstructed_train), axis=1)\n",
        "mean_error_train = np.mean(reconstructed_errors_train)\n",
        "std_error_train = np.std(reconstructed_errors_train)\n",
        "reconstruction_errors_train_normalized = (reconstructed_errors_train - mean_error_train) / std_error_train\n",
        "print(\"Max reconstruction error: \", np.max(reconstructed_errors_train))\n",
        "\n",
        "with torch.no_grad():\n",
        "    reconstructed_val = autoencoder(X_val_tensor).cpu().numpy()\n",
        "reconstructed_errors_val = np.mean(np.square(x_val.values - reconstructed_val), axis=1)\n",
        "mean_error_val = np.mean(reconstructed_errors_val)\n",
        "std_error_val = np.std(reconstructed_errors_val)\n",
        "reconstruction_errors_val_normalized = (reconstructed_errors_val - mean_error_val) / std_error_val\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred_train = original_mlp(X_train_tensor).cpu().numpy().flatten()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred_val = original_mlp(X_val_tensor).cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "original_probs_train = y_pred_train\n",
        "ground_truth_train = y_train.values\n",
        "\n",
        "original_probs_val = y_pred_val\n",
        "ground_truth_val = y_val.values\n",
        "\n",
        "\n",
        "# Prepare the data for training\n",
        "# features, target_bias = prepare_bias_data(reconstruction_errors_normalized, x_train, original_probs, ground_truth)\n",
        "\n",
        "# if model does not exist\n",
        "if not os.path.exists('./models/bias_predictor.pth'):\n",
        "    features_train, target_bias_train = prepare_bias_data_scaled(reconstruction_errors_train_normalized, x_train, original_probs_train, ground_truth_train, reconstruction_scale=50)\n",
        "    features_val, target_bias_val = prepare_bias_data_scaled(reconstruction_errors_val_normalized, x_val, original_probs_val, ground_truth_val, reconstruction_scale=50)\n",
        "\n",
        "    # Move tensors to the same device as the model\n",
        "    features_train, target_bias_train = features_train.to(device), target_bias_train.to(device)\n",
        "    features_val, target_bias_val = features_val.to(device), target_bias_val.to(device)\n",
        "\n",
        "    # Train the bias predictor\n",
        "    bias_predictor = train_bias_predictor(features_train, target_bias_train, features_val, target_bias_val, epochs=5000, learning_rate=0.0001, patience=10)\n",
        "\n",
        "    # Apply bias correction during inference\n",
        "    corrected_probs = apply_bias_correction(bias_predictor, reconstruction_errors_train_normalized, x_train, original_probs_train)\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    corrected_predictions = (corrected_probs > 0.5).astype(int)\n",
        "    # Calculate accuracy\n",
        "    corrected_accuracy = accuracy_score(y_train, corrected_predictions)\n",
        "    print(f\"\\nCorrected Accuracy: {corrected_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    conf_matrix_corrected = confusion_matrix(y_train, corrected_predictions)\n",
        "\n",
        "    tn, fp, fn, tp = conf_matrix_corrected.ravel()\n",
        "    print(\"True Positives: \", tp)\n",
        "    print(\"True Negatives: \", tn)\n",
        "    print(\"False Positives: \", fp)\n",
        "    print(\"False Negatives: \", fn)\n",
        "\n",
        "    # Evaluate the original MLP model\n",
        "    with torch.no_grad():\n",
        "        x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
        "        y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
        "        y_pred_original = original_mlp(x_train_tensor).cpu().numpy().flatten()\n",
        "        y_pred_original = (y_pred_original > 0.5).astype(int)  # Convert to binary\n",
        "\n",
        "    original_accuracy = accuracy_score(y_train, y_pred_original)\n",
        "    print(f\"\\nOriginal MLP Accuracy: {original_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    conf_matrix_original = confusion_matrix(y_train, y_pred_original)\n",
        "\n",
        "    original_tn, original_fp, original_fn, original_tp = conf_matrix_original.ravel()\n",
        "    print(\"True Positives: \", original_tp)\n",
        "    print(\"True Negatives: \", original_tn)\n",
        "    print(\"False Positives: \", original_fp)\n",
        "    print(\"False Negatives: \", original_fn)\n",
        "\n",
        "else:\n",
        "    # Load the trained bias predictor model\n",
        "    bias_predictor = BiasPredictor().to(device)\n",
        "    bias_predictor.load_state_dict(torch.load('./models/bias_predictor.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsf6-YRLRTFe",
        "outputId": "cd395baf-1264-46c3-e42a-f91efeb93315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.10538434\n",
            "0.18783206\n",
            "Number of changed predictions: 6\n",
            "\n",
            "Corrected Accuracy on Test Set: 97.18%\n",
            "True Positives:  846\n",
            "True Negatives:  13731\n",
            "False Positives:  3\n",
            "False Negatives:  420\n",
            "\n",
            "Original Accuracy on Test Set: 97.18%\n",
            "True Positives:  847\n",
            "True Negatives:  13730\n",
            "False Positives:  4\n",
            "False Negatives:  419\n",
            "Improvement:  0.0\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    reconstructed_test = autoencoder(X_test_tensor).cpu().numpy()\n",
        "reconstructed_errors_test = np.mean(np.square(x_test.values - reconstructed_test), axis=1)\n",
        "mean_error_test = np.mean(reconstructed_errors_test)\n",
        "std_error_test = np.std(reconstructed_errors_test)\n",
        "reconstruction_errors_test_normalized = (reconstructed_errors_test - mean_error_test) / std_error_test\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
        "    y_pred_test = original_mlp(x_test_tensor).cpu().numpy().flatten()\n",
        "\n",
        "original_probs_test = y_pred_test\n",
        "\n",
        "original_accuracy_test = accuracy_score(y_test, (original_probs_test > 0.5).astype(int))\n",
        "\n",
        "\n",
        "corrected_probs_test = apply_bias_correction(bias_predictor, reconstruction_errors_test_normalized, x_test, original_probs_test)\n",
        "\n",
        "corrected_predictions_test = (corrected_probs_test > 0.5).astype(int)\n",
        "\n",
        "corrected_accuracy_test = accuracy_score(y_test, corrected_predictions_test)\n",
        "print(f\"\\nCorrected Accuracy on Test Set: {corrected_accuracy_test * 100:.2f}%\")\n",
        "\n",
        "conf_matrix_corrected_test = confusion_matrix(y_test, corrected_predictions_test)\n",
        "\n",
        "tn, fp, fn, tp = conf_matrix_corrected_test.ravel()\n",
        "print(\"True Positives: \", tp)\n",
        "print(\"True Negatives: \", tn)\n",
        "print(\"False Positives: \", fp)\n",
        "print(\"False Negatives: \", fn)\n",
        "\n",
        "print(f\"\\nOriginal Accuracy on Test Set: {original_accuracy_test * 100:.2f}%\")\n",
        "\n",
        "conf_matrix_original_test = confusion_matrix(y_test, (original_probs_test > 0.5).astype(int))\n",
        "\n",
        "tn, fp, fn, tp = conf_matrix_original_test.ravel()\n",
        "print(\"True Positives: \", tp)\n",
        "print(\"True Negatives: \", tn)\n",
        "print(\"False Positives: \", fp)\n",
        "print(\"False Negatives: \", fn)\n",
        "\n",
        "difference = corrected_accuracy_test - original_accuracy_test\n",
        "difference_percent = difference/(1-original_accuracy_test)\n",
        "print(\"Improvement: \", difference_percent*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "id": "EYLDTn-oRTFg"
      },
      "outputs": [],
      "source": [
        "# if (corrected_accuracy_test > original_accuracy_test + 0.0010) and not os.path.exists('./models/bias_predictor.pth'):\n",
        "#     # save the model\n",
        "#     torch.save(bias_predictor.state_dict(), './models/bias_predictor.pth')\n",
        "#     print(\"Model saved\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "id": "TbAEfm6DRTFh"
      },
      "outputs": [],
      "source": [
        "# # Identify the indices where the predictions changed\n",
        "# changed_indices = np.where((original_probs_test > 0.5).astype(int) != corrected_predictions_test)[0]\n",
        "\n",
        "# # Print the indices of the changed data points\n",
        "# # print(\"Indices of changed data points:\", changed_indices)\n",
        "\n",
        "# print(\"Number of changed indices:\", len(changed_indices))\n",
        "# # Get the reconstruction error of the changed data points\n",
        "# changed_reconstruction_errors = reconstruction_errors_test_normalized[changed_indices]\n",
        "\n",
        "# # Define the ranges for the reconstruction errors\n",
        "# ranges = [-1.0, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0, 2.0, 3.0, 4.0, 5.0, 20]\n",
        "\n",
        "# # Count the number of changed indices within each range\n",
        "# counts, bin_edges = np.histogram(changed_reconstruction_errors, bins=ranges)\n",
        "\n",
        "# # Print the counts for each range\n",
        "# for i in range(len(ranges) - 1):\n",
        "#     print(f\"Range {ranges[i]} to {ranges[i+1]}: {counts[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaLBiw8ORTFi"
      },
      "source": [
        "### Trying bias correction with individual reconstruction errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "id": "pnrBHxVmRTFj"
      },
      "outputs": [],
      "source": [
        "def compute_feature_importance(model, X, y_true, metric=accuracy_score, n_repeats=5):\n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "    X_base = X.clone()  # Keep a copy of the original data\n",
        "\n",
        "    # Get baseline predictions and accuracy\n",
        "    with torch.no_grad():\n",
        "        y_pred_base = (model(X).detach().cpu().numpy().flatten() > 0.5).astype(int)\n",
        "        baseline_score = metric(y_true.cpu().numpy().flatten(), y_pred_base)\n",
        "\n",
        "    feature_importance = np.zeros(X.shape[1])  # Array to store importance for each feature\n",
        "\n",
        "    for i in range(X.shape[1]):  # Loop through features\n",
        "        scores = []\n",
        "        for _ in range(n_repeats):  # Repeat for robustness\n",
        "            X_permuted = X_base.clone()  # Clone original dataset\n",
        "            permuted_feature = X_permuted[:, i].clone()  # Extract feature\n",
        "            X_permuted[:, i] = permuted_feature[torch.randperm(X.shape[0])]  # Shuffle the feature\n",
        "\n",
        "            # Get predictions with shuffled feature\n",
        "            with torch.no_grad():\n",
        "                y_pred_permuted = (model(X_permuted).detach().cpu().numpy().flatten() > 0.5).astype(int)\n",
        "                score = metric(y_true.cpu().numpy().flatten(), y_pred_permuted)\n",
        "            scores.append(score)\n",
        "\n",
        "        feature_importance[i] = baseline_score - np.mean(scores)  # Drop in performance\n",
        "\n",
        "    # Normalize so that importance values sum to 1\n",
        "    feature_importance /= feature_importance.sum()\n",
        "\n",
        "    return feature_importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "O68svPMLRTFl",
        "outputId": "c4084d69-7051-4ae0-9e80-a7898554ace3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/4AAAIjCAYAAAC6S8CIAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAApwVJREFUeJzs3Xl8Dtf////nJYkkstsitsSWCLXv1E4tpUJLaYjYtdRWa1tLULQfSkt1l6B0oareVZQSW5XYQonUEkvbKK1KGiqyzO8PX/NzSZBESHt53G+3ud0y55w55zVzXXrr6zpnZiyGYRgCAAAAAAA2KV9eBwAAAAAAAB4cEn8AAAAAAGwYiT8AAAAAADaMxB8AAAAAABtG4g8AAAAAgA0j8QcAAAAAwIaR+AMAAAAAYMNI/AEAAAAAsGEk/gAAAAAA2DASfwAAAOS5c+fOycnJSTt37szrUB4ai8WiKVOmmPsRERGyWCw6ffr0Q43Dz89PoaGh5v769evl6uqqixcvPtQ4ADw4JP4AAOSCm//DbrFYtGPHjgz1hmGoVKlSslgs6tChg1WdxWLR0KFD79p/s2bNzP4tFosKFiyoOnXqaNGiRUpPT89ybLdv48ePz/7JZsEPP/ygKVOm6PLlyw+k//tx83rs3bs3r0PJsYULFyoiIiKvw8hVU6dOVb169dSoUSOzLDQ0VBaLRVWrVpVhGBmOycq/HWRf27ZtVb58ec2cOTOvQwGQS0j8AQDIRU5OTlq+fHmG8q1bt+qXX36Ro6NjjvsuWbKkli5dqqVLl2rixIlKTU1Vv3799PLLL2fp+KlTp5rH39y6d++e43ju5ocfflBYWNi/MvG3BbaW+F+8eFGLFy/W4MGDM60/fPiwVq1a9ZCjevh69eqlf/75R76+vnkdigYNGqT3339ff//9d16HAiAXkPgDAJCL2rdvrxUrVig1NdWqfPny5apVq5aKFSuW4749PDzUs2dP9ezZUyNHjtTOnTtVsmRJLViwQCkpKfc8vl27dubxN7fq1avnOJ68cOXKlbwOIU9dvXo1r0N4ID755BPZ29urY8eOGeqcnZ3l7++vqVOnZjrrn1tSU1N1/fr1B9Z/VtjZ2cnJyUkWiyVP45Ckp59+WsnJyVqxYkVehwIgF5D4AwCQi3r06KE///xTGzduNMuuX7+ulStX6rnnnsvVsQoUKKD69evrypUruXIv7rp169S4cWO5uLjIzc1NTz75pI4cOWLV5tChQwoNDVXZsmXl5OSkYsWKqW/fvvrzzz/NNlOmTNGYMWMkSWXKlDFvKzh9+rROnz4ti8WS6Wz17fc7T5kyRRaLRUePHtVzzz0nLy8vPf7442b9J598olq1asnZ2VkFCxZU9+7dde7cuRyde2hoqFxdXXX27Fl16NBBrq6uKlGihN555x1JN2acW7RoIRcXF/n6+mZY1XHz9oFt27Zp0KBBKlSokNzd3RUSEqK//vorw3gLFy5U5cqV5ejoqOLFi2vIkCEZVkc0a9ZMjz32mPbt26cmTZqoQIECevnll+Xn56cjR45o69at5rVt1qyZJOnSpUsaPXq0qlSpIldXV7m7u6tdu3aKjo626jsyMlIWi0VffPGFXnvtNZUsWVJOTk5q2bKlTpw4kSHe3bt3q3379vLy8pKLi4uqVq2qt956y6rNsWPH9Mwzz6hgwYJycnJS7dq1tWbNmixd/9WrV6tevXpydXXNUJcvXz69+uqrOnTokL766qt79nXhwgX169dP3t7ecnJyUrVq1bR48WKrNje/h7Nnz9a8efNUrlw5OTo66ujRo+b37ueff1bPnj3l4eGhIkWKaOLEiTIMQ+fOnVOnTp3k7u6uYsWKac6cOVZ9X79+XZMmTVKtWrXk4eEhFxcXNW7cWFu2bLln7Lff438zlsy2W+/JT09P17x581S5cmU5OTnJ29tbgwYNyvDdMwxD06dPV8mSJVWgQAE1b948w7/xm4oWLaqqVavq66+/vmfcAP797PM6AAAAbImfn58aNGigTz/9VO3atZN0I6FOSEhQ9+7d9fbbb+fqeKdOnZKdnZ08PT3v2TYhIUF//PGHVVnhwoUlSUuXLlXv3r3Vpk0bvf7667p69areffddPf744zpw4ID8/PwkSRs3btSpU6fUp08fFStWTEeOHNEHH3ygI0eO6Mcff5TFYlGXLl30888/69NPP9XcuXPNMYoUKZKjHyi6du2qChUqaMaMGeaM72uvvaaJEyeqW7du6t+/vy5evKj58+erSZMmOnDgQJaux+3S0tLUrl07NWnSRG+88YaWLVumoUOHysXFRa+88oqCg4PVpUsXvffeewoJCVGDBg1UpkwZqz6GDh0qT09PTZkyRbGxsXr33Xd15swZM9GWbiRzYWFhatWqlZ5//nmzXVRUlHbu3CkHBwezvz///FPt2rVT9+7d1bNnT3l7e6tZs2Z68cUX5erqqldeeUWS5O3tLenG92H16tXq2rWrypQpo99//13vv/++mjZtqqNHj6p48eJW8c6aNUv58uXT6NGjlZCQoDfeeEPBwcHavXu32Wbjxo3q0KGDfHx8NHz4cBUrVkwxMTH65ptvNHz4cEnSkSNH1KhRI5UoUULjx4+Xi4uLvvjiCwUFBenLL79U586d73jdU1JSFBUVpeeff/6ObZ577jlNmzZNU6dOVefOne84I/7PP/+oWbNmOnHihIYOHaoyZcpoxYoVCg0N1eXLl814bwoPD9e1a9c0cOBAOTo6qmDBgmbds88+q8DAQM2aNUtr167V9OnTVbBgQb3//vtq0aKFXn/9dS1btkyjR49WnTp11KRJE0lSYmKiPvroI/Xo0UMDBgzQ33//rY8//lht2rTRnj17srXKpkuXLipfvrxV2b59+zRv3jwVLVrULBs0aJAiIiLUp08fDRs2THFxcVqwYIEOHDhg9Z2aNGmSpk+frvbt26t9+/bav3+/nnjiiTuudKhVq5ZWr16d5XgB/IsZAADgvoWHhxuSjKioKGPBggWGm5ubcfXqVcMwDKNr165G8+bNDcMwDF9fX+PJJ5+0OlaSMWTIkLv237RpU6NixYrGxYsXjYsXLxoxMTHGsGHDDElGx44dsxRbZpthGMbff/9teHp6GgMGDLA67vz584aHh4dV+c1zutWnn35qSDK2bdtmlv3f//2fIcmIi4uzahsXF2dIMsLDwzP0I8mYPHmyuT958mRDktGjRw+rdqdPnzbs7OyM1157zar88OHDhr29fYbyO12PqKgos6x3796GJGPGjBlm2V9//WU4OzsbFovF+Oyzz8zyY8eOZYj1Zp+1atUyrl+/bpa/8cYbhiTj66+/NgzDMC5cuGDkz5/feOKJJ4y0tDSz3YIFCwxJxqJFi8yypk2bGpKM9957L8M5VK5c2WjatGmG8mvXrln1axg3rrmjo6MxdepUs2zLli2GJCMwMNBITk42y9966y1DknH48GHDMAwjNTXVKFOmjOHr62v89ddfVv2mp6ebf7ds2dKoUqWKce3aNav6hg0bGhUqVMgQ561OnDhhSDLmz5+foa53796Gi4uLYRiGsXjxYkOSsWrVKrP+9n878+bNMyQZn3zyiVl2/fp1o0GDBoarq6uRmJhoXhNJhru7u3HhwgWrMW9+7wYOHGiWpaamGiVLljQsFosxa9Yss/zmd6R3795WbW+9pjfbeXt7G3379rUqv9P36PZ/NzddvHjRKF26tFGlShUjKSnJMAzD2L59uyHJWLZsmVXb9evXW5Xf/O49+eSTVp/dyy+/bEiyOoebZsyYYUgyfv/990zjAfDfwVJ/AAByWbdu3fTPP//om2++0d9//61vvvkmV5b5Hzt2TEWKFFGRIkUUGBio+fPn68knn9SiRYuydPw777yjjRs3Wm3SjRndy5cvq0ePHvrjjz/Mzc7OTvXq1bNaouzs7Gz+fe3aNf3xxx+qX7++JGn//v33fY6Zuf2Bb6tWrVJ6erq6detmFW+xYsVUoUKFLC2pvpP+/fubf3t6eiogIEAuLi7q1q2bWR4QECBPT0+dOnUqw/EDBw60mrF//vnnZW9vr2+//VaStGnTJl2/fl0jRoxQvnz///+GDRgwQO7u7lq7dq1Vf46OjurTp0+W43d0dDT7TUtL059//ilXV1cFBARk+vn06dNH+fPnN/cbN24sSea5HThwQHFxcRoxYkSGVRQ3Z90vXbqkzZs3q1u3bvr777/Nz+PPP/9UmzZtdPz4cf366693jPnmbSJeXl53Pbfg4GBVqFDhrvf6f/vttypWrJh69Ohhljk4OGjYsGFKSkrS1q1brdo//fTTKlKkSKZ93fpdsLOzU+3atWUYhvr162eW3/yO3PpdsLOzM69penq6Ll26pNTUVNWuXfu+/o2kpaWpR48e+vvvv/XVV1/JxcVFkrRixQp5eHiodevWVv8eatWqJVdXV/Pfw83v3osvvmi1YmLEiBF3HPPmZ3L7SiEA/z0s9QcAIJcVKVJErVq10vLly3X16lWlpaXpmWeeue9+/fz89OGHH8piscjJyUkVKlSwWu57L3Xr1lXt2rUzlB8/flyS1KJFi0yPc3d3N/++dOmSwsLC9Nlnn+nChQtW7RISErIcS3bcvpz++PHjMgxDFSpUyLT9rYl3djg5OWVIAj08PFSyZMkMS8s9PDwyvXf/9phcXV3l4+Nj3rN95swZSTd+PLhV/vz5VbZsWbP+phIlSlgl5veSnp6ut956SwsXLlRcXJzS0tLMukKFCmVoX7p0aav9m4nezXM7efKkJOmxxx6745gnTpyQYRiaOHGiJk6cmGmbCxcuqESJEneN/U7J/E12dnZ69dVX1bt3b61evTrT2wfOnDmjChUqWP2oIkmBgYFm/a1u/27d6vZr4+HhIScnJ/PWlVvLb33GhSQtXrxYc+bM0bFjx6wevHm38e7l1Vdf1ebNm7V27VqVK1fOLD9+/LgSEhLu+N+Cm/9Ob5777d/RIkWK3PFHl5ufyb/hYYMA7g+JPwAAD8Bzzz2nAQMG6Pz582rXrl2O7jm/nYuLi1q1anX/wd0mPT1d0o37/DN764C9/f//vwvdunXTDz/8oDFjxqh69epydXVVenq62rZta/ZzN3dKIG5NUG936yqDm/FaLBatW7dOdnZ2Gdpn9oC4rMisr7uV3ytRzQ23n/u9zJgxQxMnTlTfvn01bdo0FSxYUPny5dOIESMy/Xxy49xu9jt69Gi1adMm0za336d+q5s/SGT2Q8rtgoODzXv9g4KCshzjndzt+mZ2bbJyvT755BOFhoYqKChIY8aMUdGiRWVnZ6eZM2eaP6Rk1+rVq/X6669r2rRpatu2rVVdenq6ihYtqmXLlmV67J1WNGTFzc/k9h87APz3kPgDAPAAdO7cWYMGDdKPP/6ozz//PK/Duaubs4dFixa96w8Lf/31l77//nuFhYVp0qRJZvnNFQO3ulOCf3Nm8fYn2N8+E3uveA3DUJkyZeTv75/l4x6G48ePq3nz5uZ+UlKS4uPj1b59e0ky388eGxursmXLmu2uX7+uuLi4LP+wc6fru3LlSjVv3lwff/yxVfnly5dzlLzd/G789NNPd4zt5nk4ODjk6Iep0qVLy9nZWXFxcfdse3PWPzQ0NNOnzfv6+urQoUNKT0+3mvU/duyYWf+grVy5UmXLltWqVausPqfJkyfnqL+ff/5ZvXv3VlBQkF5++eUM9eXKldOmTZvUqFGju/6QcfPcjx8/bvXdu3jx4h1/dImLi1PhwoXv68cDAP8O3OMPAMAD4OrqqnfffVdTpkzJ9N3k/yZt2rSRu7u7ZsyYYbUs+aabT+K/Odt5+2zwvHnzMhxz8/7j2xN8d3d3FS5cWNu2bbMqX7hwYZbj7dKli+zs7BQWFpYhFsMwMiy7fpg++OADq2v47rvvKjU11XzDQ6tWrZQ/f369/fbbVrF//PHHSkhI0JNPPpmlcVxcXDJcW+nGZ3T7NVmxYsVd77G/m5o1a6pMmTKaN29ehvFujlO0aFE1a9ZM77//vuLj4zP0ca83OTg4OKh27drau3dvlmLq2bOnypcvr7CwsAx17du31/nz561+bEtNTdX8+fPl6uqqpk2bZmmM+5HZv5Pdu3dr165d2e4rKSlJnTt3VokSJbR48eJMf/Dp1q2b0tLSNG3atAx1qamp5ufWqlUrOTg4aP78+VaxZfbv96Z9+/apQYMG2Y4bwL8PM/4AADwgvXv3znLbvXv3avr06RnKmzVrZvXu+gfB3d1d7777rnr16qWaNWuqe/fuKlKkiM6ePau1a9eqUaNGWrBggdzd3c1X3aWkpKhEiRL67rvvMp2prVWrliTplVdeUffu3eXg4KCOHTvKxcVF/fv316xZs9S/f3/Vrl1b27Zt088//5zleMuVK6fp06drwoQJOn36tIKCguTm5qa4uDh99dVXGjhwoEaPHp1r1yc7rl+/rpYtW6pbt26KjY3VwoUL9fjjj+upp56SdGPZ9YQJExQWFqa2bdvqqaeeMtvVqVNHPXv2zNI4tWrV0rvvvqvp06erfPnyKlq0qFq0aKEOHTpo6tSp6tOnjxo2bKjDhw9r2bJlVjO82ZEvXz69++676tixo6pXr64+ffrIx8dHx44d05EjR7RhwwZJNx4c+fjjj6tKlSoaMGCAypYtq99//127du3SL7/8oujo6LuO06lTJ73yyitKTEy0eqZEZuzs7PTKK69k+tDDgQMH6v3331doaKj27dsnPz8/rVy5Ujt37tS8efPk5uaWo+uQHR06dNCqVavUuXNnPfnkk4qLi9N7772nSpUqKSkpKVt9hYWF6ejRo3r11VczrHAoV66cGjRooKZNm2rQoEGaOXOmDh48qCeeeEIODg46fvy4VqxYobfeekvPPPOMihQpotGjR2vmzJnq0KGD2rdvrwMHDmjdunWZrga5cOGCDh06pCFDhtzX9QDw70DiDwDAv8Du3but3p1+07Rp0x544i/deCZB8eLFNWvWLP3f//2fkpOTVaJECTVu3NgqwVq+fLlefPFFvfPOOzIMQ0888YTWrVuX4f3wderU0bRp0/Tee+9p/fr1Sk9PV1xcnFxcXDRp0iRdvHhRK1eu1BdffKF27dpp3bp12XpQ4fjx4+Xv76+5c+eaM7+lSpXSE088YSbZeWHBggVatmyZJk2apJSUFPXo0UNvv/221UztlClTVKRIES1YsEAjR45UwYIFNXDgQM2YMSPLDyacNGmSzpw5ozfeeEN///23mjZtqhYtWujll1/WlStXtHz5cn3++eeqWbOm1q5dq/Hjx+f4nNq0aaMtW7YoLCxMc+bMUXp6usqVK6cBAwaYbSpVqqS9e/cqLCxMERER+vPPP1W0aFHVqFHD6raQO+nVq5fGjx+vNWvWZOnHj549e2r69OkZ7pl3dnZWZGSkxo8fr8WLFysxMVEBAQEKDw9XaGhots89J0JDQ3X+/Hm9//772rBhgypVqqRPPvlEK1asUGRkZLb6urlaIrMfBXv37m3Oxr/33nuqVauW3n//fb388suyt7eXn5+fevbsqUaNGpnHTJ8+XU5OTnrvvfe0ZcsW1atXT999912mK01WrVolR0dHqzdaAPjvshgP48k0AAAANiwiIkJ9+vRRVFRUpm9OwL3169dPP//8s7Zv357XoUBSjRo11KxZM82dOzevQwGQC5jxBwAAQJ6bPHmy/P39tXPnTqtZajx869ev1/Hjx81bOQD895H4AwAAIM+VLl1a165dy+swIKlt27bZfh4BgH83nuoPAAAAAIAN4x5/AAAAAABsGDP+AAAAAADYMBJ/AAAAAABsGA/3A/5D0tPT9dtvv8nNzc3qndAAAAAAHi2GYejvv/9W8eLFlS/f3ef0SfyB/5DffvtNpUqVyuswAAAAAPxLnDt3TiVLlrxrGxJ/4D/Ezc1N0o1/3O7u7nkcDQAAAIC8kpiYqFKlSpk5wt2Q+AP/ITeX97u7u5P4AwAAAMjSLcA83A8AAAAAABtG4g8AAAAAgA0j8QcAAAAAwIaR+AMAAAAAYMNI/AEAAAAAsGEk/gAAAAAA2DASfwAAAAAAbBiJPwAAAAAANozEHwAAAAAAG0biDwAAAACADSPxBwAAAADAhpH4AwAAAABgw0j8AQAAAACwYST+AAAAAADYMBJ/AAAAAABsGIk/AAAAAAA2jMQfAAAAAAAbRuIPAAAAAIANs8/rAAD8t7V44az59+aFpfMwEgAAAACZYcYfAAAAAAAbRuIPAAAAAIANI/EHAAAAAMCGkfgDAAAAAGDDSPwBAAAAALBhJP4AAAAAANgwEn8AAAAAAGwYiT8AAAAAADaMxB8AAAAAABtG4g8AAAAAgA0j8QeyaP369Xr88cfl6empQoUKqUOHDjp58qRZ/8MPP6h69epycnJS7dq1tXr1alksFh08eNBs89NPP6ldu3ZydXWVt7e3evXqpT/++CMPzgYAAADAo4LEH8iiK1euaNSoUdq7d6++//575cuXT507d1Z6eroSExPVsWNHValSRfv379e0adM0btw4q+MvX76sFi1aqEaNGtq7d6/Wr1+v33//Xd26dbvjmMnJyUpMTLTaAAAAACA77PM6AOC/4umnn7baX7RokYoUKaKjR49qx44dslgs+vDDD+Xk5KRKlSrp119/1YABA8z2CxYsUI0aNTRjxgyrPkqVKqWff/5Z/v7+GcacOXOmwsLCHtxJAQAAALB5zPgDWXT8+HH16NFDZcuWlbu7u/z8/CRJZ8+eVWxsrKpWrSonJyezfd26da2Oj46O1pYtW+Tq6mpuFStWlCSrWwZuNWHCBCUkJJjbuXPnHszJAQAAALBZzPgDWdSxY0f5+vrqww8/VPHixZWenq7HHntM169fz9LxSUlJ6tixo15//fUMdT4+Ppke4+joKEdHx/uKGwAAAMCjjcQfyII///xTsbGx+vDDD9W4cWNJ0o4dO8z6gIAAffLJJ0pOTjYT9aioKKs+atasqS+//FJ+fn6yt+efHgAAAICHg6X+QBZ4eXmpUKFC+uCDD3TixAlt3rxZo0aNMuufe+45paena+DAgYqJidGGDRs0e/ZsSZLFYpEkDRkyRJcuXVKPHj0UFRWlkydPasOGDerTp4/S0tLy5LwAAAAA2D4SfyAL8uXLp88++0z79u3TY489ppEjR+r//u//zHp3d3f973//08GDB1W9enW98sormjRpkiSZ9/0XL15cO3fuVFpamp544glVqVJFI0aMkKenp/Ll458iAAAAgAeD9cZAFrVq1UpHjx61KjMMw/y7YcOGio6ONveXLVsmBwcHlS5d2iyrUKGCVq1a9eCDBQAAAID/h8QfyCVLlixR2bJlVaJECUVHR2vcuHHq1q2bnJ2d8zo0AAAAAI8wEn8gl5w/f16TJk3S+fPn5ePjo65du+q1117L67AAAAAAPOJI/IFcMnbsWI0dOzavwwAAAAAAKzxRDAAAAAAAG0biDwAAAACADSPxBwAAAADAhnGPP4D7snlh6Xs3AgAAAJBnmPEHAAAAAMCGkfgDAAAAAGDDSPwBAAAAALBhJP4AAAAAANgwEn8AAAAAAGwYiT8AAAAAADaMxB8AAAAAABtG4g8AAAAAgA0j8QcAAAAAwIaR+AMAAAAAYMNI/AEAAAAAsGEk/gAAAAAA2DASfwAAAAAAbBiJPwAAAAAANozEH/fUrFkzjRgxIq/D+FfhmgAAAAD4r7DP6wCA+xEaGqrLly9r9erVD3XcVatWycHB4aGOCQAAAAA5QeKP/6S0tDRZLJY8G79gwYJ5NjYAAAAAZAdL/ZEl6enpGjt2rAoWLKhixYppypQpkqS+ffuqQ4cOVm1TUlJUtGhRffzxx5JuLIsfOnSohg4dKg8PDxUuXFgTJ06UYRjmMcnJyRo9erRKlCghFxcX1atXT5GRkWZ9RESEPD09tWbNGlWqVEmOjo7q27evFi9erK+//loWi0UWi8U85ty5c+rWrZs8PT1VsGBBderUSadPnzb7Cw0NVVBQkGbPni0fHx8VKlRIQ4YMUUpKitlm4cKFqlChgpycnOTt7a1nnnnGrLt9qf9ff/2lkJAQeXl5qUCBAmrXrp2OHz+eIf4NGzYoMDBQrq6uatu2reLj43P6kQAAAABAlpD4I0sWL14sFxcX7d69W2+88YamTp2qjRs3qn///lq/fr1VAvvNN9/o6tWrevbZZ62Ot7e31549e/TWW2/pzTff1EcffWTWDx06VLt27dJnn32mQ4cOqWvXrmrbtq1V8nz16lW9/vrr+uijj3TkyBG9/fbb6tatm5lAx8fHq2HDhkpJSVGbNm3k5uam7du3a+fOnWaiff36dbO/LVu26OTJk9qyZYsWL16siIgIRURESJL27t2rYcOGaerUqYqNjdX69evVpEmTO16f0NBQ7d27V2vWrNGuXbtkGIbat29v9UPC1atXNXv2bC1dulTbtm3T2bNnNXr06Lte9+TkZCUmJlptAAAAAJAtBnAPTZs2NR5//HGrsjp16hjjxo0zDMMwKlWqZLz++utmXceOHY3Q0FCr4wMDA4309HSzbNy4cUZgYKBhGIZx5swZw87Ozvj111+txmjZsqUxYcIEwzAMIzw83JBkHDx40KpN7969jU6dOlmVLV261AgICLAaLzk52XB2djY2bNhgHufr62ukpqaabbp27Wo8++yzhmEYxpdffmm4u7sbiYmJd7wmw4cPNwzDMH7++WdDkrFz506z/o8//jCcnZ2NL774wir+EydOmG3eeecdw9vbO9P+b5o8ebIhKcOWkJBw1+MAAAAA2LaEhIQs5wbM+CNLqlatarXv4+OjCxcuSJL69++v8PBwSdLvv/+udevWqW/fvlbt69evb3VPfoMGDXT8+HGlpaXp8OHDSktLk7+/v1xdXc1t69atOnnypHlM/vz5M8SRmejoaJ04cUJubm5mXwULFtS1a9es+qtcubLs7OwyPafWrVvL19dXZcuWVa9evbRs2TJdvXo10/FiYmJkb2+vevXqmWWFChVSQECAYmJizLICBQqoXLlymY53JxMmTFBCQoK5nTt37p7nDwAAAAC34uF+yJLbn2BvsViUnp4uSQoJCdH48eO1a9cu/fDDDypTpowaN26c5b6TkpJkZ2enffv2WSXikuTq6mr+7ezsnKUH+iUlJalWrVpatmxZhroiRYpk6Zzc3Ny0f/9+RUZG6rvvvtOkSZM0ZcoURUVFydPTM8vndqvMxjNuec5BZhwdHeXo6Jij8QAAAABAIvFHLihUqJCCgoIUHh6uXbt2qU+fPhna7N6922r/xx9/VIUKFWRnZ6caNWooLS1NFy5cyNYPBtKNVQBpaWlWZTVr1tTnn3+uokWLyt3dPfsn9P/Y29urVatWatWqlSZPnixPT09t3rxZXbp0sWoXGBio1NRU7d69Ww0bNpQk/fnnn4qNjVWlSpVyPD4AAAAA5AaW+iNX9O/fX4sXL1ZMTIx69+6dof7s2bMaNWqUYmNj9emnn2r+/PkaPny4JMnf31/BwcEKCQnRqlWrFBcXpz179mjmzJlau3btXcf18/PToUOHFBsbqz/++EMpKSkKDg5W4cKF1alTJ23fvl1xcXGKjIzUsGHD9Msvv2TpfL755hu9/fbbOnjwoM6cOaMlS5YoPT1dAQEBGdpWqFBBnTp10oABA7Rjxw5FR0erZ8+eKlGihDp16pSl8QAAAADgQSHxR65o1aqVfHx81KZNGxUvXjxDfUhIiP755x/VrVtXQ4YM0fDhwzVw4ECzPjw8XCEhIXrppZcUEBCgoKAgRUVFqXTp0ncdd8CAAQoICFDt2rVVpEgR7dy5UwUKFNC2bdtUunRpdenSRYGBgerXr5+uXbuW5RUAnp6eWrVqlVq0aKHAwEC99957+vTTT1W5cuVM24eHh6tWrVrq0KGDGjRoIMMw9O2332ZY3g8AAAAAD5vFuNdNxkAWJCUlqUSJEgoPD8+wFL5Zs2aqXr265s2blzfB2ZDExER5eHgoISHhvm5jAAAAAPDflp3cgHv8cV/S09P1xx9/aM6cOfL09NRTTz2V1yEBAAAAAG5B4o/7cvbsWZUpU0YlS5ZURESE7O35SgEAAADAvwlZGu6Ln5/fPV9JFxkZ+XCCAQAAAABkwMP9AAAAAACwYST+AAAAAADYMBJ/AAAAAABsGIk/AAAAAAA2jMQfAAAAAAAbRuIPAAAAAIANI/EHAAAAAMCGkfgDAAAAAGDDSPwBAAAAALBhJP4A7kuLF87mdQgAAAAA7oLEHwAAAAAAG0biDwAAAACADSPxBwAAAADAhpH4AwAAAABgw0j8AQAAAACwYST+AAAAAADYMBJ/AAAAAABsGIk/AAAAAAA2jMQfVpo1a6YRI0bkdRg5FhoaqqCgIHP/v34+AAAAAHC/7PM6AECSTp8+rTJlyujAgQOqXr16rvW7atUqOTg45Fp/AAAAAPBfQ+KPPHf9+vUH1nfBggUfWN8AAAAA8F/AUn9kkJ6errFjx6pgwYIqVqyYpkyZYtZdvnxZ/fv3V5EiReTu7q4WLVooOjrarD958qQ6deokb29vubq6qk6dOtq0aZNV/35+fpo2bZpCQkLk7u6ugQMHqkyZMpKkGjVqyGKxqFmzZveMMy0tTaNGjZKnp6cKFSqksWPHyjAMqza3L/VfuHChKlSoICcnJ3l7e+uZZ56xOu+ZM2eqTJkycnZ2VrVq1bRy5Uqr8fr162fWBwQE6K233rIaLzIyUnXr1pWLi4s8PT3VqFEjnTlzxqz/+uuvVbNmTTk5Oals2bIKCwtTamrqHc8xOTlZiYmJVhsAAAAAZAeJPzJYvHixXFxctHv3br3xxhuaOnWqNm7cKEnq2rWrLly4oHXr1mnfvn2qWbOmWrZsqUuXLkmSkpKS1L59e33//fc6cOCA2rZtq44dO+rs2bNWY8yePVvVqlXTgQMHNHHiRO3Zs0eStGnTJsXHx2vVqlX3jHPOnDmKiIjQokWLtGPHDl26dElfffXVHdvv3btXw4YN09SpUxUbG6v169erSZMmZv3MmTO1ZMkSvffeezpy5IhGjhypnj17auvWrZJu/DBQsmRJrVixQkePHtWkSZP08ssv64svvpAkpaamKigoSE2bNtWhQ4e0a9cuDRw4UBaLRZK0fft2hYSEaPjw4Tp69Kjef/99RURE6LXXXrtjzDNnzpSHh4e5lSpV6p7XBQAAAABuZTFunyLFI61Zs2ZKS0vT9u3bzbK6deuqRYsW6tChg5588klduHBBjo6OZn358uU1duxYDRw4MNM+H3vsMQ0ePFhDhw6VdGPGv0aNGlZJek7u8S9evLhGjhypMWPGSLqReJcpU0a1atXS6tWrzfOpXr265s2bp1WrVqlPnz765Zdf5ObmZtVXcnKyChYsqE2bNqlBgwZmef/+/XX16lUtX7480xiGDh2q8+fPa+XKlbp06ZIKFSqkyMhINW3aNEPbVq1aqWXLlpowYYJZ9sknn2js2LH67bffMu0/OTlZycnJ5n5iYqJKlSqlhIQEubu7Z+k6PWgtXjirzQtL53UYAAAAwCMlMTFRHh4eWcoNuMcfGVStWtVq38fHRxcuXFB0dLSSkpJUqFAhq/p//vlHJ0+elHRjxn/KlClau3at4uPjlZqaqn/++SfDjH/t2rXvK8aEhATFx8erXr16Zpm9vb1q166dYbn/Ta1bt5avr6/Kli2rtm3bqm3bturcubMKFCigEydO6OrVq2rdurXVMdevX1eNGjXM/XfeeUeLFi3S2bNn9c8//+j69evmDxUFCxZUaGio2rRpo9atW6tVq1bq1q2bfHx8JEnR0dHauXOn1Qx/Wlqarl27pqtXr6pAgQIZYnZ0dLT6kQUAAAAAsovEHxnc/hR8i8Wi9PR0JSUlycfHR5GRkRmO8fT0lCSNHj1aGzdu1OzZs1W+fHk5OzvrmWeeyfAAPxcXlwcV/h25ublp//79ioyM1HfffadJkyZpypQpioqKUlJSkiRp7dq1KlGihNVxNxPvzz77TKNHj9acOXPUoEEDubm56f/+7/+0e/dus214eLiGDRum9evX6/PPP9err76qjRs3qn79+kpKSlJYWJi6dOmSITYnJ6cHeOYAAAAAHmUk/siymjVr6vz587K3t5efn1+mbXbu3KnQ0FB17txZ0o0VAKdPn75n3/nz55d0YwY8Kzw8POTj46Pdu3eb9+mnpqaazx24E3t7e7Vq1UqtWrXS5MmT5enpqc2bN6t169ZydHTU2bNnM12mf/PcGjZsqBdeeMEsu7nS4VY1atRQjRo1NGHCBDVo0EDLly9X/fr1VbNmTcXGxqp8+fJZOkcAAAAAyA0k/siyVq1aqUGDBgoKCtIbb7whf39//fbbb1q7dq06d+6s2rVrq0KFClq1apU6duwoi8WiiRMnKj09/Z59Fy1aVM7Ozlq/fr1KliwpJycneXh43PWY4cOHa9asWapQoYIqVqyoN998U5cvX75j+2+++UanTp1SkyZN5OXlpW+//Vbp6ekKCAiQm5ubRo8erZEjRyo9PV2PP/64EhIStHPnTrm7u6t3796qUKGClixZog0bNqhMmTJaunSpoqKizDcSxMXF6YMPPtBTTz2l4sWLKzY2VsePH1dISIgkadKkSerQoYNKly6tZ555Rvny5VN0dLR++uknTZ8+PesfBAAAAABkA0/1R5ZZLBZ9++23atKkifr06SN/f391795dZ86ckbe3tyTpzTfflJeXlxo2bKiOHTuqTZs2d52Bv8ne3l5vv/223n//fRUvXlydOnW65zEvvfSSevXqpd69e5tL72+uNMiMp6enVq1apRYtWigwMFDvvfeePv30U1WuXFmSNG3aNE2cOFEzZ85UYGCg2rZtq7Vr15qJ/aBBg9SlSxc9++yzqlevnv7880+r2f8CBQro2LFjevrpp+Xv76+BAwdqyJAhGjRokCSpTZs2+uabb/Tdd9+pTp06ql+/vubOnStfX997nisAAAAA5BRP9Qf+Q7Lz5M6Hhaf6AwAAAA9fdnIDZvwBAAAAALBhJP7413J1db3jtn379rwODwAAAAD+E3i4H/61Dh48eMe621+5BwAAAADIHIk//rV47R0AAAAA3D+W+gMAAAAAYMNI/AEAAAAAsGEk/gDuC6/yAwAAAP7dSPwBAAAAALBhJP4AAAAAANgwEn8AAAAAAGwYiT8AAAAAADaMxB8AAAAAABtmn9cBAPhva/HCWfNvnvAPAAAA/Psw4w8AAAAAgA0j8QcAAAAAwIaR+AMAAAAAYMNI/AEAAAAAsGEk/gAAAAAA2DASfwAAAAAAbBiJPwAAAAAANozEH8iCZs2aacSIEbnaZ0REhDw9PXO1TwAAAAC4HYk/kEeeffZZ/fzzz3kdBgAAAAAbZ5/XAQCPKmdnZzk7O+d1GAAAAABsHDP+QBalpqZq6NCh8vDwUOHChTVx4kQZhiFJ8vPz0/Tp0xUSEiJXV1f5+vpqzZo1unjxojp16iRXV1dVrVpVe/fuNftjqT8AAACAh4HEH8iixYsXy97eXnv27NFbb72lN998Ux999JFZP3fuXDVq1EgHDhzQk08+qV69eikkJEQ9e/bU/v37Va5cOYWEhJg/FmRFcnKyEhMTrTYAAAAAyA4SfyCLSpUqpblz5yogIEDBwcF68cUXNXfuXLO+ffv2GjRokCpUqKBJkyYpMTFRderUUdeuXeXv769x48YpJiZGv//+e5bHnDlzpjw8PMytVKlSD+LUAAAAANgwEn8gi+rXry+LxWLuN2jQQMePH1daWpokqWrVqmadt7e3JKlKlSoZyi5cuJDlMSdMmKCEhARzO3fu3H2dAwAAAIBHDw/3A3KJg4OD+ffNHwgyK0tPT89yn46OjnJ0dMylCAEAAAA8ipjxB7Jo9+7dVvs//vijKlSoIDs7uzyKCAAAAADujcQfyKKzZ89q1KhRio2N1aeffqr58+dr+PDheR0WAAAAANwVS/2BLAoJCdE///yjunXrys7OTsOHD9fAgQPzOiwAAAAAuCuLkZ13iwHIU4mJifLw8FBCQoLc3d3zOhxJUosXzpp/b15YOg8jAQAAAB4d2ckNWOoPAAAAAIANI/EHAAAAAMCGkfgDAAAAAGDDSPwBAAAAALBhJP4AAAAAANgwEn8AAAAAAGyYfV4HAOC/jVf4AQAAAP9uzPgDAAAAAGDDSPwBAAAAALBhJP4AAAAAANgwEn8AAAAAAGwYiT8AAAAAADaMxB8AAAAAABvG6/wA3JcWL5zN6xAAAACAh+a/+DprZvwBAAAAALBhJP4AAAAAANgwEn8AAAAAAGwYiT8AAAAAADaMxB8AAAAAABtG4g8AAAAAgA0j8QcAAAAAwIaR+OOBioiIkKenZ16HcU9TpkxR9erVH+qY/5VrAwAAAOC/jcQfORYaGqqgoKAM5ZGRkbJYLLp8+XK2+vvnn39UsGBBFS5cWMnJyRnqP/jgAzVr1kzu7u456h8AAAAAHkUk/vjX+PLLL1W5cmVVrFhRq1evzlB/9epVtW3bVi+//PLDDw4AAAAA/qNI/PFQrF69WhUqVJCTk5PatGmjc+fOZWjz8ccfq2fPnurZs6c+/vjjDPUjRozQ+PHjVb9+/TuO88svv6hHjx4qWLCgXFxcVLt2be3evTtHMX/00UcKDAyUk5OTKlasqIULF5p1DRs21Lhx46zaX7x4UQ4ODtq2bZskKTk5WaNHj1aJEiXk4uKievXqKTIyMkexAAAAAEBOkfjjgbt69apee+01LVmyRDt37tTly5fVvXt3qzYnT57Url271K1bN3Xr1k3bt2/XmTNnsjVOUlKSmjZtql9//VVr1qxRdHS0xo4dq/T09GzHvGzZMk2aNEmvvfaaYmJiNGPGDE2cOFGLFy+WJAUHB+uzzz6TYRjmMZ9//rmKFy+uxo0bS5KGDh2qXbt26bPPPtOhQ4fUtWtXtW3bVsePH89yHMnJyUpMTLTaAAAAACA77PM6APy3ffPNN3J1dbUqS0tLs9pPSUnRggULVK9ePUnS4sWLFRgYqD179qhu3bqSpEWLFqldu3by8vKSJLVp00bh4eGaMmVKlmNZvny5Ll68qKioKBUsWFCSVL58+Ryd1+TJkzVnzhx16dJFklSmTBkdPXpU77//vnr37q1u3bppxIgR2rFjh5noL1++XD169JDFYtHZs2cVHh6us2fPqnjx4pKk0aNHa/369QoPD9eMGTOyFMfMmTMVFhaWo3MAAAAAAIkZf9yn5s2b6+DBg1bbRx99ZNXG3t5ederUMfcrVqwoT09PxcTESLrxQ8HixYvVs2dPs03Pnj0VERGRrdn6gwcPqkaNGmbSn1NXrlzRyZMn1a9fP7m6uprb9OnTdfLkSUlSkSJF9MQTT2jZsmWSpLi4OO3atUvBwcGSpMOHDystLU3+/v5WfWzdutXsIysmTJighIQEc8vsFgkAAAAAuBtm/HFfXFxcMsyq//LLL9nqY8OGDfr111/17LPPWpWnpaXp+++/V+vWrbPUj7Ozc7bGvZOkpCRJ0ocffmiuUrjJzs7O/Ds4OFjDhg3T/PnztXz5clWpUkVVqlQx+7Czs9O+ffusjpGUYYXE3Tg6OsrR0TGnpwIAAAAAzPjjwUtNTdXevXvN/djYWF2+fFmBgYGSbjzUr3v37hlWDnTv3j3Th/zdSdWqVXXw4EFdunTpvuL19vZW8eLFderUKZUvX95qK1OmjNmuU6dOunbtmtavX6/ly5ebs/2SVKNGDaWlpenChQsZ+ihWrNh9xQcAAAAA2cGMPx44BwcHvfjii3r77bdlb2+voUOHqn79+qpbt64uXryo//3vf1qzZo0ee+wxq+NCQkLUuXNnXbp0SQULFtT58+d1/vx5nThxQtKN5fRubm4qXbq0ChYsqB49emjGjBkKCgrSzJkz5ePjowMHDqh48eJq0KBBtmIOCwvTsGHD5OHhobZt2yo5OVl79+7VX3/9pVGjRkm6sdohKChIEydOVExMjHr06GEe7+/vr+DgYIWEhGjOnDmqUaOGLl68qO+//15Vq1bVk08+eZ9XFQAAAACyhhl/PHAFChTQuHHj9Nxzz6lRo0ZydXXV559/LklasmSJXFxc1LJlywzHtWzZUs7Ozvrkk08kSe+9955q1KihAQMGSJKaNGmiGjVqaM2aNZKk/Pnz67vvvlPRokXVvn17ValSRbNmzcqw1D4r+vfvr48++kjh4eGqUqWKmjZtqoiICKsZf+nGcv/o6Gg1btxYpUuXtqoLDw9XSEiIXnrpJQUEBCgoKEhRUVEZ2gEAAADAg2Qxbn0fGYB/tcTERHl4eCghIUHu7u55HY4kqcULZ/M6BAAAAOCh2bzw3zGRl53cgBl/AAAAAABsGIk/HgmVK1e2eq3erdvNV/IBAAAAgC3i4X54JHz77bdKSUnJtM7b2/shRwMAAAAADw+JPx4Jvr6+eR0CAAAAAOQJlvoDAAAAAGDDSPwBAAAAALBhLPUHcF/+La8zAQAAAJA5ZvwBAAAAALBhJP4AAAAAANgwEn8AAAAAAGwYiT8AAAAAADaMxB8AAAAAABtG4g8AAAAAgA3jdX4A7kuLF87mdQgAAOAOeO0uAIkZfwAAAAAAbBqJPwAAAAAANozEHwAAAAAAG0biDwAAAACADSPxBwAAAADAhpH4AwAAAABgw0j8AQAAAACwYST+D0mzZs00YsSIO9b7+flp3rx5DzwOi8Wi1atX50pfkZGRslgsunz5cq70l9ce1mdwq3t9LwAAAADgfpH4AwAAAABgw0j8AQAAAACwYST+D1FqaqqGDh0qDw8PFS5cWBMnTpRhGJm2PXv2rDp16iRXV1e5u7urW7du+v33363avPvuuypXrpzy58+vgIAALV261Kr++PHjatKkiZycnFSpUiVt3LgxW/H+8MMPql69upycnFS7dm2tXr1aFotFBw8ezLT9lClTVL16dauyefPmyc/Pz6ps0aJFqly5shwdHeXj46OhQ4dm+byjo6PVvHlzubm5yd3dXbVq1dLevXvN+h07dqhx48ZydnZWqVKlNGzYMF25ciVb533T5cuX1b9/fxUpUkTu7u5q0aKFoqOjJUk///yzLBaLjh07ZnXM3LlzVa5cOXP/p59+Urt27eTq6ipvb2/16tVLf/zxR47iAQAAAICcIPF/iBYvXix7e3vt2bNHb731lt5880199NFHGdqlp6erU6dOunTpkrZu3aqNGzfq1KlTevbZZ802X331lYYPH66XXnpJP/30kwYNGqQ+ffpoy5YtZh9dunRR/vz5tXv3br333nsaN25clmNNTExUx44dVaVKFe3fv1/Tpk3L1vF38u6772rIkCEaOHCgDh8+rDVr1qh8+fJZPu/g4GCVLFlSUVFR2rdvn8aPHy8HBwdJ0smTJ9W2bVs9/fTTOnTokD7//HPt2LHD6oeF7OjatasuXLigdevWad++fapZs6ZatmypS5cuyd/fX7Vr19ayZcusjlm2bJmee+45STd+OGjRooVq1KihvXv3av369fr999/VrVu3LMeQnJysxMREqw0AAAAAssM+rwN4lJQqVUpz586VxWJRQECADh8+rLlz52rAgAFW7b7//nsdPnxYcXFxKlWqlCRpyZIlqly5sqKiolSnTh3Nnj1boaGheuGFFyRJo0aN0o8//qjZs2erefPm2rRpk44dO6YNGzaoePHikqQZM2aoXbt2WYp1+fLlslgs+vDDD80VA7/++muGWLNr+vTpeumllzR8+HCzrE6dOlk+77Nnz2rMmDGqWLGiJKlChQpmPzNnzlRwcLD5sLwKFSro7bffVtOmTfXuu+/Kyckpy3Hu2LFDe/bs0YULF+To6ChJmj17tlavXq2VK1dq4MCBCg4O1oIFCzRt2jRJN1YB7Nu3T5988okkacGCBapRo4ZmzJhh9rto0SKVKlVKP//8s/z9/e8Zx8yZMxUWFpbluAEAAADgdsz4P0T169eXxWIx9xs0aKDjx48rLS3Nql1MTIxKlSplJr+SVKlSJXl6eiomJsZs06hRI6vjGjVqZFVfqlQpM+m/OV5WxcbGqmrVqlbJct26dbN8fGYuXLig3377TS1btsy0PivnPWrUKPXv31+tWrXSrFmzdPLkSbNtdHS0IiIi5Orqam5t2rRRenq64uLishVrdHS0kpKSVKhQIav+4uLizDG7d++u06dP68cff5R0Y7a/Zs2a5o8S0dHR2rJli9XxN+tujftuJkyYoISEBHM7d+5cts4DAAAAAJjxR67Jly9fhmcWpKSkmH87Ozvf9xhTpkzRc889p7Vr12rdunWaPHmyPvvsM3Xu3FlJSUkaNGiQhg0bluG40qVLZ2ucpKQk+fj4KDIyMkOdp6enJKlYsWJq0aKFli9frvr162v58uV6/vnnrfro2LGjXn/99Qx9+Pj4ZCkOR0dHc8UBAAAAAOQEM/4P0e7du632f/zxR1WoUEF2dnZW5YGBgTp37pzV7O7Ro0d1+fJlVapUyWyzc+dOq+N27txpVX/u3DnFx8dbjZdVN29FSE5ONsuioqLuekyRIkV0/vx5q+T/1gcBurm5yc/PT99//32mx2flvCXJ399fI0eO1HfffacuXbooPDxcklSzZk0dPXpU5cuXz7Dlz58/y+d+s6/z58/L3t4+Q1+FCxc22wUHB+vzzz/Xrl27dOrUKXXv3t2qjyNHjsjPzy9DHy4uLtmKBwAAAAByisT/ITp79qxGjRql2NhYffrpp5o/f77Vve43tWrVSlWqVFFwcLD279+vPXv2KCQkRE2bNlXt2rUlSWPGjFFERITeffddHT9+XG+++aZWrVql0aNHm334+/urd+/eio6O1vbt2/XKK69kOdbnnntO6enpGjhwoGJiYrRhwwbNnj1bkqxuV7hVs2bNdPHiRb3xxhs6efKk3nnnHa1bt86qzZQpUzRnzhy9/fbbOn78uPbv36/58+dn6bz/+ecfDR06VJGRkTpz5ox27typqKgoBQYGSpLGjRunH374QUOHDtXBgwd1/Phxff311zl6uF+rVq3UoEEDBQUF6bvvvtPp06f1ww8/6JVXXrF6i0CXLl30999/6/nnn1fz5s2tbq0YMmSILl26pB49eigqKkonT57Uhg0b1KdPnwy3dwAAAADAg0Li/xCFhITon3/+Ud26dTVkyBANHz5cAwcOzNDOYrHo66+/lpeXl5o0aaJWrVqpbNmy+vzzz802QUFBeuuttzR79mxVrlxZ77//vsLDw9WsWTNJN5bdf/XVV+Z4/fv312uvvZblWN3d3fW///1PBw8eVPXq1fXKK69o0qRJknTHh+QFBgZq4cKFeuedd1StWjXt2bPH/CHipt69e2vevHlauHChKleurA4dOuj48eNZOm87Ozv9+eefCgkJkb+/v7p166Z27dqZD7+rWrWqtm7dqp9//lmNGzdWjRo1NGnSJKtkPKssFou+/fZbNWnSRH369JG/v7+6d++uM2fOyNvb22zn5uamjh07Kjo6WsHBwVZ9FC9eXDt37lRaWpqeeOIJValSRSNGjJCnp6fy5eOfHgAAAICHw2Lc6UXywG2WLVumPn36KCEhIVfu10f2JSYmysPDQwkJCXJ3d8/rcCRJLV44m9chAACAO9i8MHvPOQLw35Gd3ICH++GOlixZorJly6pEiRKKjo7WuHHj1K1bN5J+AAAAAPgPYb3xI2rGjBlWr5m7dWvXrp0k6fz58+rZs6cCAwM1cuRIde3aVR988EEeR54z27dvv+P5urq65nV4AAAAAPDAsNT/EXXp0iVdunQp0zpnZ2eVKFHiIUf0YP3zzz/69ddf71hfvnz5hxhNzrHUHwAAZAdL/QHbxVJ/3FPBggVVsGDBvA7joXF2dv7PJPcAAAAAkJtyvNR/6dKlatSokYoXL64zZ85IkubNm6evv/4614IDAAAAAAD3J0eJ/7vvvqtRo0apffv2unz5svlOck9PT82bNy834wMAAAAAAPchR/f4V6pUSTNmzFBQUJDc3NwUHR2tsmXL6qefflKzZs30xx9/PIhYgUfev/EefwAAAAAPX3ZygxzN+MfFxalGjRoZyh0dHXXlypWcdAkAAAAAAB6AHCX+ZcqU0cGDBzOUr1+/XoGBgfcbEwAAAAAAyCU5eqr/qFGjNGTIEF27dk2GYWjPnj369NNPNXPmTH300Ue5HSMAAAAAAMihHCX+/fv3l7Ozs1599VVdvXpVzz33nIoXL6633npL3bt3z+0YAQAAAABADmU78U9NTdXy5cvVpk0bBQcH6+rVq0pKSlLRokUfRHwAAAAAAOA+ZPsef3t7ew0ePFjXrl2TJBUoUICkHwAAAACAf6kcPdyvbt26OnDgQG7HAgAAAAAAclmO7vF/4YUX9NJLL+mXX35RrVq15OLiYlVftWrVXAkOAAAAAADcH4thGEZ2D8qXL+NCAYvFIsMwZLFYlJaWlivBAbCWmJgoDw8PJSQkyN3dPa/DAQAAAJBHspMb5GjGPy4uLkeBAQAAAACAhytHib+vr29uxwEAAAAAAB6AHCX+S5YsuWt9SEhIjoIBAAAAAAC5K0f3+Ht5eVntp6Sk6OrVq8qfP78KFCigS5cu5VqAAP5/3OMPAAAAQMpebpCj1/n99ddfVltSUpJiY2P1+OOP69NPP81R0AAAAAAAIPflKPHPTIUKFTRr1iwNHz48t7oETKGhoQoKCsrrMLLNz89P8+bNy+swAAAAADzCci3xlyR7e3v99ttvudkl8NBERETIYrEoMDAwQ92KFStksVjk5+f38AMDAAAAgPuQo4f7rVmzxmrfMAzFx8drwYIFatSoUa4EBuQmwzCUlpYme/u7f+VdXFx04cIF7dq1Sw0aNDDLP/74Y5UuXfpBhwkAAAAAuS5HM/5BQUFWW5cuXTRlyhRVrVpVixYtyu0Y8S/y999/Kzg4WC4uLvLx8dHcuXPVrFkzjRgxQpKUnJys0aNHq0SJEnJxcVG9evUUGRlpHh8RESFPT09t2LBBgYGBcnV1Vdu2bRUfH2+2SUtL06hRo+Tp6alChQpp7Nixuv0ZlOnp6Zo5c6bKlCkjZ2dnVatWTStXrjTrIyMjZbFYtG7dOtWqVUuOjo7asWPHPc/P3t5ezz33nNX3+JdfflFkZKSee+45q7YnT55Up06d5O3tLVdXV9WpU0ebNm26a/+XL19W//79VaRIEbm7u6tFixaKjo6+Z1wAAAAAkFM5SvzT09OttrS0NJ0/f17Lly+Xj49PbseIf5FRo0Zp586dWrNmjTZu3Kjt27dr//79Zv3QoUO1a9cuffbZZzp06JC6du2qtm3b6vjx42abq1evavbs2Vq6dKm2bdums2fPavTo0Wb9nDlzFBERoUWLFmnHjh26dOmSvvrqK6s4Zs6cqSVLlui9997TkSNHNHLkSPXs2VNbt261ajd+/HjNmjVLMTExqlq1apbOsW/fvvriiy909epVSTd+rGjbtq28vb2t2iUlJal9+/b6/vvvdeDAAbVt21YdO3bU2bNn79h3165ddeHCBa1bt0779u1TzZo11bJlyzu+CSM5OVmJiYlWGwAAAABki5EDYWFhxpUrVzKUX7161QgLC8tJl/gPSExMNBwcHIwVK1aYZZcvXzYKFChgDB8+3Dhz5oxhZ2dn/Prrr1bHtWzZ0pgwYYJhGIYRHh5uSDJOnDhh1r/zzjuGt7e3ue/j42O88cYb5n5KSopRsmRJo1OnToZhGMa1a9eMAgUKGD/88IPVOP369TN69OhhGIZhbNmyxZBkrF69OsvnFx4ebnh4eBiGYRjVq1c3Fi9ebKSnpxvlypUzvv76a2Pu3LmGr6/vXfuoXLmyMX/+fHPf19fXmDt3rmEYhrF9+3bD3d3duHbtmtUx5cqVM95///1M+5s8ebIhKcOWkJCQ5fMCAAAAYHsSEhKynBvkaMY/LCxMSUlJGcqvXr2qsLCwHP8IgX+3U6dOKSUlRXXr1jXLPDw8FBAQIEk6fPiw0tLS5O/vL1dXV3PbunWrTp48aR5ToEABlStXztz38fHRhQsXJEkJCQmKj49XvXr1zHp7e3vVrl3b3D9x4oSuXr2q1q1bW42zZMkSq3EkWR2XHX379lV4eLi2bt2qK1euqH379hnaJCUlafTo0QoMDJSnp6dcXV0VExNzxxn/6OhoJSUlqVChQlZxx8XFZYj7pgkTJighIcHczp07l6PzAQAAAPDoytHD/QzDkMViyVAeHR2tggUL3ndQ+G9KSkqSnZ2d9u3bJzs7O6s6V1dX828HBwerOovFkuEe/nuNI0lr165ViRIlrOocHR2t9l1cXLLc762Cg4M1duxYTZkyRb169cr0oYCjR4/Wxo0bNXv2bJUvX17Ozs565plndP369TvG7ePjY/XMg5s8PT0zPcbR0THDOQEAAABAdmQr8ffy8pLFYpHFYpG/v79V8p+WlqakpCQNHjw414PEv0PZsmXl4OCgqKgo8wn3CQkJ+vnnn9WkSRPVqFFDaWlpunDhgho3bpyjMTw8POTj46Pdu3erSZMmkqTU1FTzfnhJqlSpkhwdHXX27Fk1bdo0d07uNgULFtRTTz2lL774Qu+9916mbXbu3KnQ0FB17txZ0o3E/vTp03fss2bNmjp//rzs7e15LSAAAACAhyZbif+8efNkGIb69u2rsLAweXh4mHX58+eXn5+f1SvQYFvc3NzUu3dvjRkzRgULFlTRokU1efJk5cuXz/wxKDg4WCEhIZozZ45q1Kihixcv6vvvv1fVqlX15JNPZmmc4cOHa9asWapQoYIqVqyoN998U5cvX7aKY/To0Ro5cqTS09P1+OOPKyEhQTt37pS7u7t69+6dK+cbERGhhQsXqlChQpnWV6hQQatWrVLHjh1lsVg0ceJEpaen37G/Vq1aqUGDBgoKCtIbb7whf39//fbbb1q7dq06d+6c49sSAAAAAOBuspX430yoypQpo4YNG2ZYsg3b9+abb2rw4MHq0KGD3N3dNXbsWJ07d05OTk6SpPDwcE2fPl0vvfSSfv31VxUuXFj169dXhw4dsjzGSy+9pPj4ePXu3Vv58uVT37591blzZyUkJJhtpk2bpiJFimjmzJk6deqUPD09VbNmTb388su5dq7Ozs5ydna+Y/2bb76pvn37qmHDhipcuLDGjRt316fuWywWffvtt3rllVfUp08fXbx4UcWKFVOTJk0yvDEAAAAAAHKLxcjOzdWZuHbtWoZ7mt3d3e8rKPx3XLlyRSVKlNCcOXPUr1+/vA7H5iUmJsrDw0MJCQn8OwMAAAAeYdnJDXL0cL+rV69q7Nix+uKLL/Tnn39mqE9LS8tJt/gPOHDggI4dO6a6desqISFBU6dOlSR16tQpjyMDAAAAAGQmR6/zGzNmjDZv3qx3331Xjo6O+uijjxQWFqbixYtryZIluR0j/mVmz56tatWqqVWrVrpy5Yq2b9+uwoUL53VY91S5cmWr1+jdui1btiyvwwMAAACAByJHS/1Lly6tJUuWqFmzZnJ3d9f+/ftVvnx5LV26VJ9++qm+/fbbBxErcF/OnDmjlJSUTOu8vb3l5ub2kCPKPpb6AwAAAJAewlL/S5cuqWzZspJu3M9/6dIlSdLjjz+u559/PiddAg+cr69vXocAAAAAAA9djpb6ly1bVnFxcZKkihUr6osvvpAk/e9//5Onp2euBQcAAAAAAO5PjhL/Pn36KDo6WpI0fvx4vfPOO3JyctLIkSM1ZsyYXA0QAAAAAADk3H2/zk+6ce/0vn37VL58eVWtWjU34gKQCe7xBwAAACA9hHv8b3Xt2jX5+vpy/zQAAAAAAP9COVrqn5aWpmnTpqlEiRJydXXVqVOnJEkTJ07Uxx9/nKsBAgAAAACAnMtR4v/aa68pIiJCb7zxhvLnz2+WP/bYY/roo49yLTgAAAAAAHB/cpT4L1myRB988IGCg4NlZ2dnllerVk3Hjh3LteAAAAAAAMD9yVHi/+uvv6p8+fIZytPT05WSknLfQQEAAAAAgNyRo8S/UqVK2r59e4bylStXqkaNGvcdFAAAAAAAyB05eqr/pEmT1Lt3b/36669KT0/XqlWrFBsbqyVLluibb77J7RgBAAAAAEAOZWvG/9SpUzIMQ506ddL//vc/bdq0SS4uLpo0aZJiYmL0v//9T61bt35QsQIAAAAAgGzK1ox/hQoVFB8fr6JFi6px48YqWLCgDh8+LG9v7wcVHwAAAAAAuA/ZmvE3DMNqf926dbpy5UquBgQAAAAAAHJPjh7ud9PtPwQAAAAAAIB/l2wl/haLRRaLJUMZAAAAAAD4d8rWPf6GYSg0NFSOjo6SpGvXrmnw4MFycXGxardq1arcixAAAAAAAORYthL/3r17W+337NkzV4MBAAAAAAC5K1uJf3h4+IOKA1BoaKguX76s1atX53Uod3T69GmVKVNGBw4cUPXq1fM6HAAAAAC4p/t6uB/wX/TPP/9o8uTJ8vf3l6OjowoXLqyuXbvqyJEjVu1CQ0MVFBSUN0ECAAAAQC4h8YfNMAxDqampd22TnJysVq1aadGiRZo+fbp+/vlnffvtt0pNTVW9evX0448/PqRorV2/fj1PxgUAAABg+0j8kcHff/+t4OBgubi4yMfHR3PnzlWzZs00YsQISTeS59GjR6tEiRJycXFRvXr1FBkZaR4fEREhT09PbdiwQYGBgXJ1dVXbtm0VHx9vtklLS9OoUaPk6empQoUKaezYsRleD5menq6ZM2eqTJkycnZ2VrVq1bRy5UqzPjIyUhaLRevWrVOtWrXk6OioHTt23PXc5s2bp127dumbb75Rt27d5Ovrq7p16+rLL79UYGCg+vXrJ8MwNGXKFC1evFhff/21+TaLW8/x1KlTat68uQoUKKBq1app165dVuPs2LFDjRs3lrOzs0qVKqVhw4bpypUrZr2fn5+mTZumkJAQubu7a+DAgVn9eAAAAAAgW0j8kcGoUaO0c+dOrVmzRhs3btT27du1f/9+s37o0KHatWuXPvvsMx06dEhdu3ZV27Ztdfz4cbPN1atXNXv2bC1dulTbtm3T2bNnNXr0aLN+zpw5ioiI0KJFi7Rjxw5dunRJX331lVUcM2fO1JIlS/Tee+/pyJEjGjlypHr27KmtW7datRs/frxmzZqlmJgYVa1a9a7ntnz5crVu3VrVqlWzKs+XL59Gjhypo0ePKjo6WqNHj1a3bt3MHyzi4+PVsGFDs/0rr7yi0aNH6+DBg/L391ePHj3M1QYnT55U27Zt9fTTT+vQoUP6/PPPtWPHDg0dOtRqzNmzZ6tatWo6cOCAJk6cmGm8ycnJSkxMtNoAAAAAIFsM4BaJiYmGg4ODsWLFCrPs8uXLRoECBYzhw4cbZ86cMezs7Ixff/3V6riWLVsaEyZMMAzDMMLDww1JxokTJ8z6d955x/D29jb3fXx8jDfeeMPcT0lJMUqWLGl06tTJMAzDuHbtmlGgQAHjhx9+sBqnX79+Ro8ePQzDMIwtW7YYkozVq1dn+fycnJyM4cOHZ1q3f/9+Q5Lx+eefG4ZhGL179zbjuSkuLs6QZHz00Udm2ZEjRwxJRkxMjBnjwIEDrY7bvn27kS9fPuOff/4xDMMwfH19jaCgoHvGO3nyZENShi0hISGrpwwAAADABiUkJGQ5N8jWU/1h+06dOqWUlBTVrVvXLPPw8FBAQIAk6fDhw0pLS5O/v7/VccnJySpUqJC5X6BAAZUrV87c9/Hx0YULFyRJCQkJio+PV7169cx6e3t71a5d21zuf+LECV29elWtW7e2Guf69euqUaOGVVnt2rWzdY7GbbcU5MStKwt8fHwkSRcuXFDFihUVHR2tQ4cOadmyZVZjpqenKy4uToGBgVmOe8KECRo1apS5n5iYqFKlSt13/AAAAAAeHST+yJakpCTZ2dlp3759srOzs6pzdXU1/3ZwcLCqs1gs2Uq4k5KSJElr165ViRIlrOocHR2t9l1cXLLcr7+/v2JiYjKtu1l++48ambn1/CwWi6QbzySQbsQ+aNAgDRs2LMNxpUuXzlbcjo6OGc4XAAAAALKDxB9WypYtKwcHB0VFRZlJakJCgn7++Wc1adJENWrUUFpami5cuKDGjRvnaAwPDw/5+Pho9+7datKkiSQpNTVV+/btU82aNSVJlSpVkqOjo86ePaumTZvmzslJ6t69u1555RVFR0db3eefnp6uuXPnqlKlSmZ5/vz5lZaWlu0xatasqaNHj6p8+fK5FjcAAAAA5BSJP6y4ubmpd+/eGjNmjAoWLKiiRYtq8uTJypcvnywWi/z9/RUcHKyQkBDNmTNHNWrU0MWLF/X999+ratWqevLJJ7M0zvDhwzVr1ixVqFBBFStW1JtvvqnLly9bxTF69GiNHDlS6enpevzxx5WQkKCdO3fK3d1dvXv3ztH5jRw5Ul9//bU6duyoOXPmqF69evr99981Y8YMxcTEaNOmTeYMvp+fnzZs2KDY2FgVKlRIHh4eWRpj3Lhxql+/voYOHar+/fvLxcVFR48e1caNG7VgwYIcxQ0AAAAAOUXijwzefPNNDR48WB06dJC7u7vGjh2rc+fOycnJSZIUHh6u6dOn66WXXtKvv/6qwoULq379+urQoUOWx3jppZcUHx+v3r17K1++fOrbt686d+6shIQEs820adNUpEgRzZw5U6dOnZKnp6dq1qypl19+Ocfn5uTkpM2bN2vGjBl6+eWXdebMGbm5ual58+b68ccf9dhjj5ltBwwYoMjISNWuXVtJSUnasmWL/Pz87jlG1apVtXXrVr3yyitq3LixDMNQuXLl9Oyzz+Y4bgAAAADIKYuRG086g027cuWKSpQooTlz5qhfv355Hc4jLTExUR4eHkpISJC7u3tehwMAAAAgj2QnN2DGHxkcOHBAx44dU926dZWQkKCpU6dKkjp16pTHkQEAAAAAsitfXgeAf6fZs2erWrVqatWqla5cuaLt27ercOHCeR3WPVWuXFmurq6Zbre+Xg8AAAAAHhUs9YdNOXPmjFJSUjKt8/b2lpub20OOKHex1B8AAACAxFJ/PMJ8fX3zOgQAAAAA+FdhqT8AAAAAADaMxB8AAAAAABtG4g8AAAAAgA0j8QcAAAAAwIaR+AMAAAAAYMNI/AEAAAAAsGEk/gAAAAAA2DASfwAAAAAAbBiJPwAAAAAANozEHwAAAAAAG0biDwAAAACADSPxBwAAAADAhpH4AwAAAABgw0j8AQAAAACwYST+AAAAAADYMBJ/AAAAAABsGIk/AAAAAAA2jMT/AQkNDVVQUNAd66dMmaLq1as/tHgkyWKxaPXq1Xesj4yMlMVi0eXLlx9aTAAAAACAB4vEP4+MHj1a33//fV6HYaVhw4aKj4+Xh4fHPdvyI8H9udcPQwAAAACQW0j884irq6sKFSqU12FYyZ8/v4oVKyaLxfLQxjQMQ6mpqQ9tvKy6fv16hrK0tDSlp6fnQTQAAAAAkHOPTOK/cuVKValSRc7OzipUqJBatWqlK1eumDOvM2bMkLe3tzw9PTV16lSlpqZqzJgxKliwoEqWLKnw8HCr/g4fPqwWLVqY/Q0cOFBJSUl3HD8qKkpFihTR66+/LinjUv+bccyePVs+Pj4qVKiQhgwZopSUFLNNfHy8nnzySTk7O6tMmTJavny5/Pz8NG/evCxfhz/++EOdO3dWgQIFVKFCBa1Zs8asu30W/8yZM+rYsaO8vLzk4uKiypUr69tvv9Xp06fVvHlzSZKXl5csFotCQ0MlScnJyRo2bJiKFi0qJycnPf7444qKisowxrp161SrVi05Ojrqk08+Ub58+bR3716rWOfNmydfX98sJdtHjhxRhw4d5O7uLjc3NzVu3FgnT56UJDVr1kwjRoywah8UFGTGLEl+fn6aNm2aQkJC5O7uroEDByoiIkKenp5as2aNKlWqJEdHR509e1bJyckaPXq0SpQoIRcXF9WrV0+RkZFmXzeP27BhgwIDA+Xq6qq2bdsqPj5e0o3PfvHixfr6669lsVhksVisjr9VcnKyEhMTrTYAAAAAyI5HIvGPj49Xjx491LdvX8XExCgyMlJdunSRYRiSpM2bN+u3337Ttm3b9Oabb2ry5Mnq0KGDvLy8tHv3bg0ePFiDBg3SL7/8Ikm6cuWK2rRpIy8vL0VFRWnFihXatGmThg4dmun4mzdvVuvWrfXaa69p3Lhxd4xzy5YtOnnypLZs2aLFixcrIiJCERERZn1ISIh+++03RUZG6ssvv9QHH3ygCxcuZOtahIWFqVu3bjp06JDat2+v4OBgXbp0KdO2Q4YMUXJysrZt26bDhw/r9ddfl6urq0qVKqUvv/xSkhQbG6v4+Hi99dZbkqSxY8fqyy+/1OLFi7V//36VL19ebdq0yTDG+PHjNWvWLMXExOipp55Sq1atMvy4Eh4ertDQUOXLd/ev6a+//qomTZrI0dFRmzdv1r59+9S3b99srySYPXu2qlWrpgMHDmjixImSpKtXr+r111/XRx99pCNHjqho0aIaOnSodu3apc8++0yHDh1S165d1bZtWx0/ftzs6+rVq5o9e7aWLl2qbdu26ezZsxo9erSkG7d5dOvWzfwxID4+Xg0bNsw0ppkzZ8rDw8PcSpUqla1zAgAAAAAZj4B9+/YZkozTp09nqOvdu7fh6+trpKWlmWUBAQFG48aNzf3U1FTDxcXF+PTTTw3DMIwPPvjA8PLyMpKSksw2a9euNfLly2ecP3/e7LdTp07GqlWrDFdXV+Ozzz6zGnfy5MlGtWrVMsSRmppqlnXt2tV49tlnDcMwjJiYGEOSERUVZdYfP37ckGTMnTs3S9dBkvHqq6+a+0lJSYYkY926dYZhGMaWLVsMScZff/1lGIZhVKlSxZgyZUqmfd3e9mZ/Dg4OxrJly8yy69evG8WLFzfeeOMNq+NWr15t1d/nn39ueHl5GdeuXTMM48ZnZrFYjLi4uHue14QJE4wyZcoY169fz7S+adOmxvDhw63KOnXqZPTu3dvc9/X1NYKCgqzahIeHG5KMgwcPmmVnzpwx7OzsjF9//dWqbcuWLY0JEyZYHXfixAmz/p133jG8vb3N/Zvfj3u5du2akZCQYG7nzp0zJBkJCQn3PBYAAACA7UpISMhybvBIzPhXq1ZNLVu2VJUqVdS1a1d9+OGH+uuvv8z6ypUrW80qe3t7q0qVKua+nZ2dChUqZM6ux8TEqFq1anJxcTHbNGrUSOnp6YqNjTXLdu/era5du2rp0qV69tln7xln5cqVZWdnZ+77+PiYY8bGxsre3l41a9Y068uXLy8vL6/sXApVrVrV/NvFxUXu7u53XDUwbNgwTZ8+XY0aNdLkyZN16NChu/Z98uRJpaSkqFGjRmaZg4OD6tatq5iYGKu2tWvXttoPCgqSnZ2dvvrqK0k3lss3b95cfn5+9zyngwcPqnHjxnJwcLhn27u5PSbpxnMPbr1mhw8fVlpamvz9/eXq6mpuW7duNW8tkKQCBQqoXLly5v6tn2V2ODo6yt3d3WoDAAAAgOx4JBJ/Ozs7bdy4UevWrVOlSpU0f/58BQQEKC4uTpIyJIwWiyXTsuw+2K1cuXKqWLGiFi1aZHWv/p3kxpi5OUb//v116tQp9erVS4cPH1bt2rU1f/78XInj1h9NpBsJdkhIiMLDw3X9+nUtX75cffv2zVJfzs7Od63Ply+feVvHTZl9HrfHdLPvWx92mJSUJDs7O+3bt08HDx40t5iYGPN2Bynz63x7DAAAAADwMDwSib90I/Fq1KiRwsLCdODAAeXPn9+cXc6uwMBARUdH68qVK2bZzp07lS9fPgUEBJhlhQsX1ubNm3XixAl169YtS8n/nQQEBCg1NVUHDhwwy06cOGG1cuFBKFWqlAYPHqxVq1bppZde0ocffijpRqIu3XjS/U3lypVT/vz5tXPnTrMsJSVFUVFRqlSp0j3H6t+/vzZt2qSFCxcqNTVVXbp0yVKMVatW1fbt2+94fYsUKWI+WO9mzD/99FOW+r5djRo1lJaWpgsXLqh8+fJWW7FixbLcT/78+a2uHQAAAAA8KI9E4r97927NmDFDe/fu1dmzZ7Vq1SpdvHhRgYGBOeovODhYTk5O6t27t3766Sdt2bJFL774onr16iVvb2+rtkWLFtXmzZt17Ngx9ejRI8evrqtYsaJatWqlgQMHas+ePTpw4IAGDhyYYUY6N40YMUIbNmxQXFyc9u/fry1btpjXzNfXVxaLRd98840uXryopKQkubi46Pnnn9eYMWO0fv16HT16VAMGDNDVq1fVr1+/e44XGBio+vXra9y4cerRo8c9Z/JvGjp0qBITE9W9e3ft3btXx48f19KlS83bLlq0aKG1a9dq7dq1OnbsmJ5//nnzzQXZ5e/vr+DgYIWEhGjVqlWKi4vTnj17NHPmTK1duzbL/fj5+enQoUOKjY3VH3/8cV8/CgEAAADA3TwSib+7u7u2bdum9u3by9/fX6+++qrmzJmjdu3a5ai/AgUKaMOGDbp06ZLq1KmjZ555Ri1bttSCBQsybV+sWDFt3rxZhw8fVnBwcI5nepcsWSJvb281adJEnTt31oABA+Tm5iYnJ6cc9XcvaWlpGjJkiAIDA9W2bVv5+/tr4cKFkqQSJUooLCxM48ePl7e3t/lGg1mzZunpp59Wr169VLNmTZ04cUIbNmzI8rMI+vXrp+vXr2d5mb8kFSpUSJs3b1ZSUpKaNm2qWrVq6cMPPzSX2/ft21e9e/dWSEiImjZtqrJly5qvI8yJ8PBwhYSE6KWXXlJAQICCgoIUFRWl0qVLZ7mPAQMGKCAgQLVr11aRIkWsVkkAAAAAQG6yGNx4/J/1yy+/qFSpUtq0aZNatmyZ1+HkimnTpmnFihX3fJDgoyoxMVEeHh5KSEjgQX8AAADAIyw7uYH9Q4oJueDmrHaVKlUUHx+vsWPHys/PT02aNMnr0O5bUlKSTp8+rQULFmj69Ol5HQ4AAAAA2IxHYqm/rUhJSdHLL7+sypUrq3PnzipSpIgiIyPl4OCgZcuWWb1e7tatcuXKeR36PQ0dOlS1atVSs2bNMizzHzx48B3PbfDgwXkUMQAAAAD8N7DU30b8/fff+v333zOtc3BwkK+v70OOKPdcuHBBiYmJmda5u7uraNGiDzmivMNSfwAAAAASS/0fSW5ubnJzc8vrMB6IokWLPlLJPQAAAADkJpb6AwAAAABgw0j8AQAAAACwYST+AAAAAADYMBJ/AAAAAABsGIk/AAAAAAA2jMQfAAAAAAAbRuIPAAAAAIANI/EHAAAAAMCGkfgDyLEWL5zN6xAAAAAA3AOJPwAAAAAANozEHwAAAAAAG0biDwAAAACADSPxBwAAAADAhpH4AwAAAABgw0j8AQAAAACwYST+AAAAAADYMBL//4jQ0FAFBQXdsX7KlCmqXr36Q4tHkiwWi1avXn3H+sjISFksFl2+fPmhxQQAAAAAsEbibyNGjx6t77//Pq/DsNKwYUPFx8fLw8Pjnm35kQAAAAAAHgwSfxvh6uqqQoUK5XUYVvLnz69ixYrJYrE8tDENw1BqaupDGy83paSk5HUIAAAAAGwQiX8OrVy5UlWqVJGzs7MKFSqkVq1a6cqVK+aS/BkzZsjb21uenp6aOnWqUlNTNWbMGBUsWFAlS5ZUeHi4VX+HDx9WixYtzP4GDhyopKSkO44fFRWlIkWK6PXXX5eUcan/zThmz54tHx8fFSpUSEOGDLFKLuPj4/Xkk0/K2dlZZcqU0fLly+Xn56d58+Zl+Tr88ccf6ty5swoUKKAKFSpozZo1Zt3ts/hnzpxRx44d5eXlJRcXF1WuXFnffvutTp8+rebNm0uSvLy8ZLFYFBoaKklKTk7WsGHDVLRoUTk5Oenxxx9XVFRUhjHWrVunWrVqydHRUZ988ony5cunvXv3WsU6b948+fr6Kj09/Z7n9dNPP6ldu3ZydXWVt7e3evXqpT/++EOS9MEHH6h48eIZ+unUqZP69u1r7n/99deqWbOmnJycVLZsWYWFhVn9KGGxWPTuu+/qqaeekouLi1577bUsXHEAAAAAyB4S/xyIj49Xjx491LdvX8XExCgyMlJdunSRYRiSpM2bN+u3337Ttm3b9Oabb2ry5Mnq0KGDvLy8tHv3bg0ePFiDBg3SL7/8Ikm6cuWK2rRpIy8vL0VFRWnFihXatGmThg4dmun4mzdvVuvWrfXaa69p3Lhxd4xzy5YtOnnypLZs2aLFixcrIiJCERERZn1ISIh+++03RUZG6ssvv9QHH3ygCxcuZOtahIWFqVu3bjp06JDat2+v4OBgXbp0KdO2Q4YMUXJysrZt26bDhw/r9ddfl6urq0qVKqUvv/xSkhQbG6v4+Hi99dZbkqSxY8fqyy+/1OLFi7V//36VL19ebdq0yTDG+PHjNWvWLMXExOipp55Sq1atMvy4Eh4ertDQUOXLd/ev/eXLl9WiRQvVqFFDe/fu1fr16/X777+rW7dukqSuXbvqzz//1JYtW8xjLl26pPXr1ys4OFiStH37doWEhGj48OE6evSo3n//fUVERGRI7qdMmaLOnTvr8OHDVj8a3JScnKzExESrDQAAAACyxUC27du3z5BknD59OkNd7969DV9fXyMtLc0sCwgIMBo3bmzup6amGi4uLsann35qGIZhfPDBB4aXl5eRlJRktlm7dq2RL18+4/z582a/nTp1MlatWmW4uroan332mdW4kydPNqpVq5YhjtTUVLOsa9euxrPPPmsYhmHExMQYkoyoqCiz/vjx44YkY+7cuVm6DpKMV1991dxPSkoyJBnr1q0zDMMwtmzZYkgy/vrrL8MwDKNKlSrGlClTMu3r9rY3+3NwcDCWLVtmll2/ft0oXry48cYbb1gdt3r1aqv+Pv/8c8PLy8u4du2aYRg3PjOLxWLExcXd87ymTZtmPPHEE1Zl586dMyQZsbGxhmEYRqdOnYy+ffua9e+//75RvHhx83Nv2bKlMWPGDKs+li5davj4+Jj7kowRI0bcNZbJkycbkjJsCQkJ9zyPh6H582fyOgQAAADgkZSQkJDl3IAZ/xyoVq2aWrZsqSpVqqhr16768MMP9ddff5n1lStXtppV9vb2VpUqVcx9Ozs7FSpUyJxdj4mJUbVq1eTi4mK2adSokdLT0xUbG2uW7d69W127dtXSpUv17LPP3jPOypUry87Oztz38fExx4yNjZW9vb1q1qxp1pcvX15eXl7ZuRSqWrWq+beLi4vc3d3vuGpg2LBhmj59uho1aqTJkyfr0KFDd+375MmTSklJUaNGjcwyBwcH1a1bVzExMVZta9eubbUfFBQkOzs7ffXVV5KkiIgINW/eXH5+fvc8p+joaG3ZskWurq7mVrFiRTMmSQoODtaXX36p5ORkSdKyZcvUvXt383OPjo7W1KlTrfoYMGCA4uPjdfXq1TvGfbsJEyYoISHB3M6dO3fP+AEAAADgViT+OWBnZ6eNGzdq3bp1qlSpkubPn6+AgADFxcVJupGc3spisWRalpV7zW9Vrlw5VaxYUYsWLcrSg+ByY8zcHKN///46deqUevXqpcOHD6t27dqaP39+rsRx648m0o0HC4aEhCg8PFzXr1/X8uXLM11Kn5mkpCR17NhRBw8etNqOHz+uJk2aSJI6duwowzC0du1anTt3Ttu3bzeX+d/sIywszOr4w4cP6/jx43Jycrpj3LdzdHSUu7u71QYAAAAA2UHin0MWi0WNGjVSWFiYDhw4oPz585uzy9kVGBio6OhoXblyxSzbuXOn8uXLp4CAALOscOHC2rx5s06cOKFu3brd11PgAwIClJqaqgMHDphlJ06csFq58CCUKlVKgwcP1qpVq/TSSy/pww8/lHQjUZektLQ0s225cuWUP39+7dy50yxLSUlRVFSUKlWqdM+x+vfvr02bNmnhwoVKTU1Vly5dshRjzZo1deTIEfn5+al8+fJW281E3cnJSV26dNGyZcv06aefKiAgwGr1RM2aNRUbG5vh+PLly9/zGQMAAAAAkJvIQHJg9+7dmjFjhvbu3auzZ89q1apVunjxogIDA3PUX3BwsJycnNS7d2/99NNP2rJli1588UX16tVL3t7eVm2LFi2qzZs369ixY+rRo0eOX11XsWJFtWrVSgMHDtSePXt04MABDRw4UM7Ozg/s9XsjRozQhg0bFBcXp/3792vLli3mNfP19ZXFYtE333yjixcvKikpSS4uLnr++ec1ZswYrV+/XkePHtWAAQN09epV9evX757jBQYGqn79+ho3bpx69OghZ2fnLMU5ZMgQXbp0ST169FBUVJROnjypDRs2qE+fPlY/TAQHB2vt2rVatGiR1Wy/JE2aNElLlixRWFiYjhw5opiYGH322Wd69dVXs3HFAAAAAOD+kfjngLu7u7Zt26b27dvL399fr776qubMmaN27drlqL8CBQpow4YNunTpkurUqaNnnnlGLVu21IIFCzJtX6xYMW3evFmHDx9WcHCwVTKaHUuWLJG3t7eaNGmizp07a8CAAXJzc7Naip6b0tLSNGTIEAUGBqpt27by9/fXwoULJUklSpRQWFiYxo8fL29vb/ONBrNmzdLTTz+tXr16qWbNmjpx4oQ2bNiQ5WcR9OvXT9evX8/yMn9JKl68uHbu3Km0tDQ98cQTqlKlikaMGCFPT0+r2foWLVqoYMGCio2N1XPPPWfVR5s2bfTNN9/ou+++U506dVS/fn3NnTtXvr6+WY4DAAAAAHKDxTD+3zvo8Mj75ZdfVKpUKW3atEktW7bM63ByxbRp07RixYp7PkjwvyIxMVEeHh5KSEj4V9zv3+KFs9q8sHRehwEAAAA8crKTG9g/pJjwL7R582YlJSWpSpUqio+P19ixY+Xn52c+wO6/LCkpSadPn9aCBQs0ffr0vA4HAAAAAPIMS/0fYSkpKXr55ZdVuXJlde7cWUWKFFFkZKQcHBy0bNkyq1fR3bpVrlw5r0O/p6FDh6pWrVpq1qxZhmX+gwcPvuO5DR48OI8iBgAAAIAHg6X+yNTff/+t33//PdM6BweH//S96hcuXFBiYmKmde7u7ipatOhDjijrWOoPAAAAQGKpP3KBm5ub3Nzc8jqMB6Jo0aL/6uQeAAAAAHITS/0BAAAAALBhJP4AAAAAANgwEn8AOcb9/QAAAMC/H4k/AAAAAAA2jMQfAAAAAAAbRuIPAAAAAIANI/EHAAAAAMCGkfgDAAAAAGDDSPwBAAAAALBhJP4A7kuLF87mdQgAAAAA7oLEHwAAAAAAG0biDwAAAACADSPxBwAAAADAhpH4AwAAAABgw0j8AQAAAACwYST+AAAAAADYMBJ/AAAAAABsGIn/QxIaGqqgoKA71k+ZMkXVq1d/aPFIksVi0erVq+9YHxkZKYvFosuXLz+0mHLLzp07VaVKFTk4ONz1ugMAAACArSPx/5cYPXq0vv/++7wOw0rDhg0VHx8vDw+Pe7b9t/1IMGrUKFWvXl1xcXGKiIjI63AAAAAAIM+Q+P9LuLq6qlChQnkdhpX8+fOrWLFislgsD21MwzCUmpp63/2cPHlSLVq0UMmSJeXp6ZmjPq5fv37fcWRVbp03AAAAANzukU38V65cqSpVqsjZ2VmFChVSq1atdOXKFXNJ/owZM+Tt7S1PT09NnTpVqampGjNmjAoWLKiSJUsqPDzcqr/Dhw+rRYsWZn8DBw5UUlLSHcePiopSkSJF9Prrr0vKuNT/ZhyzZ8+Wj4+PChUqpCFDhiglJcVsEx8fryeffFLOzs4qU6aMli9fLj8/P82bNy/L1+GPP/5Q586dVaBAAVWoUEFr1qwx626fxT9z5ow6duwoLy8vubi4qHLlyvr22291+vRpNW/eXJLk5eUli8Wi0NBQSVJycrKGDRumokWLysnJSY8//riioqIyjLFu3TrVqlVLjo6O+uSTT5QvXz7t3bvXKtZ58+bJ19dX6enpdzyf06dPy2Kx6M8//1Tfvn1lsVjMGf+tW7eqbt26cnR0lI+Pj8aPH2+VbDdr1kxDhw7ViBEjVLhwYbVp08aMb8OGDapRo4acnZ3VokULXbhwQevWrVNgYKDc3d313HPP6erVq2Zf6enpmjlzpsqUKSNnZ2dVq1ZNK1euvOt579ixI8ufGwAAAABk1SOZ+MfHx6tHjx7q27evYmJiFBkZqS5dusgwDEnS5s2b9dtvv2nbtm168803NXnyZHXo0EFeXl7avXu3Bg8erEGDBumXX36RJF25ckVt2rSRl5eXoqKitGLFCm3atElDhw7NdPzNmzerdevWeu211zRu3Lg7xrllyxadPHlSW7Zs0eLFixUREWG1bD0kJES//fabIiMj9eWXX+qDDz7QhQsXsnUtwsLC1K1bNx06dEjt27dXcHCwLl26lGnbIUOGKDk5Wdu2bdPhw4f1+uuvy9XVVaVKldKXX34pSYqNjVV8fLzeeustSdLYsWP15ZdfavHixdq/f7/Kly+vNm3aZBhj/PjxmjVrlmJiYvTUU0+pVatWGX5cCQ8PV2hoqPLlu/PXtlSpUoqPj5e7u7vmzZun+Ph4Pfvss/r111/Vvn171alTR9HR0Xr33Xf18ccfa/r06VbHL168WPnz59fOnTv13nvvmeVTpkzRggUL9MMPP+jcuXPq1q2b5s2bp+XLl2vt2rX67rvvNH/+fLP9zJkztWTJEr333ns6cuSIRo4cqZ49e2rr1q13PO+qVatmOJ/k5GQlJiZabQAAAACQLcYjaN++fYYk4/Tp0xnqevfubfj6+hppaWlmWUBAgNG4cWNzPzU11XBxcTE+/fRTwzAM44MPPjC8vLyMpKQks83atWuNfPnyGefPnzf77dSpk7Fq1SrD1dXV+Oyzz6zGnTx5slGtWrUMcaSmppplXbt2NZ599lnDMAwjJibGkGRERUWZ9cePHzckGXPnzs3SdZBkvPrqq+Z+UlKSIclYt26dYRiGsWXLFkOS8ddffxmGYRhVqlQxpkyZkmlft7e92Z+Dg4OxbNkys+z69etG8eLFjTfeeMPquNWrV1v19/nnnxteXl7GtWvXDMO48ZlZLBYjLi4uS+fm4eFhhIeHm/svv/yyERAQYKSnp5tl77zzjuHq6mp+1k2bNjVq1KiR6Xlt2rTJLJs5c6YhyTh58qRZNmjQIKNNmzaGYRjGtWvXjAIFChg//PCDVV/9+vUzevTocdfzvt3kyZMNSRm2hISELF2Hh6H582fyOgQAAADgkZOQkJDl3OCRnPGvVq2aWrZsqSpVqqhr16768MMP9ddff5n1lStXtppV9vb2VpUqVcx9Ozs7FSpUyJxdj4mJUbVq1eTi4mK2adSokdLT0xUbG2uW7d69W127dtXSpUv17LPP3jPOypUry87Oztz38fExx4yNjZW9vb1q1qxp1pcvX15eXl7ZuRRWs8wuLi5yd3e/46qBYcOGafr06WrUqJEmT56sQ4cO3bXvkydPKiUlRY0aNTLLHBwcVLduXcXExFi1rV27ttV+UFCQ7Ozs9NVXX0mSIiIi1Lx5c/n5+WXn9EwxMTFq0KCB1fMKGjVqpKSkJHPlhiTVqlUr0+NvvU7e3t4qUKCAypYta1V287qdOHFCV69eVevWreXq6mpuS5Ys0cmTJ+963rebMGGCEhISzO3cuXNZP2kAAAAA0CO61N/Ozk4bN27UunXrVKlSJc2fP18BAQGKi4uTdCM5vZXFYsm07G73mmemXLlyqlixohYtWmR1r/6d5MaYuTlG//79derUKfXq1UuHDx9W7dq1rZa3349bfzSRbjxYMCQkROHh4bp+/bqWL1+uvn375spY2Ynjpluv072+Dzef7bB27VodPHjQ3I4ePWp1n//dxrvJ0dFR7u7uVhsAAAAAZMcjmfhLNxK1Ro0aKSwsTAcOHFD+/PnN2eXsCgwMVHR0tK5cuWKW7dy5U/ny5VNAQIBZVrhwYW3evFknTpxQt27dspT830lAQIBSU1N14MABs+zEiRNWKxcehFKlSmnw4MFatWqVXnrpJX344YeSbiTqkpSWlma2LVeunHm//E0pKSmKiopSpUqV7jlW//79tWnTJi1cuFCpqanq0qVLjuMODAzUrl27zOc4SDc+Izc3N5UsWTLH/WamUqVKcnR01NmzZ1W+fHmrrVSpUrk6FgAAAADcyyOZ+O/evVszZszQ3r17dfbsWa1atUoXL15UYGBgjvoLDg6Wk5OTevfurZ9++klbtmzRiy++qF69esnb29uqbdGiRbV582YdO3ZMPXr0yPEr3CpWrKhWrVpp4MCB2rNnjw4cOKCBAwfK2dn5gb1+b8SIEdqwYYPi4uK0f/9+bdmyxbxmvr6+slgs+uabb3Tx4kUlJSXJxcVFzz//vMaMGaP169fr6NGjGjBggK5evap+/frdc7zAwEDVr19f48aNU48ePeTs7Jzj2F944QWdO3dOL774oo4dO6avv/5akydP1qhRo+76sMCccHNz0+jRozVy5EgtXrxYJ0+e1P79+zV//nwtXrw4V8cCAAAAgHt5JBN/d3d3bdu2Te3bt5e/v79effVVzZkzR+3atctRfwUKFNCGDRt06dIl1alTR88884xatmypBQsWZNq+WLFi2rx5sw4fPqzg4GCrWfLsWLJkiby9vdWkSRN17txZAwYMkJubm5ycnHLU372kpaVpyJAhCgwMVNu2beXv76+FCxdKkkqUKKGwsDCNHz9e3t7e5hsNZs2apaefflq9evVSzZo1deLECW3YsCHLzyLo16+frl+/ft/L/EuUKKFvv/1We/bsUbVq1TR48GD169dPr7766n31eyfTpk3TxIkTNXPmTPN6rV27VmXKlHkg4wEAAADAnViMW9c+4z/tl19+UalSpbRp0ya1bNkyr8PJFdOmTdOKFSvu+SDBR0ViYqI8PDyUkJDwr7nfv8ULZ7V5Yem8DgMAAAB4pGQnN7B/SDHhAdi8ebOSkpJUpUoVxcfHa+zYsfLz81OTJk3yOrT7lpSUpNOnT2vBggWaPn16XocDAAAAAP9Zj+RSf1uRkpKil19+WZUrV1bnzp1VpEgRRUZGysHBQcuWLbN6ldytW+XKlfM69HsaOnSoatWqpWbNmmVY5j948OA7ntvgwYPzKGIAAAAA+Hdiqb+N+vvvv/X7779nWufg4CBfX9+HHFHuuXDhghITEzOtc3d3V9GiRR9yRA8PS/0BAAAASCz1h248Wd7NzS2vw3ggihYtatPJPQAAAADkJpb6AwAAAABgw0j8AQAAAACwYST+AO4L9/cDAAAA/24k/gAAAAAA2DASfwAAAAAAbBiJPwAAAAAANozEHwAAAAAAG0biDwAAAACADSPxBwAAAADAhpH4A7gvLV44m9chAAAAALgLEn8AAAAAAGwYiT8AAAAAADaMxB8AAAAAABtG4g8AAAAAgA0j8QcAAAAAwIaR+AMAAAAAYMNI/AEAAAAAsGEk/v8CoaGhCgoKumP9lClTVL169YcWjyRZLBatXr36jvWRkZGyWCy6fPnyQ4sJAAAAAJB9JP7/AaNHj9b333+f12FYadiwoeLj4+Xh4XHPtvxIAAAAAAB5xz6vA8C9ubq6ytXVNa/DsJI/f34VK1bsoY5pGIbS0tJkb297X1tbPjcAAAAAeYsZ/0ysXLlSVapUkbOzswoVKqRWrVrpypUr5pL8GTNmyNvbW56enpo6dapSU1M1ZswYFSxYUCVLllR4eLhVf4cPH1aLFi3M/gYOHKikpKQ7jh8VFaUiRYro9ddfl5Rxqf/NOGbPni0fHx8VKlRIQ4YMUUpKitkmPj5eTz75pJydnVWmTBktX75cfn5+mjdvXpavwx9//KHOnTurQIECqlChgtasWWPW3T6Lf+bMGXXs2FFeXl5ycXFR5cqV9e233+r06dNq3ry5JMnLy0sWi0WhoaGSpOTkZA0bNkxFixaVk5OTHn/8cUVFRWUYY926dapVq5YcHR31ySefKF++fNq7d69VrPPmzZOvr6/S09Pvek43+/z+++9Vu3ZtFShQQA0bNlRsbKxVu6+//lo1a9aUk5OTypYtq7CwMKWmpkqSnnvuOT377LNW7VNSUlS4cGEtWbJEkpSenq6ZM2eqTJkycnZ2VrVq1bRy5cq7ntuOHTvu9ZEAAAAAQLaR+N8mPj5ePXr0UN++fRUTE6PIyEh16dJFhmFIkjZv3qzffvtN27Zt05tvvqnJkyerQ4cO8vLy0u7duzV48GANGjRIv/zyiyTpypUratOmjby8vBQVFaUVK1Zo06ZNGjp0aKbjb968Wa1bt9Zrr72mcePG3THOLVu26OTJk9qyZYsWL16s/6+9e4/r8f7/B/54d04HciixtzKHVAspG/o4Z2EfUw71aX0q0xwmI2OYsZAPsdpmzPGzVczwIZqPU6MDW/NpRRFaFA1TDkNK0/H1+8PP9fXWW/V+l9723uN+u123m+u6Xu/X9Xxdz7cbz+t6Xdc7Ojoa0dHR0v6AgABcv34dycnJiI2NxaZNm3Dz5k2VzsWSJUvg7e2NM2fOYOTIkfDz88OdO3eUtg0ODkZZWRmOHz+OrKwsrFy5EqamppDL5YiNjQUA5OTkoKCgAKtXrwYAzJ07F7GxsYiJicGpU6fQuXNneHh41DjG/PnzER4ejuzsbLz55ptwd3evcXElKioKEyZMgI5O/b7SH330ESIjI5Geng49PT1MnDhR2vfDDz8gICAAM2fOxPnz57Fx40ZER0fjX//6FwDAz88P//3vfxUu3sTHx6O0tBReXl4AgBUrVmDLli3YsGEDzp07h1mzZuGf//wnjh079syxde/evUacZWVluH//vsJCRERERESkEkEKTp48KQCI/Pz8GvsCAwOFjY2NqKqqkrbZ2dmJ/v37S+uVlZXCxMREbN++XQghxKZNm4SFhYUoKSmR2hw4cEDo6OiIwsJCqd/Ro0eLPXv2CFNTU7Fjxw6F44aGhooePXrUiKOyslLaNn78eOHj4yOEECI7O1sAEGlpadL+ixcvCgDis88+q9d5ACAWLlworZeUlAgA4tChQ0IIIZKSkgQAcffuXSGEEE5OTmLx4sVK+3q67eP+9PX1xbZt26Rt5eXlol27dmLVqlUKn4uLi1Pob+fOncLCwkI8fPhQCPEoZzKZTFy+fLnOcT3u8+jRo9K2AwcOCADijz/+EEIIMXToULF8+XKFz23dulVYW1sLIYSoqKgQrVu3Flu2bJH2+/r6Suf/4cOHolmzZuKnn35S6CMoKEj4+vrWOranhYaGCgA1lqKiojrH2lQGv/urpkMgIiIiIvrLKSoqqndtwDv+T+nRoweGDh0KJycnjB8/Hps3b8bdu3el/Y6Ojgp3la2srODk5CSt6+rqolWrVtLd9ezsbPTo0QMmJiZSGzc3N1RXVytML09NTcX48eOxdevWGtPIlXF0dISurq60bm1tLR0zJycHenp66NWrl7S/c+fOsLCwUOVUKNyBNjExgbm5+TNnDcyYMQPLli2Dm5sbQkNDcebMmVr7zsvLQ0VFBdzc3KRt+vr6ePXVV5Gdna3Q1tXVVWHd09MTurq62Lt3LwAgOjoagwcPhq2trVpjs7a2BgBpbKdPn8bSpUuldyuYmppi0qRJKCgoQGlpKfT09ODt7Y1t27YBeDSr47vvvoOfnx8AIDc3F6WlpRg2bJhCH1u2bEFeXl6tY3vahx9+iKKiImm5evVqvcdIREREREQEcKp/Dbq6ujhy5AgOHToEBwcHrFmzBnZ2drh8+TKAR8Xpk2QymdJtdT1r/rROnTqhW7du+PrrrxWe1X+WxjhmYx7jnXfewaVLl+Dv74+srCy4urpizZo1jRLHkxdNgEcvFgwICEBUVBTKy8vx7bffKkzVr48nxyaTyQBAGltJSQmWLFmCzMxMacnKysLFixdhZGQE4NF0/4SEBNy8eRNxcXEwNjbG8OHDpc8DwIEDBxT6OH/+vMJz/srG9jRDQ0OYm5srLERERERERKpg4a+ETCaDm5sblixZgoyMDBgYGEh3l1Vlb2+P06dP48GDB9K2lJQU6OjowM7OTtrWunVrJCYmIjc3F97e3vUq/p/Fzs4OlZWVyMjIkLbl5uYqzFx4HuRyOaZOnYo9e/Zg9uzZ2Lx5M4BHhToAVFVVSW07deoEAwMDpKSkSNsqKiqQlpYGBweHOo/1zjvv4OjRo1i3bh0qKysxZsyYRhtHr169kJOTg86dO9dYHs/26NevH+RyOXbu3Ilt27Zh/Pjx0sUEBwcHGBoa4sqVKzU+L5fLGy1OIiIiIiKi+uBvhz0lNTUVCQkJeP3112FpaYnU1FTcunUL9vb2dU5fV8bPzw+hoaEIDAzE4sWLcevWLbz33nvw9/eHlZWVQltLS0skJiZi8ODB8PX1xY4dO9T6ebdu3brB3d0dkydPxvr166Gvr4/Zs2fD2NhYurvd2EJCQjBixAh07doVd+/eRVJSEuzt7QEANjY2kMlk2L9/P0aOHAljY2OYmpri3XfflX4NoUOHDli1ahVKS0sRFBRU5/Hs7e3Rp08fzJs3DxMnToSxsXGjjeXjjz/G3//+d3To0AHjxo2Djo4OTp8+jbNnz2LZsmVSu7feegsbNmzAhQsXkJSUJG03MzPDnDlzMGvWLFRXV+Nvf/sbioqKkJKSAnNzcwQGBjZarERERERERHXhHf+nmJub4/jx4xg5ciS6du2KhQsXIjIyEiNGjFCrv2bNmiE+Ph537txB7969MW7cOAwdOhRr165V2r5t27ZITExEVlYW/Pz8FO6Sq2LLli2wsrLCgAED4OXlhUmTJsHMzEyaqt7YqqqqEBwcDHt7ewwfPhxdu3bFunXrAADt27fHkiVLMH/+fFhZWUm/aBAeHo6xY8fC398fvXr1Qm5uLuLj4+v9LoKgoCCUl5erPM2/Lh4eHti/fz++//579O7dG3369MFnn30GGxsbhXZ+fn44f/482rdvr/CuAgAICwvDokWLsGLFCumcHDhwAB07dmzUWImIiIiIiOoiE+L//04dabVr165BLpfj6NGjGDp0qKbDaRRhYWHYtWuXWjMx/qzu37+P5s2bo6io6IV53n/ItCtIXNdB02EQEREREf2lqFIbcKq/lkpMTERJSQmcnJxQUFCAuXPnwtbWFgMGDNB0aA1WUlKC/Px8rF27VmHqPREREREREdXEqf5aqqKiAgsWLICjoyO8vLzQpk0bJCcnQ19fH9u2bVP4mbknF0dHR02HXqfp06fDxcUFgwYNqjHNf+rUqc8c29SpUzUUMRERERERkeZwqv9fUHFxMW7cuKF0n76+fo1n2f9Mbt68ifv37yvdZ25uDktLyyaOqHFxqj8REREREQGc6k91MDMzg5mZmabDeC4sLS3/9MU9ERERERFRY+JUfyIiIiIiIiItxsKfiIiIiIiISIux8CeiBuHz/URERERELzYW/kRERERERERajIU/ERERERERkRZj4U9ERERERESkxVj4ExEREREREWkxFv5EREREREREWoyFPxGpbci0K5oOgYiIiIiI6sDCn4iIiIiIiEiLsfAnIiIiIiIi0mIs/ImIiIiIiIi0GAt/IiIiIiIiIi3Gwp+IiIiIiIhIi7HwJyIiIiIiItJiLPyJiIiIiIiItBgLfyIiIiIiIiIt9pcr/CdMmABPT89n7l+8eDF69uzZZPEAgEwmQ1xc3DP3JycnQyaT4d69e00WExEREREREWmHv1zhX5c5c+YgISFB02Eo6NevHwoKCtC8efM622rjRYK6Loz8WWniIhMREREREf31sPB/iqmpKVq1aqXpMBQYGBigbdu2kMlkTXZMIQQqKyub7HgvovLy8hrbeF6IiIiIiOjPRqOF/+7du+Hk5ARjY2O0atUK7u7uePDgAYD/m5K/fPlyWFlZoUWLFli6dCkqKyvxwQcfoGXLlnjppZcQFRWl0GdWVhaGDBki9Tl58mSUlJQ8M4a0tDS0adMGK1euBFDzLuzjOCIiImBtbY1WrVohODgYFRUVUpuCggK88cYbMDY2RseOHfHtt9/C1tYWn3/+eb3Pxe3bt+Hl5YVmzZqhS5cu2Ldvn7Tv6bv4v/76K0aNGgULCwuYmJjA0dERBw8eRH5+PgYPHgwAsLCwgEwmw4QJEwAAZWVlmDFjBiwtLWFkZIS//e1vSEtLq3GMQ4cOwcXFBYaGhvjmm2+go6OD9PR0hVg///xz2NjYoLq6utYxPe4zISEBrq6uaNasGfr164ecnByFduvXr0enTp1gYGAAOzs7bN26Vdpna2sLAPDy8oJMJpPWlbl27Rp8fX3RsmVLmJiYwNXVFampqQCUP+IREhKCQYMGSeuDBg3C9OnTERISgtatW8PDw0Ppefnxxx9RXV2NFStWoGPHjjA2NkaPHj2we/fueo89OjoaS5YswenTpyGTySCTyRAdHV3r+SQiIiIiIlKHxgr/goIC+Pr6YuLEicjOzkZycjLGjBkDIYTUJjExEdevX8fx48fx6aefIjQ0FH//+99hYWGB1NRUTJ06FVOmTMG1a9cAAA8ePICHhwcsLCyQlpaGXbt24ejRo5g+fbrSGBITEzFs2DD861//wrx5854Za1JSEvLy8pCUlISYmBhER0crFGkBAQG4fv06kpOTERsbi02bNuHmzZsqnY8lS5bA29sbZ86cwciRI+Hn54c7d+4obRscHIyysjIcP34cWVlZWLlyJUxNTSGXyxEbGwsAyMnJQUFBAVavXg0AmDt3LmJjYxETE4NTp06hc+fO8PDwqHGM+fPnIzw8HNnZ2XjzzTfh7u5e4+JKVFQUJkyYAB2d+n19PvroI0RGRiI9PR16enqYOHGitG/v3r2YOXMmZs+ejbNnz2LKlCl4++23kZSUBADSxYmoqCgUFBQoXKx4UklJCQYOHIjffvsN+/btw+nTpzF37tw6L048LSYmBgYGBkhJScGGDRuUnpfu3btjxYoV2LJlCzZs2IBz585h1qxZ+Oc//4ljx47Va+w+Pj6YPXs2HB0dUVBQgIKCAvj4+NSIp6ysDPfv31dYiIiIiIiIVCI05OTJkwKAyM/PV7o/MDBQ2NjYiKqqKmmbnZ2d6N+/v7ReWVkpTExMxPbt24UQQmzatElYWFiIkpISqc2BAweEjo6OKCwslPodPXq02LNnjzA1NRU7duxQOG5oaKjo0aNHjTgqKyulbePHjxc+Pj5CCCGys7MFAJGWlibtv3jxogAgPvvss3qdCwBi4cKF0npJSYkAIA4dOiSEECIpKUkAEHfv3hVCCOHk5CQWL16stK+n2z7uT19fX2zbtk3aVl5eLtq1aydWrVql8Lm4uDiF/nbu3CksLCzEw4cPhRCP8iaTycTly5frHNfjPo8ePSptO3DggAAg/vjjDyGEEP369ROTJk1S+Nz48ePFyJEjFc7P3r17az3Wxo0bhZmZmfj999+V7n+c9yfNnDlTDBw4UFofOHCgcHZ2VjqGJ8/Lw4cPRbNmzcRPP/2k0DYoKEj4+vrWe+xPf9eUCQ0NFQBqLEVFRbV+rqkMfvdXTYdARERERPSXVFRUVO/aQGN3/Hv06IGhQ4fCyckJ48ePx+bNm3H37l2FNo6Ojgp3la2srODk5CSt6+rqolWrVtLd9ezsbPTo0QMmJiZSGzc3N1RXVytML09NTcX48eOxdetWpXdZn+bo6AhdXV1p3draWjpmTk4O9PT00KtXL2l/586dYWFhUd9TAQDo3r279GcTExOYm5s/c9bAjBkzsGzZMri5uSE0NBRnzpypte+8vDxUVFTAzc1N2qavr49XX30V2dnZCm1dXV0V1j09PaGrq4u9e/cCeDRFffDgwbVOua9tbNbW1gCgkLMn4wIe5ezpuOqSmZkJZ2dntGzZUqXPPc3FxUXp9ifPS25uLkpLSzFs2DCYmppKy5YtW5CXl6fwudrGXh8ffvghioqKpOXq1auqDIeIiIiIiEhzU/11dXVx5MgRHDp0CA4ODlizZg3s7Oxw+fJlqY2+vr7CZ2QymdJtqk7n7tSpE7p164avv/5a4Vn9Z2mMYzbmMd555x1cunQJ/v7+yMrKgqurK9asWdMocTx50QR49GLBgIAAREVFoby8HN9++63CVP36eHJsj19Q2Njnz9jYuNb9Ojo6Co+RAFCa+6fHr2z743dGHDhwAJmZmdJy/vx5hef8gYaP3dDQEObm5goLERERERGRKjT6cj+ZTAY3NzcsWbIEGRkZMDAwkO4sq8Pe3h6nT5+WXhAIACkpKdDR0YGdnZ20rXXr1khMTERubi68vb3rVfw/i52dHSorK5GRkSFty83NrTF7obHJ5XJMnToVe/bswezZs7F582YAjwp1AKiqqpLaPn5xXkpKirStoqICaWlpcHBwqPNY77zzDo4ePYp169ahsrISY8aMabRx2NvbK8QFPMrZk3Hp6+srjEeZ7t27IzMz85nvRWjTpg0KCgoUtmVmZqoVs4ODAwwNDXHlyhV07txZYZHL5fXux8DAoM5xERERERERNZTGCv/U1FQsX74c6enpuHLlCvbs2YNbt27B3t5e7T79/PxgZGSEwMBAnD17FklJSXjvvffg7+8PKysrhbaWlpZITEzEL7/8Al9fX7V/oq1bt25wd3fH5MmT8fPPPyMjIwOTJ0+GsbHxc/v5vZCQEMTHx+Py5cs4deoUkpKSpPNmY2MDmUyG/fv349atWygpKYGJiQneffddfPDBBzh8+DDOnz+PSZMmobS0FEFBQXUez97eHn369MG8efPg6+tb5911VXzwwQeIjo7G+vXrcfHiRXz66afYs2cP5syZI7WxtbVFQkICCgsLn3lBxdfXF23btoWnpydSUlJw6dIlxMbG4sSJEwCAIUOGID09HVu2bMHFixcRGhqKs2fPqhWzmZkZ5syZg1mzZiEmJgZ5eXk4deoU1qxZg5iYmHr3Y2tri8uXLyMzMxO3b99GWVmZWvEQERERERHVRmOFv7m5OY4fP46RI0eia9euWLhwISIjIzFixAi1+2zWrBni4+Nx584d9O7dG+PGjcPQoUOxdu1ape3btm2LxMREZGVlwc/PT+27r1u2bIGVlRUGDBgALy8vTJo0CWZmZjAyMlJ7LLWpqqpCcHAw7O3tMXz4cHTt2hXr1q0DALRv3x5LlizB/PnzYWVlJf2iQXh4OMaOHQt/f3/06tULubm5iI+Pr/e7CIKCglBeXq7yNP+6eHp6YvXq1YiIiICjoyM2btyIqKgohZ/Zi4yMxJEjRyCXy+Hs7Ky0HwMDA3z//fewtLTEyJEj4eTkhPDwcOndDB4eHli0aBHmzp2L3r17o7i4GAEBAWrHHRYWhkWLFmHFihVSHg4cOICOHTvWu4+xY8di+PDhGDx4MNq0aYPt27erHQ8REREREdGzyMTTDz5Tg127dg1yuRxHjx7F0KFDNR1OowgLC8OuXbvqfJEgPV/3799H8+bNUVRU9EI87z9k2hUkruug6TCIiIiIiP5yVKkN9JooJq2WmJiIkpISODk5oaCgAHPnzoWtrS0GDBig6dAarKSkBPn5+Vi7di2WLVum6XCIiIiIiIhIRRp9uZ+2qKiowIIFC+Do6AgvLy+0adMGycnJ0NfXx7Zt2xR+8u3JxdHRUdOh12n69OlwcXHBoEGDakzznzp16jPHNnXqVA1FTERERERERE/iVP/nrLi4GDdu3FC6T19fHzY2Nk0cUeO5efMm7t+/r3Sfubk5LC0tmzgi7cep/kREREREBHCq/wvFzMwMZmZmmg7jubC0tGRxT0RERERE9ILjVH8iIiIiIiIiLcbCn4iIiIiIiEiLsfAnIrXx+X4iIiIiohcfC38iIiIiIiIiLcbCn4iIiIiIiEiLsfAnIiIiIiIi0mIs/ImIiIiIiIi0GAt/IiIiIiIiIi3Gwp+IiIiIiIhIi7HwJyIiIiIiItJiLPyJiIiIiIiItBgLfyIiIiIiIiItxsKfiIiIiIiISIux8CciIiIiIiLSYiz8iYiIiIiIiLQYC38iIiIiIiIiLcbCn4iIiIiIiEiLsfAnIiIiIiIi0mIs/ImIiIiIiIi0GAt/IiIiIiIiIi3Gwp+IiIiIiIhIi7HwJyIiIiIiItJiepoOgIjqTwgBALh//76GIyEiIiIiIk16XBM8rhFqw8Kf6E+kuLgYACCXyzUcCRERERERvQiKi4vRvHnzWtvIRH0uDxDRC6G6uhrXr1+HmZkZZDKZpsPB/fv3IZfLcfXqVZibm2s6HGpkzK/2Y461G/Or/Zhj7cb8ar+G5lgIgeLiYrRr1w46OrU/xc87/kR/Ijo6OnjppZc0HUYN5ubm/AdJizG/2o851m7Mr/ZjjrUb86v9GpLjuu70P8aX+xERERERERFpMRb+RERERERERFqMhT8Rqc3Q0BChoaEwNDTUdCj0HDC/2o851m7Mr/ZjjrUb86v9mjLHfLkfERERERERkRbjHX8iIiIiIiIiLcbCn4iIiIiIiEiLsfAnIiIiIiIi0mIs/ImIiIiIiIi0GAt/IqrVl19+CVtbWxgZGeG1117Dzz//XGv7Xbt2oVu3bjAyMoKTkxMOHjzYRJGSOlTJ77lz5zB27FjY2tpCJpPh888/b7pASW2q5Hjz5s3o378/LCwsYGFhAXd39zr/zpNmqZLfPXv2wNXVFS1atICJiQl69uyJrVu3NmG0pA5V/x1+bMeOHZDJZPD09Hy+AVKDqJLf6OhoyGQyhcXIyKgJoyV1qPp3+N69ewgODoa1tTUMDQ3RtWvXRvn/NAt/InqmnTt34v3330doaChOnTqFHj16wMPDAzdv3lTa/qeffoKvry+CgoKQkZEBT09PeHp64uzZs00cOdWHqvktLS3Fyy+/jPDwcLRt27aJoyV1qJrj5ORk+Pr6IikpCSdOnIBcLsfrr7+O3377rYkjp/pQNb8tW7bERx99hBMnTuDMmTN4++238fbbbyM+Pr6JI6f6UjXHj+Xn52POnDno379/E0VK6lAnv+bm5igoKJCWX3/9tQkjJlWpmuPy8nIMGzYM+fn52L17N3JycrB582a0b9++4cEIIqJnePXVV0VwcLC0XlVVJdq1aydWrFihtL23t7d44403FLa99tprYsqUKc81TlKPqvl9ko2Njfjss8+eY3TUGBqSYyGEqKysFGZmZiImJuZ5hUgN0ND8CiGEs7OzWLhw4fMIjxqBOjmurKwU/fr1E//+979FYGCgGD16dBNESupQNb9RUVGiefPmTRQdNQZVc7x+/Xrx8ssvi/Ly8kaPhXf8iUip8vJynDx5Eu7u7tI2HR0duLu748SJE0o/c+LECYX2AODh4fHM9qQ56uSX/lwaI8elpaWoqKhAy5Ytn1eYpKaG5lcIgYSEBOTk5GDAgAHPM1RSk7o5Xrp0KSwtLREUFNQUYZKa1M1vSUkJbGxsIJfLMXr0aJw7d64pwiU1qJPjffv2oW/fvggODoaVlRVeeeUVLF++HFVVVQ2Oh4U/ESl1+/ZtVFVVwcrKSmG7lZUVCgsLlX6msLBQpfakOerkl/5cGiPH8+bNQ7t27Wpc0CPNUze/RUVFMDU1hYGBAd544w2sWbMGw4YNe97hkhrUyfGPP/6Ir776Cps3b26KEKkB1MmvnZ0dvv76a3z33Xf45ptvUF1djX79+uHatWtNETKpSJ0cX7p0Cbt370ZVVRUOHjyIRYsWITIyEsuWLWtwPHoN7oGIiIi0Tnh4OHbs2IHk5GS+PEqLmJmZITMzEyUlJUhISMD777+Pl19+GYMGDdJ0aNRAxcXF8Pf3x+bNm9G6dWtNh0PPQd++fdG3b19pvV+/frC3t8fGjRsRFhamwciosVRXV8PS0hKbNm2Crq4uXFxc8Ntvv+GTTz5BaGhog/pm4U9ESrVu3Rq6urq4ceOGwvYbN24888Vubdu2Vak9aY46+aU/l4bkOCIiAuHh4Th69Ci6d+/+PMMkNambXx0dHXTu3BkA0LNnT2RnZ2PFihUs/F9AquY4Ly8P+fn5GDVqlLSturoaAKCnp4ecnBx06tTp+QZN9dYY/w7r6+vD2dkZubm5zyNEaiB1cmxtbQ19fX3o6upK2+zt7VFYWIjy8nIYGBioHQ+n+hORUgYGBnBxcUFCQoK0rbq6GgkJCQpXm5/Ut29fhfYAcOTIkWe2J81RJ7/056JujletWoWwsDAcPnwYrq6uTREqqaGx/g5XV1ejrKzseYRIDaRqjrt164asrCxkZmZKy5tvvonBgwcjMzMTcrm8KcOnOjTG3+GqqipkZWXB2tr6eYVJDaBOjt3c3JCbmytdtAOACxcuwNraukFFPwC+1Z+Inm3Hjh3C0NBQREdHi/Pnz4vJkyeLFi1aiMLCQiGEEP7+/mL+/PlS+5SUFKGnpyciIiJEdna2CA0NFfr6+iIrK0tTQ6BaqJrfsrIykZGRITIyMoS1tbWYM2eOyMjIEBcvXtTUEKgOquY4PDxcGBgYiN27d4uCggJpKS4u1tQQqBaq5nf58uXi+++/F3l5eeL8+fMiIiJC6Onpic2bN2tqCFQHVXP8NL7V/8Wman6XLFki4uPjRV5enjh58qT4xz/+IYyMjMS5c+c0NQSqg6o5vnLlijAzMxPTp08XOTk5Yv/+/cLS0lIsW7aswbFwqj8RPZOPjw9u3bqFjz/+GIWFhejZsycOHz4svaTkypUr0NH5v4lD/fr1w7fffouFCxdiwYIF6NKlC+Li4vDKK69oaghUC1Xze/36dTg7O0vrERERiIiIwMCBA5GcnNzU4VM9qJrj9evXo7y8HOPGjVPoJzQ0FIsXL27K0KkeVM3vgwcPMG3aNFy7dg3Gxsbo1q0bvvnmG/j4+GhqCFQHVXNMfy6q5vfu3buYNGkSCgsLYWFhARcXF/z0009wcHDQ1BCoDqrmWC6XIz4+HrNmzUL37t3Rvn17zJw5E/PmzWtwLDIhhGhwL0RERERERET0QuIlQiIiIiIiIiItxsKfiIiIiIiISIux8CciIiIiIiLSYiz8iYiIiIiIiLQYC38iIiIiIiIiLcbCn4iIiIiIiEiLsfAnIiIiIiIi0mIs/ImIiIiIiIi0GAt/IiIi+tNKTk6GTCbDvXv3AADR0dFo0aLFcz3mhAkT4Onp+VyPQURE1JhY+BMREREmTJgAmUyG8PBwhe1xcXGQyWQaikp1Pj4+uHDhgkZjeHwx4ull4cKFjXYMmUyGuLi4RutPVU9fcHkRDRo0CCEhIZoOg4johaCn6QCIiIjoxWBkZISVK1diypQpsLCwaLR+y8vLYWBg0Gj91cbY2BjGxsZNcqy65OTkwNzcXFo3NTXVYDTKNWVumoo2jomIqKF4x5+IiIgAAO7u7mjbti1WrFhRa7vY2Fg4OjrC0NAQtra2iIyMVNhva2uLsLAwBAQEwNzcHJMnT5am4O/fvx92dnZo1qwZxo0bh9LSUsTExMDW1hYWFhaYMWMGqqqqpL62bt0KV1dXmJmZoW3btnjrrbdw8+bNZ8b29FR/W1tbpXffH7t69Sq8vb3RokULtGzZEqNHj0Z+fr60v6qqCu+//z5atGiBVq1aYe7cuRBC1Ot8Wlpaom3bttLyuPCv65hpaWkYNmwYWrdujebNm2PgwIE4deqUwpgAwMvLCzKZTFpX9ghCSEgIBg0aJK0PGjQI06dPR0hICFq3bg0PDw8AwNmzZzFixAiYmprCysoK/v7+uH37dr3GCUDt/D7+rvj6+sLExATt27fHl19+qdD3lStXMHr0aJiamsLc3Bze3t64ceOGtH/x4sXo2bMn/v3vf6Njx44wMjLChAkTcOzYMaxevVrKeX5+PqqqqhAUFISOHTvC2NgYdnZ2WL16tcLxHp/HiIgIWFtbo1WrVggODkZFRYXUpqysDPPmzYNcLoehoSE6d+6Mr776Strf0PNJRNTYWPgTERERAEBXVxfLly/HmjVrcO3aNaVtTp48CW9vb/zjH/9AVlYWFi9ejEWLFiE6OlqhXUREBHr06IGMjAwsWrQIAFBaWoovvvgCO3bswOHDh5GcnAwvLy8cPHgQBw8exNatW7Fx40bs3r1b6qeiogJhYWE4ffo04uLikJ+fjwkTJtR7TGlpaSgoKEBBQQGuXbuGPn36oH///lLfHh4eMDMzww8//ICUlBSYmppi+PDhKC8vBwBERkYiOjoaX3/9NX788UfcuXMHe/fuVeGsKqrPMYuLixEYGIgff/wR//vf/9ClSxeMHDkSxcXF0pgAICoqCgUFBdJ6fcXExMDAwAApKSnYsGED7t27hyFDhsDZ2Rnp6ek4fPgwbty4AW9vb5X6VSe/APDJJ59I35X58+dj5syZOHLkCACguroao0ePxp07d3Ds2DEcOXIEly5dgo+Pj0Ifubm5iI2NxZ49e5CZmYnVq1ejb9++mDRpkpR/uVyO6upqvPTSS9i1axfOnz+Pjz/+GAsWLMB//vMfhf6SkpKQl5eHpKQkxMTEIDo6WuE7HhAQgO3bt+OLL75AdnY2Nm7cKF3YaazzSUTUqAQRERH95QUGBorRo0cLIYTo06ePmDhxohBCiL1794on/7vw1ltviWHDhil89oMPPhAODg7Suo2NjfD09FRoExUVJQCI3NxcaduUKVNEs2bNRHFxsbTNw8NDTJky5ZlxpqWlCQDSZ5KSkgQAcffuXek4zZs3V/rZGTNmCBsbG3Hz5k0hhBBbt24VdnZ2orq6WmpTVlYmjI2NRXx8vBBCCGtra7Fq1Sppf0VFhXjppZekc6XM45hMTEwUltu3b9frmE+rqqoSZmZm4r///a+0DYDYu3evQrsnc/jYzJkzxcCBA6X1gQMHCmdnZ4U2YWFh4vXXX1fYdvXqVQFA5OTk1DrGJ8+7Ovm1sbERw4cPV+jbx8dHjBgxQgghxPfffy90dXXFlStXpP3nzp0TAMTPP/8shBAiNDRU6OvrS3l9cqwzZ85UGv+TgoODxdixY6X1wMBAYWNjIyorK6Vt48ePFz4+PkIIIXJycgQAceTIEaX9qXM+iYieN97xJyIiIgUrV65ETEwMsrOza+zLzs6Gm5ubwjY3NzdcvHhRYQq3q6trjc82a9YMnTp1ktatrKxga2ur8Oy7lZWVwlT+kydPYtSoUejQoQPMzMwwcOBAAI+mf6ti06ZN+Oqrr7Bv3z60adMGAHD69Gnk5ubCzMwMpqamMDU1RcuWLfHw4UPk5eWhqKgIBQUFeO2116R+9PT0lI5NmR9++AGZmZnSYmFhUecxAeDGjRuYNGkSunTpgubNm8Pc3BwlJSUqj/lZXFxcFNZPnz6NpKQkKR5TU1N069YNAKSY6kOd/AJA3759a6w//u5lZ2dDLpdDLpdL+x0cHNCiRQuF76eNjY2U17p8+eWXcHFxQZs2bWBqaopNmzbVOLeOjo7Q1dWV1q2traW4MzMzoaurK30Xn9ZY55OIqDHx5X5ERESkYMCAAfDw8MCHH36o0rT6J5mYmNTYpq+vr7Auk8mUbquurgYAPHjwAB4eHvDw8MC2bdvQpk0bXLlyBR4eHtK0+PpISkrCe++9h+3bt6N79+7S9pKSEri4uGDbtm01PlPfIrI2HTt2rPHTgvU5ZmBgIH7//XesXr0aNjY2MDQ0RN++fescs46OTo33Dzz5XPpjT+empKQEo0aNwsqVK2u0tba2rvWYT1I1v41J2fdNmR07dmDOnDmIjIxE3759YWZmhk8++QSpqakK7WqLu66XRzbW+SQiakws/ImIiKiG8PBw9OzZE3Z2dgrb7e3tkZKSorAtJSUFXbt2VbhD2hh++eUX/P777wgPD5fu+Kanp6vUR25uLsaNG4cFCxZgzJgxCvt69eqFnTt3wtLSUuHt+0+ytrZGamoqBgwYAACorKzEyZMn0atXLzVGVL9jpqSkYN26dRg5ciSARy8DfPrFcPr6+gozLIBHFw7Onj2rsC0zM7NGEassptjYWNja2kJPr+n/a/i///2vxrq9vT2AR9+3q1ev4urVq9J34Pz587h37x4cHBxq7dfAwKDGOUpJSUG/fv0wbdo0aZuqd+GdnJxQXV2NY8eOwd3dvcZ+TZ9PIiJlONWfiIiIanBycoKfnx+++OILhe2zZ89GQkICwsLCcOHCBcTExGDt2rWYM2dOo8fQoUMHGBgYYM2aNbh06RL27duHsLCwen/+jz/+wKhRo+Ds7IzJkyejsLBQWgDAz88PrVu3xujRo/HDDz/g8uXLSE5OxowZM6SXG86cORPh4eGIi4vDL7/8gmnTpjXot+vrc8wuXbpg69atyM7ORmpqKvz8/GrcZba1tUVCQgIKCwtx9+5dAMCQIUOQnp6OLVu24OLFiwgNDa1xIUCZ4OBg3LlzB76+vkhLS0NeXh7i4+Px9ttv1yicn4eUlBSsWrUKFy5cwJdffoldu3Zh5syZAB790sTj7+KpU6fw888/IyAgAAMHDqzzkQtbW1ukpqYiPz8ft2/fRnV1Nbp06YL09HTEx8fjwoULWLRokcovR7S1tUVgYCAmTpyIuLg4KYePXxCo6fNJRKQMC38iIiJSaunSpTWmZffq1Qv/+c9/sGPHDrzyyiv4+OOPsXTpUrUfCahNmzZtEB0djV27dsHBwQHh4eGIiIio9+dv3LiBX375BQkJCWjXrh2sra2lBXj0TPrx48fRoUMHjBkzBvb29ggKCsLDhw+lu/GzZ8+Gv78/AgMDpanhXl5eao+pPsf86quvcPfuXfTq1Qv+/v6YMWMGLC0tFfqJjIzEkSNHIJfL4ezsDADw8PDAokWLMHfuXPTu3RvFxcUICAioM6Z27dohJSUFVVVVeP311+Hk5ISQkBC0aNECOjrP/7+Ks2fPRnp6OpydnbFs2TJ8+umn0s8MymQyfPfdd7CwsMCAAQPg7u6Ol19+GTt37qyz3zlz5kBXVxcODg7SYyJTpkzBmDFj4OPjg9deew2///67wt3/+lq/fj3GjRuHadOmoVu3bpg0aRIePHgAQPPnk4hIGZl4+mEwIiIiIqImYGtri5CQEISEhGg6FCIircbLjkRERERERERajIU/ERERERERkRbjVH8iIiIiIiIiLcY7/kRERERERERajIU/ERERERERkRZj4U9ERERERESkxVj4ExEREREREWkxFv5EREREREREWoyFPxEREREREZEWY+FPREREREREpMVY+BMRERERERFpsf8HfFwPxMBbrBAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age: 0.0029\n",
            "hypertension: 0.0000\n",
            "heart_disease: 0.0013\n",
            "bmi: 0.0025\n",
            "HbA1c_level: 0.5841\n",
            "blood_glucose_level: 0.4066\n",
            "gender_Male: 0.0003\n",
            "gender_Other: 0.0000\n",
            "smoking_history_current: 0.0001\n",
            "smoking_history_ever: -0.0001\n",
            "smoking_history_former: 0.0015\n",
            "smoking_history_never: 0.0011\n",
            "smoking_history_not current: -0.0003\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Compute feature importance on validation data\n",
        "feature_importance = compute_feature_importance(original_mlp, X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Get feature names if using pandas DataFrame\n",
        "feature_names = x_test.columns.tolist()\n",
        "\n",
        "# Plot normalized feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_names, feature_importance, color=\"royalblue\")\n",
        "plt.xlabel(\"Normalized Feature Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"MLP Feature Importance (Normalized)\")\n",
        "plt.gca().invert_yaxis()  # Highest importance at the top\n",
        "plt.show()\n",
        "\n",
        "for i in range(len(feature_names)):\n",
        "    print(f\"{feature_names[i]}: {feature_importance[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "id": "dZOV7FGTRTFn"
      },
      "outputs": [],
      "source": [
        "# Get reconstruction error by column\n",
        "\n",
        "with torch.no_grad():\n",
        "    reconstructed_train = autoencoder(X_train_tensor).cpu().numpy()\n",
        "    reconstructed_val = autoencoder(X_val_tensor).cpu().numpy()\n",
        "    reconstructed_test = autoencoder(X_test_tensor).cpu().numpy()\n",
        "\n",
        "\n",
        "reconstructed_errors_train = np.abs((x_train.values - reconstructed_train) / (x_train.values+1e-8)) * 100\n",
        "reconstructed_errors_val = np.abs((x_val.values - reconstructed_val) / (x_val.values+1e-8)) * 100\n",
        "reconstructed_errors_test = np.abs((x_test.values - reconstructed_test) / (x_test.values+1e-8)) * 100\n",
        "\n",
        "# Normalize based on column\n",
        "# x_scaler = StandardScaler()\n",
        "# x_scaler = RobustScaler()\n",
        "x_scaler = MinMaxScaler()\n",
        "\n",
        "reconstructed_train_normalized = x_scaler.fit_transform(reconstructed_errors_train)\n",
        "reconstructed_val_normalized = x_scaler.transform(reconstructed_errors_val)\n",
        "reconstructed_test_normalized = x_scaler.transform(reconstructed_errors_test)\n",
        "\n",
        "# only keep the 4th and 5th columns of the normalized reconstruction errors\n",
        "reconstructed_train_normalized = reconstructed_train_normalized[:, 4:6]\n",
        "reconstructed_val_normalized = reconstructed_val_normalized[:, 4:6]\n",
        "reconstructed_test_normalized = reconstructed_test_normalized[:, 4:6]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "id": "y77G6JusRTFo"
      },
      "outputs": [],
      "source": [
        "# Get MLP prediction\n",
        "with torch.no_grad():\n",
        "    y_pred_train = original_mlp(X_train_tensor).cpu().numpy().flatten()\n",
        "    y_pred_val = original_mlp(X_val_tensor).cpu().numpy().flatten()\n",
        "    y_pred_test = original_mlp(X_test_tensor).cpu().numpy().flatten()\n",
        "\n",
        "ground_truth_train = y_train.values\n",
        "ground_truth_val = y_val.values\n",
        "ground_truth_test = y_test.values\n",
        "\n",
        "bias_train = ground_truth_train - y_pred_train\n",
        "bias_val = ground_truth_val - y_pred_val\n",
        "bias_test = ground_truth_test - y_pred_test\n",
        "\n",
        "# y_scaler = StandardScaler()\n",
        "# y_scaler = RobustScaler()\n",
        "y_scaler = MinMaxScaler()\n",
        "\n",
        "bias_train_normalized = y_scaler.fit_transform(bias_train.reshape(-1, 1)).flatten()\n",
        "bias_val_normalized = y_scaler.transform(bias_val.reshape(-1, 1)).flatten()\n",
        "bias_test_normalized = y_scaler.transform(bias_test.reshape(-1, 1)).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "TCmg3iH2RTFo"
      },
      "outputs": [],
      "source": [
        "# def train_bias_predictor(features, target_bias, val_features, val_target_bias,\n",
        "#                          epochs=1000, learning_rate=0.0001, patience=10,\n",
        "#                          input_dim=2, hidden_size=128, dropout=0.2, batch_size=32):\n",
        "\n",
        "#     model = BiasPredictor(input_dim=input_dim, hidden_size=hidden_size, dropout=dropout).to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#     criterion = nn.MSELoss()\n",
        "\n",
        "#     best_val_loss = float('inf')\n",
        "#     patience_counter = 0\n",
        "#     best_model_state = None\n",
        "\n",
        "#     # Reshape target_bias to (batch_size, 1)\n",
        "#     target_bias = target_bias.view(-1, 1)\n",
        "#     val_target_bias = val_target_bias.view(-1, 1)\n",
        "\n",
        "#     # Create DataLoaders\n",
        "#     train_dataset = TensorDataset(features, target_bias)\n",
        "#     val_dataset = TensorDataset(val_features, val_target_bias)\n",
        "\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         total_train_loss = 0.0\n",
        "\n",
        "#         # Training loop\n",
        "#         for batch_features, batch_targets in train_loader:\n",
        "#             batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             predictions = model(batch_features).squeeze()\n",
        "#             loss = criterion(predictions, batch_targets)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             total_train_loss += loss.item()\n",
        "\n",
        "#         # Validation loop\n",
        "#         model.eval()\n",
        "#         total_val_loss = 0.0\n",
        "#         with torch.no_grad():\n",
        "#             for val_batch_features, val_batch_targets in val_loader:\n",
        "#                 val_batch_features, val_batch_targets = val_batch_features.to(device), val_batch_targets.to(device)\n",
        "\n",
        "#                 val_predictions = model(val_batch_features).squeeze()\n",
        "#                 val_loss = criterion(val_predictions, val_batch_targets)\n",
        "\n",
        "#                 total_val_loss += val_loss.item()\n",
        "\n",
        "#         avg_train_loss = total_train_loss / len(train_loader)\n",
        "#         avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "#         # Early stopping check\n",
        "#         if avg_val_loss < best_val_loss:\n",
        "#             best_val_loss = avg_val_loss\n",
        "#             patience_counter = 0\n",
        "#             best_model_state = model.state_dict()  # Save best model\n",
        "#         else:\n",
        "#             patience_counter += 1\n",
        "\n",
        "#         # Print progress\n",
        "#         print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "#         if patience_counter >= patience:\n",
        "#             print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
        "#             break\n",
        "\n",
        "#     # Load best model before returning\n",
        "#     if best_model_state is not None:\n",
        "#         model.load_state_dict(best_model_state)\n",
        "\n",
        "#     return model\n",
        "\n",
        "class WeightedMSELoss(nn.Module):\n",
        "    def __init__(self, scale_factor=2.0):\n",
        "        super(WeightedMSELoss, self).__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        weights = 1 + self.scale_factor * torch.abs(targets)  # Higher weight for large biases\n",
        "        return torch.mean(weights * (predictions - targets) ** 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_bias_predictor(features, target_bias, val_features, val_target_bias,\n",
        "                         epochs=1000, learning_rate=0.0001, patience=10,\n",
        "                         input_dim=2, hidden_size=128, dropout=0.2, batch_size=32, scale_factor=1.25):\n",
        "\n",
        "    model = BiasPredictor(input_dim=input_dim, hidden_size=hidden_size, dropout=dropout).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    # criterion = nn.MSELoss()\n",
        "    criterion = WeightedMSELoss(scale_factor=scale_factor)\n",
        "\n",
        "    # Reshape target_bias to (batch_size, 1)\n",
        "    target_bias = target_bias.view(-1, 1)\n",
        "    val_target_bias = val_target_bias.view(-1, 1)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataset = TensorDataset(features, target_bias)\n",
        "    val_dataset = TensorDataset(val_features, val_target_bias)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for batch_features, batch_targets in train_loader:\n",
        "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(batch_features)  # Ensure output shape is (batch_size, 1)\n",
        "            loss = criterion(predictions, batch_targets.view(-1, 1))  # Match shapes\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for val_batch_features, val_batch_targets in val_loader:\n",
        "                val_batch_features, val_batch_targets = val_batch_features.to(device), val_batch_targets.to(device)\n",
        "\n",
        "                val_predictions = model(val_batch_features)  # Ensure correct shape\n",
        "                val_loss = criterion(val_predictions, val_batch_targets.view(-1, 1))  # Match shapes\n",
        "\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict()  # Save best model\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
        "            break\n",
        "\n",
        "    # Load best model before returning\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "id": "vGZTycOGRTFp"
      },
      "outputs": [],
      "source": [
        "reconstructed_train_tensor = torch.tensor(reconstructed_train_normalized, dtype=torch.float32).to(device)\n",
        "bias_train_tensor = torch.tensor(bias_train_normalized, dtype=torch.float32).to(device)\n",
        "bias_train_tensor = bias_train_tensor.view(-1,1)\n",
        "\n",
        "reconstructed_val_tensor = torch.tensor(reconstructed_val_normalized, dtype=torch.float32).to(device)\n",
        "bias_val_tensor = torch.tensor(bias_val_normalized, dtype=torch.float32).to(device)\n",
        "bias_val_tensor = bias_val_tensor.view(-1,1)\n",
        "\n",
        "# model = train_bias_predictor(\n",
        "#     reconstructed_train_tensor, bias_train_tensor,\n",
        "#     reconstructed_val_tensor, bias_val_tensor,\n",
        "#     epochs=1000, learning_rate=0.0005, patience=10,\n",
        "#     input_dim=2, hidden_size=64, dropout=0.2, batch_size=32\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "Mhz_rXzHRTFp"
      },
      "outputs": [],
      "source": [
        "# model.eval()\n",
        "\n",
        "# # try on test set\n",
        "# reconstructed_test_tensor = torch.tensor(reconstructed_test_normalized, dtype=torch.float32).to(device)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     bias_test_pred = model(reconstructed_test_tensor).cpu().numpy().flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "ZPDD_LfIRTFq"
      },
      "outputs": [],
      "source": [
        "# bias_test_pred_unscaled = y_scaler.inverse_transform(bias_test_pred.reshape(-1, 1)).flatten()\n",
        "\n",
        "# bias_test_pred_unscaled_scaled = bias_test_pred_unscaled\n",
        "# # bias_test_pred_unscaled_scaled = np.clip(bias_test_pred_unscaled_scaled, -0.5, 0.5)\n",
        "# corrected_probs_test = y_pred_test + bias_test_pred_unscaled_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "id": "JqIavc2xRTFq"
      },
      "outputs": [],
      "source": [
        "# # convert corrected prob test to binary\n",
        "# corrected_predictions_test = (corrected_probs_test > 0.5).astype(int)\n",
        "\n",
        "# # Calculate accuracy\n",
        "# corrected_accuracy_test = accuracy_score(ground_truth_test, corrected_predictions_test)\n",
        "# print(f\"Corrected Accuracy on Test Set: {corrected_accuracy_test * 100:.2f}%\")\n",
        "\n",
        "# original_accuracy_test = accuracy_score(ground_truth_test, (y_pred_test > 0.5).astype(int))\n",
        "# print(f\"Original Accuracy on Test Set: {original_accuracy_test * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "id": "SxbH9DZPRTFq"
      },
      "outputs": [],
      "source": [
        "# conf_matrix_original_test = confusion_matrix(y_test, (original_probs_test > 0.5).astype(int))\n",
        "\n",
        "# tn, fp, fn, tp = conf_matrix_original_test.ravel()\n",
        "# recall_original = tp / (tp + fn)\n",
        "# precision_original = tp / (tp + fp)\n",
        "# f1_score_original = 2 * (precision_original * recall_original) / (precision_original + recall_original)\n",
        "# print(\"Original:\")\n",
        "# print(\"True Positives: \", tp)\n",
        "# print(\"True Negatives: \", tn)\n",
        "# print(\"False Positives: \", fp)\n",
        "# print(\"False Negatives: \", fn)\n",
        "# print(\"Recall: \", recall_original)\n",
        "# print(\"Precision: \", precision_original)\n",
        "# print(\"F1 Score: \", f1_score_original)\n",
        "\n",
        "\n",
        "# conf_matrix_corrected_test = confusion_matrix(y_test, corrected_predictions_test)\n",
        "\n",
        "# tn, fp, fn, tp = conf_matrix_corrected_test.ravel()\n",
        "# recall_corrected = tp / (tp + fn)\n",
        "# precision_corrected = tp / (tp + fp)\n",
        "# f1_score_corrected = 2 * (precision_corrected * recall_corrected) / (precision_corrected + recall_corrected)\n",
        "# print(\"\\n\\nCorrected:\")\n",
        "# print(\"True Positives: \", tp)\n",
        "# print(\"True Negatives: \", tn)\n",
        "# print(\"False Positives: \", fp)\n",
        "# print(\"False Negatives: \", fn)\n",
        "# print(\"Recall: \", recall_corrected)\n",
        "# print(\"Precision: \", precision_corrected)\n",
        "# print(\"F1 Score: \", f1_score_corrected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx-baSAkRTFq"
      },
      "source": [
        "### Trying bias correction with combined reconstruction errors (percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "id": "Erndbtv6RTFr"
      },
      "outputs": [],
      "source": [
        "x_train, x_temp, y_train, y_temp = train_test_split(x_scaled, y, test_size=0.3, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC9A4oE4Squs",
        "outputId": "28b3f506-c466-4261-ab64-5e33f0dc31b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-317-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 0 1 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 0 1 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 1 0 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 1 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 1 0 ... 0 0 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 0 1 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-317-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n"
          ]
        }
      ],
      "source": [
        "x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
        "x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
        "x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {
        "id": "DXT-UmWkRTFr"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "X_val_tensor = torch.tensor(x_val.values, dtype=torch.float32).to(device)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "X_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {
        "id": "ApwTmkqjRTFr"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    reconstructed_train = autoencoder(X_train_tensor).cpu().numpy()\n",
        "    reconstructed_val = autoencoder(X_val_tensor).cpu().numpy()\n",
        "    reconstructed_test = autoencoder(X_test_tensor).cpu().numpy()\n",
        "\n",
        "temp_train = np.abs((x_train.values - reconstructed_train) / (x_train.values+1e-8)) * 100\n",
        "temp_val = np.abs((x_val.values - reconstructed_val) / (x_val.values+1e-8)) * 100\n",
        "temp_test = np.abs((x_test.values - reconstructed_test) / (x_test.values+1e-8)) * 100\n",
        "\n",
        "\n",
        "# get mean per for each row of temp\n",
        "reconstruction_percent_train = np.mean(temp_train, axis=1)\n",
        "reconstruction_percent_val = np.mean(temp_val, axis=1)\n",
        "reconstruction_percent_test = np.mean(temp_test, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {
        "id": "qn2-X3lQRTFr"
      },
      "outputs": [],
      "source": [
        "x_scaler = StandardScaler()\n",
        "# x_scaler = RobustScaler()\n",
        "# x_scaler = MinMaxScaler()\n",
        "\n",
        "reconstruction_percent_train_normalized = x_scaler.fit_transform(reconstruction_percent_train.reshape(-1, 1)).flatten()\n",
        "reconstruction_percent_val_normalized = x_scaler.transform(reconstruction_percent_val.reshape(-1, 1)).flatten()\n",
        "reconstruction_percent_test_normalized = x_scaler.transform(reconstruction_percent_test.reshape(-1, 1)).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "id": "iqlgDCXgRTFs"
      },
      "outputs": [],
      "source": [
        "# Get MLP prediction\n",
        "with torch.no_grad():\n",
        "    y_pred_train = original_mlp(X_train_tensor).cpu().numpy().flatten()\n",
        "    y_pred_val = original_mlp(X_val_tensor).cpu().numpy().flatten()\n",
        "    y_pred_test = original_mlp(X_test_tensor).cpu().numpy().flatten()\n",
        "\n",
        "ground_truth_train = y_train.values\n",
        "ground_truth_val = y_val.values\n",
        "ground_truth_test = y_test.values\n",
        "\n",
        "bias_train = ground_truth_train - y_pred_train\n",
        "bias_val = ground_truth_val - y_pred_val\n",
        "bias_test = ground_truth_test - y_pred_test\n",
        "\n",
        "# y_scaler = StandardScaler()\n",
        "# y_scaler = RobustScaler()\n",
        "y_scaler = StandardScaler()\n",
        "\n",
        "bias_train_normalized = y_scaler.fit_transform(bias_train.reshape(-1, 1)).flatten()\n",
        "bias_val_normalized = y_scaler.transform(bias_val.reshape(-1, 1)).flatten()\n",
        "bias_test_normalized = y_scaler.transform(bias_test.reshape(-1, 1)).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {
        "id": "ELvYsI65RTFs"
      },
      "outputs": [],
      "source": [
        "reconstructed_train_tensor = torch.tensor(reconstruction_percent_train_normalized, dtype=torch.float32).view(-1,1).to(device)\n",
        "bias_train_tensor = torch.tensor(bias_train_normalized, dtype=torch.float32).to(device)\n",
        "bias_train_tensor = bias_train_tensor.view(-1,1)\n",
        "\n",
        "reconstructed_val_tensor = torch.tensor(reconstruction_percent_val_normalized, dtype=torch.float32).view(-1,1).to(device)\n",
        "bias_val_tensor = torch.tensor(bias_val_normalized, dtype=torch.float32).to(device)\n",
        "bias_val_tensor = bias_val_tensor.view(-1,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "id": "9034IrH6RTFs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# model = train_bias_predictor(\n",
        "#     reconstructed_train_tensor, bias_train_tensor,\n",
        "#     reconstructed_val_tensor, bias_val_tensor,\n",
        "#     epochs=1000, learning_rate=0.01, patience=10,\n",
        "#     input_dim=1, hidden_size=64, dropout=0.1, batch_size=128, scale_factor=1.25\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {
        "id": "wFq8ri4jRTFs"
      },
      "outputs": [],
      "source": [
        "# model.eval()\n",
        "\n",
        "# # try on test set\n",
        "reconstructed_test_tensor = torch.tensor(reconstruction_percent_test_normalized, dtype=torch.float32).view(-1,1).to(device)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     bias_test_pred = model(reconstructed_test_tensor).cpu().numpy().flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "id": "ZCtaI0zERTFt"
      },
      "outputs": [],
      "source": [
        "# bias_test_pred_unscaled = y_scaler.inverse_transform(bias_test_pred.reshape(-1, 1)).flatten()\n",
        "\n",
        "# corrected_probs_test = y_pred_test + bias_test_pred_unscaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "id": "HCyPsLyiRTFt"
      },
      "outputs": [],
      "source": [
        "# # convert corrected prob test to binary\n",
        "# corrected_predictions_test = (corrected_probs_test > 0.5).astype(int)\n",
        "\n",
        "# # Calculate accuracy\n",
        "# corrected_accuracy_test = accuracy_score(ground_truth_test, corrected_predictions_test)\n",
        "# print(f\"Corrected Accuracy on Test Set: {corrected_accuracy_test * 100:.2f}%\")\n",
        "\n",
        "# original_accuracy_test = accuracy_score(ground_truth_test, (y_pred_test > 0.5).astype(int))\n",
        "# print(f\"Original Accuracy on Test Set: {original_accuracy_test * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {
        "id": "_2xZTXJfRTFt"
      },
      "outputs": [],
      "source": [
        "# conf_matrix_original_test = confusion_matrix(y_test, (original_probs_test > 0.5).astype(int))\n",
        "\n",
        "# tn, fp, fn, tp = conf_matrix_original_test.ravel()\n",
        "# recall_original = tp / (tp + fn)\n",
        "# precision_original = tp / (tp + fp)\n",
        "# f1_score_original = 2 * (precision_original * recall_original) / (precision_original + recall_original)\n",
        "# print(\"Original:\")\n",
        "# print(\"True Positives: \", tp)\n",
        "# print(\"True Negatives: \", tn)\n",
        "# print(\"False Positives: \", fp)\n",
        "# print(\"False Negatives: \", fn)\n",
        "# print(\"Recall: \", recall_original)\n",
        "# print(\"Precision: \", precision_original)\n",
        "# print(\"F1 Score: \", f1_score_original)\n",
        "\n",
        "\n",
        "# conf_matrix_corrected_test = confusion_matrix(y_test, corrected_predictions_test)\n",
        "\n",
        "# tn, fp, fn, tp = conf_matrix_corrected_test.ravel()\n",
        "# recall_corrected = tp / (tp + fn)\n",
        "# precision_corrected = tp / (tp + fp)\n",
        "# f1_score_corrected = 2 * (precision_corrected * recall_corrected) / (precision_corrected + recall_corrected)\n",
        "# print(\"\\n\\nCorrected:\")\n",
        "# print(\"True Positives: \", tp)\n",
        "# print(\"True Negatives: \", tn)\n",
        "# print(\"False Positives: \", fp)\n",
        "# print(\"False Negatives: \", fn)\n",
        "# print(\"Recall: \", recall_corrected)\n",
        "# print(\"Precision: \", precision_corrected)\n",
        "# print(\"F1 Score: \", f1_score_corrected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "id": "YOHlTi2qRTFu"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "\n",
        "# # Histogram\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.hist(y_pred_train, bins=50, edgecolor='black', alpha=0.7)\n",
        "# plt.xlabel(\"Bias Values\")\n",
        "# plt.ylabel(\"Frequency\")\n",
        "# plt.title(\"Histogram of bias_train\")\n",
        "\n",
        "# # Box Plot\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.boxplot(y_pred_train, vert=False)\n",
        "# plt.xlabel(\"Bias Values\")\n",
        "# plt.title(\"Box Plot of bias_train\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {
        "id": "_jI4kg41RTFu"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, reconstructed_test_tensor, y_pred_test, ground_truth_test):\n",
        "    \"\"\"Runs evaluation on the test set and returns accuracy, recall, precision, F1 score\"\"\"\n",
        "    with torch.no_grad():\n",
        "        bias_test_pred = model(reconstructed_test_tensor).cpu().numpy().flatten()\n",
        "\n",
        "    bias_test_pred_unscaled = y_scaler.inverse_transform(bias_test_pred.reshape(-1, 1)).flatten()\n",
        "    corrected_probs_test = y_pred_test + bias_test_pred_unscaled\n",
        "    corrected_predictions_test = (corrected_probs_test > 0.5).astype(int)\n",
        "\n",
        "    # Compute metrics\n",
        "    original_accuracy = accuracy_score(ground_truth_test, (y_pred_test > 0.5).astype(int))\n",
        "    corrected_accuracy = accuracy_score(ground_truth_test, corrected_predictions_test)\n",
        "\n",
        "    conf_matrix_original = confusion_matrix(ground_truth_test, (y_pred_test > 0.5).astype(int))\n",
        "    tn, fp, fn, tp = conf_matrix_original.ravel()\n",
        "    recall_original = tp / (tp + fn)\n",
        "    precision_original = tp / (tp + fp)\n",
        "    f1_score_original = 2 * (precision_original * recall_original) / (precision_original + recall_original)\n",
        "    tn_ori, fp_ori, fn_ori, tp_ori = conf_matrix_original.ravel()\n",
        "\n",
        "    conf_matrix_corrected = confusion_matrix(ground_truth_test, corrected_predictions_test)\n",
        "    tn, fp, fn, tp = conf_matrix_corrected.ravel()\n",
        "    recall_corrected = tp / (tp + fn)\n",
        "    precision_corrected = tp / (tp + fp)\n",
        "    f1_score_corrected = 2 * (precision_corrected * recall_corrected) / (precision_corrected + recall_corrected)\n",
        "    tn_corr, fp_corr, fn_corr, tp_corr = conf_matrix_corrected.ravel()\n",
        "\n",
        "    return (original_accuracy, corrected_accuracy, recall_original, precision_original, f1_score_original, tn_ori, fp_ori, fn_ori, tp_ori,\n",
        "            recall_corrected, precision_corrected, f1_score_corrected, tn_corr, fp_corr, fn_corr, tp_corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh47t19tRTFu",
        "outputId": "76eda18d-4466-48fa-d440-faaa8b1ef7f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 10 trials with Best LR=0.005, Hidden=16, Dropout=0.3, Scale Factor=1.3\n",
            "Trial 1/10\n",
            "Epoch 1/1000, Train Loss: 6.8447, Val Loss: 6.8082\n",
            "Epoch 2/1000, Train Loss: 6.8225, Val Loss: 6.8185\n",
            "Epoch 3/1000, Train Loss: 6.8351, Val Loss: 6.8078\n",
            "Epoch 4/1000, Train Loss: 6.8240, Val Loss: 6.8225\n",
            "Epoch 5/1000, Train Loss: 6.8292, Val Loss: 6.7991\n",
            "Epoch 6/1000, Train Loss: 6.8316, Val Loss: 6.8117\n",
            "Epoch 7/1000, Train Loss: 6.8191, Val Loss: 6.8029\n",
            "Epoch 8/1000, Train Loss: 6.8305, Val Loss: 6.8035\n",
            "Epoch 9/1000, Train Loss: 6.8282, Val Loss: 6.7932\n",
            "Epoch 10/1000, Train Loss: 6.8232, Val Loss: 6.8013\n",
            "Epoch 11/1000, Train Loss: 6.8230, Val Loss: 6.8035\n",
            "Epoch 12/1000, Train Loss: 6.8299, Val Loss: 6.8000\n",
            "Epoch 13/1000, Train Loss: 6.8297, Val Loss: 6.8001\n",
            "Epoch 14/1000, Train Loss: 6.8367, Val Loss: 6.8029\n",
            "Epoch 15/1000, Train Loss: 6.8273, Val Loss: 6.8117\n",
            "Epoch 16/1000, Train Loss: 6.8209, Val Loss: 6.8055\n",
            "Epoch 17/1000, Train Loss: 6.8216, Val Loss: 6.8027\n",
            "Epoch 18/1000, Train Loss: 6.8216, Val Loss: 6.8044\n",
            "Epoch 19/1000, Train Loss: 6.8238, Val Loss: 6.7996\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 6.7932\n",
            "Trial 2/10\n",
            "Epoch 1/1000, Train Loss: 6.8625, Val Loss: 6.8034\n",
            "Epoch 2/1000, Train Loss: 6.8225, Val Loss: 6.8040\n",
            "Epoch 3/1000, Train Loss: 6.8294, Val Loss: 6.7993\n",
            "Epoch 4/1000, Train Loss: 6.8375, Val Loss: 6.8008\n",
            "Epoch 5/1000, Train Loss: 6.8350, Val Loss: 6.7988\n",
            "Epoch 6/1000, Train Loss: 6.8228, Val Loss: 6.8109\n",
            "Epoch 7/1000, Train Loss: 6.8238, Val Loss: 6.7992\n",
            "Epoch 8/1000, Train Loss: 6.8328, Val Loss: 6.8014\n",
            "Epoch 9/1000, Train Loss: 6.8276, Val Loss: 6.7992\n",
            "Epoch 10/1000, Train Loss: 6.8324, Val Loss: 6.8009\n",
            "Epoch 11/1000, Train Loss: 6.8267, Val Loss: 6.8050\n",
            "Epoch 12/1000, Train Loss: 6.8282, Val Loss: 6.7951\n",
            "Epoch 13/1000, Train Loss: 6.8233, Val Loss: 6.8052\n",
            "Epoch 14/1000, Train Loss: 6.8302, Val Loss: 6.8006\n",
            "Epoch 15/1000, Train Loss: 6.8300, Val Loss: 6.8012\n",
            "Epoch 16/1000, Train Loss: 6.8276, Val Loss: 6.8056\n",
            "Epoch 17/1000, Train Loss: 6.8247, Val Loss: 6.8022\n",
            "Epoch 18/1000, Train Loss: 6.8257, Val Loss: 6.8022\n",
            "Epoch 19/1000, Train Loss: 6.8237, Val Loss: 6.8036\n",
            "Epoch 20/1000, Train Loss: 6.8200, Val Loss: 6.8037\n",
            "Epoch 21/1000, Train Loss: 6.8205, Val Loss: 6.8057\n",
            "Epoch 22/1000, Train Loss: 6.8267, Val Loss: 6.8028\n",
            "Early stopping triggered at epoch 22. Best Val Loss: 6.7951\n",
            "Trial 3/10\n",
            "Epoch 1/1000, Train Loss: 6.8480, Val Loss: 6.8025\n",
            "Epoch 2/1000, Train Loss: 6.8286, Val Loss: 6.8057\n",
            "Epoch 3/1000, Train Loss: 6.8259, Val Loss: 6.8141\n",
            "Epoch 4/1000, Train Loss: 6.8246, Val Loss: 6.7973\n",
            "Epoch 5/1000, Train Loss: 6.8237, Val Loss: 6.8097\n",
            "Epoch 6/1000, Train Loss: 6.8235, Val Loss: 6.8002\n",
            "Epoch 7/1000, Train Loss: 6.8226, Val Loss: 6.8018\n",
            "Epoch 8/1000, Train Loss: 6.8318, Val Loss: 6.7990\n",
            "Epoch 9/1000, Train Loss: 6.8268, Val Loss: 6.8147\n",
            "Epoch 10/1000, Train Loss: 6.8308, Val Loss: 6.8063\n",
            "Epoch 11/1000, Train Loss: 6.8166, Val Loss: 6.8023\n",
            "Epoch 12/1000, Train Loss: 6.8190, Val Loss: 6.8084\n",
            "Epoch 13/1000, Train Loss: 6.8254, Val Loss: 6.7993\n",
            "Epoch 14/1000, Train Loss: 6.8207, Val Loss: 6.8032\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 6.7973\n",
            "Trial 4/10\n",
            "Epoch 1/1000, Train Loss: 6.8275, Val Loss: 6.8085\n",
            "Epoch 2/1000, Train Loss: 6.8358, Val Loss: 6.8038\n",
            "Epoch 3/1000, Train Loss: 6.8252, Val Loss: 6.8123\n",
            "Epoch 4/1000, Train Loss: 6.8287, Val Loss: 6.8094\n",
            "Epoch 5/1000, Train Loss: 6.8244, Val Loss: 6.8103\n",
            "Epoch 6/1000, Train Loss: 6.8322, Val Loss: 6.7988\n",
            "Epoch 7/1000, Train Loss: 6.8297, Val Loss: 6.7975\n",
            "Epoch 8/1000, Train Loss: 6.8377, Val Loss: 6.8029\n",
            "Epoch 9/1000, Train Loss: 6.8202, Val Loss: 6.8084\n",
            "Epoch 10/1000, Train Loss: 6.8255, Val Loss: 6.8181\n",
            "Epoch 11/1000, Train Loss: 6.8315, Val Loss: 6.8095\n",
            "Epoch 12/1000, Train Loss: 6.8178, Val Loss: 6.7976\n",
            "Epoch 13/1000, Train Loss: 6.8203, Val Loss: 6.8082\n",
            "Epoch 14/1000, Train Loss: 6.8278, Val Loss: 6.7975\n",
            "Epoch 15/1000, Train Loss: 6.8223, Val Loss: 6.7880\n",
            "Epoch 16/1000, Train Loss: 6.8258, Val Loss: 6.8012\n",
            "Epoch 17/1000, Train Loss: 6.8250, Val Loss: 6.7980\n",
            "Epoch 18/1000, Train Loss: 6.8306, Val Loss: 6.8050\n",
            "Epoch 19/1000, Train Loss: 6.8208, Val Loss: 6.8068\n",
            "Epoch 20/1000, Train Loss: 6.8245, Val Loss: 6.7972\n",
            "Epoch 21/1000, Train Loss: 6.8269, Val Loss: 6.8039\n",
            "Epoch 22/1000, Train Loss: 6.8259, Val Loss: 6.8034\n",
            "Epoch 23/1000, Train Loss: 6.8206, Val Loss: 6.8071\n",
            "Epoch 24/1000, Train Loss: 6.8282, Val Loss: 6.8050\n",
            "Epoch 25/1000, Train Loss: 6.8225, Val Loss: 6.8061\n",
            "Early stopping triggered at epoch 25. Best Val Loss: 6.7880\n",
            "Trial 5/10\n",
            "Epoch 1/1000, Train Loss: 6.8498, Val Loss: 6.8030\n",
            "Epoch 2/1000, Train Loss: 6.8251, Val Loss: 6.8005\n",
            "Epoch 3/1000, Train Loss: 6.8229, Val Loss: 6.8111\n",
            "Epoch 4/1000, Train Loss: 6.8292, Val Loss: 6.7999\n",
            "Epoch 5/1000, Train Loss: 6.8272, Val Loss: 6.8017\n",
            "Epoch 6/1000, Train Loss: 6.8278, Val Loss: 6.8034\n",
            "Epoch 7/1000, Train Loss: 6.8209, Val Loss: 6.7940\n",
            "Epoch 8/1000, Train Loss: 6.8276, Val Loss: 6.8056\n",
            "Epoch 9/1000, Train Loss: 6.8195, Val Loss: 6.7993\n",
            "Epoch 10/1000, Train Loss: 6.8220, Val Loss: 6.7978\n",
            "Epoch 11/1000, Train Loss: 6.8257, Val Loss: 6.7963\n",
            "Epoch 12/1000, Train Loss: 6.8237, Val Loss: 6.7969\n",
            "Epoch 13/1000, Train Loss: 6.8176, Val Loss: 6.8022\n",
            "Epoch 14/1000, Train Loss: 6.8280, Val Loss: 6.7999\n",
            "Epoch 15/1000, Train Loss: 6.8155, Val Loss: 6.8113\n",
            "Epoch 16/1000, Train Loss: 6.8211, Val Loss: 6.8026\n",
            "Epoch 17/1000, Train Loss: 6.8265, Val Loss: 6.8001\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 6.7940\n",
            "Trial 6/10\n",
            "Epoch 1/1000, Train Loss: 6.8609, Val Loss: 6.8031\n",
            "Epoch 2/1000, Train Loss: 6.8358, Val Loss: 6.8160\n",
            "Epoch 3/1000, Train Loss: 6.8281, Val Loss: 6.8020\n",
            "Epoch 4/1000, Train Loss: 6.8334, Val Loss: 6.7997\n",
            "Epoch 5/1000, Train Loss: 6.8299, Val Loss: 6.8059\n",
            "Epoch 6/1000, Train Loss: 6.8260, Val Loss: 6.8100\n",
            "Epoch 7/1000, Train Loss: 6.8278, Val Loss: 6.7992\n",
            "Epoch 8/1000, Train Loss: 6.8232, Val Loss: 6.8043\n",
            "Epoch 9/1000, Train Loss: 6.8257, Val Loss: 6.8006\n",
            "Epoch 10/1000, Train Loss: 6.8219, Val Loss: 6.8024\n",
            "Epoch 11/1000, Train Loss: 6.8297, Val Loss: 6.8042\n",
            "Epoch 12/1000, Train Loss: 6.8257, Val Loss: 6.8144\n",
            "Epoch 13/1000, Train Loss: 6.8291, Val Loss: 6.8008\n",
            "Epoch 14/1000, Train Loss: 6.8248, Val Loss: 6.8061\n",
            "Epoch 15/1000, Train Loss: 6.8203, Val Loss: 6.7982\n",
            "Epoch 16/1000, Train Loss: 6.8206, Val Loss: 6.8052\n",
            "Epoch 17/1000, Train Loss: 6.8277, Val Loss: 6.7858\n",
            "Epoch 18/1000, Train Loss: 6.8237, Val Loss: 6.7939\n",
            "Epoch 19/1000, Train Loss: 6.8245, Val Loss: 6.7989\n",
            "Epoch 20/1000, Train Loss: 6.8164, Val Loss: 6.7930\n",
            "Epoch 21/1000, Train Loss: 6.8250, Val Loss: 6.7981\n",
            "Epoch 22/1000, Train Loss: 6.8181, Val Loss: 6.8023\n",
            "Epoch 23/1000, Train Loss: 6.8277, Val Loss: 6.7954\n",
            "Epoch 24/1000, Train Loss: 6.8177, Val Loss: 6.8046\n",
            "Epoch 25/1000, Train Loss: 6.8245, Val Loss: 6.8041\n",
            "Epoch 26/1000, Train Loss: 6.8181, Val Loss: 6.8236\n",
            "Epoch 27/1000, Train Loss: 6.8154, Val Loss: 6.8002\n",
            "Early stopping triggered at epoch 27. Best Val Loss: 6.7858\n",
            "Trial 7/10\n",
            "Epoch 1/1000, Train Loss: 6.8386, Val Loss: 6.8071\n",
            "Epoch 2/1000, Train Loss: 6.8258, Val Loss: 6.8054\n",
            "Epoch 3/1000, Train Loss: 6.8207, Val Loss: 6.7998\n",
            "Epoch 4/1000, Train Loss: 6.8277, Val Loss: 6.8022\n",
            "Epoch 5/1000, Train Loss: 6.8268, Val Loss: 6.8058\n",
            "Epoch 6/1000, Train Loss: 6.8271, Val Loss: 6.8061\n",
            "Epoch 7/1000, Train Loss: 6.8238, Val Loss: 6.7980\n",
            "Epoch 8/1000, Train Loss: 6.8239, Val Loss: 6.8180\n",
            "Epoch 9/1000, Train Loss: 6.8238, Val Loss: 6.8110\n",
            "Epoch 10/1000, Train Loss: 6.8264, Val Loss: 6.7922\n",
            "Epoch 11/1000, Train Loss: 6.8288, Val Loss: 6.7921\n",
            "Epoch 12/1000, Train Loss: 6.8283, Val Loss: 6.7984\n",
            "Epoch 13/1000, Train Loss: 6.8277, Val Loss: 6.7972\n",
            "Epoch 14/1000, Train Loss: 6.8191, Val Loss: 6.8052\n",
            "Epoch 15/1000, Train Loss: 6.8215, Val Loss: 6.8052\n",
            "Epoch 16/1000, Train Loss: 6.8259, Val Loss: 6.7933\n",
            "Epoch 17/1000, Train Loss: 6.8216, Val Loss: 6.7980\n",
            "Epoch 18/1000, Train Loss: 6.8186, Val Loss: 6.7963\n",
            "Epoch 19/1000, Train Loss: 6.8353, Val Loss: 6.8003\n",
            "Epoch 20/1000, Train Loss: 6.8257, Val Loss: 6.7977\n",
            "Epoch 21/1000, Train Loss: 6.8294, Val Loss: 6.8019\n",
            "Early stopping triggered at epoch 21. Best Val Loss: 6.7921\n",
            "Trial 8/10\n",
            "Epoch 1/1000, Train Loss: 6.8412, Val Loss: 6.8002\n",
            "Epoch 2/1000, Train Loss: 6.8310, Val Loss: 6.7984\n",
            "Epoch 3/1000, Train Loss: 6.8178, Val Loss: 6.8039\n",
            "Epoch 4/1000, Train Loss: 6.8327, Val Loss: 6.8163\n",
            "Epoch 5/1000, Train Loss: 6.8294, Val Loss: 6.8054\n",
            "Epoch 6/1000, Train Loss: 6.8271, Val Loss: 6.8118\n",
            "Epoch 7/1000, Train Loss: 6.8282, Val Loss: 6.8039\n",
            "Epoch 8/1000, Train Loss: 6.8375, Val Loss: 6.8003\n",
            "Epoch 9/1000, Train Loss: 6.8251, Val Loss: 6.7929\n",
            "Epoch 10/1000, Train Loss: 6.8336, Val Loss: 6.8021\n",
            "Epoch 11/1000, Train Loss: 6.8329, Val Loss: 6.8076\n",
            "Epoch 12/1000, Train Loss: 6.8277, Val Loss: 6.7970\n",
            "Epoch 13/1000, Train Loss: 6.8220, Val Loss: 6.8096\n",
            "Epoch 14/1000, Train Loss: 6.8317, Val Loss: 6.8002\n",
            "Epoch 15/1000, Train Loss: 6.8285, Val Loss: 6.8143\n",
            "Epoch 16/1000, Train Loss: 6.8319, Val Loss: 6.8037\n",
            "Epoch 17/1000, Train Loss: 6.8244, Val Loss: 6.8047\n",
            "Epoch 18/1000, Train Loss: 6.8194, Val Loss: 6.8144\n",
            "Epoch 19/1000, Train Loss: 6.8281, Val Loss: 6.8055\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 6.7929\n",
            "Trial 9/10\n",
            "Epoch 1/1000, Train Loss: 6.8521, Val Loss: 6.8386\n",
            "Epoch 2/1000, Train Loss: 6.8196, Val Loss: 6.8061\n",
            "Epoch 3/1000, Train Loss: 6.8267, Val Loss: 6.8040\n",
            "Epoch 4/1000, Train Loss: 6.8316, Val Loss: 6.7980\n",
            "Epoch 5/1000, Train Loss: 6.8183, Val Loss: 6.8166\n",
            "Epoch 6/1000, Train Loss: 6.8231, Val Loss: 6.8073\n",
            "Epoch 7/1000, Train Loss: 6.8326, Val Loss: 6.7910\n",
            "Epoch 8/1000, Train Loss: 6.8274, Val Loss: 6.8021\n",
            "Epoch 9/1000, Train Loss: 6.8239, Val Loss: 6.7969\n",
            "Epoch 10/1000, Train Loss: 6.8238, Val Loss: 6.7950\n",
            "Epoch 11/1000, Train Loss: 6.8263, Val Loss: 6.7964\n",
            "Epoch 12/1000, Train Loss: 6.8346, Val Loss: 6.8027\n",
            "Epoch 13/1000, Train Loss: 6.8237, Val Loss: 6.8042\n",
            "Epoch 14/1000, Train Loss: 6.8236, Val Loss: 6.7981\n",
            "Epoch 15/1000, Train Loss: 6.8227, Val Loss: 6.7954\n",
            "Epoch 16/1000, Train Loss: 6.8177, Val Loss: 6.8115\n",
            "Epoch 17/1000, Train Loss: 6.8263, Val Loss: 6.8022\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 6.7910\n",
            "Trial 10/10\n",
            "Epoch 1/1000, Train Loss: 6.8639, Val Loss: 6.7970\n",
            "Epoch 2/1000, Train Loss: 6.8208, Val Loss: 6.8080\n",
            "Epoch 3/1000, Train Loss: 6.8206, Val Loss: 6.8184\n",
            "Epoch 4/1000, Train Loss: 6.8280, Val Loss: 6.8044\n",
            "Epoch 5/1000, Train Loss: 6.8264, Val Loss: 6.8047\n",
            "Epoch 6/1000, Train Loss: 6.8235, Val Loss: 6.8138\n",
            "Epoch 7/1000, Train Loss: 6.8335, Val Loss: 6.8159\n",
            "Epoch 8/1000, Train Loss: 6.8214, Val Loss: 6.8124\n",
            "Epoch 9/1000, Train Loss: 6.8373, Val Loss: 6.8127\n",
            "Epoch 10/1000, Train Loss: 6.8276, Val Loss: 6.8205\n",
            "Epoch 11/1000, Train Loss: 6.8348, Val Loss: 6.8063\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 6.7970\n",
            "Trials complete. Results saved to best_hyperparameter_trials.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Best hyperparameters (replace with your actual best values)\n",
        "best_lr = 0.005\n",
        "best_hidden_size = 16\n",
        "best_dropout = 0.3\n",
        "best_scale_factor = 1.3\n",
        "\n",
        "# Number of trials\n",
        "num_trials = 10\n",
        "\n",
        "# Store results\n",
        "trial_results = []\n",
        "\n",
        "print(f\"Running {num_trials} trials with Best LR={best_lr}, Hidden={best_hidden_size}, Dropout={best_dropout}, Scale Factor={best_scale_factor}\")\n",
        "\n",
        "for trial in range(1, num_trials + 1):\n",
        "    print(f\"Trial {trial}/{num_trials}\")\n",
        "\n",
        "    # Train model with best hyperparameters\n",
        "    model = train_bias_predictor(\n",
        "        reconstructed_train_tensor, bias_train_tensor,\n",
        "        reconstructed_val_tensor, bias_val_tensor,\n",
        "        epochs=1000, learning_rate=best_lr, patience=10,\n",
        "        input_dim=1, hidden_size=best_hidden_size, dropout=best_dropout, scale_factor=best_scale_factor\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Evaluate the model\n",
        "    trial_results.append(evaluate_model(model, reconstructed_test_tensor, y_pred_test, ground_truth_test))\n",
        "\n",
        "# Convert results to DataFrame\n",
        "columns = [\"Original Accuracy\", \"Corrected Accuracy\",\n",
        "           \"Original Recall\", \"Original Precision\", \"Original F1 Score\", \"Original TN\", \"Original FP\", \"Original FN\", \"Original TP\",\n",
        "           \"Corrected Recall\", \"Corrected Precision\", \"Corrected F1 Score\", \"Corrected TN\", \"Corrected FP\", \"Corrected FN\", \"Corrected TP\"]\n",
        "df_trials = pd.DataFrame(trial_results, columns=columns)\n",
        "\n",
        "# Save to CSV\n",
        "df_trials.to_csv(\"best_hyperparameter_trials.csv\", index=False)\n",
        "print(\"Trials complete. Results saved to best_hyperparameter_trials.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "_OF5nFm9RXkt",
        "outputId": "2c629829-a557-4eed-dfc5-8088442d4b57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Original Accuracy  Corrected Accuracy  Original Recall  Original Precision  \\\n",
              "0             0.9718            0.972067         0.669036              0.9953   \n",
              "2             0.9718            0.972000         0.669036              0.9953   \n",
              "1             0.9718            0.971933         0.669036              0.9953   \n",
              "5             0.9718            0.971933         0.669036              0.9953   \n",
              "8             0.9718            0.971933         0.669036              0.9953   \n",
              "4             0.9718            0.971867         0.669036              0.9953   \n",
              "6             0.9718            0.971867         0.669036              0.9953   \n",
              "7             0.9718            0.971867         0.669036              0.9953   \n",
              "9             0.9718            0.971867         0.669036              0.9953   \n",
              "3             0.9718            0.971800         0.669036              0.9953   \n",
              "\n",
              "   Original F1 Score  Original TN  Original FP  Original FN  Original TP  \\\n",
              "0           0.800189        13730            4          419          847   \n",
              "2           0.800189        13730            4          419          847   \n",
              "1           0.800189        13730            4          419          847   \n",
              "5           0.800189        13730            4          419          847   \n",
              "8           0.800189        13730            4          419          847   \n",
              "4           0.800189        13730            4          419          847   \n",
              "6           0.800189        13730            4          419          847   \n",
              "7           0.800189        13730            4          419          847   \n",
              "9           0.800189        13730            4          419          847   \n",
              "3           0.800189        13730            4          419          847   \n",
              "\n",
              "   Corrected Recall  Corrected Precision  Corrected F1 Score  Corrected TN  \\\n",
              "0          0.676145             0.989595            0.803379         13725   \n",
              "2          0.674566             0.990719            0.802632         13726   \n",
              "1          0.674566             0.989571            0.802255         13725   \n",
              "5          0.674566             0.989571            0.802255         13725   \n",
              "8          0.674566             0.989571            0.802255         13725   \n",
              "4          0.676145             0.986175            0.802249         13722   \n",
              "6          0.676145             0.986175            0.802249         13722   \n",
              "7          0.674566             0.988426            0.801878         13724   \n",
              "9          0.674566             0.988426            0.801878         13724   \n",
              "3          0.674566             0.987283            0.801502         13723   \n",
              "\n",
              "   Corrected FP  Corrected FN  Corrected TP  \n",
              "0             9           410           856  \n",
              "2             8           412           854  \n",
              "1             9           412           854  \n",
              "5             9           412           854  \n",
              "8             9           412           854  \n",
              "4            12           410           856  \n",
              "6            12           410           856  \n",
              "7            10           412           854  \n",
              "9            10           412           854  \n",
              "3            11           412           854  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-97aa511e-9f3f-4c87-b214-92298bf1697b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original Accuracy</th>\n",
              "      <th>Corrected Accuracy</th>\n",
              "      <th>Original Recall</th>\n",
              "      <th>Original Precision</th>\n",
              "      <th>Original F1 Score</th>\n",
              "      <th>Original TN</th>\n",
              "      <th>Original FP</th>\n",
              "      <th>Original FN</th>\n",
              "      <th>Original TP</th>\n",
              "      <th>Corrected Recall</th>\n",
              "      <th>Corrected Precision</th>\n",
              "      <th>Corrected F1 Score</th>\n",
              "      <th>Corrected TN</th>\n",
              "      <th>Corrected FP</th>\n",
              "      <th>Corrected FN</th>\n",
              "      <th>Corrected TP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.972067</td>\n",
              "      <td>0.669036</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730</td>\n",
              "      <td>4</td>\n",
              "      <td>419</td>\n",
              "      <td>847</td>\n",
              "      <td>0.676145</td>\n",
              "      <td>0.989595</td>\n",
              "      <td>0.803379</td>\n",
              "      <td>13725</td>\n",
              "      <td>9</td>\n",
              "      <td>410</td>\n",
              "      <td>856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.972000</td>\n",
              "      <td>0.669036</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730</td>\n",
              "      <td>4</td>\n",
              "      <td>419</td>\n",
              "      <td>847</td>\n",
              "      <td>0.674566</td>\n",
              "      <td>0.990719</td>\n",
              "      <td>0.802632</td>\n",
              "      <td>13726</td>\n",
              "      <td>8</td>\n",
              "      <td>412</td>\n",
              "      <td>854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.971933</td>\n",
              "      <td>0.669036</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730</td>\n",
              "      <td>4</td>\n",
              "      <td>419</td>\n",
              "      <td>847</td>\n",
              "      <td>0.674566</td>\n",
              "      <td>0.989571</td>\n",
              "      <td>0.802255</td>\n",
              "      <td>13725</td>\n",
              "      <td>9</td>\n",
              "      <td>412</td>\n",
              "      <td>854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.971933</td>\n",
              "      <td>0.669036</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730</td>\n",
              "      <td>4</td>\n",
              "      <td>419</td>\n",
              "      <td>847</td>\n",
              "      <td>0.674566</td>\n",
              "      <td>0.989571</td>\n",
              "      <td>0.802255</td>\n",
              "      <td>13725</td>\n",
              "      <td>9</td>\n",
              "      <td>412</td>\n",
              "      <td>854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.971933</td>\n",
              "      <td>0.669036</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730</td>\n",
              "      <td>4</td>\n",
              "      <td>419</td>\n",
              "      <td>847</td>\n",
              "      <td>0.674566</td>\n",
              "      <td>0.989571</td>\n",
              "      <td>0.802255</td>\n",
              "      <td>13725</td>\n",
              "      <td>9</td>\n",
              "      <td>412</td>\n",
              "      <td>854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.971867</td>\n",
              "      <td>0.669036</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730</td>\n",
              "      <td>4</td>\n",
              "      <td>419</td>\n",
              "      <td>847</td>\n",
              "      <td>0.676145</td>\n",
              "      <td>0.986175</td>\n",
              "      <td>0.802249</td>\n",
              "      <td>13722</td>\n",
              "      <td>12</td>\n",
              "      <td>410</td>\n",
              "      <td>856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.971867</td>\n",
              "      <td>0.669036</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730</td>\n",
              "      <td>4</td>\n",
              "      <td>419</td>\n",
              "      <td>847</td>\n",
              "      <td>0.676145</td>\n",
              "      <td>0.986175</td>\n",
              "      <td>0.802249</td>\n",
              "      <td>13722</td>\n",
              "      <td>12</td>\n",
              "      <td>410</td>\n",
              "      <td>856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.971867</td>\n",
              "      <td>0.669036</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730</td>\n",
              "      <td>4</td>\n",
              "      <td>419</td>\n",
              "      <td>847</td>\n",
              "      <td>0.674566</td>\n",
              "      <td>0.988426</td>\n",
              "      <td>0.801878</td>\n",
              "      <td>13724</td>\n",
              "      <td>10</td>\n",
              "      <td>412</td>\n",
              "      <td>854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.971867</td>\n",
              "      <td>0.669036</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730</td>\n",
              "      <td>4</td>\n",
              "      <td>419</td>\n",
              "      <td>847</td>\n",
              "      <td>0.674566</td>\n",
              "      <td>0.988426</td>\n",
              "      <td>0.801878</td>\n",
              "      <td>13724</td>\n",
              "      <td>10</td>\n",
              "      <td>412</td>\n",
              "      <td>854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.971800</td>\n",
              "      <td>0.669036</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730</td>\n",
              "      <td>4</td>\n",
              "      <td>419</td>\n",
              "      <td>847</td>\n",
              "      <td>0.674566</td>\n",
              "      <td>0.987283</td>\n",
              "      <td>0.801502</td>\n",
              "      <td>13723</td>\n",
              "      <td>11</td>\n",
              "      <td>412</td>\n",
              "      <td>854</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97aa511e-9f3f-4c87-b214-92298bf1697b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-97aa511e-9f3f-4c87-b214-92298bf1697b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-97aa511e-9f3f-4c87-b214-92298bf1697b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bf6a7b5b-9375-49b4-b233-80a0bf8b8b3d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bf6a7b5b-9375-49b4-b233-80a0bf8b8b3d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bf6a7b5b-9375-49b4-b233-80a0bf8b8b3d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_trials",
              "summary": "{\n  \"name\": \"df_trials\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Original Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.9718,\n        \"max\": 0.9718,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9718\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.730012058188521e-05,\n        \"min\": 0.9718,\n        \"max\": 0.9720666666666666,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.972\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1702778228589004e-16,\n        \"min\": 0.6690363349131122,\n        \"max\": 0.6690363349131122,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6690363349131122\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.9952996474735605,\n        \"max\": 0.9952996474735605,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9952996474735605\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original F1 Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.8001889466225791,\n        \"max\": 0.8001889466225791,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.8001889466225791\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original TN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 13730,\n        \"max\": 13730,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          13730\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original FP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original FN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 419,\n        \"max\": 419,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          419\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original TP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 847,\n        \"max\": 847,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          847\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0007631056738382784,\n        \"min\": 0.674565560821485,\n        \"max\": 0.6761453396524486,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.674565560821485\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0015578845051382288,\n        \"min\": 0.9861751152073732,\n        \"max\": 0.9907192575406032,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.9895953757225433\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected F1 Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0005007143762935005,\n        \"min\": 0.8015016424213984,\n        \"max\": 0.8033786954481463,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.8033786954481463\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected TN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 13722,\n        \"max\": 13726,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          13726\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected FP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 8,\n        \"max\": 12,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected FN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 410,\n        \"max\": 412,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected TP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 854,\n        \"max\": 856,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          854\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 355
        }
      ],
      "source": [
        "# sort by corrected F1 score\n",
        "df_trials = df_trials.sort_values(by='Corrected F1 Score', ascending=False)\n",
        "df_trials.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_trials.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "zqGzfYzV2lv2",
        "outputId": "e47e3473-b5b6-4a69-ec9e-26e681a686a0"
      },
      "execution_count": 356,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Original Accuracy  Corrected Accuracy  Original Recall  \\\n",
              "count            10.0000           10.000000     1.000000e+01   \n",
              "mean              0.9718            0.971913     6.690363e-01   \n",
              "std               0.0000            0.000077     1.170278e-16   \n",
              "min               0.9718            0.971800     6.690363e-01   \n",
              "25%               0.9718            0.971867     6.690363e-01   \n",
              "50%               0.9718            0.971900     6.690363e-01   \n",
              "75%               0.9718            0.971933     6.690363e-01   \n",
              "max               0.9718            0.972067     6.690363e-01   \n",
              "\n",
              "       Original Precision  Original F1 Score  Original TN  Original FP  \\\n",
              "count             10.0000          10.000000         10.0         10.0   \n",
              "mean               0.9953           0.800189      13730.0          4.0   \n",
              "std                0.0000           0.000000          0.0          0.0   \n",
              "min                0.9953           0.800189      13730.0          4.0   \n",
              "25%                0.9953           0.800189      13730.0          4.0   \n",
              "50%                0.9953           0.800189      13730.0          4.0   \n",
              "75%                0.9953           0.800189      13730.0          4.0   \n",
              "max                0.9953           0.800189      13730.0          4.0   \n",
              "\n",
              "       Original FN  Original TP  Corrected Recall  Corrected Precision  \\\n",
              "count         10.0         10.0         10.000000            10.000000   \n",
              "mean         419.0        847.0          0.675039             0.988551   \n",
              "std            0.0          0.0          0.000763             0.001558   \n",
              "min          419.0        847.0          0.674566             0.986175   \n",
              "25%          419.0        847.0          0.674566             0.987569   \n",
              "50%          419.0        847.0          0.674566             0.988999   \n",
              "75%          419.0        847.0          0.675750             0.989571   \n",
              "max          419.0        847.0          0.676145             0.990719   \n",
              "\n",
              "       Corrected F1 Score  Corrected TN  Corrected FP  Corrected FN  \\\n",
              "count           10.000000      10.00000      10.00000     10.000000   \n",
              "mean             0.802253   13724.10000       9.90000    411.400000   \n",
              "std              0.000501       1.37032       1.37032      0.966092   \n",
              "min              0.801502   13722.00000       8.00000    410.000000   \n",
              "25%              0.801971   13723.25000       9.00000    410.500000   \n",
              "50%              0.802252   13724.50000       9.50000    412.000000   \n",
              "75%              0.802255   13725.00000      10.75000    412.000000   \n",
              "max              0.803379   13726.00000      12.00000    412.000000   \n",
              "\n",
              "       Corrected TP  \n",
              "count     10.000000  \n",
              "mean     854.600000  \n",
              "std        0.966092  \n",
              "min      854.000000  \n",
              "25%      854.000000  \n",
              "50%      854.000000  \n",
              "75%      855.500000  \n",
              "max      856.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3500f0e-d7b4-4451-878d-c235e8eedf16\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original Accuracy</th>\n",
              "      <th>Corrected Accuracy</th>\n",
              "      <th>Original Recall</th>\n",
              "      <th>Original Precision</th>\n",
              "      <th>Original F1 Score</th>\n",
              "      <th>Original TN</th>\n",
              "      <th>Original FP</th>\n",
              "      <th>Original FN</th>\n",
              "      <th>Original TP</th>\n",
              "      <th>Corrected Recall</th>\n",
              "      <th>Corrected Precision</th>\n",
              "      <th>Corrected F1 Score</th>\n",
              "      <th>Corrected TN</th>\n",
              "      <th>Corrected FP</th>\n",
              "      <th>Corrected FN</th>\n",
              "      <th>Corrected TP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10.0000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>10.0000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.971913</td>\n",
              "      <td>6.690363e-01</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>419.0</td>\n",
              "      <td>847.0</td>\n",
              "      <td>0.675039</td>\n",
              "      <td>0.988551</td>\n",
              "      <td>0.802253</td>\n",
              "      <td>13724.10000</td>\n",
              "      <td>9.90000</td>\n",
              "      <td>411.400000</td>\n",
              "      <td>854.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>1.170278e-16</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.001558</td>\n",
              "      <td>0.000501</td>\n",
              "      <td>1.37032</td>\n",
              "      <td>1.37032</td>\n",
              "      <td>0.966092</td>\n",
              "      <td>0.966092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.971800</td>\n",
              "      <td>6.690363e-01</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>419.0</td>\n",
              "      <td>847.0</td>\n",
              "      <td>0.674566</td>\n",
              "      <td>0.986175</td>\n",
              "      <td>0.801502</td>\n",
              "      <td>13722.00000</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>410.000000</td>\n",
              "      <td>854.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.971867</td>\n",
              "      <td>6.690363e-01</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>419.0</td>\n",
              "      <td>847.0</td>\n",
              "      <td>0.674566</td>\n",
              "      <td>0.987569</td>\n",
              "      <td>0.801971</td>\n",
              "      <td>13723.25000</td>\n",
              "      <td>9.00000</td>\n",
              "      <td>410.500000</td>\n",
              "      <td>854.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.971900</td>\n",
              "      <td>6.690363e-01</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>419.0</td>\n",
              "      <td>847.0</td>\n",
              "      <td>0.674566</td>\n",
              "      <td>0.988999</td>\n",
              "      <td>0.802252</td>\n",
              "      <td>13724.50000</td>\n",
              "      <td>9.50000</td>\n",
              "      <td>412.000000</td>\n",
              "      <td>854.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.971933</td>\n",
              "      <td>6.690363e-01</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>419.0</td>\n",
              "      <td>847.0</td>\n",
              "      <td>0.675750</td>\n",
              "      <td>0.989571</td>\n",
              "      <td>0.802255</td>\n",
              "      <td>13725.00000</td>\n",
              "      <td>10.75000</td>\n",
              "      <td>412.000000</td>\n",
              "      <td>855.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.9718</td>\n",
              "      <td>0.972067</td>\n",
              "      <td>6.690363e-01</td>\n",
              "      <td>0.9953</td>\n",
              "      <td>0.800189</td>\n",
              "      <td>13730.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>419.0</td>\n",
              "      <td>847.0</td>\n",
              "      <td>0.676145</td>\n",
              "      <td>0.990719</td>\n",
              "      <td>0.803379</td>\n",
              "      <td>13726.00000</td>\n",
              "      <td>12.00000</td>\n",
              "      <td>412.000000</td>\n",
              "      <td>856.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3500f0e-d7b4-4451-878d-c235e8eedf16')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b3500f0e-d7b4-4451-878d-c235e8eedf16 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b3500f0e-d7b4-4451-878d-c235e8eedf16');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7433bd31-6718-4194-ad54-38ba3efbad41\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7433bd31-6718-4194-ad54-38ba3efbad41')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7433bd31-6718-4194-ad54-38ba3efbad41 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_trials\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Original Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2588252234728485,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          0.9718,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.258788499510164,\n        \"min\": 7.730012058188521e-05,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.9719133333333334,\n          0.9719,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3409979134224956,\n        \"min\": 1.1702778228589004e-16,\n        \"max\": 10.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.6690363349131123,\n          0.6690363349131122,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2526129577461713,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          0.9952996474735605,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original F1 Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.304923152901846,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          0.8001889466225791,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original TN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6353.440996814246,\n        \"min\": 0.0,\n        \"max\": 13730.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          13730.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original FP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.712405363721075,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          4.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original FN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 191.66339541721874,\n        \"min\": 0.0,\n        \"max\": 419.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          419.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original TP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 389.77942480331103,\n        \"min\": 0.0,\n        \"max\": 847.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10.0,\n          847.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3392555492926115,\n        \"min\": 0.0007631056738382784,\n        \"max\": 10.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          10.0,\n          0.6750394944707739,\n          0.6761453396524486\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.254246363741148,\n        \"min\": 0.0015578845051382288,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.9885513741631728,\n          0.9889985944809236,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected F1 Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3043169415506184,\n        \"min\": 0.0005007143762935005,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8022530118395357,\n          0.8022519383547502,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected TN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6350.411913739521,\n        \"min\": 1.3703203194062976,\n        \"max\": 13726.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          13724.1,\n          13724.5,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected FP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.230675084998693,\n        \"min\": 1.3703203194062976,\n        \"max\": 12.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          9.9,\n          9.5,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected FN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 187.88142374127648,\n        \"min\": 0.9660917830792959,\n        \"max\": 412.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          10.0,\n          411.4,\n          412.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected TP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 393.1114539104303,\n        \"min\": 0.9660917830792959,\n        \"max\": 856.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          10.0,\n          854.6,\n          856.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 356
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "metadata": {
        "id": "VkHT0L0vUsuH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}