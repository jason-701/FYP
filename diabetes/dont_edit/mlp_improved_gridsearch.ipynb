{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da68MaKdRTFR",
        "outputId": "3dd17ccd-83cc-4b14-d50c-216c93573466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('diabetes_prediction_dataset.csv')\n",
        "df_encoded = pd.get_dummies(df, columns=['gender', 'smoking_history'], drop_first=True)\n",
        "testDF = df_encoded.sample(frac=1).reset_index(drop=True)\n",
        "x_unscaled = testDF.drop(['diabetes'], axis=1)\n",
        "y = testDF['diabetes']\n",
        "\n",
        "# Normalize the data\n",
        "numerical_columns = x_unscaled.select_dtypes(include=np.number).columns\n",
        "boolean_columns = x_unscaled.select_dtypes(include=bool).columns\n",
        "scaler = StandardScaler()\n",
        "temp = pd.DataFrame(scaler.fit_transform(x_unscaled[numerical_columns]), columns=numerical_columns)\n",
        "x_scaled = pd.concat([temp, x_unscaled[boolean_columns]], axis=1)\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(x_scaled, y, test_size=0.3, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
        "x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
        "x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOKaWwwUSJn7",
        "outputId": "be81408b-3be6-4aaf-e3c7-7fe7f7a483ab"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-92-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 1 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 0 0 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 1 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 1 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 1 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 1 0 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-92-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "5JAtg4mkRTFU"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "X_val_tensor = torch.tensor(x_val.values, dtype=torch.float32).to(device)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "X_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "y0fZOoJcRTFV"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size, encoding_dim, dropout):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 2, encoding_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size // 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZbq7Ol_RTFX",
        "outputId": "c467d639-87a4-4e15-e67c-6b23dbfc5d16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-95-8fad8731368b>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  autoencoder.load_state_dict(torch.load(autoencoder_path, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Autoencoder(\n",
              "  (encoder): Sequential(\n",
              "    (0): Linear(in_features=13, out_features=32, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.0, inplace=False)\n",
              "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.0, inplace=False)\n",
              "    (6): Linear(in_features=16, out_features=8, bias=True)\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=8, out_features=16, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.0, inplace=False)\n",
              "    (3): Linear(in_features=16, out_features=32, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.0, inplace=False)\n",
              "    (6): Linear(in_features=32, out_features=13, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "# Define input dimensions\n",
        "input_dim = x_train.shape[1]\n",
        "encoding_dim = 8\n",
        "\n",
        "autoencoder_path = './models/autoencoder.pth'\n",
        "\n",
        "# Initialize the model\n",
        "autoencoder = Autoencoder(input_dim, 32, encoding_dim, 0.0).to(device)\n",
        "\n",
        "# Load the model\n",
        "autoencoder.load_state_dict(torch.load(autoencoder_path, map_location=torch.device('cpu')))\n",
        "autoencoder.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS37HZflRTFX"
      },
      "source": [
        "## Original MLP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dthvENP7RTFZ",
        "outputId": "054ad74c-671c-47e2-826e-2a8332c399b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-96-5db66acc508a>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  original_mlp.load_state_dict(torch.load(mlp_model_file, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 97.33%\n",
            "TP: 808\n",
            "TN: 13792\n",
            "FP: 2\n",
            "FN: 398\n",
            "Recall: 0.669983\n",
            "Precision: 0.997531\n",
            "F1 Score: 0.801587\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size, dropout):\n",
        "        super(MLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Load the original MLP model\n",
        "mlp_model_file = './models/mlp_model.pth'\n",
        "input_dim = x_train.shape[1]\n",
        "original_mlp = MLP(input_dim, 128, 0.3).to(device)\n",
        "original_mlp.load_state_dict(torch.load(mlp_model_file, map_location=torch.device('cpu')))\n",
        "original_mlp.eval()\n",
        "\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize lists to store predictions and true labels\n",
        "y_true_list = []\n",
        "y_pred_list = []\n",
        "\n",
        "# Evaluate the original MLP model\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        outputs = original_mlp(X_batch)  # Get predictions (probabilities from sigmoid)\n",
        "        predictions = (outputs > 0.5).float()  # Convert probabilities to binary (0 or 1)\n",
        "\n",
        "        y_true_list.extend(y_batch.cpu().numpy())  # Store true labels\n",
        "        y_pred_list.extend(predictions.cpu().numpy())  # Store predictions\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "y_true = np.array(y_true_list)\n",
        "y_pred = np.array(y_pred_list)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print('TP:', conf_matrix[1, 1])\n",
        "print('TN:', conf_matrix[0, 0])\n",
        "print('FP:', conf_matrix[0, 1])\n",
        "print('FN:', conf_matrix[1, 0])\n",
        "\n",
        "# Calculate recall, precision, and F1 score\n",
        "recall = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 0])\n",
        "precision = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[0, 1])\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "print(f'Recall: {recall:.6f}')\n",
        "print(f'Precision: {precision:.6f}')\n",
        "print(f'F1 Score: {f1_score:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrkWThhKRTFa"
      },
      "source": [
        "### TRYING OUT ERROR-CORRECTING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "i-CReA7eRTFb"
      },
      "outputs": [],
      "source": [
        "# Define the Bias Predictor Model\n",
        "# class BiasPredictor(nn.Module):\n",
        "#     def __init__(self, input_dim=1, hidden_size=128, dropout=0.2):\n",
        "#         super(BiasPredictor, self).__init__()\n",
        "#         self.fc1 = nn.Linear(input_dim, hidden_size)\n",
        "#         self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "#         self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "#         self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "#         self.fc3 = nn.Linear(hidden_size // 2, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.dropout1(x)\n",
        "\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = self.dropout2(x)\n",
        "\n",
        "#         return self.fc3(x)\n",
        "\n",
        "class BiasPredictor(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_size=128, dropout=0.2):\n",
        "        super(BiasPredictor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_size // 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.leaky_relu(self.fc2(x), negative_slope=0.01)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        return torch.tanh(self.fc3(x))\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the training data for the bias predictor\n",
        "def prepare_bias_data(reconstruction_errors, x_test, original_probs, ground_truth):\n",
        "    # Combine reconstruction errors with the existing normalized features\n",
        "    features = np.hstack((reconstruction_errors.reshape(-1, 1), x_test.values))\n",
        "\n",
        "    # Compute target biases\n",
        "    target_bias = ground_truth - original_probs\n",
        "    print(\"Minimum bias: \", np.min(target_bias))\n",
        "    print(\"Maximum bias: \", np.max(target_bias))\n",
        "\n",
        "    return torch.tensor(features, dtype=torch.float32), torch.tensor(target_bias, dtype=torch.float32)\n",
        "\n",
        "def prepare_bias_data_scaled(reconstruction_errors, x_data, original_probs, ground_truth, reconstruction_scale):\n",
        "    \"\"\"\n",
        "    Prepares data for the bias predictor by combining features and scaling reconstruction errors.\n",
        "    \"\"\"\n",
        "    # Scale reconstruction errors\n",
        "    scaled_reconstruction_errors = reconstruction_errors * reconstruction_scale\n",
        "\n",
        "    # Combine scaled reconstruction errors with the existing features\n",
        "    # features = np.hstack((scaled_reconstruction_errors.reshape(-1, 1), x_data.values))\n",
        "\n",
        "    #######################################################################################################\n",
        "    # use this for only reconstruction errors as the input feature\n",
        "\n",
        "    features = scaled_reconstruction_errors.reshape(-1, 1)\n",
        "    # print(\"Minimum scaled reconstruction error: \", np.min(scaled_reconstruction_errors))\n",
        "    # print(\"Maximum scaled reconstruction error: \", np.max(scaled_reconstruction_errors))\n",
        "\n",
        "    #######################################################################################################\n",
        "\n",
        "    # Compute target biases\n",
        "    target_bias = ground_truth - original_probs\n",
        "    print(\"Minimum bias: \", np.min(target_bias))\n",
        "    print(\"Maximum bias: \", np.max(target_bias))\n",
        "\n",
        "    return torch.tensor(features, dtype=torch.float32), torch.tensor(target_bias, dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "def train_bias_predictor(features, target_bias, val_features, val_target_bias, epochs=1000, learning_rate=0.0001, patience=10, input_dim=1, hidden_size=128, dropout=0.2):\n",
        "    model = BiasPredictor(input_dim = input_dim, hidden_size=hidden_size, dropout = dropout).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
        "    patience_counter = 0  # Counter for early stopping\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(features).squeeze()\n",
        "        loss = criterion(predictions, target_bias)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_predictions = model(val_features).squeeze()\n",
        "            val_loss = criterion(val_predictions, val_target_bias)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict()  # Save the best model state\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "        # Stop training if patience is exceeded\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
        "            break\n",
        "\n",
        "    # Load the best model state before returning\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "#####################################################################################################\n",
        "\n",
        "# Train the Bias Predictor with Class Weights\n",
        "# def train_bias_predictor_with_weights(features, target_bias, ground_truth, epochs=100, learning_rate=0.001):\n",
        "# def train_bias_predictor_with_weights(features, target_bias, ground_truth, epochs=100, learning_rate=0.001, weight_positive_scale=1.0, weight_negative_scale=1.0):\n",
        "#     model = BiasPredictor().to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#     # Define class weights\n",
        "#     num_positive = np.sum(ground_truth == 1)\n",
        "#     num_negative = np.sum(ground_truth == 0)\n",
        "#     total = len(ground_truth)\n",
        "\n",
        "#     weight_positive = (total / (2 * num_positive)) * weight_positive_scale\n",
        "#     weight_negative = (total / (2 * num_negative)) * weight_negative_scale\n",
        "\n",
        "#     # Convert weights to tensors\n",
        "#     class_weights = torch.tensor([weight_negative, weight_positive], dtype=torch.float32).to(device)\n",
        "#     criterion = nn.MSELoss(reduction='none')  # Use 'none' to apply weights manually\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         predictions = model(features).squeeze()\n",
        "\n",
        "#         # Calculate weighted loss\n",
        "#         loss = criterion(predictions, target_bias)\n",
        "#         weights = torch.where(\n",
        "#             target_bias > 0,  # Assign weights based on ground truth class\n",
        "#             class_weights[1],  # Positive class weight\n",
        "#             class_weights[0]   # Negative class weight\n",
        "#         )\n",
        "#         weighted_loss = torch.mean(loss * weights)\n",
        "\n",
        "#         weighted_loss.backward()\n",
        "#         optimizer.step()\n",
        "#     return model\n",
        "\n",
        "#####################################################################################################\n",
        "\n",
        "def apply_bias_correction(bias_predictor, reconstruction_errors, x_test, original_probs):\n",
        "    # Combine normalized reconstruction errors with other features\n",
        "    # combined_features = np.hstack((reconstruction_errors.reshape(-1, 1), x_test.values))\n",
        "    # features = torch.tensor(combined_features, dtype=torch.float32).to(next(bias_predictor.parameters()).device)\n",
        "\n",
        "    #######################################################################################################\n",
        "    # use this for only reconstruction errors as the input feature\n",
        "\n",
        "    combined_features = reconstruction_errors.reshape(-1, 1)\n",
        "    features = torch.tensor(combined_features, dtype=torch.float32).to(next(bias_predictor.parameters()).device)\n",
        "\n",
        "    #######################################################################################################\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bias_correction = bias_predictor(features).squeeze().cpu().numpy()\n",
        "\n",
        "    print(np.min(bias_correction))\n",
        "    print(np.max(bias_correction))\n",
        "\n",
        "    # Multiply with a scaling factor\n",
        "    bias_correction *= 1.2\n",
        "\n",
        "    # Apply the correction and clip probabilities to [0, 1]\n",
        "    corrected_probs = np.clip(original_probs + bias_correction, 0, 1)\n",
        "\n",
        "    # Number of predictions that changed\n",
        "    changed_predictions = np.sum((original_probs > 0.5).astype(int) != (corrected_probs > 0.5).astype(int))\n",
        "    print(f\"Number of changed predictions: {changed_predictions}\")\n",
        "\n",
        "    return corrected_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMariswORTFc",
        "outputId": "9d69dd99-b79a-4438-bf06-d581feff9174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max reconstruction error:  3.2539681865347374\n",
            "Minimum bias:  -0.7354510426521301\n",
            "Maximum bias:  0.999342622992117\n",
            "Minimum bias:  -0.7070267200469971\n",
            "Maximum bias:  0.9974858066998422\n",
            "Early stopping triggered at epoch 49. Best Val Loss: 0.0287\n",
            "-0.098697215\n",
            "0.030277092\n",
            "Number of changed predictions: 18\n",
            "\n",
            "Corrected Accuracy: 97.19%\n",
            "True Positives:  4062\n",
            "True Negatives:  63968\n",
            "False Positives:  14\n",
            "False Negatives:  1956\n",
            "\n",
            "Original MLP Accuracy: 97.20%\n",
            "True Positives:  4075\n",
            "True Negatives:  63963\n",
            "False Positives:  19\n",
            "False Negatives:  1943\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "with torch.no_grad():\n",
        "    reconstructed_train = autoencoder(X_train_tensor).cpu().numpy()\n",
        "reconstructed_errors_train = np.mean(np.square(x_train.values - reconstructed_train), axis=1)\n",
        "mean_error_train = np.mean(reconstructed_errors_train)\n",
        "std_error_train = np.std(reconstructed_errors_train)\n",
        "reconstruction_errors_train_normalized = (reconstructed_errors_train - mean_error_train) / std_error_train\n",
        "print(\"Max reconstruction error: \", np.max(reconstructed_errors_train))\n",
        "\n",
        "with torch.no_grad():\n",
        "    reconstructed_val = autoencoder(X_val_tensor).cpu().numpy()\n",
        "reconstructed_errors_val = np.mean(np.square(x_val.values - reconstructed_val), axis=1)\n",
        "mean_error_val = np.mean(reconstructed_errors_val)\n",
        "std_error_val = np.std(reconstructed_errors_val)\n",
        "reconstruction_errors_val_normalized = (reconstructed_errors_val - mean_error_val) / std_error_val\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred_train = original_mlp(X_train_tensor).cpu().numpy().flatten()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred_val = original_mlp(X_val_tensor).cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "original_probs_train = y_pred_train\n",
        "ground_truth_train = y_train.values\n",
        "\n",
        "original_probs_val = y_pred_val\n",
        "ground_truth_val = y_val.values\n",
        "\n",
        "\n",
        "# Prepare the data for training\n",
        "# features, target_bias = prepare_bias_data(reconstruction_errors_normalized, x_train, original_probs, ground_truth)\n",
        "\n",
        "# if model does not exist\n",
        "if not os.path.exists('./models/bias_predictor.pth'):\n",
        "    features_train, target_bias_train = prepare_bias_data_scaled(reconstruction_errors_train_normalized, x_train, original_probs_train, ground_truth_train, reconstruction_scale=50)\n",
        "    features_val, target_bias_val = prepare_bias_data_scaled(reconstruction_errors_val_normalized, x_val, original_probs_val, ground_truth_val, reconstruction_scale=50)\n",
        "\n",
        "    # Move tensors to the same device as the model\n",
        "    features_train, target_bias_train = features_train.to(device), target_bias_train.to(device)\n",
        "    features_val, target_bias_val = features_val.to(device), target_bias_val.to(device)\n",
        "\n",
        "    # Train the bias predictor\n",
        "    bias_predictor = train_bias_predictor(features_train, target_bias_train, features_val, target_bias_val, epochs=5000, learning_rate=0.0001, patience=10)\n",
        "\n",
        "    # Apply bias correction during inference\n",
        "    corrected_probs = apply_bias_correction(bias_predictor, reconstruction_errors_train_normalized, x_train, original_probs_train)\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    corrected_predictions = (corrected_probs > 0.5).astype(int)\n",
        "    # Calculate accuracy\n",
        "    corrected_accuracy = accuracy_score(y_train, corrected_predictions)\n",
        "    print(f\"\\nCorrected Accuracy: {corrected_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    conf_matrix_corrected = confusion_matrix(y_train, corrected_predictions)\n",
        "\n",
        "    tn, fp, fn, tp = conf_matrix_corrected.ravel()\n",
        "    print(\"True Positives: \", tp)\n",
        "    print(\"True Negatives: \", tn)\n",
        "    print(\"False Positives: \", fp)\n",
        "    print(\"False Negatives: \", fn)\n",
        "\n",
        "    # Evaluate the original MLP model\n",
        "    with torch.no_grad():\n",
        "        x_train_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
        "        y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
        "        y_pred_original = original_mlp(x_train_tensor).cpu().numpy().flatten()\n",
        "        y_pred_original = (y_pred_original > 0.5).astype(int)  # Convert to binary\n",
        "\n",
        "    original_accuracy = accuracy_score(y_train, y_pred_original)\n",
        "    print(f\"\\nOriginal MLP Accuracy: {original_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    conf_matrix_original = confusion_matrix(y_train, y_pred_original)\n",
        "\n",
        "    original_tn, original_fp, original_fn, original_tp = conf_matrix_original.ravel()\n",
        "    print(\"True Positives: \", original_tp)\n",
        "    print(\"True Negatives: \", original_tn)\n",
        "    print(\"False Positives: \", original_fp)\n",
        "    print(\"False Negatives: \", original_fn)\n",
        "\n",
        "else:\n",
        "    # Load the trained bias predictor model\n",
        "    bias_predictor = BiasPredictor().to(device)\n",
        "    bias_predictor.load_state_dict(torch.load('./models/bias_predictor.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsf6-YRLRTFe",
        "outputId": "d2a329a0-1dbd-44a2-bdfc-3b5c238fcebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.084019795\n",
            "0.030274093\n",
            "Number of changed predictions: 5\n",
            "\n",
            "Corrected Accuracy on Test Set: 97.30%\n",
            "True Positives:  803\n",
            "True Negatives:  13792\n",
            "False Positives:  2\n",
            "False Negatives:  403\n",
            "\n",
            "Original Accuracy on Test Set: 97.33%\n",
            "True Positives:  808\n",
            "True Negatives:  13792\n",
            "False Positives:  2\n",
            "False Negatives:  398\n",
            "Improvement:  -1.250000000000281\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    reconstructed_test = autoencoder(X_test_tensor).cpu().numpy()\n",
        "reconstructed_errors_test = np.mean(np.square(x_test.values - reconstructed_test), axis=1)\n",
        "mean_error_test = np.mean(reconstructed_errors_test)\n",
        "std_error_test = np.std(reconstructed_errors_test)\n",
        "reconstruction_errors_test_normalized = (reconstructed_errors_test - mean_error_test) / std_error_test\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
        "    y_pred_test = original_mlp(x_test_tensor).cpu().numpy().flatten()\n",
        "\n",
        "original_probs_test = y_pred_test\n",
        "\n",
        "original_accuracy_test = accuracy_score(y_test, (original_probs_test > 0.5).astype(int))\n",
        "\n",
        "\n",
        "corrected_probs_test = apply_bias_correction(bias_predictor, reconstruction_errors_test_normalized, x_test, original_probs_test)\n",
        "\n",
        "corrected_predictions_test = (corrected_probs_test > 0.5).astype(int)\n",
        "\n",
        "corrected_accuracy_test = accuracy_score(y_test, corrected_predictions_test)\n",
        "print(f\"\\nCorrected Accuracy on Test Set: {corrected_accuracy_test * 100:.2f}%\")\n",
        "\n",
        "conf_matrix_corrected_test = confusion_matrix(y_test, corrected_predictions_test)\n",
        "\n",
        "tn, fp, fn, tp = conf_matrix_corrected_test.ravel()\n",
        "print(\"True Positives: \", tp)\n",
        "print(\"True Negatives: \", tn)\n",
        "print(\"False Positives: \", fp)\n",
        "print(\"False Negatives: \", fn)\n",
        "\n",
        "print(f\"\\nOriginal Accuracy on Test Set: {original_accuracy_test * 100:.2f}%\")\n",
        "\n",
        "conf_matrix_original_test = confusion_matrix(y_test, (original_probs_test > 0.5).astype(int))\n",
        "\n",
        "tn, fp, fn, tp = conf_matrix_original_test.ravel()\n",
        "print(\"True Positives: \", tp)\n",
        "print(\"True Negatives: \", tn)\n",
        "print(\"False Positives: \", fp)\n",
        "print(\"False Negatives: \", fn)\n",
        "\n",
        "difference = corrected_accuracy_test - original_accuracy_test\n",
        "difference_percent = difference/(1-original_accuracy_test)\n",
        "print(\"Improvement: \", difference_percent*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "EYLDTn-oRTFg"
      },
      "outputs": [],
      "source": [
        "# if (corrected_accuracy_test > original_accuracy_test + 0.0010) and not os.path.exists('./models/bias_predictor.pth'):\n",
        "#     # save the model\n",
        "#     torch.save(bias_predictor.state_dict(), './models/bias_predictor.pth')\n",
        "#     print(\"Model saved\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "TbAEfm6DRTFh"
      },
      "outputs": [],
      "source": [
        "# # Identify the indices where the predictions changed\n",
        "# changed_indices = np.where((original_probs_test > 0.5).astype(int) != corrected_predictions_test)[0]\n",
        "\n",
        "# # Print the indices of the changed data points\n",
        "# # print(\"Indices of changed data points:\", changed_indices)\n",
        "\n",
        "# print(\"Number of changed indices:\", len(changed_indices))\n",
        "# # Get the reconstruction error of the changed data points\n",
        "# changed_reconstruction_errors = reconstruction_errors_test_normalized[changed_indices]\n",
        "\n",
        "# # Define the ranges for the reconstruction errors\n",
        "# ranges = [-1.0, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0, 2.0, 3.0, 4.0, 5.0, 20]\n",
        "\n",
        "# # Count the number of changed indices within each range\n",
        "# counts, bin_edges = np.histogram(changed_reconstruction_errors, bins=ranges)\n",
        "\n",
        "# # Print the counts for each range\n",
        "# for i in range(len(ranges) - 1):\n",
        "#     print(f\"Range {ranges[i]} to {ranges[i+1]}: {counts[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaLBiw8ORTFi"
      },
      "source": [
        "### Trying bias correction with individual reconstruction errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "pnrBHxVmRTFj"
      },
      "outputs": [],
      "source": [
        "def compute_feature_importance(model, X, y_true, metric=accuracy_score, n_repeats=5):\n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "    X_base = X.clone()  # Keep a copy of the original data\n",
        "\n",
        "    # Get baseline predictions and accuracy\n",
        "    with torch.no_grad():\n",
        "        y_pred_base = (model(X).detach().cpu().numpy().flatten() > 0.5).astype(int)\n",
        "        baseline_score = metric(y_true.cpu().numpy().flatten(), y_pred_base)\n",
        "\n",
        "    feature_importance = np.zeros(X.shape[1])  # Array to store importance for each feature\n",
        "\n",
        "    for i in range(X.shape[1]):  # Loop through features\n",
        "        scores = []\n",
        "        for _ in range(n_repeats):  # Repeat for robustness\n",
        "            X_permuted = X_base.clone()  # Clone original dataset\n",
        "            permuted_feature = X_permuted[:, i].clone()  # Extract feature\n",
        "            X_permuted[:, i] = permuted_feature[torch.randperm(X.shape[0])]  # Shuffle the feature\n",
        "\n",
        "            # Get predictions with shuffled feature\n",
        "            with torch.no_grad():\n",
        "                y_pred_permuted = (model(X_permuted).detach().cpu().numpy().flatten() > 0.5).astype(int)\n",
        "                score = metric(y_true.cpu().numpy().flatten(), y_pred_permuted)\n",
        "            scores.append(score)\n",
        "\n",
        "        feature_importance[i] = baseline_score - np.mean(scores)  # Drop in performance\n",
        "\n",
        "    # Normalize so that importance values sum to 1\n",
        "    feature_importance /= feature_importance.sum()\n",
        "\n",
        "    return feature_importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "O68svPMLRTFl",
        "outputId": "708c4bc4-5d8d-4b43-885c-85dc3a967e81"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age: 0.0044\n",
            "hypertension: 0.0005\n",
            "heart_disease: 0.0063\n",
            "bmi: 0.0063\n",
            "HbA1c_level: 0.5770\n",
            "blood_glucose_level: 0.4037\n",
            "gender_Male: -0.0003\n",
            "gender_Other: 0.0000\n",
            "smoking_history_current: 0.0000\n",
            "smoking_history_ever: 0.0004\n",
            "smoking_history_former: 0.0007\n",
            "smoking_history_never: 0.0004\n",
            "smoking_history_not current: 0.0005\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Compute feature importance on validation data\n",
        "feature_importance = compute_feature_importance(original_mlp, X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Get feature names if using pandas DataFrame\n",
        "feature_names = x_test.columns.tolist()\n",
        "\n",
        "# Plot normalized feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_names, feature_importance, color=\"royalblue\")\n",
        "plt.xlabel(\"Normalized Feature Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"MLP Feature Importance (Normalized)\")\n",
        "plt.gca().invert_yaxis()  # Highest importance at the top\n",
        "plt.show()\n",
        "\n",
        "for i in range(len(feature_names)):\n",
        "    print(f\"{feature_names[i]}: {feature_importance[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "dZOV7FGTRTFn"
      },
      "outputs": [],
      "source": [
        "# Get reconstruction error by column\n",
        "\n",
        "with torch.no_grad():\n",
        "    reconstructed_train = autoencoder(X_train_tensor).cpu().numpy()\n",
        "    reconstructed_val = autoencoder(X_val_tensor).cpu().numpy()\n",
        "    reconstructed_test = autoencoder(X_test_tensor).cpu().numpy()\n",
        "\n",
        "\n",
        "reconstructed_errors_train = np.abs((x_train.values - reconstructed_train) / (x_train.values+1e-8)) * 100\n",
        "reconstructed_errors_val = np.abs((x_val.values - reconstructed_val) / (x_val.values+1e-8)) * 100\n",
        "reconstructed_errors_test = np.abs((x_test.values - reconstructed_test) / (x_test.values+1e-8)) * 100\n",
        "\n",
        "# Normalize based on column\n",
        "# x_scaler = StandardScaler()\n",
        "# x_scaler = RobustScaler()\n",
        "x_scaler = MinMaxScaler()\n",
        "\n",
        "reconstructed_train_normalized = x_scaler.fit_transform(reconstructed_errors_train)\n",
        "reconstructed_val_normalized = x_scaler.transform(reconstructed_errors_val)\n",
        "reconstructed_test_normalized = x_scaler.transform(reconstructed_errors_test)\n",
        "\n",
        "# only keep the 4th and 5th columns of the normalized reconstruction errors\n",
        "reconstructed_train_normalized = reconstructed_train_normalized[:, 4:6]\n",
        "reconstructed_val_normalized = reconstructed_val_normalized[:, 4:6]\n",
        "reconstructed_test_normalized = reconstructed_test_normalized[:, 4:6]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "y77G6JusRTFo"
      },
      "outputs": [],
      "source": [
        "# Get MLP prediction\n",
        "with torch.no_grad():\n",
        "    y_pred_train = original_mlp(X_train_tensor).cpu().numpy().flatten()\n",
        "    y_pred_val = original_mlp(X_val_tensor).cpu().numpy().flatten()\n",
        "    y_pred_test = original_mlp(X_test_tensor).cpu().numpy().flatten()\n",
        "\n",
        "ground_truth_train = y_train.values\n",
        "ground_truth_val = y_val.values\n",
        "ground_truth_test = y_test.values\n",
        "\n",
        "bias_train = ground_truth_train - y_pred_train\n",
        "bias_val = ground_truth_val - y_pred_val\n",
        "bias_test = ground_truth_test - y_pred_test\n",
        "\n",
        "# y_scaler = StandardScaler()\n",
        "# y_scaler = RobustScaler()\n",
        "y_scaler = MinMaxScaler()\n",
        "\n",
        "bias_train_normalized = y_scaler.fit_transform(bias_train.reshape(-1, 1)).flatten()\n",
        "bias_val_normalized = y_scaler.transform(bias_val.reshape(-1, 1)).flatten()\n",
        "bias_test_normalized = y_scaler.transform(bias_test.reshape(-1, 1)).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "TCmg3iH2RTFo"
      },
      "outputs": [],
      "source": [
        "# def train_bias_predictor(features, target_bias, val_features, val_target_bias,\n",
        "#                          epochs=1000, learning_rate=0.0001, patience=10,\n",
        "#                          input_dim=2, hidden_size=128, dropout=0.2, batch_size=32):\n",
        "\n",
        "#     model = BiasPredictor(input_dim=input_dim, hidden_size=hidden_size, dropout=dropout).to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#     criterion = nn.MSELoss()\n",
        "\n",
        "#     best_val_loss = float('inf')\n",
        "#     patience_counter = 0\n",
        "#     best_model_state = None\n",
        "\n",
        "#     # Reshape target_bias to (batch_size, 1)\n",
        "#     target_bias = target_bias.view(-1, 1)\n",
        "#     val_target_bias = val_target_bias.view(-1, 1)\n",
        "\n",
        "#     # Create DataLoaders\n",
        "#     train_dataset = TensorDataset(features, target_bias)\n",
        "#     val_dataset = TensorDataset(val_features, val_target_bias)\n",
        "\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         model.train()\n",
        "#         total_train_loss = 0.0\n",
        "\n",
        "#         # Training loop\n",
        "#         for batch_features, batch_targets in train_loader:\n",
        "#             batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             predictions = model(batch_features).squeeze()\n",
        "#             loss = criterion(predictions, batch_targets)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             total_train_loss += loss.item()\n",
        "\n",
        "#         # Validation loop\n",
        "#         model.eval()\n",
        "#         total_val_loss = 0.0\n",
        "#         with torch.no_grad():\n",
        "#             for val_batch_features, val_batch_targets in val_loader:\n",
        "#                 val_batch_features, val_batch_targets = val_batch_features.to(device), val_batch_targets.to(device)\n",
        "\n",
        "#                 val_predictions = model(val_batch_features).squeeze()\n",
        "#                 val_loss = criterion(val_predictions, val_batch_targets)\n",
        "\n",
        "#                 total_val_loss += val_loss.item()\n",
        "\n",
        "#         avg_train_loss = total_train_loss / len(train_loader)\n",
        "#         avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "#         # Early stopping check\n",
        "#         if avg_val_loss < best_val_loss:\n",
        "#             best_val_loss = avg_val_loss\n",
        "#             patience_counter = 0\n",
        "#             best_model_state = model.state_dict()  # Save best model\n",
        "#         else:\n",
        "#             patience_counter += 1\n",
        "\n",
        "#         # Print progress\n",
        "#         print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "#         if patience_counter >= patience:\n",
        "#             print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
        "#             break\n",
        "\n",
        "#     # Load best model before returning\n",
        "#     if best_model_state is not None:\n",
        "#         model.load_state_dict(best_model_state)\n",
        "\n",
        "#     return model\n",
        "\n",
        "class WeightedMSELoss(nn.Module):\n",
        "    def __init__(self, scale_factor=2.0):\n",
        "        super(WeightedMSELoss, self).__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        weights = 1 + self.scale_factor * torch.abs(targets)  # Higher weight for large biases\n",
        "        return torch.mean(weights * (predictions - targets) ** 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_bias_predictor(features, target_bias, val_features, val_target_bias,\n",
        "                         epochs=1000, learning_rate=0.0001, patience=10,\n",
        "                         input_dim=2, hidden_size=128, dropout=0.2, batch_size=32, scale_factor=1.25):\n",
        "\n",
        "    model = BiasPredictor(input_dim=input_dim, hidden_size=hidden_size, dropout=dropout).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    # criterion = nn.MSELoss()\n",
        "    criterion = WeightedMSELoss(scale_factor=scale_factor)\n",
        "\n",
        "    # Reshape target_bias to (batch_size, 1)\n",
        "    target_bias = target_bias.view(-1, 1)\n",
        "    val_target_bias = val_target_bias.view(-1, 1)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataset = TensorDataset(features, target_bias)\n",
        "    val_dataset = TensorDataset(val_features, val_target_bias)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for batch_features, batch_targets in train_loader:\n",
        "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(batch_features)  # Ensure output shape is (batch_size, 1)\n",
        "            loss = criterion(predictions, batch_targets.view(-1, 1))  # Match shapes\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for val_batch_features, val_batch_targets in val_loader:\n",
        "                val_batch_features, val_batch_targets = val_batch_features.to(device), val_batch_targets.to(device)\n",
        "\n",
        "                val_predictions = model(val_batch_features)  # Ensure correct shape\n",
        "                val_loss = criterion(val_predictions, val_batch_targets.view(-1, 1))  # Match shapes\n",
        "\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict()  # Save best model\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
        "            break\n",
        "\n",
        "    # Load best model before returning\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "vGZTycOGRTFp"
      },
      "outputs": [],
      "source": [
        "reconstructed_train_tensor = torch.tensor(reconstructed_train_normalized, dtype=torch.float32).to(device)\n",
        "bias_train_tensor = torch.tensor(bias_train_normalized, dtype=torch.float32).to(device)\n",
        "bias_train_tensor = bias_train_tensor.view(-1,1)\n",
        "\n",
        "reconstructed_val_tensor = torch.tensor(reconstructed_val_normalized, dtype=torch.float32).to(device)\n",
        "bias_val_tensor = torch.tensor(bias_val_normalized, dtype=torch.float32).to(device)\n",
        "bias_val_tensor = bias_val_tensor.view(-1,1)\n",
        "\n",
        "# model = train_bias_predictor(\n",
        "#     reconstructed_train_tensor, bias_train_tensor,\n",
        "#     reconstructed_val_tensor, bias_val_tensor,\n",
        "#     epochs=1000, learning_rate=0.0005, patience=10,\n",
        "#     input_dim=2, hidden_size=64, dropout=0.2, batch_size=32\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "Mhz_rXzHRTFp"
      },
      "outputs": [],
      "source": [
        "# model.eval()\n",
        "\n",
        "# # try on test set\n",
        "# reconstructed_test_tensor = torch.tensor(reconstructed_test_normalized, dtype=torch.float32).to(device)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     bias_test_pred = model(reconstructed_test_tensor).cpu().numpy().flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "ZPDD_LfIRTFq"
      },
      "outputs": [],
      "source": [
        "# bias_test_pred_unscaled = y_scaler.inverse_transform(bias_test_pred.reshape(-1, 1)).flatten()\n",
        "\n",
        "# bias_test_pred_unscaled_scaled = bias_test_pred_unscaled\n",
        "# # bias_test_pred_unscaled_scaled = np.clip(bias_test_pred_unscaled_scaled, -0.5, 0.5)\n",
        "# corrected_probs_test = y_pred_test + bias_test_pred_unscaled_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "JqIavc2xRTFq"
      },
      "outputs": [],
      "source": [
        "# # convert corrected prob test to binary\n",
        "# corrected_predictions_test = (corrected_probs_test > 0.5).astype(int)\n",
        "\n",
        "# # Calculate accuracy\n",
        "# corrected_accuracy_test = accuracy_score(ground_truth_test, corrected_predictions_test)\n",
        "# print(f\"Corrected Accuracy on Test Set: {corrected_accuracy_test * 100:.2f}%\")\n",
        "\n",
        "# original_accuracy_test = accuracy_score(ground_truth_test, (y_pred_test > 0.5).astype(int))\n",
        "# print(f\"Original Accuracy on Test Set: {original_accuracy_test * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "SxbH9DZPRTFq"
      },
      "outputs": [],
      "source": [
        "# conf_matrix_original_test = confusion_matrix(y_test, (original_probs_test > 0.5).astype(int))\n",
        "\n",
        "# tn, fp, fn, tp = conf_matrix_original_test.ravel()\n",
        "# recall_original = tp / (tp + fn)\n",
        "# precision_original = tp / (tp + fp)\n",
        "# f1_score_original = 2 * (precision_original * recall_original) / (precision_original + recall_original)\n",
        "# print(\"Original:\")\n",
        "# print(\"True Positives: \", tp)\n",
        "# print(\"True Negatives: \", tn)\n",
        "# print(\"False Positives: \", fp)\n",
        "# print(\"False Negatives: \", fn)\n",
        "# print(\"Recall: \", recall_original)\n",
        "# print(\"Precision: \", precision_original)\n",
        "# print(\"F1 Score: \", f1_score_original)\n",
        "\n",
        "\n",
        "# conf_matrix_corrected_test = confusion_matrix(y_test, corrected_predictions_test)\n",
        "\n",
        "# tn, fp, fn, tp = conf_matrix_corrected_test.ravel()\n",
        "# recall_corrected = tp / (tp + fn)\n",
        "# precision_corrected = tp / (tp + fp)\n",
        "# f1_score_corrected = 2 * (precision_corrected * recall_corrected) / (precision_corrected + recall_corrected)\n",
        "# print(\"\\n\\nCorrected:\")\n",
        "# print(\"True Positives: \", tp)\n",
        "# print(\"True Negatives: \", tn)\n",
        "# print(\"False Positives: \", fp)\n",
        "# print(\"False Negatives: \", fn)\n",
        "# print(\"Recall: \", recall_corrected)\n",
        "# print(\"Precision: \", precision_corrected)\n",
        "# print(\"F1 Score: \", f1_score_corrected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx-baSAkRTFq"
      },
      "source": [
        "### Trying bias correction with combined reconstruction errors (percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "Erndbtv6RTFr"
      },
      "outputs": [],
      "source": [
        "x_train, x_temp, y_train, y_temp = train_test_split(x_scaled, y, test_size=0.3, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
        "x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
        "x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC9A4oE4Squs",
        "outputId": "83792b81-b197-48e2-e196-0cfbca426a74"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-113-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 1 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 0 0 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 1 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_train.loc[:, x_train.select_dtypes('bool').columns] = x_train.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_val.loc[:, x_val.select_dtypes('bool').columns] = x_val.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 1 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 1 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 1 ... 1 0 1]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n",
            "<ipython-input-113-523639d1353c>:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0 0 0 ... 0 0 0]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
            "  x_test.loc[:, x_test.select_dtypes('bool').columns] = x_test.select_dtypes('bool').astype(int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "DXT-UmWkRTFr"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "X_val_tensor = torch.tensor(x_val.values, dtype=torch.float32).to(device)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "X_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "ApwTmkqjRTFr"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    reconstructed_train = autoencoder(X_train_tensor).cpu().numpy()\n",
        "    reconstructed_val = autoencoder(X_val_tensor).cpu().numpy()\n",
        "    reconstructed_test = autoencoder(X_test_tensor).cpu().numpy()\n",
        "\n",
        "temp_train = np.abs((x_train.values - reconstructed_train) / (x_train.values+1e-8)) * 100\n",
        "temp_val = np.abs((x_val.values - reconstructed_val) / (x_val.values+1e-8)) * 100\n",
        "temp_test = np.abs((x_test.values - reconstructed_test) / (x_test.values+1e-8)) * 100\n",
        "\n",
        "\n",
        "# get mean per for each row of temp\n",
        "reconstruction_percent_train = np.mean(temp_train, axis=1)\n",
        "reconstruction_percent_val = np.mean(temp_val, axis=1)\n",
        "reconstruction_percent_test = np.mean(temp_test, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "qn2-X3lQRTFr"
      },
      "outputs": [],
      "source": [
        "x_scaler = StandardScaler()\n",
        "# x_scaler = RobustScaler()\n",
        "# x_scaler = MinMaxScaler()\n",
        "\n",
        "reconstruction_percent_train_normalized = x_scaler.fit_transform(reconstruction_percent_train.reshape(-1, 1)).flatten()\n",
        "reconstruction_percent_val_normalized = x_scaler.transform(reconstruction_percent_val.reshape(-1, 1)).flatten()\n",
        "reconstruction_percent_test_normalized = x_scaler.transform(reconstruction_percent_test.reshape(-1, 1)).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "iqlgDCXgRTFs"
      },
      "outputs": [],
      "source": [
        "# Get MLP prediction\n",
        "with torch.no_grad():\n",
        "    y_pred_train = original_mlp(X_train_tensor).cpu().numpy().flatten()\n",
        "    y_pred_val = original_mlp(X_val_tensor).cpu().numpy().flatten()\n",
        "    y_pred_test = original_mlp(X_test_tensor).cpu().numpy().flatten()\n",
        "\n",
        "ground_truth_train = y_train.values\n",
        "ground_truth_val = y_val.values\n",
        "ground_truth_test = y_test.values\n",
        "\n",
        "bias_train = ground_truth_train - y_pred_train\n",
        "bias_val = ground_truth_val - y_pred_val\n",
        "bias_test = ground_truth_test - y_pred_test\n",
        "\n",
        "# y_scaler = StandardScaler()\n",
        "# y_scaler = RobustScaler()\n",
        "y_scaler = StandardScaler()\n",
        "\n",
        "bias_train_normalized = y_scaler.fit_transform(bias_train.reshape(-1, 1)).flatten()\n",
        "bias_val_normalized = y_scaler.transform(bias_val.reshape(-1, 1)).flatten()\n",
        "bias_test_normalized = y_scaler.transform(bias_test.reshape(-1, 1)).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "ELvYsI65RTFs"
      },
      "outputs": [],
      "source": [
        "reconstructed_train_tensor = torch.tensor(reconstruction_percent_train_normalized, dtype=torch.float32).view(-1,1).to(device)\n",
        "bias_train_tensor = torch.tensor(bias_train_normalized, dtype=torch.float32).to(device)\n",
        "bias_train_tensor = bias_train_tensor.view(-1,1)\n",
        "\n",
        "reconstructed_val_tensor = torch.tensor(reconstruction_percent_val_normalized, dtype=torch.float32).view(-1,1).to(device)\n",
        "bias_val_tensor = torch.tensor(bias_val_normalized, dtype=torch.float32).to(device)\n",
        "bias_val_tensor = bias_val_tensor.view(-1,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "9034IrH6RTFs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# model = train_bias_predictor(\n",
        "#     reconstructed_train_tensor, bias_train_tensor,\n",
        "#     reconstructed_val_tensor, bias_val_tensor,\n",
        "#     epochs=1000, learning_rate=0.01, patience=10,\n",
        "#     input_dim=1, hidden_size=64, dropout=0.1, batch_size=128, scale_factor=1.25\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "wFq8ri4jRTFs"
      },
      "outputs": [],
      "source": [
        "# model.eval()\n",
        "\n",
        "# # try on test set\n",
        "reconstructed_test_tensor = torch.tensor(reconstruction_percent_test_normalized, dtype=torch.float32).view(-1,1).to(device)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     bias_test_pred = model(reconstructed_test_tensor).cpu().numpy().flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "ZCtaI0zERTFt"
      },
      "outputs": [],
      "source": [
        "# bias_test_pred_unscaled = y_scaler.inverse_transform(bias_test_pred.reshape(-1, 1)).flatten()\n",
        "\n",
        "# corrected_probs_test = y_pred_test + bias_test_pred_unscaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "HCyPsLyiRTFt"
      },
      "outputs": [],
      "source": [
        "# # convert corrected prob test to binary\n",
        "# corrected_predictions_test = (corrected_probs_test > 0.5).astype(int)\n",
        "\n",
        "# # Calculate accuracy\n",
        "# corrected_accuracy_test = accuracy_score(ground_truth_test, corrected_predictions_test)\n",
        "# print(f\"Corrected Accuracy on Test Set: {corrected_accuracy_test * 100:.2f}%\")\n",
        "\n",
        "# original_accuracy_test = accuracy_score(ground_truth_test, (y_pred_test > 0.5).astype(int))\n",
        "# print(f\"Original Accuracy on Test Set: {original_accuracy_test * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "_2xZTXJfRTFt"
      },
      "outputs": [],
      "source": [
        "# conf_matrix_original_test = confusion_matrix(y_test, (original_probs_test > 0.5).astype(int))\n",
        "\n",
        "# tn, fp, fn, tp = conf_matrix_original_test.ravel()\n",
        "# recall_original = tp / (tp + fn)\n",
        "# precision_original = tp / (tp + fp)\n",
        "# f1_score_original = 2 * (precision_original * recall_original) / (precision_original + recall_original)\n",
        "# print(\"Original:\")\n",
        "# print(\"True Positives: \", tp)\n",
        "# print(\"True Negatives: \", tn)\n",
        "# print(\"False Positives: \", fp)\n",
        "# print(\"False Negatives: \", fn)\n",
        "# print(\"Recall: \", recall_original)\n",
        "# print(\"Precision: \", precision_original)\n",
        "# print(\"F1 Score: \", f1_score_original)\n",
        "\n",
        "\n",
        "# conf_matrix_corrected_test = confusion_matrix(y_test, corrected_predictions_test)\n",
        "\n",
        "# tn, fp, fn, tp = conf_matrix_corrected_test.ravel()\n",
        "# recall_corrected = tp / (tp + fn)\n",
        "# precision_corrected = tp / (tp + fp)\n",
        "# f1_score_corrected = 2 * (precision_corrected * recall_corrected) / (precision_corrected + recall_corrected)\n",
        "# print(\"\\n\\nCorrected:\")\n",
        "# print(\"True Positives: \", tp)\n",
        "# print(\"True Negatives: \", tn)\n",
        "# print(\"False Positives: \", fp)\n",
        "# print(\"False Negatives: \", fn)\n",
        "# print(\"Recall: \", recall_corrected)\n",
        "# print(\"Precision: \", precision_corrected)\n",
        "# print(\"F1 Score: \", f1_score_corrected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "YOHlTi2qRTFu"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "\n",
        "# # Histogram\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.hist(y_pred_train, bins=50, edgecolor='black', alpha=0.7)\n",
        "# plt.xlabel(\"Bias Values\")\n",
        "# plt.ylabel(\"Frequency\")\n",
        "# plt.title(\"Histogram of bias_train\")\n",
        "\n",
        "# # Box Plot\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.boxplot(y_pred_train, vert=False)\n",
        "# plt.xlabel(\"Bias Values\")\n",
        "# plt.title(\"Box Plot of bias_train\")\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "_jI4kg41RTFu"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, reconstructed_test_tensor, y_pred_test, ground_truth_test):\n",
        "    \"\"\"Runs evaluation on the test set and returns accuracy, recall, precision, F1 score\"\"\"\n",
        "    with torch.no_grad():\n",
        "        bias_test_pred = model(reconstructed_test_tensor).cpu().numpy().flatten()\n",
        "\n",
        "    bias_test_pred_unscaled = y_scaler.inverse_transform(bias_test_pred.reshape(-1, 1)).flatten()\n",
        "    corrected_probs_test = y_pred_test + bias_test_pred_unscaled\n",
        "    corrected_predictions_test = (corrected_probs_test > 0.5).astype(int)\n",
        "\n",
        "    # Compute metrics\n",
        "    original_accuracy = accuracy_score(ground_truth_test, (y_pred_test > 0.5).astype(int))\n",
        "    corrected_accuracy = accuracy_score(ground_truth_test, corrected_predictions_test)\n",
        "\n",
        "    conf_matrix_original = confusion_matrix(ground_truth_test, (y_pred_test > 0.5).astype(int))\n",
        "    tn, fp, fn, tp = conf_matrix_original.ravel()\n",
        "    recall_original = tp / (tp + fn)\n",
        "    precision_original = tp / (tp + fp)\n",
        "    f1_score_original = 2 * (precision_original * recall_original) / (precision_original + recall_original)\n",
        "\n",
        "    conf_matrix_corrected = confusion_matrix(ground_truth_test, corrected_predictions_test)\n",
        "    tn, fp, fn, tp = conf_matrix_corrected.ravel()\n",
        "    recall_corrected = tp / (tp + fn)\n",
        "    precision_corrected = tp / (tp + fp)\n",
        "    f1_score_corrected = 2 * (precision_corrected * recall_corrected) / (precision_corrected + recall_corrected)\n",
        "\n",
        "    return (original_accuracy, corrected_accuracy, recall_original, precision_original, f1_score_original,\n",
        "            recall_corrected, precision_corrected, f1_score_corrected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh47t19tRTFu",
        "outputId": "4a413f04-b4d2-4faa-e0bb-0f507a1ad058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with LR=0.001, Hidden=16, Dropout=0.0, Scale Factor=1.0\n",
            "Combination 1/81\n",
            "Epoch 1/1000, Train Loss: 5.5452, Val Loss: 5.6667\n",
            "Epoch 2/1000, Train Loss: 5.5380, Val Loss: 5.6670\n",
            "Epoch 3/1000, Train Loss: 5.5361, Val Loss: 5.6901\n",
            "Epoch 4/1000, Train Loss: 5.5376, Val Loss: 5.6749\n",
            "Epoch 5/1000, Train Loss: 5.5405, Val Loss: 5.6636\n",
            "Epoch 6/1000, Train Loss: 5.5380, Val Loss: 5.6705\n",
            "Epoch 7/1000, Train Loss: 5.5377, Val Loss: 5.6660\n",
            "Epoch 8/1000, Train Loss: 5.5365, Val Loss: 5.6657\n",
            "Epoch 9/1000, Train Loss: 5.5383, Val Loss: 5.6670\n",
            "Epoch 10/1000, Train Loss: 5.5370, Val Loss: 5.6664\n",
            "Epoch 11/1000, Train Loss: 5.5384, Val Loss: 5.6680\n",
            "Epoch 12/1000, Train Loss: 5.5387, Val Loss: 5.6679\n",
            "Epoch 13/1000, Train Loss: 5.5357, Val Loss: 5.6659\n",
            "Epoch 14/1000, Train Loss: 5.5364, Val Loss: 5.6680\n",
            "Epoch 15/1000, Train Loss: 5.5351, Val Loss: 5.6649\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 5.6636\n",
            "Epoch 1/1000, Train Loss: 5.5637, Val Loss: 5.6687\n",
            "Epoch 2/1000, Train Loss: 5.5412, Val Loss: 5.6749\n",
            "Epoch 3/1000, Train Loss: 5.5412, Val Loss: 5.6692\n",
            "Epoch 4/1000, Train Loss: 5.5402, Val Loss: 5.6676\n",
            "Epoch 5/1000, Train Loss: 5.5391, Val Loss: 5.6714\n",
            "Epoch 6/1000, Train Loss: 5.5381, Val Loss: 5.6741\n",
            "Epoch 7/1000, Train Loss: 5.5390, Val Loss: 5.6664\n",
            "Epoch 8/1000, Train Loss: 5.5385, Val Loss: 5.6668\n",
            "Epoch 9/1000, Train Loss: 5.5431, Val Loss: 5.6695\n",
            "Epoch 10/1000, Train Loss: 5.5429, Val Loss: 5.6700\n",
            "Epoch 11/1000, Train Loss: 5.5391, Val Loss: 5.6696\n",
            "Epoch 12/1000, Train Loss: 5.5378, Val Loss: 5.6792\n",
            "Epoch 13/1000, Train Loss: 5.5388, Val Loss: 5.6706\n",
            "Epoch 14/1000, Train Loss: 5.5370, Val Loss: 5.6709\n",
            "Epoch 15/1000, Train Loss: 5.5406, Val Loss: 5.6711\n",
            "Epoch 16/1000, Train Loss: 5.5361, Val Loss: 5.6783\n",
            "Epoch 17/1000, Train Loss: 5.5378, Val Loss: 5.6659\n",
            "Epoch 18/1000, Train Loss: 5.5395, Val Loss: 5.6687\n",
            "Epoch 19/1000, Train Loss: 5.5372, Val Loss: 5.6698\n",
            "Epoch 20/1000, Train Loss: 5.5365, Val Loss: 5.6712\n",
            "Epoch 21/1000, Train Loss: 5.5376, Val Loss: 5.6719\n",
            "Epoch 22/1000, Train Loss: 5.5368, Val Loss: 5.6734\n",
            "Epoch 23/1000, Train Loss: 5.5363, Val Loss: 5.6732\n",
            "Epoch 24/1000, Train Loss: 5.5399, Val Loss: 5.6690\n",
            "Epoch 25/1000, Train Loss: 5.5389, Val Loss: 5.6690\n",
            "Epoch 26/1000, Train Loss: 5.5349, Val Loss: 5.6728\n",
            "Epoch 27/1000, Train Loss: 5.5370, Val Loss: 5.6717\n",
            "Early stopping triggered at epoch 27. Best Val Loss: 5.6659\n",
            "Epoch 1/1000, Train Loss: 5.5499, Val Loss: 5.6694\n",
            "Epoch 2/1000, Train Loss: 5.5425, Val Loss: 5.6709\n",
            "Epoch 3/1000, Train Loss: 5.5453, Val Loss: 5.6687\n",
            "Epoch 4/1000, Train Loss: 5.5405, Val Loss: 5.6653\n",
            "Epoch 5/1000, Train Loss: 5.5406, Val Loss: 5.6665\n",
            "Epoch 6/1000, Train Loss: 5.5391, Val Loss: 5.6670\n",
            "Epoch 7/1000, Train Loss: 5.5412, Val Loss: 5.6637\n",
            "Epoch 8/1000, Train Loss: 5.5385, Val Loss: 5.6638\n",
            "Epoch 9/1000, Train Loss: 5.5384, Val Loss: 5.6708\n",
            "Epoch 10/1000, Train Loss: 5.5407, Val Loss: 5.6632\n",
            "Epoch 11/1000, Train Loss: 5.5385, Val Loss: 5.6682\n",
            "Epoch 12/1000, Train Loss: 5.5372, Val Loss: 5.6700\n",
            "Epoch 13/1000, Train Loss: 5.5420, Val Loss: 5.6656\n",
            "Epoch 14/1000, Train Loss: 5.5413, Val Loss: 5.6663\n",
            "Epoch 15/1000, Train Loss: 5.5409, Val Loss: 5.6663\n",
            "Epoch 16/1000, Train Loss: 5.5382, Val Loss: 5.6672\n",
            "Epoch 17/1000, Train Loss: 5.5382, Val Loss: 5.6672\n",
            "Epoch 18/1000, Train Loss: 5.5372, Val Loss: 5.6695\n",
            "Epoch 19/1000, Train Loss: 5.5365, Val Loss: 5.6735\n",
            "Epoch 20/1000, Train Loss: 5.5394, Val Loss: 5.6687\n",
            "Early stopping triggered at epoch 20. Best Val Loss: 5.6632\n",
            "Training with LR=0.001, Hidden=16, Dropout=0.0, Scale Factor=1.25\n",
            "Combination 2/81\n",
            "Epoch 1/1000, Train Loss: 6.7057, Val Loss: 6.7671\n",
            "Epoch 2/1000, Train Loss: 6.5984, Val Loss: 6.7547\n",
            "Epoch 3/1000, Train Loss: 6.5968, Val Loss: 6.7559\n",
            "Epoch 4/1000, Train Loss: 6.5952, Val Loss: 6.7538\n",
            "Epoch 5/1000, Train Loss: 6.5970, Val Loss: 6.7616\n",
            "Epoch 6/1000, Train Loss: 6.6018, Val Loss: 6.7550\n",
            "Epoch 7/1000, Train Loss: 6.5962, Val Loss: 6.7561\n",
            "Epoch 8/1000, Train Loss: 6.5953, Val Loss: 6.7672\n",
            "Epoch 9/1000, Train Loss: 6.5964, Val Loss: 6.7576\n",
            "Epoch 10/1000, Train Loss: 6.5934, Val Loss: 6.7634\n",
            "Epoch 11/1000, Train Loss: 6.5943, Val Loss: 6.7582\n",
            "Epoch 12/1000, Train Loss: 6.5991, Val Loss: 6.7584\n",
            "Epoch 13/1000, Train Loss: 6.5943, Val Loss: 6.7587\n",
            "Epoch 14/1000, Train Loss: 6.5984, Val Loss: 6.7583\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 6.7538\n",
            "Epoch 1/1000, Train Loss: 6.6166, Val Loss: 6.7527\n",
            "Epoch 2/1000, Train Loss: 6.5999, Val Loss: 6.7507\n",
            "Epoch 3/1000, Train Loss: 6.5982, Val Loss: 6.7533\n",
            "Epoch 4/1000, Train Loss: 6.5961, Val Loss: 6.7552\n",
            "Epoch 5/1000, Train Loss: 6.5982, Val Loss: 6.7561\n",
            "Epoch 6/1000, Train Loss: 6.5939, Val Loss: 6.7533\n",
            "Epoch 7/1000, Train Loss: 6.5938, Val Loss: 6.7557\n",
            "Epoch 8/1000, Train Loss: 6.5938, Val Loss: 6.7551\n",
            "Epoch 9/1000, Train Loss: 6.5921, Val Loss: 6.7532\n",
            "Epoch 10/1000, Train Loss: 6.6027, Val Loss: 6.7559\n",
            "Epoch 11/1000, Train Loss: 6.6005, Val Loss: 6.7516\n",
            "Epoch 12/1000, Train Loss: 6.5931, Val Loss: 6.7521\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 6.7507\n",
            "Epoch 1/1000, Train Loss: 6.6092, Val Loss: 6.7494\n",
            "Epoch 2/1000, Train Loss: 6.5967, Val Loss: 6.7642\n",
            "Epoch 3/1000, Train Loss: 6.5995, Val Loss: 6.7566\n",
            "Epoch 4/1000, Train Loss: 6.5986, Val Loss: 6.7534\n",
            "Epoch 5/1000, Train Loss: 6.5943, Val Loss: 6.7541\n",
            "Epoch 6/1000, Train Loss: 6.5939, Val Loss: 6.7582\n",
            "Epoch 7/1000, Train Loss: 6.5950, Val Loss: 6.7504\n",
            "Epoch 8/1000, Train Loss: 6.5981, Val Loss: 6.7638\n",
            "Epoch 9/1000, Train Loss: 6.5959, Val Loss: 6.7530\n",
            "Epoch 10/1000, Train Loss: 6.5927, Val Loss: 6.7600\n",
            "Epoch 11/1000, Train Loss: 6.5968, Val Loss: 6.7522\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 6.7494\n",
            "Training with LR=0.001, Hidden=16, Dropout=0.0, Scale Factor=1.5\n",
            "Combination 3/81\n",
            "Epoch 1/1000, Train Loss: 7.6662, Val Loss: 7.8154\n",
            "Epoch 2/1000, Train Loss: 7.6349, Val Loss: 7.8175\n",
            "Epoch 3/1000, Train Loss: 7.6365, Val Loss: 7.8215\n",
            "Epoch 4/1000, Train Loss: 7.6363, Val Loss: 7.8211\n",
            "Epoch 5/1000, Train Loss: 7.6330, Val Loss: 7.8182\n",
            "Epoch 6/1000, Train Loss: 7.6314, Val Loss: 7.8246\n",
            "Epoch 7/1000, Train Loss: 7.6326, Val Loss: 7.8203\n",
            "Epoch 8/1000, Train Loss: 7.6320, Val Loss: 7.8203\n",
            "Epoch 9/1000, Train Loss: 7.6288, Val Loss: 7.8218\n",
            "Epoch 10/1000, Train Loss: 7.6327, Val Loss: 7.8203\n",
            "Epoch 11/1000, Train Loss: 7.6323, Val Loss: 7.8232\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8154\n",
            "Epoch 1/1000, Train Loss: 7.6504, Val Loss: 7.8209\n",
            "Epoch 2/1000, Train Loss: 7.6337, Val Loss: 7.8208\n",
            "Epoch 3/1000, Train Loss: 7.6347, Val Loss: 7.8242\n",
            "Epoch 4/1000, Train Loss: 7.6349, Val Loss: 7.8209\n",
            "Epoch 5/1000, Train Loss: 7.6315, Val Loss: 7.8237\n",
            "Epoch 6/1000, Train Loss: 7.6371, Val Loss: 7.8261\n",
            "Epoch 7/1000, Train Loss: 7.6313, Val Loss: 7.8212\n",
            "Epoch 8/1000, Train Loss: 7.6334, Val Loss: 7.8370\n",
            "Epoch 9/1000, Train Loss: 7.6311, Val Loss: 7.8207\n",
            "Epoch 10/1000, Train Loss: 7.6356, Val Loss: 7.8199\n",
            "Epoch 11/1000, Train Loss: 7.6352, Val Loss: 7.8228\n",
            "Epoch 12/1000, Train Loss: 7.6311, Val Loss: 7.8213\n",
            "Epoch 13/1000, Train Loss: 7.6289, Val Loss: 7.8213\n",
            "Epoch 14/1000, Train Loss: 7.6309, Val Loss: 7.8262\n",
            "Epoch 15/1000, Train Loss: 7.6307, Val Loss: 7.8217\n",
            "Epoch 16/1000, Train Loss: 7.6286, Val Loss: 7.8234\n",
            "Epoch 17/1000, Train Loss: 7.6273, Val Loss: 7.8452\n",
            "Epoch 18/1000, Train Loss: 7.6310, Val Loss: 7.8220\n",
            "Epoch 19/1000, Train Loss: 7.6326, Val Loss: 7.8221\n",
            "Epoch 20/1000, Train Loss: 7.6283, Val Loss: 7.8215\n",
            "Early stopping triggered at epoch 20. Best Val Loss: 7.8199\n",
            "Epoch 1/1000, Train Loss: 7.6485, Val Loss: 7.8405\n",
            "Epoch 2/1000, Train Loss: 7.6393, Val Loss: 7.8218\n",
            "Epoch 3/1000, Train Loss: 7.6346, Val Loss: 7.8185\n",
            "Epoch 4/1000, Train Loss: 7.6330, Val Loss: 7.8186\n",
            "Epoch 5/1000, Train Loss: 7.6325, Val Loss: 7.8174\n",
            "Epoch 6/1000, Train Loss: 7.6313, Val Loss: 7.8223\n",
            "Epoch 7/1000, Train Loss: 7.6310, Val Loss: 7.8193\n",
            "Epoch 8/1000, Train Loss: 7.6321, Val Loss: 7.8191\n",
            "Epoch 9/1000, Train Loss: 7.6325, Val Loss: 7.8204\n",
            "Epoch 10/1000, Train Loss: 7.6339, Val Loss: 7.8189\n",
            "Epoch 11/1000, Train Loss: 7.6327, Val Loss: 7.8169\n",
            "Epoch 12/1000, Train Loss: 7.6304, Val Loss: 7.8196\n",
            "Epoch 13/1000, Train Loss: 7.6268, Val Loss: 7.8255\n",
            "Epoch 14/1000, Train Loss: 7.6317, Val Loss: 7.8202\n",
            "Epoch 15/1000, Train Loss: 7.6289, Val Loss: 7.8326\n",
            "Epoch 16/1000, Train Loss: 7.6295, Val Loss: 7.8176\n",
            "Epoch 17/1000, Train Loss: 7.6329, Val Loss: 7.8194\n",
            "Epoch 18/1000, Train Loss: 7.6292, Val Loss: 7.8202\n",
            "Epoch 19/1000, Train Loss: 7.6275, Val Loss: 7.8332\n",
            "Epoch 20/1000, Train Loss: 7.6311, Val Loss: 7.8216\n",
            "Epoch 21/1000, Train Loss: 7.6270, Val Loss: 7.8279\n",
            "Early stopping triggered at epoch 21. Best Val Loss: 7.8169\n",
            "Training with LR=0.001, Hidden=16, Dropout=0.1, Scale Factor=1.0\n",
            "Combination 4/81\n",
            "Epoch 1/1000, Train Loss: 5.5872, Val Loss: 5.6700\n",
            "Epoch 2/1000, Train Loss: 5.5528, Val Loss: 5.6686\n",
            "Epoch 3/1000, Train Loss: 5.5510, Val Loss: 5.6711\n",
            "Epoch 4/1000, Train Loss: 5.5423, Val Loss: 5.6688\n",
            "Epoch 5/1000, Train Loss: 5.5490, Val Loss: 5.6668\n",
            "Epoch 6/1000, Train Loss: 5.5457, Val Loss: 5.6708\n",
            "Epoch 7/1000, Train Loss: 5.5492, Val Loss: 5.6674\n",
            "Epoch 8/1000, Train Loss: 5.5414, Val Loss: 5.6710\n",
            "Epoch 9/1000, Train Loss: 5.5465, Val Loss: 5.6691\n",
            "Epoch 10/1000, Train Loss: 5.5440, Val Loss: 5.6691\n",
            "Epoch 11/1000, Train Loss: 5.5379, Val Loss: 5.6683\n",
            "Epoch 12/1000, Train Loss: 5.5417, Val Loss: 5.6712\n",
            "Epoch 13/1000, Train Loss: 5.5394, Val Loss: 5.6732\n",
            "Epoch 14/1000, Train Loss: 5.5424, Val Loss: 5.6650\n",
            "Epoch 15/1000, Train Loss: 5.5391, Val Loss: 5.6671\n",
            "Epoch 16/1000, Train Loss: 5.5347, Val Loss: 5.6674\n",
            "Epoch 17/1000, Train Loss: 5.5423, Val Loss: 5.6654\n",
            "Epoch 18/1000, Train Loss: 5.5378, Val Loss: 5.6655\n",
            "Epoch 19/1000, Train Loss: 5.5357, Val Loss: 5.6694\n",
            "Epoch 20/1000, Train Loss: 5.5382, Val Loss: 5.6669\n",
            "Epoch 21/1000, Train Loss: 5.5367, Val Loss: 5.6673\n",
            "Epoch 22/1000, Train Loss: 5.5381, Val Loss: 5.6663\n",
            "Epoch 23/1000, Train Loss: 5.5384, Val Loss: 5.6692\n",
            "Epoch 24/1000, Train Loss: 5.5388, Val Loss: 5.6670\n",
            "Early stopping triggered at epoch 24. Best Val Loss: 5.6650\n",
            "Epoch 1/1000, Train Loss: 5.5724, Val Loss: 5.6699\n",
            "Epoch 2/1000, Train Loss: 5.5606, Val Loss: 5.6680\n",
            "Epoch 3/1000, Train Loss: 5.5467, Val Loss: 5.6703\n",
            "Epoch 4/1000, Train Loss: 5.5494, Val Loss: 5.6668\n",
            "Epoch 5/1000, Train Loss: 5.5397, Val Loss: 5.6679\n",
            "Epoch 6/1000, Train Loss: 5.5408, Val Loss: 5.6690\n",
            "Epoch 7/1000, Train Loss: 5.5371, Val Loss: 5.6689\n",
            "Epoch 8/1000, Train Loss: 5.5369, Val Loss: 5.6747\n",
            "Epoch 9/1000, Train Loss: 5.5383, Val Loss: 5.6670\n",
            "Epoch 10/1000, Train Loss: 5.5433, Val Loss: 5.6670\n",
            "Epoch 11/1000, Train Loss: 5.5390, Val Loss: 5.6700\n",
            "Epoch 12/1000, Train Loss: 5.5392, Val Loss: 5.6665\n",
            "Epoch 13/1000, Train Loss: 5.5370, Val Loss: 5.6701\n",
            "Epoch 14/1000, Train Loss: 5.5370, Val Loss: 5.6605\n",
            "Epoch 15/1000, Train Loss: 5.5404, Val Loss: 5.6719\n",
            "Epoch 16/1000, Train Loss: 5.5453, Val Loss: 5.6687\n",
            "Epoch 17/1000, Train Loss: 5.5406, Val Loss: 5.6660\n",
            "Epoch 18/1000, Train Loss: 5.5412, Val Loss: 5.6689\n",
            "Epoch 19/1000, Train Loss: 5.5376, Val Loss: 5.6684\n",
            "Epoch 20/1000, Train Loss: 5.5359, Val Loss: 5.6720\n",
            "Epoch 21/1000, Train Loss: 5.5356, Val Loss: 5.6652\n",
            "Epoch 22/1000, Train Loss: 5.5385, Val Loss: 5.6704\n",
            "Epoch 23/1000, Train Loss: 5.5384, Val Loss: 5.6677\n",
            "Epoch 24/1000, Train Loss: 5.5390, Val Loss: 5.6663\n",
            "Early stopping triggered at epoch 24. Best Val Loss: 5.6605\n",
            "Epoch 1/1000, Train Loss: 5.5524, Val Loss: 5.6631\n",
            "Epoch 2/1000, Train Loss: 5.5429, Val Loss: 5.6806\n",
            "Epoch 3/1000, Train Loss: 5.5470, Val Loss: 5.6664\n",
            "Epoch 4/1000, Train Loss: 5.5411, Val Loss: 5.6654\n",
            "Epoch 5/1000, Train Loss: 5.5393, Val Loss: 5.6668\n",
            "Epoch 6/1000, Train Loss: 5.5404, Val Loss: 5.6710\n",
            "Epoch 7/1000, Train Loss: 5.5381, Val Loss: 5.6727\n",
            "Epoch 8/1000, Train Loss: 5.5357, Val Loss: 5.6707\n",
            "Epoch 9/1000, Train Loss: 5.5347, Val Loss: 5.6718\n",
            "Epoch 10/1000, Train Loss: 5.5397, Val Loss: 5.6643\n",
            "Epoch 11/1000, Train Loss: 5.5406, Val Loss: 5.6681\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 5.6631\n",
            "Training with LR=0.001, Hidden=16, Dropout=0.1, Scale Factor=1.25\n",
            "Combination 5/81\n",
            "Epoch 1/1000, Train Loss: 6.6202, Val Loss: 6.7518\n",
            "Epoch 2/1000, Train Loss: 6.5935, Val Loss: 6.7567\n",
            "Epoch 3/1000, Train Loss: 6.5943, Val Loss: 6.7556\n",
            "Epoch 4/1000, Train Loss: 6.5953, Val Loss: 6.7594\n",
            "Epoch 5/1000, Train Loss: 6.5973, Val Loss: 6.7556\n",
            "Epoch 6/1000, Train Loss: 6.5946, Val Loss: 6.7592\n",
            "Epoch 7/1000, Train Loss: 6.5932, Val Loss: 6.7577\n",
            "Epoch 8/1000, Train Loss: 6.5950, Val Loss: 6.7558\n",
            "Epoch 9/1000, Train Loss: 6.5948, Val Loss: 6.7542\n",
            "Epoch 10/1000, Train Loss: 6.5957, Val Loss: 6.7576\n",
            "Epoch 11/1000, Train Loss: 6.5934, Val Loss: 6.7568\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 6.7518\n",
            "Epoch 1/1000, Train Loss: 6.6336, Val Loss: 6.7561\n",
            "Epoch 2/1000, Train Loss: 6.6156, Val Loss: 6.7573\n",
            "Epoch 3/1000, Train Loss: 6.6050, Val Loss: 6.7631\n",
            "Epoch 4/1000, Train Loss: 6.6014, Val Loss: 6.7534\n",
            "Epoch 5/1000, Train Loss: 6.5979, Val Loss: 6.7569\n",
            "Epoch 6/1000, Train Loss: 6.6028, Val Loss: 6.7612\n",
            "Epoch 7/1000, Train Loss: 6.6042, Val Loss: 6.7573\n",
            "Epoch 8/1000, Train Loss: 6.5987, Val Loss: 6.7558\n",
            "Epoch 9/1000, Train Loss: 6.5998, Val Loss: 6.7596\n",
            "Epoch 10/1000, Train Loss: 6.5970, Val Loss: 6.7567\n",
            "Epoch 11/1000, Train Loss: 6.5987, Val Loss: 6.7570\n",
            "Epoch 12/1000, Train Loss: 6.5953, Val Loss: 6.7552\n",
            "Epoch 13/1000, Train Loss: 6.5967, Val Loss: 6.7583\n",
            "Epoch 14/1000, Train Loss: 6.5990, Val Loss: 6.7571\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 6.7534\n",
            "Epoch 1/1000, Train Loss: 6.6114, Val Loss: 6.7578\n",
            "Epoch 2/1000, Train Loss: 6.6020, Val Loss: 6.7648\n",
            "Epoch 3/1000, Train Loss: 6.6053, Val Loss: 6.7566\n",
            "Epoch 4/1000, Train Loss: 6.6014, Val Loss: 6.7630\n",
            "Epoch 5/1000, Train Loss: 6.6023, Val Loss: 6.7609\n",
            "Epoch 6/1000, Train Loss: 6.6019, Val Loss: 6.7621\n",
            "Epoch 7/1000, Train Loss: 6.5976, Val Loss: 6.7617\n",
            "Epoch 8/1000, Train Loss: 6.5980, Val Loss: 6.7599\n",
            "Epoch 9/1000, Train Loss: 6.5955, Val Loss: 6.7557\n",
            "Epoch 10/1000, Train Loss: 6.5984, Val Loss: 6.7598\n",
            "Epoch 11/1000, Train Loss: 6.5954, Val Loss: 6.7588\n",
            "Epoch 12/1000, Train Loss: 6.5988, Val Loss: 6.7583\n",
            "Epoch 13/1000, Train Loss: 6.5970, Val Loss: 6.7576\n",
            "Epoch 14/1000, Train Loss: 6.5982, Val Loss: 6.7637\n",
            "Epoch 15/1000, Train Loss: 6.6047, Val Loss: 6.7550\n",
            "Epoch 16/1000, Train Loss: 6.5943, Val Loss: 6.7587\n",
            "Epoch 17/1000, Train Loss: 6.5897, Val Loss: 6.7566\n",
            "Epoch 18/1000, Train Loss: 6.5935, Val Loss: 6.7564\n",
            "Epoch 19/1000, Train Loss: 6.5963, Val Loss: 6.7583\n",
            "Epoch 20/1000, Train Loss: 6.5939, Val Loss: 6.7599\n",
            "Epoch 21/1000, Train Loss: 6.5980, Val Loss: 6.7568\n",
            "Epoch 22/1000, Train Loss: 6.5962, Val Loss: 6.7598\n",
            "Epoch 23/1000, Train Loss: 6.5926, Val Loss: 6.7568\n",
            "Epoch 24/1000, Train Loss: 6.5938, Val Loss: 6.7582\n",
            "Epoch 25/1000, Train Loss: 6.5964, Val Loss: 6.7586\n",
            "Early stopping triggered at epoch 25. Best Val Loss: 6.7550\n",
            "Training with LR=0.001, Hidden=16, Dropout=0.1, Scale Factor=1.5\n",
            "Combination 6/81\n",
            "Epoch 1/1000, Train Loss: 7.7372, Val Loss: 7.8245\n",
            "Epoch 2/1000, Train Loss: 7.6747, Val Loss: 7.8224\n",
            "Epoch 3/1000, Train Loss: 7.6540, Val Loss: 7.8181\n",
            "Epoch 4/1000, Train Loss: 7.6485, Val Loss: 7.8249\n",
            "Epoch 5/1000, Train Loss: 7.6423, Val Loss: 7.8193\n",
            "Epoch 6/1000, Train Loss: 7.6423, Val Loss: 7.8214\n",
            "Epoch 7/1000, Train Loss: 7.6462, Val Loss: 7.8277\n",
            "Epoch 8/1000, Train Loss: 7.6378, Val Loss: 7.8177\n",
            "Epoch 9/1000, Train Loss: 7.6550, Val Loss: 7.8167\n",
            "Epoch 10/1000, Train Loss: 7.6410, Val Loss: 7.8194\n",
            "Epoch 11/1000, Train Loss: 7.6358, Val Loss: 7.8282\n",
            "Epoch 12/1000, Train Loss: 7.6419, Val Loss: 7.8203\n",
            "Epoch 13/1000, Train Loss: 7.6455, Val Loss: 7.8198\n",
            "Epoch 14/1000, Train Loss: 7.6346, Val Loss: 7.8221\n",
            "Epoch 15/1000, Train Loss: 7.6374, Val Loss: 7.8194\n",
            "Epoch 16/1000, Train Loss: 7.6368, Val Loss: 7.8183\n",
            "Epoch 17/1000, Train Loss: 7.6410, Val Loss: 7.8210\n",
            "Epoch 18/1000, Train Loss: 7.6354, Val Loss: 7.8189\n",
            "Epoch 19/1000, Train Loss: 7.6374, Val Loss: 7.8174\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 7.8167\n",
            "Epoch 1/1000, Train Loss: 7.7082, Val Loss: 7.8175\n",
            "Epoch 2/1000, Train Loss: 7.6376, Val Loss: 7.8160\n",
            "Epoch 3/1000, Train Loss: 7.6413, Val Loss: 7.8156\n",
            "Epoch 4/1000, Train Loss: 7.6275, Val Loss: 7.8156\n",
            "Epoch 5/1000, Train Loss: 7.6414, Val Loss: 7.8151\n",
            "Epoch 6/1000, Train Loss: 7.6363, Val Loss: 7.8171\n",
            "Epoch 7/1000, Train Loss: 7.6333, Val Loss: 7.8173\n",
            "Epoch 8/1000, Train Loss: 7.6352, Val Loss: 7.8190\n",
            "Epoch 9/1000, Train Loss: 7.6401, Val Loss: 7.8202\n",
            "Epoch 10/1000, Train Loss: 7.6365, Val Loss: 7.8175\n",
            "Epoch 11/1000, Train Loss: 7.6341, Val Loss: 7.8206\n",
            "Epoch 12/1000, Train Loss: 7.6350, Val Loss: 7.8189\n",
            "Epoch 13/1000, Train Loss: 7.6327, Val Loss: 7.8195\n",
            "Epoch 14/1000, Train Loss: 7.6330, Val Loss: 7.8216\n",
            "Epoch 15/1000, Train Loss: 7.6357, Val Loss: 7.8191\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 7.8151\n",
            "Epoch 1/1000, Train Loss: 7.6936, Val Loss: 7.8268\n",
            "Epoch 2/1000, Train Loss: 7.6439, Val Loss: 7.8201\n",
            "Epoch 3/1000, Train Loss: 7.6405, Val Loss: 7.8249\n",
            "Epoch 4/1000, Train Loss: 7.6404, Val Loss: 7.8200\n",
            "Epoch 5/1000, Train Loss: 7.6504, Val Loss: 7.8258\n",
            "Epoch 6/1000, Train Loss: 7.6393, Val Loss: 7.8228\n",
            "Epoch 7/1000, Train Loss: 7.6401, Val Loss: 7.8237\n",
            "Epoch 8/1000, Train Loss: 7.6340, Val Loss: 7.8353\n",
            "Epoch 9/1000, Train Loss: 7.6378, Val Loss: 7.8252\n",
            "Epoch 10/1000, Train Loss: 7.6373, Val Loss: 7.8218\n",
            "Epoch 11/1000, Train Loss: 7.6412, Val Loss: 7.8240\n",
            "Epoch 12/1000, Train Loss: 7.6392, Val Loss: 7.8288\n",
            "Epoch 13/1000, Train Loss: 7.6434, Val Loss: 7.8207\n",
            "Epoch 14/1000, Train Loss: 7.6310, Val Loss: 7.8235\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 7.8200\n",
            "Training with LR=0.001, Hidden=16, Dropout=0.2, Scale Factor=1.0\n",
            "Combination 7/81\n",
            "Epoch 1/1000, Train Loss: 5.5522, Val Loss: 5.6673\n",
            "Epoch 2/1000, Train Loss: 5.5430, Val Loss: 5.6686\n",
            "Epoch 3/1000, Train Loss: 5.5419, Val Loss: 5.6668\n",
            "Epoch 4/1000, Train Loss: 5.5416, Val Loss: 5.6669\n",
            "Epoch 5/1000, Train Loss: 5.5426, Val Loss: 5.6714\n",
            "Epoch 6/1000, Train Loss: 5.5375, Val Loss: 5.6659\n",
            "Epoch 7/1000, Train Loss: 5.5426, Val Loss: 5.6694\n",
            "Epoch 8/1000, Train Loss: 5.5389, Val Loss: 5.6727\n",
            "Epoch 9/1000, Train Loss: 5.5375, Val Loss: 5.6671\n",
            "Epoch 10/1000, Train Loss: 5.5397, Val Loss: 5.6682\n",
            "Epoch 11/1000, Train Loss: 5.5384, Val Loss: 5.6716\n",
            "Epoch 12/1000, Train Loss: 5.5349, Val Loss: 5.6719\n",
            "Epoch 13/1000, Train Loss: 5.5357, Val Loss: 5.6697\n",
            "Epoch 14/1000, Train Loss: 5.5398, Val Loss: 5.6672\n",
            "Epoch 15/1000, Train Loss: 5.5453, Val Loss: 5.6671\n",
            "Epoch 16/1000, Train Loss: 5.5380, Val Loss: 5.6683\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 5.6659\n",
            "Epoch 1/1000, Train Loss: 5.6526, Val Loss: 5.6752\n",
            "Epoch 2/1000, Train Loss: 5.5685, Val Loss: 5.6761\n",
            "Epoch 3/1000, Train Loss: 5.5522, Val Loss: 5.6706\n",
            "Epoch 4/1000, Train Loss: 5.5416, Val Loss: 5.6751\n",
            "Epoch 5/1000, Train Loss: 5.5471, Val Loss: 5.6691\n",
            "Epoch 6/1000, Train Loss: 5.5399, Val Loss: 5.6700\n",
            "Epoch 7/1000, Train Loss: 5.5485, Val Loss: 5.6652\n",
            "Epoch 8/1000, Train Loss: 5.5424, Val Loss: 5.6700\n",
            "Epoch 9/1000, Train Loss: 5.5434, Val Loss: 5.6628\n",
            "Epoch 10/1000, Train Loss: 5.5400, Val Loss: 5.6660\n",
            "Epoch 11/1000, Train Loss: 5.5419, Val Loss: 5.6684\n",
            "Epoch 12/1000, Train Loss: 5.5351, Val Loss: 5.6720\n",
            "Epoch 13/1000, Train Loss: 5.5465, Val Loss: 5.6678\n",
            "Epoch 14/1000, Train Loss: 5.5396, Val Loss: 5.6723\n",
            "Epoch 15/1000, Train Loss: 5.5453, Val Loss: 5.6659\n",
            "Epoch 16/1000, Train Loss: 5.5411, Val Loss: 5.6673\n",
            "Epoch 17/1000, Train Loss: 5.5419, Val Loss: 5.6669\n",
            "Epoch 18/1000, Train Loss: 5.5414, Val Loss: 5.6660\n",
            "Epoch 19/1000, Train Loss: 5.5426, Val Loss: 5.6726\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 5.6628\n",
            "Epoch 1/1000, Train Loss: 5.6121, Val Loss: 5.6706\n",
            "Epoch 2/1000, Train Loss: 5.5587, Val Loss: 5.6699\n",
            "Epoch 3/1000, Train Loss: 5.5583, Val Loss: 5.6694\n",
            "Epoch 4/1000, Train Loss: 5.5482, Val Loss: 5.6681\n",
            "Epoch 5/1000, Train Loss: 5.5488, Val Loss: 5.6682\n",
            "Epoch 6/1000, Train Loss: 5.5490, Val Loss: 5.6672\n",
            "Epoch 7/1000, Train Loss: 5.5439, Val Loss: 5.6671\n",
            "Epoch 8/1000, Train Loss: 5.5473, Val Loss: 5.6649\n",
            "Epoch 9/1000, Train Loss: 5.5429, Val Loss: 5.6673\n",
            "Epoch 10/1000, Train Loss: 5.5414, Val Loss: 5.6648\n",
            "Epoch 11/1000, Train Loss: 5.5432, Val Loss: 5.6660\n",
            "Epoch 12/1000, Train Loss: 5.5404, Val Loss: 5.6657\n",
            "Epoch 13/1000, Train Loss: 5.5376, Val Loss: 5.6669\n",
            "Epoch 14/1000, Train Loss: 5.5371, Val Loss: 5.6704\n",
            "Epoch 15/1000, Train Loss: 5.5438, Val Loss: 5.6710\n",
            "Epoch 16/1000, Train Loss: 5.5358, Val Loss: 5.6659\n",
            "Epoch 17/1000, Train Loss: 5.5380, Val Loss: 5.6686\n",
            "Epoch 18/1000, Train Loss: 5.5380, Val Loss: 5.6670\n",
            "Epoch 19/1000, Train Loss: 5.5393, Val Loss: 5.6676\n",
            "Epoch 20/1000, Train Loss: 5.5394, Val Loss: 5.6687\n",
            "Early stopping triggered at epoch 20. Best Val Loss: 5.6648\n",
            "Training with LR=0.001, Hidden=16, Dropout=0.2, Scale Factor=1.25\n",
            "Combination 8/81\n",
            "Epoch 1/1000, Train Loss: 6.6233, Val Loss: 6.7564\n",
            "Epoch 2/1000, Train Loss: 6.6154, Val Loss: 6.7539\n",
            "Epoch 3/1000, Train Loss: 6.5993, Val Loss: 6.7599\n",
            "Epoch 4/1000, Train Loss: 6.6016, Val Loss: 6.7561\n",
            "Epoch 5/1000, Train Loss: 6.6007, Val Loss: 6.7524\n",
            "Epoch 6/1000, Train Loss: 6.5978, Val Loss: 6.7552\n",
            "Epoch 7/1000, Train Loss: 6.5998, Val Loss: 6.7547\n",
            "Epoch 8/1000, Train Loss: 6.5973, Val Loss: 6.7552\n",
            "Epoch 9/1000, Train Loss: 6.5965, Val Loss: 6.7558\n",
            "Epoch 10/1000, Train Loss: 6.6050, Val Loss: 6.7567\n",
            "Epoch 11/1000, Train Loss: 6.5981, Val Loss: 6.7559\n",
            "Epoch 12/1000, Train Loss: 6.5993, Val Loss: 6.7538\n",
            "Epoch 13/1000, Train Loss: 6.5986, Val Loss: 6.7608\n",
            "Epoch 14/1000, Train Loss: 6.5986, Val Loss: 6.7608\n",
            "Epoch 15/1000, Train Loss: 6.5955, Val Loss: 6.7546\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 6.7524\n",
            "Epoch 1/1000, Train Loss: 6.6848, Val Loss: 6.7490\n",
            "Epoch 2/1000, Train Loss: 6.6134, Val Loss: 6.7501\n",
            "Epoch 3/1000, Train Loss: 6.6030, Val Loss: 6.7514\n",
            "Epoch 4/1000, Train Loss: 6.6028, Val Loss: 6.7518\n",
            "Epoch 5/1000, Train Loss: 6.6049, Val Loss: 6.7520\n",
            "Epoch 6/1000, Train Loss: 6.5966, Val Loss: 6.7505\n",
            "Epoch 7/1000, Train Loss: 6.6021, Val Loss: 6.7518\n",
            "Epoch 8/1000, Train Loss: 6.5967, Val Loss: 6.7515\n",
            "Epoch 9/1000, Train Loss: 6.5964, Val Loss: 6.7542\n",
            "Epoch 10/1000, Train Loss: 6.5987, Val Loss: 6.7504\n",
            "Epoch 11/1000, Train Loss: 6.5968, Val Loss: 6.7545\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 6.7490\n",
            "Epoch 1/1000, Train Loss: 6.6944, Val Loss: 6.7555\n",
            "Epoch 2/1000, Train Loss: 6.6098, Val Loss: 6.7549\n",
            "Epoch 3/1000, Train Loss: 6.6080, Val Loss: 6.7555\n",
            "Epoch 4/1000, Train Loss: 6.6026, Val Loss: 6.7545\n",
            "Epoch 5/1000, Train Loss: 6.6126, Val Loss: 6.7557\n",
            "Epoch 6/1000, Train Loss: 6.6047, Val Loss: 6.7576\n",
            "Epoch 7/1000, Train Loss: 6.6004, Val Loss: 6.7575\n",
            "Epoch 8/1000, Train Loss: 6.6102, Val Loss: 6.7560\n",
            "Epoch 9/1000, Train Loss: 6.5988, Val Loss: 6.7560\n",
            "Epoch 10/1000, Train Loss: 6.6026, Val Loss: 6.7551\n",
            "Epoch 11/1000, Train Loss: 6.6077, Val Loss: 6.7536\n",
            "Epoch 12/1000, Train Loss: 6.5934, Val Loss: 6.7557\n",
            "Epoch 13/1000, Train Loss: 6.5938, Val Loss: 6.7574\n",
            "Epoch 14/1000, Train Loss: 6.5979, Val Loss: 6.7586\n",
            "Epoch 15/1000, Train Loss: 6.5908, Val Loss: 6.7570\n",
            "Epoch 16/1000, Train Loss: 6.5966, Val Loss: 6.7540\n",
            "Epoch 17/1000, Train Loss: 6.5959, Val Loss: 6.7547\n",
            "Epoch 18/1000, Train Loss: 6.6002, Val Loss: 6.7566\n",
            "Epoch 19/1000, Train Loss: 6.5973, Val Loss: 6.7536\n",
            "Epoch 20/1000, Train Loss: 6.5979, Val Loss: 6.7544\n",
            "Epoch 21/1000, Train Loss: 6.5991, Val Loss: 6.7538\n",
            "Epoch 22/1000, Train Loss: 6.5987, Val Loss: 6.7580\n",
            "Epoch 23/1000, Train Loss: 6.5990, Val Loss: 6.7579\n",
            "Epoch 24/1000, Train Loss: 6.5940, Val Loss: 6.7588\n",
            "Epoch 25/1000, Train Loss: 6.5932, Val Loss: 6.7600\n",
            "Epoch 26/1000, Train Loss: 6.5946, Val Loss: 6.7576\n",
            "Epoch 27/1000, Train Loss: 6.5929, Val Loss: 6.7649\n",
            "Epoch 28/1000, Train Loss: 6.5993, Val Loss: 6.7607\n",
            "Epoch 29/1000, Train Loss: 6.5981, Val Loss: 6.7599\n",
            "Early stopping triggered at epoch 29. Best Val Loss: 6.7536\n",
            "Training with LR=0.001, Hidden=16, Dropout=0.2, Scale Factor=1.5\n",
            "Combination 9/81\n",
            "Epoch 1/1000, Train Loss: 7.7075, Val Loss: 7.8223\n",
            "Epoch 2/1000, Train Loss: 7.6563, Val Loss: 7.8214\n",
            "Epoch 3/1000, Train Loss: 7.6600, Val Loss: 7.8225\n",
            "Epoch 4/1000, Train Loss: 7.6523, Val Loss: 7.8401\n",
            "Epoch 5/1000, Train Loss: 7.6445, Val Loss: 7.8214\n",
            "Epoch 6/1000, Train Loss: 7.6490, Val Loss: 7.8263\n",
            "Epoch 7/1000, Train Loss: 7.6425, Val Loss: 7.8208\n",
            "Epoch 8/1000, Train Loss: 7.6427, Val Loss: 7.8211\n",
            "Epoch 9/1000, Train Loss: 7.6347, Val Loss: 7.8220\n",
            "Epoch 10/1000, Train Loss: 7.6366, Val Loss: 7.8196\n",
            "Epoch 11/1000, Train Loss: 7.6331, Val Loss: 7.8220\n",
            "Epoch 12/1000, Train Loss: 7.6383, Val Loss: 7.8208\n",
            "Epoch 13/1000, Train Loss: 7.6361, Val Loss: 7.8225\n",
            "Epoch 14/1000, Train Loss: 7.6402, Val Loss: 7.8267\n",
            "Epoch 15/1000, Train Loss: 7.6382, Val Loss: 7.8196\n",
            "Epoch 16/1000, Train Loss: 7.6335, Val Loss: 7.8222\n",
            "Epoch 17/1000, Train Loss: 7.6333, Val Loss: 7.8254\n",
            "Epoch 18/1000, Train Loss: 7.6360, Val Loss: 7.8248\n",
            "Epoch 19/1000, Train Loss: 7.6314, Val Loss: 7.8207\n",
            "Epoch 20/1000, Train Loss: 7.6339, Val Loss: 7.8213\n",
            "Early stopping triggered at epoch 20. Best Val Loss: 7.8196\n",
            "Epoch 1/1000, Train Loss: 7.7224, Val Loss: 7.8184\n",
            "Epoch 2/1000, Train Loss: 7.6565, Val Loss: 7.8201\n",
            "Epoch 3/1000, Train Loss: 7.6516, Val Loss: 7.8200\n",
            "Epoch 4/1000, Train Loss: 7.6404, Val Loss: 7.8201\n",
            "Epoch 5/1000, Train Loss: 7.6373, Val Loss: 7.8160\n",
            "Epoch 6/1000, Train Loss: 7.6347, Val Loss: 7.8210\n",
            "Epoch 7/1000, Train Loss: 7.6323, Val Loss: 7.8192\n",
            "Epoch 8/1000, Train Loss: 7.6400, Val Loss: 7.8187\n",
            "Epoch 9/1000, Train Loss: 7.6346, Val Loss: 7.8174\n",
            "Epoch 10/1000, Train Loss: 7.6332, Val Loss: 7.8174\n",
            "Epoch 11/1000, Train Loss: 7.6287, Val Loss: 7.8178\n",
            "Epoch 12/1000, Train Loss: 7.6360, Val Loss: 7.8221\n",
            "Epoch 13/1000, Train Loss: 7.6311, Val Loss: 7.8195\n",
            "Epoch 14/1000, Train Loss: 7.6356, Val Loss: 7.8229\n",
            "Epoch 15/1000, Train Loss: 7.6371, Val Loss: 7.8204\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 7.8160\n",
            "Epoch 1/1000, Train Loss: 7.6929, Val Loss: 7.8121\n",
            "Epoch 2/1000, Train Loss: 7.6639, Val Loss: 7.8140\n",
            "Epoch 3/1000, Train Loss: 7.6428, Val Loss: 7.8160\n",
            "Epoch 4/1000, Train Loss: 7.6418, Val Loss: 7.8176\n",
            "Epoch 5/1000, Train Loss: 7.6411, Val Loss: 7.8174\n",
            "Epoch 6/1000, Train Loss: 7.6424, Val Loss: 7.8183\n",
            "Epoch 7/1000, Train Loss: 7.6459, Val Loss: 7.8195\n",
            "Epoch 8/1000, Train Loss: 7.6403, Val Loss: 7.8163\n",
            "Epoch 9/1000, Train Loss: 7.6283, Val Loss: 7.8279\n",
            "Epoch 10/1000, Train Loss: 7.6325, Val Loss: 7.8197\n",
            "Epoch 11/1000, Train Loss: 7.6397, Val Loss: 7.8193\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8121\n",
            "Training with LR=0.001, Hidden=32, Dropout=0.0, Scale Factor=1.0\n",
            "Combination 10/81\n",
            "Epoch 1/1000, Train Loss: 5.5474, Val Loss: 5.6946\n",
            "Epoch 2/1000, Train Loss: 5.5423, Val Loss: 5.6774\n",
            "Epoch 3/1000, Train Loss: 5.5410, Val Loss: 5.6712\n",
            "Epoch 4/1000, Train Loss: 5.5390, Val Loss: 5.6772\n",
            "Epoch 5/1000, Train Loss: 5.5385, Val Loss: 5.6682\n",
            "Epoch 6/1000, Train Loss: 5.5390, Val Loss: 5.6806\n",
            "Epoch 7/1000, Train Loss: 5.5379, Val Loss: 5.6667\n",
            "Epoch 8/1000, Train Loss: 5.5404, Val Loss: 5.6710\n",
            "Epoch 9/1000, Train Loss: 5.5362, Val Loss: 5.6660\n",
            "Epoch 10/1000, Train Loss: 5.5360, Val Loss: 5.6648\n",
            "Epoch 11/1000, Train Loss: 5.5371, Val Loss: 5.6694\n",
            "Epoch 12/1000, Train Loss: 5.5364, Val Loss: 5.6719\n",
            "Epoch 13/1000, Train Loss: 5.5347, Val Loss: 5.6754\n",
            "Epoch 14/1000, Train Loss: 5.5381, Val Loss: 5.6646\n",
            "Epoch 15/1000, Train Loss: 5.5358, Val Loss: 5.6740\n",
            "Epoch 16/1000, Train Loss: 5.5364, Val Loss: 5.6687\n",
            "Epoch 17/1000, Train Loss: 5.5358, Val Loss: 5.6701\n",
            "Epoch 18/1000, Train Loss: 5.5345, Val Loss: 5.6665\n",
            "Epoch 19/1000, Train Loss: 5.5348, Val Loss: 5.6733\n",
            "Epoch 20/1000, Train Loss: 5.5355, Val Loss: 5.6667\n",
            "Epoch 21/1000, Train Loss: 5.5345, Val Loss: 5.6704\n",
            "Epoch 22/1000, Train Loss: 5.5375, Val Loss: 5.6716\n",
            "Epoch 23/1000, Train Loss: 5.5362, Val Loss: 5.6699\n",
            "Epoch 24/1000, Train Loss: 5.5390, Val Loss: 5.6707\n",
            "Early stopping triggered at epoch 24. Best Val Loss: 5.6646\n",
            "Epoch 1/1000, Train Loss: 5.5631, Val Loss: 5.6698\n",
            "Epoch 2/1000, Train Loss: 5.5447, Val Loss: 5.6738\n",
            "Epoch 3/1000, Train Loss: 5.5434, Val Loss: 5.6723\n",
            "Epoch 4/1000, Train Loss: 5.5411, Val Loss: 5.6706\n",
            "Epoch 5/1000, Train Loss: 5.5386, Val Loss: 5.7176\n",
            "Epoch 6/1000, Train Loss: 5.5394, Val Loss: 5.6667\n",
            "Epoch 7/1000, Train Loss: 5.5378, Val Loss: 5.6941\n",
            "Epoch 8/1000, Train Loss: 5.5389, Val Loss: 5.6736\n",
            "Epoch 9/1000, Train Loss: 5.5395, Val Loss: 5.6714\n",
            "Epoch 10/1000, Train Loss: 5.5392, Val Loss: 5.6684\n",
            "Epoch 11/1000, Train Loss: 5.5358, Val Loss: 5.6696\n",
            "Epoch 12/1000, Train Loss: 5.5378, Val Loss: 5.6800\n",
            "Epoch 13/1000, Train Loss: 5.5358, Val Loss: 5.6640\n",
            "Epoch 14/1000, Train Loss: 5.5357, Val Loss: 5.6701\n",
            "Epoch 15/1000, Train Loss: 5.5392, Val Loss: 5.6679\n",
            "Epoch 16/1000, Train Loss: 5.5351, Val Loss: 5.6702\n",
            "Epoch 17/1000, Train Loss: 5.5348, Val Loss: 5.6643\n",
            "Epoch 18/1000, Train Loss: 5.5369, Val Loss: 5.6625\n",
            "Epoch 19/1000, Train Loss: 5.5353, Val Loss: 5.6703\n",
            "Epoch 20/1000, Train Loss: 5.5379, Val Loss: 5.6698\n",
            "Epoch 21/1000, Train Loss: 5.5399, Val Loss: 5.6739\n",
            "Epoch 22/1000, Train Loss: 5.5384, Val Loss: 5.6676\n",
            "Epoch 23/1000, Train Loss: 5.5365, Val Loss: 5.6768\n",
            "Epoch 24/1000, Train Loss: 5.5364, Val Loss: 5.6706\n",
            "Epoch 25/1000, Train Loss: 5.5351, Val Loss: 5.6622\n",
            "Epoch 26/1000, Train Loss: 5.5366, Val Loss: 5.6668\n",
            "Epoch 27/1000, Train Loss: 5.5355, Val Loss: 5.6688\n",
            "Epoch 28/1000, Train Loss: 5.5313, Val Loss: 5.6836\n",
            "Epoch 29/1000, Train Loss: 5.5396, Val Loss: 5.6693\n",
            "Epoch 30/1000, Train Loss: 5.5378, Val Loss: 5.6740\n",
            "Epoch 31/1000, Train Loss: 5.5375, Val Loss: 5.6640\n",
            "Epoch 32/1000, Train Loss: 5.5343, Val Loss: 5.6689\n",
            "Epoch 33/1000, Train Loss: 5.5345, Val Loss: 5.6648\n",
            "Epoch 34/1000, Train Loss: 5.5352, Val Loss: 5.6742\n",
            "Epoch 35/1000, Train Loss: 5.5340, Val Loss: 5.6730\n",
            "Early stopping triggered at epoch 35. Best Val Loss: 5.6622\n",
            "Epoch 1/1000, Train Loss: 5.5543, Val Loss: 5.6688\n",
            "Epoch 2/1000, Train Loss: 5.5450, Val Loss: 5.6737\n",
            "Epoch 3/1000, Train Loss: 5.5392, Val Loss: 5.6751\n",
            "Epoch 4/1000, Train Loss: 5.5405, Val Loss: 5.6889\n",
            "Epoch 5/1000, Train Loss: 5.5419, Val Loss: 5.6705\n",
            "Epoch 6/1000, Train Loss: 5.5399, Val Loss: 5.6718\n",
            "Epoch 7/1000, Train Loss: 5.5385, Val Loss: 5.6614\n",
            "Epoch 8/1000, Train Loss: 5.5403, Val Loss: 5.6666\n",
            "Epoch 9/1000, Train Loss: 5.5406, Val Loss: 5.6611\n",
            "Epoch 10/1000, Train Loss: 5.5446, Val Loss: 5.6669\n",
            "Epoch 11/1000, Train Loss: 5.5402, Val Loss: 5.6661\n",
            "Epoch 12/1000, Train Loss: 5.5416, Val Loss: 5.6647\n",
            "Epoch 13/1000, Train Loss: 5.5394, Val Loss: 5.6617\n",
            "Epoch 14/1000, Train Loss: 5.5422, Val Loss: 5.6737\n",
            "Epoch 15/1000, Train Loss: 5.5400, Val Loss: 5.6712\n",
            "Epoch 16/1000, Train Loss: 5.5388, Val Loss: 5.6790\n",
            "Epoch 17/1000, Train Loss: 5.5402, Val Loss: 5.6659\n",
            "Epoch 18/1000, Train Loss: 5.5360, Val Loss: 5.6719\n",
            "Epoch 19/1000, Train Loss: 5.5354, Val Loss: 5.6648\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 5.6611\n",
            "Training with LR=0.001, Hidden=32, Dropout=0.0, Scale Factor=1.25\n",
            "Combination 11/81\n",
            "Epoch 1/1000, Train Loss: 6.6184, Val Loss: 6.7490\n",
            "Epoch 2/1000, Train Loss: 6.6021, Val Loss: 6.7603\n",
            "Epoch 3/1000, Train Loss: 6.5976, Val Loss: 6.7580\n",
            "Epoch 4/1000, Train Loss: 6.6064, Val Loss: 6.7635\n",
            "Epoch 5/1000, Train Loss: 6.5983, Val Loss: 6.7595\n",
            "Epoch 6/1000, Train Loss: 6.6002, Val Loss: 6.7615\n",
            "Epoch 7/1000, Train Loss: 6.5964, Val Loss: 6.7572\n",
            "Epoch 8/1000, Train Loss: 6.5945, Val Loss: 6.7564\n",
            "Epoch 9/1000, Train Loss: 6.5969, Val Loss: 6.7606\n",
            "Epoch 10/1000, Train Loss: 6.5985, Val Loss: 6.7580\n",
            "Epoch 11/1000, Train Loss: 6.5972, Val Loss: 6.7616\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 6.7490\n",
            "Epoch 1/1000, Train Loss: 6.6191, Val Loss: 6.7718\n",
            "Epoch 2/1000, Train Loss: 6.6026, Val Loss: 6.7572\n",
            "Epoch 3/1000, Train Loss: 6.6055, Val Loss: 6.7572\n",
            "Epoch 4/1000, Train Loss: 6.5992, Val Loss: 6.7692\n",
            "Epoch 5/1000, Train Loss: 6.5986, Val Loss: 6.7556\n",
            "Epoch 6/1000, Train Loss: 6.6024, Val Loss: 6.7686\n",
            "Epoch 7/1000, Train Loss: 6.5984, Val Loss: 6.7573\n",
            "Epoch 8/1000, Train Loss: 6.5962, Val Loss: 6.7656\n",
            "Epoch 9/1000, Train Loss: 6.5965, Val Loss: 6.7624\n",
            "Epoch 10/1000, Train Loss: 6.5981, Val Loss: 6.7619\n",
            "Epoch 11/1000, Train Loss: 6.5943, Val Loss: 6.7562\n",
            "Epoch 12/1000, Train Loss: 6.5950, Val Loss: 6.7554\n",
            "Epoch 13/1000, Train Loss: 6.5946, Val Loss: 6.7588\n",
            "Epoch 14/1000, Train Loss: 6.5998, Val Loss: 6.7595\n",
            "Epoch 15/1000, Train Loss: 6.5986, Val Loss: 6.7556\n",
            "Epoch 16/1000, Train Loss: 6.5936, Val Loss: 6.7825\n",
            "Epoch 17/1000, Train Loss: 6.5936, Val Loss: 6.7570\n",
            "Epoch 18/1000, Train Loss: 6.5933, Val Loss: 6.7571\n",
            "Epoch 19/1000, Train Loss: 6.5915, Val Loss: 6.7618\n",
            "Epoch 20/1000, Train Loss: 6.5960, Val Loss: 6.7625\n",
            "Epoch 21/1000, Train Loss: 6.5907, Val Loss: 6.7537\n",
            "Epoch 22/1000, Train Loss: 6.5946, Val Loss: 6.7547\n",
            "Epoch 23/1000, Train Loss: 6.5965, Val Loss: 6.7516\n",
            "Epoch 24/1000, Train Loss: 6.5921, Val Loss: 6.7590\n",
            "Epoch 25/1000, Train Loss: 6.5930, Val Loss: 6.7507\n",
            "Epoch 26/1000, Train Loss: 6.5948, Val Loss: 6.7560\n",
            "Epoch 27/1000, Train Loss: 6.5962, Val Loss: 6.7561\n",
            "Epoch 28/1000, Train Loss: 6.5913, Val Loss: 6.7564\n",
            "Epoch 29/1000, Train Loss: 6.5939, Val Loss: 6.7562\n",
            "Epoch 30/1000, Train Loss: 6.5941, Val Loss: 6.7558\n",
            "Epoch 31/1000, Train Loss: 6.5992, Val Loss: 6.7543\n",
            "Epoch 32/1000, Train Loss: 6.5911, Val Loss: 6.7497\n",
            "Epoch 33/1000, Train Loss: 6.5959, Val Loss: 6.7573\n",
            "Epoch 34/1000, Train Loss: 6.5901, Val Loss: 6.7588\n",
            "Epoch 35/1000, Train Loss: 6.5924, Val Loss: 6.7523\n",
            "Epoch 36/1000, Train Loss: 6.5955, Val Loss: 6.7531\n",
            "Epoch 37/1000, Train Loss: 6.5953, Val Loss: 6.7540\n",
            "Epoch 38/1000, Train Loss: 6.5909, Val Loss: 6.7594\n",
            "Epoch 39/1000, Train Loss: 6.5954, Val Loss: 6.7538\n",
            "Epoch 40/1000, Train Loss: 6.5916, Val Loss: 6.7578\n",
            "Epoch 41/1000, Train Loss: 6.5912, Val Loss: 6.7626\n",
            "Epoch 42/1000, Train Loss: 6.5923, Val Loss: 6.7617\n",
            "Early stopping triggered at epoch 42. Best Val Loss: 6.7497\n",
            "Epoch 1/1000, Train Loss: 6.6350, Val Loss: 6.7576\n",
            "Epoch 2/1000, Train Loss: 6.6025, Val Loss: 6.7661\n",
            "Epoch 3/1000, Train Loss: 6.6018, Val Loss: 6.7689\n",
            "Epoch 4/1000, Train Loss: 6.5977, Val Loss: 6.7645\n",
            "Epoch 5/1000, Train Loss: 6.5976, Val Loss: 6.7563\n",
            "Epoch 6/1000, Train Loss: 6.5987, Val Loss: 6.7575\n",
            "Epoch 7/1000, Train Loss: 6.5963, Val Loss: 6.7653\n",
            "Epoch 8/1000, Train Loss: 6.5972, Val Loss: 6.7555\n",
            "Epoch 9/1000, Train Loss: 6.5949, Val Loss: 6.7582\n",
            "Epoch 10/1000, Train Loss: 6.5970, Val Loss: 6.7578\n",
            "Epoch 11/1000, Train Loss: 6.5983, Val Loss: 6.7567\n",
            "Epoch 12/1000, Train Loss: 6.5965, Val Loss: 6.7570\n",
            "Epoch 13/1000, Train Loss: 6.5944, Val Loss: 6.7651\n",
            "Epoch 14/1000, Train Loss: 6.5973, Val Loss: 6.7554\n",
            "Epoch 15/1000, Train Loss: 6.5942, Val Loss: 6.7580\n",
            "Epoch 16/1000, Train Loss: 6.5939, Val Loss: 6.7538\n",
            "Epoch 17/1000, Train Loss: 6.5986, Val Loss: 6.7574\n",
            "Epoch 18/1000, Train Loss: 6.5940, Val Loss: 6.7637\n",
            "Epoch 19/1000, Train Loss: 6.5962, Val Loss: 6.7570\n",
            "Epoch 20/1000, Train Loss: 6.5966, Val Loss: 6.7636\n",
            "Epoch 21/1000, Train Loss: 6.5971, Val Loss: 6.7537\n",
            "Epoch 22/1000, Train Loss: 6.5950, Val Loss: 6.7535\n",
            "Epoch 23/1000, Train Loss: 6.5957, Val Loss: 6.7647\n",
            "Epoch 24/1000, Train Loss: 6.5987, Val Loss: 6.7563\n",
            "Epoch 25/1000, Train Loss: 6.5906, Val Loss: 6.7638\n",
            "Epoch 26/1000, Train Loss: 6.5941, Val Loss: 6.7552\n",
            "Epoch 27/1000, Train Loss: 6.5974, Val Loss: 6.7589\n",
            "Epoch 28/1000, Train Loss: 6.5957, Val Loss: 6.7531\n",
            "Epoch 29/1000, Train Loss: 6.5940, Val Loss: 6.7535\n",
            "Epoch 30/1000, Train Loss: 6.5978, Val Loss: 6.7542\n",
            "Epoch 31/1000, Train Loss: 6.5954, Val Loss: 6.7593\n",
            "Epoch 32/1000, Train Loss: 6.5947, Val Loss: 6.8345\n",
            "Epoch 33/1000, Train Loss: 6.5939, Val Loss: 6.7576\n",
            "Epoch 34/1000, Train Loss: 6.5937, Val Loss: 6.7527\n",
            "Epoch 35/1000, Train Loss: 6.6027, Val Loss: 6.7545\n",
            "Epoch 36/1000, Train Loss: 6.5911, Val Loss: 6.7650\n",
            "Epoch 37/1000, Train Loss: 6.5917, Val Loss: 6.7557\n",
            "Epoch 38/1000, Train Loss: 6.5922, Val Loss: 6.7502\n",
            "Epoch 39/1000, Train Loss: 6.5947, Val Loss: 6.7562\n",
            "Epoch 40/1000, Train Loss: 6.5961, Val Loss: 6.7555\n",
            "Epoch 41/1000, Train Loss: 6.5862, Val Loss: 6.7519\n",
            "Epoch 42/1000, Train Loss: 6.5918, Val Loss: 6.7549\n",
            "Epoch 43/1000, Train Loss: 6.5969, Val Loss: 6.7521\n",
            "Epoch 44/1000, Train Loss: 6.5919, Val Loss: 6.7551\n",
            "Epoch 45/1000, Train Loss: 6.5952, Val Loss: 6.7491\n",
            "Epoch 46/1000, Train Loss: 6.5925, Val Loss: 6.7468\n",
            "Epoch 47/1000, Train Loss: 6.5919, Val Loss: 6.7527\n",
            "Epoch 48/1000, Train Loss: 6.5905, Val Loss: 6.7581\n",
            "Epoch 49/1000, Train Loss: 6.5953, Val Loss: 6.7519\n",
            "Epoch 50/1000, Train Loss: 6.5936, Val Loss: 6.7506\n",
            "Epoch 51/1000, Train Loss: 6.5924, Val Loss: 6.7639\n",
            "Epoch 52/1000, Train Loss: 6.5929, Val Loss: 6.7510\n",
            "Epoch 53/1000, Train Loss: 6.5921, Val Loss: 6.7502\n",
            "Epoch 54/1000, Train Loss: 6.5923, Val Loss: 6.7565\n",
            "Epoch 55/1000, Train Loss: 6.5926, Val Loss: 6.7525\n",
            "Epoch 56/1000, Train Loss: 6.5924, Val Loss: 6.7479\n",
            "Early stopping triggered at epoch 56. Best Val Loss: 6.7468\n",
            "Training with LR=0.001, Hidden=32, Dropout=0.0, Scale Factor=1.5\n",
            "Combination 12/81\n",
            "Epoch 1/1000, Train Loss: 7.6498, Val Loss: 7.8173\n",
            "Epoch 2/1000, Train Loss: 7.6387, Val Loss: 7.8186\n",
            "Epoch 3/1000, Train Loss: 7.6387, Val Loss: 7.8178\n",
            "Epoch 4/1000, Train Loss: 7.6407, Val Loss: 7.8175\n",
            "Epoch 5/1000, Train Loss: 7.6323, Val Loss: 7.8524\n",
            "Epoch 6/1000, Train Loss: 7.6368, Val Loss: 7.8201\n",
            "Epoch 7/1000, Train Loss: 7.6361, Val Loss: 7.8356\n",
            "Epoch 8/1000, Train Loss: 7.6367, Val Loss: 7.8260\n",
            "Epoch 9/1000, Train Loss: 7.6375, Val Loss: 7.8263\n",
            "Epoch 10/1000, Train Loss: 7.6409, Val Loss: 7.8192\n",
            "Epoch 11/1000, Train Loss: 7.6293, Val Loss: 7.8253\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8173\n",
            "Epoch 1/1000, Train Loss: 7.6751, Val Loss: 7.8175\n",
            "Epoch 2/1000, Train Loss: 7.6376, Val Loss: 7.8322\n",
            "Epoch 3/1000, Train Loss: 7.6355, Val Loss: 7.8288\n",
            "Epoch 4/1000, Train Loss: 7.6338, Val Loss: 7.8200\n",
            "Epoch 5/1000, Train Loss: 7.6396, Val Loss: 7.8205\n",
            "Epoch 6/1000, Train Loss: 7.6337, Val Loss: 7.8229\n",
            "Epoch 7/1000, Train Loss: 7.6316, Val Loss: 7.8242\n",
            "Epoch 8/1000, Train Loss: 7.6277, Val Loss: 7.8979\n",
            "Epoch 9/1000, Train Loss: 7.6339, Val Loss: 7.8349\n",
            "Epoch 10/1000, Train Loss: 7.6328, Val Loss: 7.8220\n",
            "Epoch 11/1000, Train Loss: 7.6376, Val Loss: 7.8206\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8175\n",
            "Epoch 1/1000, Train Loss: 7.6558, Val Loss: 7.8380\n",
            "Epoch 2/1000, Train Loss: 7.6410, Val Loss: 7.8210\n",
            "Epoch 3/1000, Train Loss: 7.6341, Val Loss: 7.8222\n",
            "Epoch 4/1000, Train Loss: 7.6342, Val Loss: 7.8196\n",
            "Epoch 5/1000, Train Loss: 7.6381, Val Loss: 7.8181\n",
            "Epoch 6/1000, Train Loss: 7.6336, Val Loss: 7.8253\n",
            "Epoch 7/1000, Train Loss: 7.6433, Val Loss: 7.8230\n",
            "Epoch 8/1000, Train Loss: 7.6328, Val Loss: 7.8185\n",
            "Epoch 9/1000, Train Loss: 7.6359, Val Loss: 7.8198\n",
            "Epoch 10/1000, Train Loss: 7.6349, Val Loss: 7.8270\n",
            "Epoch 11/1000, Train Loss: 7.6320, Val Loss: 7.8286\n",
            "Epoch 12/1000, Train Loss: 7.6322, Val Loss: 7.8415\n",
            "Epoch 13/1000, Train Loss: 7.6303, Val Loss: 7.8262\n",
            "Epoch 14/1000, Train Loss: 7.6329, Val Loss: 7.8220\n",
            "Epoch 15/1000, Train Loss: 7.6328, Val Loss: 7.8201\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 7.8181\n",
            "Training with LR=0.001, Hidden=32, Dropout=0.1, Scale Factor=1.0\n",
            "Combination 13/81\n",
            "Epoch 1/1000, Train Loss: 5.5500, Val Loss: 5.6717\n",
            "Epoch 2/1000, Train Loss: 5.5413, Val Loss: 5.6666\n",
            "Epoch 3/1000, Train Loss: 5.5408, Val Loss: 5.6727\n",
            "Epoch 4/1000, Train Loss: 5.5416, Val Loss: 5.6700\n",
            "Epoch 5/1000, Train Loss: 5.5433, Val Loss: 5.6738\n",
            "Epoch 6/1000, Train Loss: 5.5390, Val Loss: 5.6760\n",
            "Epoch 7/1000, Train Loss: 5.5425, Val Loss: 5.6691\n",
            "Epoch 8/1000, Train Loss: 5.5390, Val Loss: 5.6671\n",
            "Epoch 9/1000, Train Loss: 5.5369, Val Loss: 5.6677\n",
            "Epoch 10/1000, Train Loss: 5.5411, Val Loss: 5.6669\n",
            "Epoch 11/1000, Train Loss: 5.5380, Val Loss: 5.6732\n",
            "Epoch 12/1000, Train Loss: 5.5366, Val Loss: 5.6781\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 5.6666\n",
            "Epoch 1/1000, Train Loss: 5.5687, Val Loss: 5.6904\n",
            "Epoch 2/1000, Train Loss: 5.5557, Val Loss: 5.6795\n",
            "Epoch 3/1000, Train Loss: 5.5535, Val Loss: 5.6778\n",
            "Epoch 4/1000, Train Loss: 5.5440, Val Loss: 5.6801\n",
            "Epoch 5/1000, Train Loss: 5.5413, Val Loss: 5.6798\n",
            "Epoch 6/1000, Train Loss: 5.5434, Val Loss: 5.6776\n",
            "Epoch 7/1000, Train Loss: 5.5389, Val Loss: 5.6814\n",
            "Epoch 8/1000, Train Loss: 5.5414, Val Loss: 5.6737\n",
            "Epoch 9/1000, Train Loss: 5.5431, Val Loss: 5.6679\n",
            "Epoch 10/1000, Train Loss: 5.5413, Val Loss: 5.6725\n",
            "Epoch 11/1000, Train Loss: 5.5431, Val Loss: 5.6693\n",
            "Epoch 12/1000, Train Loss: 5.5428, Val Loss: 5.6698\n",
            "Epoch 13/1000, Train Loss: 5.5378, Val Loss: 5.6683\n",
            "Epoch 14/1000, Train Loss: 5.5472, Val Loss: 5.6658\n",
            "Epoch 15/1000, Train Loss: 5.5377, Val Loss: 5.6819\n",
            "Epoch 16/1000, Train Loss: 5.5425, Val Loss: 5.6692\n",
            "Epoch 17/1000, Train Loss: 5.5392, Val Loss: 5.6658\n",
            "Epoch 18/1000, Train Loss: 5.5385, Val Loss: 5.6687\n",
            "Epoch 19/1000, Train Loss: 5.5381, Val Loss: 5.6643\n",
            "Epoch 20/1000, Train Loss: 5.5349, Val Loss: 5.6632\n",
            "Epoch 21/1000, Train Loss: 5.5376, Val Loss: 5.6672\n",
            "Epoch 22/1000, Train Loss: 5.5359, Val Loss: 5.6689\n",
            "Epoch 23/1000, Train Loss: 5.5392, Val Loss: 5.6684\n",
            "Epoch 24/1000, Train Loss: 5.5423, Val Loss: 5.6724\n",
            "Epoch 25/1000, Train Loss: 5.5378, Val Loss: 5.6681\n",
            "Epoch 26/1000, Train Loss: 5.5405, Val Loss: 5.6673\n",
            "Epoch 27/1000, Train Loss: 5.5384, Val Loss: 5.6637\n",
            "Epoch 28/1000, Train Loss: 5.5356, Val Loss: 5.6732\n",
            "Epoch 29/1000, Train Loss: 5.5339, Val Loss: 5.6633\n",
            "Epoch 30/1000, Train Loss: 5.5359, Val Loss: 5.6699\n",
            "Early stopping triggered at epoch 30. Best Val Loss: 5.6632\n",
            "Epoch 1/1000, Train Loss: 5.5579, Val Loss: 5.6866\n",
            "Epoch 2/1000, Train Loss: 5.5470, Val Loss: 5.6789\n",
            "Epoch 3/1000, Train Loss: 5.5482, Val Loss: 5.6769\n",
            "Epoch 4/1000, Train Loss: 5.5466, Val Loss: 5.6798\n",
            "Epoch 5/1000, Train Loss: 5.5431, Val Loss: 5.6702\n",
            "Epoch 6/1000, Train Loss: 5.5450, Val Loss: 5.6659\n",
            "Epoch 7/1000, Train Loss: 5.5434, Val Loss: 5.6656\n",
            "Epoch 8/1000, Train Loss: 5.5464, Val Loss: 5.6692\n",
            "Epoch 9/1000, Train Loss: 5.5372, Val Loss: 5.6718\n",
            "Epoch 10/1000, Train Loss: 5.5410, Val Loss: 5.6645\n",
            "Epoch 11/1000, Train Loss: 5.5429, Val Loss: 5.6676\n",
            "Epoch 12/1000, Train Loss: 5.5384, Val Loss: 5.6710\n",
            "Epoch 13/1000, Train Loss: 5.5404, Val Loss: 5.6645\n",
            "Epoch 14/1000, Train Loss: 5.5444, Val Loss: 5.6694\n",
            "Epoch 15/1000, Train Loss: 5.5365, Val Loss: 5.6648\n",
            "Epoch 16/1000, Train Loss: 5.5394, Val Loss: 5.6660\n",
            "Epoch 17/1000, Train Loss: 5.5388, Val Loss: 5.6645\n",
            "Epoch 18/1000, Train Loss: 5.5401, Val Loss: 5.6688\n",
            "Epoch 19/1000, Train Loss: 5.5367, Val Loss: 5.6624\n",
            "Epoch 20/1000, Train Loss: 5.5350, Val Loss: 5.6746\n",
            "Epoch 21/1000, Train Loss: 5.5435, Val Loss: 5.6656\n",
            "Epoch 22/1000, Train Loss: 5.5376, Val Loss: 5.6644\n",
            "Epoch 23/1000, Train Loss: 5.5394, Val Loss: 5.6653\n",
            "Epoch 24/1000, Train Loss: 5.5369, Val Loss: 5.6648\n",
            "Epoch 25/1000, Train Loss: 5.5333, Val Loss: 5.6629\n",
            "Epoch 26/1000, Train Loss: 5.5370, Val Loss: 5.6677\n",
            "Epoch 27/1000, Train Loss: 5.5365, Val Loss: 5.6678\n",
            "Epoch 28/1000, Train Loss: 5.5364, Val Loss: 5.6705\n",
            "Epoch 29/1000, Train Loss: 5.5395, Val Loss: 5.6742\n",
            "Early stopping triggered at epoch 29. Best Val Loss: 5.6624\n",
            "Training with LR=0.001, Hidden=32, Dropout=0.1, Scale Factor=1.25\n",
            "Combination 14/81\n",
            "Epoch 1/1000, Train Loss: 6.6314, Val Loss: 6.7576\n",
            "Epoch 2/1000, Train Loss: 6.5996, Val Loss: 6.7582\n",
            "Epoch 3/1000, Train Loss: 6.5934, Val Loss: 6.7590\n",
            "Epoch 4/1000, Train Loss: 6.6032, Val Loss: 6.7565\n",
            "Epoch 5/1000, Train Loss: 6.5947, Val Loss: 6.7555\n",
            "Epoch 6/1000, Train Loss: 6.5929, Val Loss: 6.7709\n",
            "Epoch 7/1000, Train Loss: 6.6022, Val Loss: 6.7602\n",
            "Epoch 8/1000, Train Loss: 6.5943, Val Loss: 6.7726\n",
            "Epoch 9/1000, Train Loss: 6.5998, Val Loss: 6.7558\n",
            "Epoch 10/1000, Train Loss: 6.5998, Val Loss: 6.7681\n",
            "Epoch 11/1000, Train Loss: 6.5948, Val Loss: 6.7645\n",
            "Epoch 12/1000, Train Loss: 6.5984, Val Loss: 6.7561\n",
            "Epoch 13/1000, Train Loss: 6.5965, Val Loss: 6.7690\n",
            "Epoch 14/1000, Train Loss: 6.6016, Val Loss: 6.7578\n",
            "Epoch 15/1000, Train Loss: 6.5995, Val Loss: 6.7574\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 6.7555\n",
            "Epoch 1/1000, Train Loss: 6.6200, Val Loss: 6.7552\n",
            "Epoch 2/1000, Train Loss: 6.6082, Val Loss: 6.7601\n",
            "Epoch 3/1000, Train Loss: 6.6080, Val Loss: 6.7599\n",
            "Epoch 4/1000, Train Loss: 6.5984, Val Loss: 6.7596\n",
            "Epoch 5/1000, Train Loss: 6.5923, Val Loss: 6.7620\n",
            "Epoch 6/1000, Train Loss: 6.5943, Val Loss: 6.7593\n",
            "Epoch 7/1000, Train Loss: 6.5961, Val Loss: 6.7644\n",
            "Epoch 8/1000, Train Loss: 6.6048, Val Loss: 6.7558\n",
            "Epoch 9/1000, Train Loss: 6.5945, Val Loss: 6.7613\n",
            "Epoch 10/1000, Train Loss: 6.5921, Val Loss: 6.7639\n",
            "Epoch 11/1000, Train Loss: 6.5971, Val Loss: 6.7619\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 6.7552\n",
            "Epoch 1/1000, Train Loss: 6.6479, Val Loss: 6.7719\n",
            "Epoch 2/1000, Train Loss: 6.6208, Val Loss: 6.7596\n",
            "Epoch 3/1000, Train Loss: 6.6043, Val Loss: 6.7648\n",
            "Epoch 4/1000, Train Loss: 6.6035, Val Loss: 6.7622\n",
            "Epoch 5/1000, Train Loss: 6.6075, Val Loss: 6.7590\n",
            "Epoch 6/1000, Train Loss: 6.6041, Val Loss: 6.7612\n",
            "Epoch 7/1000, Train Loss: 6.6056, Val Loss: 6.7569\n",
            "Epoch 8/1000, Train Loss: 6.6088, Val Loss: 6.7659\n",
            "Epoch 9/1000, Train Loss: 6.5981, Val Loss: 6.7671\n",
            "Epoch 10/1000, Train Loss: 6.6065, Val Loss: 6.7567\n",
            "Epoch 11/1000, Train Loss: 6.5984, Val Loss: 6.7544\n",
            "Epoch 12/1000, Train Loss: 6.6033, Val Loss: 6.7551\n",
            "Epoch 13/1000, Train Loss: 6.5949, Val Loss: 6.7524\n",
            "Epoch 14/1000, Train Loss: 6.6012, Val Loss: 6.7535\n",
            "Epoch 15/1000, Train Loss: 6.6012, Val Loss: 6.7604\n",
            "Epoch 16/1000, Train Loss: 6.5962, Val Loss: 6.7551\n",
            "Epoch 17/1000, Train Loss: 6.6070, Val Loss: 6.7585\n",
            "Epoch 18/1000, Train Loss: 6.5929, Val Loss: 6.7587\n",
            "Epoch 19/1000, Train Loss: 6.5996, Val Loss: 6.7600\n",
            "Epoch 20/1000, Train Loss: 6.5949, Val Loss: 6.7584\n",
            "Epoch 21/1000, Train Loss: 6.5936, Val Loss: 6.7600\n",
            "Epoch 22/1000, Train Loss: 6.5976, Val Loss: 6.7533\n",
            "Epoch 23/1000, Train Loss: 6.5959, Val Loss: 6.7578\n",
            "Early stopping triggered at epoch 23. Best Val Loss: 6.7524\n",
            "Training with LR=0.001, Hidden=32, Dropout=0.1, Scale Factor=1.5\n",
            "Combination 15/81\n",
            "Epoch 1/1000, Train Loss: 7.6708, Val Loss: 7.8226\n",
            "Epoch 2/1000, Train Loss: 7.6401, Val Loss: 7.8244\n",
            "Epoch 3/1000, Train Loss: 7.6422, Val Loss: 7.8390\n",
            "Epoch 4/1000, Train Loss: 7.6468, Val Loss: 7.8186\n",
            "Epoch 5/1000, Train Loss: 7.6403, Val Loss: 7.8197\n",
            "Epoch 6/1000, Train Loss: 7.6314, Val Loss: 7.8225\n",
            "Epoch 7/1000, Train Loss: 7.6356, Val Loss: 7.8203\n",
            "Epoch 8/1000, Train Loss: 7.6462, Val Loss: 7.8277\n",
            "Epoch 9/1000, Train Loss: 7.6333, Val Loss: 7.8216\n",
            "Epoch 10/1000, Train Loss: 7.6424, Val Loss: 7.8236\n",
            "Epoch 11/1000, Train Loss: 7.6317, Val Loss: 7.8209\n",
            "Epoch 12/1000, Train Loss: 7.6342, Val Loss: 7.8241\n",
            "Epoch 13/1000, Train Loss: 7.6308, Val Loss: 7.8243\n",
            "Epoch 14/1000, Train Loss: 7.6272, Val Loss: 7.8490\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 7.8186\n",
            "Epoch 1/1000, Train Loss: 7.6592, Val Loss: 7.8198\n",
            "Epoch 2/1000, Train Loss: 7.6403, Val Loss: 7.8198\n",
            "Epoch 3/1000, Train Loss: 7.6448, Val Loss: 7.8407\n",
            "Epoch 4/1000, Train Loss: 7.6402, Val Loss: 7.8206\n",
            "Epoch 5/1000, Train Loss: 7.6425, Val Loss: 7.8309\n",
            "Epoch 6/1000, Train Loss: 7.6411, Val Loss: 7.8217\n",
            "Epoch 7/1000, Train Loss: 7.6433, Val Loss: 7.8252\n",
            "Epoch 8/1000, Train Loss: 7.6360, Val Loss: 7.8219\n",
            "Epoch 9/1000, Train Loss: 7.6403, Val Loss: 7.8196\n",
            "Epoch 10/1000, Train Loss: 7.6307, Val Loss: 7.8403\n",
            "Epoch 11/1000, Train Loss: 7.6356, Val Loss: 7.8214\n",
            "Epoch 12/1000, Train Loss: 7.6337, Val Loss: 7.8259\n",
            "Epoch 13/1000, Train Loss: 7.6330, Val Loss: 7.8229\n",
            "Epoch 14/1000, Train Loss: 7.6292, Val Loss: 7.8228\n",
            "Epoch 15/1000, Train Loss: 7.6343, Val Loss: 7.8222\n",
            "Epoch 16/1000, Train Loss: 7.6320, Val Loss: 7.8301\n",
            "Epoch 17/1000, Train Loss: 7.6326, Val Loss: 7.8264\n",
            "Epoch 18/1000, Train Loss: 7.6327, Val Loss: 7.8354\n",
            "Epoch 19/1000, Train Loss: 7.6342, Val Loss: 7.8217\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 7.8196\n",
            "Epoch 1/1000, Train Loss: 7.6613, Val Loss: 7.8195\n",
            "Epoch 2/1000, Train Loss: 7.6410, Val Loss: 7.8208\n",
            "Epoch 3/1000, Train Loss: 7.6458, Val Loss: 7.8229\n",
            "Epoch 4/1000, Train Loss: 7.6465, Val Loss: 7.8251\n",
            "Epoch 5/1000, Train Loss: 7.6432, Val Loss: 7.8254\n",
            "Epoch 6/1000, Train Loss: 7.6391, Val Loss: 7.8228\n",
            "Epoch 7/1000, Train Loss: 7.6327, Val Loss: 7.8215\n",
            "Epoch 8/1000, Train Loss: 7.6363, Val Loss: 7.8203\n",
            "Epoch 9/1000, Train Loss: 7.6373, Val Loss: 7.8209\n",
            "Epoch 10/1000, Train Loss: 7.6335, Val Loss: 7.8214\n",
            "Epoch 11/1000, Train Loss: 7.6336, Val Loss: 7.8214\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8195\n",
            "Training with LR=0.001, Hidden=32, Dropout=0.2, Scale Factor=1.0\n",
            "Combination 16/81\n",
            "Epoch 1/1000, Train Loss: 5.5829, Val Loss: 5.6940\n",
            "Epoch 2/1000, Train Loss: 5.5449, Val Loss: 5.6838\n",
            "Epoch 3/1000, Train Loss: 5.5456, Val Loss: 5.6867\n",
            "Epoch 4/1000, Train Loss: 5.5444, Val Loss: 5.6737\n",
            "Epoch 5/1000, Train Loss: 5.5474, Val Loss: 5.6721\n",
            "Epoch 6/1000, Train Loss: 5.5462, Val Loss: 5.6759\n",
            "Epoch 7/1000, Train Loss: 5.5418, Val Loss: 5.6696\n",
            "Epoch 8/1000, Train Loss: 5.5393, Val Loss: 5.6682\n",
            "Epoch 9/1000, Train Loss: 5.5411, Val Loss: 5.6686\n",
            "Epoch 10/1000, Train Loss: 5.5429, Val Loss: 5.6703\n",
            "Epoch 11/1000, Train Loss: 5.5371, Val Loss: 5.6678\n",
            "Epoch 12/1000, Train Loss: 5.5382, Val Loss: 5.6656\n",
            "Epoch 13/1000, Train Loss: 5.5418, Val Loss: 5.6766\n",
            "Epoch 14/1000, Train Loss: 5.5408, Val Loss: 5.6710\n",
            "Epoch 15/1000, Train Loss: 5.5380, Val Loss: 5.6685\n",
            "Epoch 16/1000, Train Loss: 5.5400, Val Loss: 5.6683\n",
            "Epoch 17/1000, Train Loss: 5.5427, Val Loss: 5.6669\n",
            "Epoch 18/1000, Train Loss: 5.5384, Val Loss: 5.6655\n",
            "Epoch 19/1000, Train Loss: 5.5376, Val Loss: 5.6637\n",
            "Epoch 20/1000, Train Loss: 5.5389, Val Loss: 5.6665\n",
            "Epoch 21/1000, Train Loss: 5.5381, Val Loss: 5.6680\n",
            "Epoch 22/1000, Train Loss: 5.5359, Val Loss: 5.6676\n",
            "Epoch 23/1000, Train Loss: 5.5362, Val Loss: 5.6700\n",
            "Epoch 24/1000, Train Loss: 5.5386, Val Loss: 5.6630\n",
            "Epoch 25/1000, Train Loss: 5.5356, Val Loss: 5.6692\n",
            "Epoch 26/1000, Train Loss: 5.5357, Val Loss: 5.6685\n",
            "Epoch 27/1000, Train Loss: 5.5357, Val Loss: 5.6642\n",
            "Epoch 28/1000, Train Loss: 5.5384, Val Loss: 5.6648\n",
            "Epoch 29/1000, Train Loss: 5.5368, Val Loss: 5.6637\n",
            "Epoch 30/1000, Train Loss: 5.5364, Val Loss: 5.6720\n",
            "Epoch 31/1000, Train Loss: 5.5402, Val Loss: 5.6690\n",
            "Epoch 32/1000, Train Loss: 5.5436, Val Loss: 5.6734\n",
            "Epoch 33/1000, Train Loss: 5.5353, Val Loss: 5.6665\n",
            "Epoch 34/1000, Train Loss: 5.5367, Val Loss: 5.6637\n",
            "Early stopping triggered at epoch 34. Best Val Loss: 5.6630\n",
            "Epoch 1/1000, Train Loss: 5.5598, Val Loss: 5.6779\n",
            "Epoch 2/1000, Train Loss: 5.5459, Val Loss: 5.6781\n",
            "Epoch 3/1000, Train Loss: 5.5440, Val Loss: 5.6713\n",
            "Epoch 4/1000, Train Loss: 5.5413, Val Loss: 5.6754\n",
            "Epoch 5/1000, Train Loss: 5.5386, Val Loss: 5.6734\n",
            "Epoch 6/1000, Train Loss: 5.5414, Val Loss: 5.6703\n",
            "Epoch 7/1000, Train Loss: 5.5426, Val Loss: 5.6708\n",
            "Epoch 8/1000, Train Loss: 5.5421, Val Loss: 5.6662\n",
            "Epoch 9/1000, Train Loss: 5.5386, Val Loss: 5.6717\n",
            "Epoch 10/1000, Train Loss: 5.5376, Val Loss: 5.6659\n",
            "Epoch 11/1000, Train Loss: 5.5426, Val Loss: 5.6674\n",
            "Epoch 12/1000, Train Loss: 5.5400, Val Loss: 5.6679\n",
            "Epoch 13/1000, Train Loss: 5.5375, Val Loss: 5.6671\n",
            "Epoch 14/1000, Train Loss: 5.5388, Val Loss: 5.6633\n",
            "Epoch 15/1000, Train Loss: 5.5350, Val Loss: 5.6641\n",
            "Epoch 16/1000, Train Loss: 5.5427, Val Loss: 5.6665\n",
            "Epoch 17/1000, Train Loss: 5.5366, Val Loss: 5.6691\n",
            "Epoch 18/1000, Train Loss: 5.5367, Val Loss: 5.6812\n",
            "Epoch 19/1000, Train Loss: 5.5393, Val Loss: 5.6640\n",
            "Epoch 20/1000, Train Loss: 5.5373, Val Loss: 5.6815\n",
            "Epoch 21/1000, Train Loss: 5.5407, Val Loss: 5.6672\n",
            "Epoch 22/1000, Train Loss: 5.5378, Val Loss: 5.6777\n",
            "Epoch 23/1000, Train Loss: 5.5412, Val Loss: 5.6629\n",
            "Epoch 24/1000, Train Loss: 5.5380, Val Loss: 5.6668\n",
            "Epoch 25/1000, Train Loss: 5.5351, Val Loss: 5.6627\n",
            "Epoch 26/1000, Train Loss: 5.5365, Val Loss: 5.6703\n",
            "Epoch 27/1000, Train Loss: 5.5350, Val Loss: 5.6703\n",
            "Epoch 28/1000, Train Loss: 5.5348, Val Loss: 5.6650\n",
            "Epoch 29/1000, Train Loss: 5.5407, Val Loss: 5.6637\n",
            "Epoch 30/1000, Train Loss: 5.5389, Val Loss: 5.6644\n",
            "Epoch 31/1000, Train Loss: 5.5389, Val Loss: 5.6722\n",
            "Epoch 32/1000, Train Loss: 5.5403, Val Loss: 5.6642\n",
            "Epoch 33/1000, Train Loss: 5.5391, Val Loss: 5.6631\n",
            "Epoch 34/1000, Train Loss: 5.5337, Val Loss: 5.6633\n",
            "Epoch 35/1000, Train Loss: 5.5374, Val Loss: 5.6643\n",
            "Early stopping triggered at epoch 35. Best Val Loss: 5.6627\n",
            "Epoch 1/1000, Train Loss: 5.5710, Val Loss: 5.6796\n",
            "Epoch 2/1000, Train Loss: 5.5390, Val Loss: 5.6744\n",
            "Epoch 3/1000, Train Loss: 5.5448, Val Loss: 5.6763\n",
            "Epoch 4/1000, Train Loss: 5.5479, Val Loss: 5.6719\n",
            "Epoch 5/1000, Train Loss: 5.5382, Val Loss: 5.6719\n",
            "Epoch 6/1000, Train Loss: 5.5475, Val Loss: 5.6688\n",
            "Epoch 7/1000, Train Loss: 5.5478, Val Loss: 5.6690\n",
            "Epoch 8/1000, Train Loss: 5.5375, Val Loss: 5.6723\n",
            "Epoch 9/1000, Train Loss: 5.5461, Val Loss: 5.6719\n",
            "Epoch 10/1000, Train Loss: 5.5422, Val Loss: 5.6714\n",
            "Epoch 11/1000, Train Loss: 5.5399, Val Loss: 5.6701\n",
            "Epoch 12/1000, Train Loss: 5.5411, Val Loss: 5.6720\n",
            "Epoch 13/1000, Train Loss: 5.5401, Val Loss: 5.6670\n",
            "Epoch 14/1000, Train Loss: 5.5409, Val Loss: 5.6698\n",
            "Epoch 15/1000, Train Loss: 5.5403, Val Loss: 5.6752\n",
            "Epoch 16/1000, Train Loss: 5.5408, Val Loss: 5.6711\n",
            "Epoch 17/1000, Train Loss: 5.5416, Val Loss: 5.6773\n",
            "Epoch 18/1000, Train Loss: 5.5367, Val Loss: 5.6731\n",
            "Epoch 19/1000, Train Loss: 5.5366, Val Loss: 5.6676\n",
            "Epoch 20/1000, Train Loss: 5.5429, Val Loss: 5.6662\n",
            "Epoch 21/1000, Train Loss: 5.5368, Val Loss: 5.6734\n",
            "Epoch 22/1000, Train Loss: 5.5391, Val Loss: 5.6713\n",
            "Epoch 23/1000, Train Loss: 5.5387, Val Loss: 5.6684\n",
            "Epoch 24/1000, Train Loss: 5.5402, Val Loss: 5.6683\n",
            "Epoch 25/1000, Train Loss: 5.5385, Val Loss: 5.6726\n",
            "Epoch 26/1000, Train Loss: 5.5343, Val Loss: 5.6715\n",
            "Epoch 27/1000, Train Loss: 5.5365, Val Loss: 5.6688\n",
            "Epoch 28/1000, Train Loss: 5.5382, Val Loss: 5.6721\n",
            "Epoch 29/1000, Train Loss: 5.5404, Val Loss: 5.6670\n",
            "Epoch 30/1000, Train Loss: 5.5351, Val Loss: 5.6714\n",
            "Early stopping triggered at epoch 30. Best Val Loss: 5.6662\n",
            "Training with LR=0.001, Hidden=32, Dropout=0.2, Scale Factor=1.25\n",
            "Combination 17/81\n",
            "Epoch 1/1000, Train Loss: 6.6354, Val Loss: 6.7560\n",
            "Epoch 2/1000, Train Loss: 6.6104, Val Loss: 6.7544\n",
            "Epoch 3/1000, Train Loss: 6.6032, Val Loss: 6.7576\n",
            "Epoch 4/1000, Train Loss: 6.6005, Val Loss: 6.7624\n",
            "Epoch 5/1000, Train Loss: 6.5961, Val Loss: 6.7548\n",
            "Epoch 6/1000, Train Loss: 6.5976, Val Loss: 6.7570\n",
            "Epoch 7/1000, Train Loss: 6.6036, Val Loss: 6.7576\n",
            "Epoch 8/1000, Train Loss: 6.5972, Val Loss: 6.7594\n",
            "Epoch 9/1000, Train Loss: 6.5942, Val Loss: 6.7498\n",
            "Epoch 10/1000, Train Loss: 6.5974, Val Loss: 6.7570\n",
            "Epoch 11/1000, Train Loss: 6.6006, Val Loss: 6.7595\n",
            "Epoch 12/1000, Train Loss: 6.5986, Val Loss: 6.7621\n",
            "Epoch 13/1000, Train Loss: 6.5974, Val Loss: 6.7668\n",
            "Epoch 14/1000, Train Loss: 6.6005, Val Loss: 6.7600\n",
            "Epoch 15/1000, Train Loss: 6.5989, Val Loss: 6.7627\n",
            "Epoch 16/1000, Train Loss: 6.5973, Val Loss: 6.7631\n",
            "Epoch 17/1000, Train Loss: 6.5939, Val Loss: 6.7624\n",
            "Epoch 18/1000, Train Loss: 6.5943, Val Loss: 6.7611\n",
            "Epoch 19/1000, Train Loss: 6.5974, Val Loss: 6.7578\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 6.7498\n",
            "Epoch 1/1000, Train Loss: 6.6579, Val Loss: 6.7622\n",
            "Epoch 2/1000, Train Loss: 6.6123, Val Loss: 6.7610\n",
            "Epoch 3/1000, Train Loss: 6.6116, Val Loss: 6.7619\n",
            "Epoch 4/1000, Train Loss: 6.6073, Val Loss: 6.7530\n",
            "Epoch 5/1000, Train Loss: 6.6052, Val Loss: 6.7598\n",
            "Epoch 6/1000, Train Loss: 6.6090, Val Loss: 6.7587\n",
            "Epoch 7/1000, Train Loss: 6.6052, Val Loss: 6.7588\n",
            "Epoch 8/1000, Train Loss: 6.6016, Val Loss: 6.7562\n",
            "Epoch 9/1000, Train Loss: 6.6009, Val Loss: 6.7557\n",
            "Epoch 10/1000, Train Loss: 6.6033, Val Loss: 6.7532\n",
            "Epoch 11/1000, Train Loss: 6.5975, Val Loss: 6.7554\n",
            "Epoch 12/1000, Train Loss: 6.6006, Val Loss: 6.7584\n",
            "Epoch 13/1000, Train Loss: 6.5956, Val Loss: 6.7597\n",
            "Epoch 14/1000, Train Loss: 6.5917, Val Loss: 6.7558\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 6.7530\n",
            "Epoch 1/1000, Train Loss: 6.6190, Val Loss: 6.7628\n",
            "Epoch 2/1000, Train Loss: 6.6094, Val Loss: 6.7597\n",
            "Epoch 3/1000, Train Loss: 6.5976, Val Loss: 6.7574\n",
            "Epoch 4/1000, Train Loss: 6.6117, Val Loss: 6.7639\n",
            "Epoch 5/1000, Train Loss: 6.6062, Val Loss: 6.7572\n",
            "Epoch 6/1000, Train Loss: 6.6027, Val Loss: 6.7516\n",
            "Epoch 7/1000, Train Loss: 6.5999, Val Loss: 6.7571\n",
            "Epoch 8/1000, Train Loss: 6.6066, Val Loss: 6.7589\n",
            "Epoch 9/1000, Train Loss: 6.5959, Val Loss: 6.7610\n",
            "Epoch 10/1000, Train Loss: 6.5986, Val Loss: 6.7561\n",
            "Epoch 11/1000, Train Loss: 6.5974, Val Loss: 6.7665\n",
            "Epoch 12/1000, Train Loss: 6.5963, Val Loss: 6.7486\n",
            "Epoch 13/1000, Train Loss: 6.5980, Val Loss: 6.7606\n",
            "Epoch 14/1000, Train Loss: 6.6020, Val Loss: 6.7579\n",
            "Epoch 15/1000, Train Loss: 6.5985, Val Loss: 6.7511\n",
            "Epoch 16/1000, Train Loss: 6.5958, Val Loss: 6.7579\n",
            "Epoch 17/1000, Train Loss: 6.5935, Val Loss: 6.7660\n",
            "Epoch 18/1000, Train Loss: 6.5947, Val Loss: 6.7548\n",
            "Epoch 19/1000, Train Loss: 6.5913, Val Loss: 6.7514\n",
            "Epoch 20/1000, Train Loss: 6.5932, Val Loss: 6.7584\n",
            "Epoch 21/1000, Train Loss: 6.5938, Val Loss: 6.7596\n",
            "Epoch 22/1000, Train Loss: 6.5926, Val Loss: 6.7642\n",
            "Early stopping triggered at epoch 22. Best Val Loss: 6.7486\n",
            "Training with LR=0.001, Hidden=32, Dropout=0.2, Scale Factor=1.5\n",
            "Combination 18/81\n",
            "Epoch 1/1000, Train Loss: 7.6889, Val Loss: 7.8183\n",
            "Epoch 2/1000, Train Loss: 7.6464, Val Loss: 7.8235\n",
            "Epoch 3/1000, Train Loss: 7.6400, Val Loss: 7.8228\n",
            "Epoch 4/1000, Train Loss: 7.6453, Val Loss: 7.8215\n",
            "Epoch 5/1000, Train Loss: 7.6312, Val Loss: 7.8427\n",
            "Epoch 6/1000, Train Loss: 7.6352, Val Loss: 7.8184\n",
            "Epoch 7/1000, Train Loss: 7.6463, Val Loss: 7.8225\n",
            "Epoch 8/1000, Train Loss: 7.6341, Val Loss: 7.8297\n",
            "Epoch 9/1000, Train Loss: 7.6282, Val Loss: 7.8261\n",
            "Epoch 10/1000, Train Loss: 7.6371, Val Loss: 7.8201\n",
            "Epoch 11/1000, Train Loss: 7.6355, Val Loss: 7.8238\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8183\n",
            "Epoch 1/1000, Train Loss: 7.6879, Val Loss: 7.8196\n",
            "Epoch 2/1000, Train Loss: 7.6523, Val Loss: 7.8385\n",
            "Epoch 3/1000, Train Loss: 7.6449, Val Loss: 7.8202\n",
            "Epoch 4/1000, Train Loss: 7.6402, Val Loss: 7.8338\n",
            "Epoch 5/1000, Train Loss: 7.6410, Val Loss: 7.8282\n",
            "Epoch 6/1000, Train Loss: 7.6431, Val Loss: 7.8238\n",
            "Epoch 7/1000, Train Loss: 7.6349, Val Loss: 7.8253\n",
            "Epoch 8/1000, Train Loss: 7.6424, Val Loss: 7.8239\n",
            "Epoch 9/1000, Train Loss: 7.6350, Val Loss: 7.8215\n",
            "Epoch 10/1000, Train Loss: 7.6357, Val Loss: 7.8262\n",
            "Epoch 11/1000, Train Loss: 7.6378, Val Loss: 7.8220\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8196\n",
            "Epoch 1/1000, Train Loss: 7.6795, Val Loss: 7.8179\n",
            "Epoch 2/1000, Train Loss: 7.6513, Val Loss: 7.8174\n",
            "Epoch 3/1000, Train Loss: 7.6363, Val Loss: 7.8259\n",
            "Epoch 4/1000, Train Loss: 7.6411, Val Loss: 7.8206\n",
            "Epoch 5/1000, Train Loss: 7.6390, Val Loss: 7.8213\n",
            "Epoch 6/1000, Train Loss: 7.6350, Val Loss: 7.8223\n",
            "Epoch 7/1000, Train Loss: 7.6316, Val Loss: 7.8196\n",
            "Epoch 8/1000, Train Loss: 7.6330, Val Loss: 7.8193\n",
            "Epoch 9/1000, Train Loss: 7.6345, Val Loss: 7.8257\n",
            "Epoch 10/1000, Train Loss: 7.6342, Val Loss: 7.8281\n",
            "Epoch 11/1000, Train Loss: 7.6397, Val Loss: 7.8202\n",
            "Epoch 12/1000, Train Loss: 7.6308, Val Loss: 7.8272\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 7.8174\n",
            "Training with LR=0.001, Hidden=64, Dropout=0.0, Scale Factor=1.0\n",
            "Combination 19/81\n",
            "Epoch 1/1000, Train Loss: 5.5511, Val Loss: 5.6736\n",
            "Epoch 2/1000, Train Loss: 5.5462, Val Loss: 5.6804\n",
            "Epoch 3/1000, Train Loss: 5.5447, Val Loss: 5.6766\n",
            "Epoch 4/1000, Train Loss: 5.5397, Val Loss: 5.6795\n",
            "Epoch 5/1000, Train Loss: 5.5438, Val Loss: 5.6667\n",
            "Epoch 6/1000, Train Loss: 5.5414, Val Loss: 5.6810\n",
            "Epoch 7/1000, Train Loss: 5.5412, Val Loss: 5.6682\n",
            "Epoch 8/1000, Train Loss: 5.5440, Val Loss: 5.6743\n",
            "Epoch 9/1000, Train Loss: 5.5413, Val Loss: 5.6691\n",
            "Epoch 10/1000, Train Loss: 5.5386, Val Loss: 5.6696\n",
            "Epoch 11/1000, Train Loss: 5.5378, Val Loss: 5.6822\n",
            "Epoch 12/1000, Train Loss: 5.5416, Val Loss: 5.6740\n",
            "Epoch 13/1000, Train Loss: 5.5402, Val Loss: 5.6674\n",
            "Epoch 14/1000, Train Loss: 5.5373, Val Loss: 5.6710\n",
            "Epoch 15/1000, Train Loss: 5.5370, Val Loss: 5.6829\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 5.6667\n",
            "Epoch 1/1000, Train Loss: 5.5537, Val Loss: 5.7141\n",
            "Epoch 2/1000, Train Loss: 5.5456, Val Loss: 5.6813\n",
            "Epoch 3/1000, Train Loss: 5.5429, Val Loss: 5.6723\n",
            "Epoch 4/1000, Train Loss: 5.5465, Val Loss: 5.6713\n",
            "Epoch 5/1000, Train Loss: 5.5436, Val Loss: 5.6782\n",
            "Epoch 6/1000, Train Loss: 5.5429, Val Loss: 5.6646\n",
            "Epoch 7/1000, Train Loss: 5.5425, Val Loss: 5.6699\n",
            "Epoch 8/1000, Train Loss: 5.5368, Val Loss: 5.6749\n",
            "Epoch 9/1000, Train Loss: 5.5401, Val Loss: 5.6659\n",
            "Epoch 10/1000, Train Loss: 5.5390, Val Loss: 5.6737\n",
            "Epoch 11/1000, Train Loss: 5.5376, Val Loss: 5.6646\n",
            "Epoch 12/1000, Train Loss: 5.5363, Val Loss: 5.6744\n",
            "Epoch 13/1000, Train Loss: 5.5369, Val Loss: 5.6742\n",
            "Epoch 14/1000, Train Loss: 5.5374, Val Loss: 5.6694\n",
            "Epoch 15/1000, Train Loss: 5.5387, Val Loss: 5.6720\n",
            "Epoch 16/1000, Train Loss: 5.5386, Val Loss: 5.6737\n",
            "Epoch 17/1000, Train Loss: 5.5357, Val Loss: 5.6672\n",
            "Epoch 18/1000, Train Loss: 5.5399, Val Loss: 5.6696\n",
            "Epoch 19/1000, Train Loss: 5.5365, Val Loss: 5.6658\n",
            "Epoch 20/1000, Train Loss: 5.5388, Val Loss: 5.6693\n",
            "Epoch 21/1000, Train Loss: 5.5372, Val Loss: 5.6726\n",
            "Early stopping triggered at epoch 21. Best Val Loss: 5.6646\n",
            "Epoch 1/1000, Train Loss: 5.5554, Val Loss: 5.6758\n",
            "Epoch 2/1000, Train Loss: 5.5491, Val Loss: 5.6762\n",
            "Epoch 3/1000, Train Loss: 5.5446, Val Loss: 5.6790\n",
            "Epoch 4/1000, Train Loss: 5.5424, Val Loss: 5.6813\n",
            "Epoch 5/1000, Train Loss: 5.5454, Val Loss: 5.6739\n",
            "Epoch 6/1000, Train Loss: 5.5437, Val Loss: 5.6664\n",
            "Epoch 7/1000, Train Loss: 5.5418, Val Loss: 5.6737\n",
            "Epoch 8/1000, Train Loss: 5.5444, Val Loss: 5.6880\n",
            "Epoch 9/1000, Train Loss: 5.5404, Val Loss: 5.6835\n",
            "Epoch 10/1000, Train Loss: 5.5393, Val Loss: 5.6726\n",
            "Epoch 11/1000, Train Loss: 5.5411, Val Loss: 5.6708\n",
            "Epoch 12/1000, Train Loss: 5.5361, Val Loss: 5.6640\n",
            "Epoch 13/1000, Train Loss: 5.5382, Val Loss: 5.6700\n",
            "Epoch 14/1000, Train Loss: 5.5365, Val Loss: 5.6698\n",
            "Epoch 15/1000, Train Loss: 5.5367, Val Loss: 5.6778\n",
            "Epoch 16/1000, Train Loss: 5.5374, Val Loss: 5.6770\n",
            "Epoch 17/1000, Train Loss: 5.5375, Val Loss: 5.6792\n",
            "Epoch 18/1000, Train Loss: 5.5375, Val Loss: 5.6762\n",
            "Epoch 19/1000, Train Loss: 5.5396, Val Loss: 5.6679\n",
            "Epoch 20/1000, Train Loss: 5.5399, Val Loss: 5.6620\n",
            "Epoch 21/1000, Train Loss: 5.5362, Val Loss: 5.6620\n",
            "Epoch 22/1000, Train Loss: 5.5359, Val Loss: 5.6696\n",
            "Epoch 23/1000, Train Loss: 5.5387, Val Loss: 5.6714\n",
            "Epoch 24/1000, Train Loss: 5.5390, Val Loss: 5.6676\n",
            "Epoch 25/1000, Train Loss: 5.5403, Val Loss: 5.6685\n",
            "Epoch 26/1000, Train Loss: 5.5375, Val Loss: 5.6685\n",
            "Epoch 27/1000, Train Loss: 5.5345, Val Loss: 5.6761\n",
            "Epoch 28/1000, Train Loss: 5.5383, Val Loss: 5.6777\n",
            "Epoch 29/1000, Train Loss: 5.5355, Val Loss: 5.6691\n",
            "Epoch 30/1000, Train Loss: 5.5388, Val Loss: 5.6673\n",
            "Early stopping triggered at epoch 30. Best Val Loss: 5.6620\n",
            "Training with LR=0.001, Hidden=64, Dropout=0.0, Scale Factor=1.25\n",
            "Combination 20/81\n",
            "Epoch 1/1000, Train Loss: 6.6052, Val Loss: 6.7543\n",
            "Epoch 2/1000, Train Loss: 6.6052, Val Loss: 6.7579\n",
            "Epoch 3/1000, Train Loss: 6.5983, Val Loss: 6.7664\n",
            "Epoch 4/1000, Train Loss: 6.6024, Val Loss: 6.7737\n",
            "Epoch 5/1000, Train Loss: 6.5961, Val Loss: 6.7593\n",
            "Epoch 6/1000, Train Loss: 6.6009, Val Loss: 6.7593\n",
            "Epoch 7/1000, Train Loss: 6.5961, Val Loss: 6.7594\n",
            "Epoch 8/1000, Train Loss: 6.5961, Val Loss: 6.7606\n",
            "Epoch 9/1000, Train Loss: 6.5950, Val Loss: 6.7594\n",
            "Epoch 10/1000, Train Loss: 6.6013, Val Loss: 6.7603\n",
            "Epoch 11/1000, Train Loss: 6.5973, Val Loss: 6.7537\n",
            "Epoch 12/1000, Train Loss: 6.5970, Val Loss: 6.7562\n",
            "Epoch 13/1000, Train Loss: 6.5972, Val Loss: 6.7544\n",
            "Epoch 14/1000, Train Loss: 6.5982, Val Loss: 6.7521\n",
            "Epoch 15/1000, Train Loss: 6.5948, Val Loss: 6.7619\n",
            "Epoch 16/1000, Train Loss: 6.5959, Val Loss: 6.7517\n",
            "Epoch 17/1000, Train Loss: 6.5935, Val Loss: 6.7556\n",
            "Epoch 18/1000, Train Loss: 6.5959, Val Loss: 6.7550\n",
            "Epoch 19/1000, Train Loss: 6.5937, Val Loss: 6.7512\n",
            "Epoch 20/1000, Train Loss: 6.5917, Val Loss: 6.7623\n",
            "Epoch 21/1000, Train Loss: 6.5931, Val Loss: 6.7567\n",
            "Epoch 22/1000, Train Loss: 6.5898, Val Loss: 6.7508\n",
            "Epoch 23/1000, Train Loss: 6.5938, Val Loss: 6.7538\n",
            "Epoch 24/1000, Train Loss: 6.5933, Val Loss: 6.7547\n",
            "Epoch 25/1000, Train Loss: 6.5943, Val Loss: 6.7495\n",
            "Epoch 26/1000, Train Loss: 6.5966, Val Loss: 6.7560\n",
            "Epoch 27/1000, Train Loss: 6.5935, Val Loss: 6.7574\n",
            "Epoch 28/1000, Train Loss: 6.5923, Val Loss: 6.7532\n",
            "Epoch 29/1000, Train Loss: 6.5922, Val Loss: 6.7512\n",
            "Epoch 30/1000, Train Loss: 6.5931, Val Loss: 6.7595\n",
            "Epoch 31/1000, Train Loss: 6.5927, Val Loss: 6.7511\n",
            "Epoch 32/1000, Train Loss: 6.5896, Val Loss: 6.7633\n",
            "Epoch 33/1000, Train Loss: 6.5937, Val Loss: 6.7526\n",
            "Epoch 34/1000, Train Loss: 6.5922, Val Loss: 6.7549\n",
            "Epoch 35/1000, Train Loss: 6.5928, Val Loss: 6.7601\n",
            "Early stopping triggered at epoch 35. Best Val Loss: 6.7495\n",
            "Epoch 1/1000, Train Loss: 6.6086, Val Loss: 6.7539\n",
            "Epoch 2/1000, Train Loss: 6.6023, Val Loss: 6.7673\n",
            "Epoch 3/1000, Train Loss: 6.6036, Val Loss: 6.7570\n",
            "Epoch 4/1000, Train Loss: 6.6058, Val Loss: 6.7549\n",
            "Epoch 5/1000, Train Loss: 6.5985, Val Loss: 6.7566\n",
            "Epoch 6/1000, Train Loss: 6.6007, Val Loss: 6.7612\n",
            "Epoch 7/1000, Train Loss: 6.5972, Val Loss: 6.7597\n",
            "Epoch 8/1000, Train Loss: 6.5940, Val Loss: 6.7768\n",
            "Epoch 9/1000, Train Loss: 6.5943, Val Loss: 6.7616\n",
            "Epoch 10/1000, Train Loss: 6.5965, Val Loss: 6.7680\n",
            "Epoch 11/1000, Train Loss: 6.5967, Val Loss: 6.7610\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 6.7539\n",
            "Epoch 1/1000, Train Loss: 6.6192, Val Loss: 6.7600\n",
            "Epoch 2/1000, Train Loss: 6.6052, Val Loss: 6.8048\n",
            "Epoch 3/1000, Train Loss: 6.6022, Val Loss: 6.7767\n",
            "Epoch 4/1000, Train Loss: 6.6022, Val Loss: 6.7561\n",
            "Epoch 5/1000, Train Loss: 6.5984, Val Loss: 6.7751\n",
            "Epoch 6/1000, Train Loss: 6.5978, Val Loss: 6.7569\n",
            "Epoch 7/1000, Train Loss: 6.5982, Val Loss: 6.7777\n",
            "Epoch 8/1000, Train Loss: 6.6048, Val Loss: 6.7507\n",
            "Epoch 9/1000, Train Loss: 6.5993, Val Loss: 6.7573\n",
            "Epoch 10/1000, Train Loss: 6.5968, Val Loss: 6.7701\n",
            "Epoch 11/1000, Train Loss: 6.6013, Val Loss: 6.7534\n",
            "Epoch 12/1000, Train Loss: 6.5982, Val Loss: 6.7654\n",
            "Epoch 13/1000, Train Loss: 6.6026, Val Loss: 6.7560\n",
            "Epoch 14/1000, Train Loss: 6.5952, Val Loss: 6.7562\n",
            "Epoch 15/1000, Train Loss: 6.5977, Val Loss: 6.7597\n",
            "Epoch 16/1000, Train Loss: 6.5974, Val Loss: 6.7573\n",
            "Epoch 17/1000, Train Loss: 6.5949, Val Loss: 6.7596\n",
            "Epoch 18/1000, Train Loss: 6.6015, Val Loss: 6.7601\n",
            "Early stopping triggered at epoch 18. Best Val Loss: 6.7507\n",
            "Training with LR=0.001, Hidden=64, Dropout=0.0, Scale Factor=1.5\n",
            "Combination 21/81\n",
            "Epoch 1/1000, Train Loss: 7.6594, Val Loss: 7.8216\n",
            "Epoch 2/1000, Train Loss: 7.6434, Val Loss: 7.8873\n",
            "Epoch 3/1000, Train Loss: 7.6389, Val Loss: 7.8294\n",
            "Epoch 4/1000, Train Loss: 7.6366, Val Loss: 7.8224\n",
            "Epoch 5/1000, Train Loss: 7.6439, Val Loss: 7.8241\n",
            "Epoch 6/1000, Train Loss: 7.6331, Val Loss: 7.8388\n",
            "Epoch 7/1000, Train Loss: 7.6400, Val Loss: 7.8250\n",
            "Epoch 8/1000, Train Loss: 7.6419, Val Loss: 7.8280\n",
            "Epoch 9/1000, Train Loss: 7.6324, Val Loss: 7.8282\n",
            "Epoch 10/1000, Train Loss: 7.6336, Val Loss: 7.8226\n",
            "Epoch 11/1000, Train Loss: 7.6382, Val Loss: 7.8219\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8216\n",
            "Epoch 1/1000, Train Loss: 7.6543, Val Loss: 7.8175\n",
            "Epoch 2/1000, Train Loss: 7.6401, Val Loss: 7.8201\n",
            "Epoch 3/1000, Train Loss: 7.6357, Val Loss: 7.8228\n",
            "Epoch 4/1000, Train Loss: 7.6380, Val Loss: 7.8177\n",
            "Epoch 5/1000, Train Loss: 7.6385, Val Loss: 7.8225\n",
            "Epoch 6/1000, Train Loss: 7.6359, Val Loss: 7.8210\n",
            "Epoch 7/1000, Train Loss: 7.6337, Val Loss: 7.8210\n",
            "Epoch 8/1000, Train Loss: 7.6383, Val Loss: 7.8227\n",
            "Epoch 9/1000, Train Loss: 7.6294, Val Loss: 7.8240\n",
            "Epoch 10/1000, Train Loss: 7.6334, Val Loss: 7.8218\n",
            "Epoch 11/1000, Train Loss: 7.6319, Val Loss: 7.8254\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8175\n",
            "Epoch 1/1000, Train Loss: 7.6546, Val Loss: 7.8195\n",
            "Epoch 2/1000, Train Loss: 7.6415, Val Loss: 7.8206\n",
            "Epoch 3/1000, Train Loss: 7.6355, Val Loss: 7.9082\n",
            "Epoch 4/1000, Train Loss: 7.6453, Val Loss: 7.8216\n",
            "Epoch 5/1000, Train Loss: 7.6345, Val Loss: 7.8252\n",
            "Epoch 6/1000, Train Loss: 7.6335, Val Loss: 7.8240\n",
            "Epoch 7/1000, Train Loss: 7.6342, Val Loss: 7.8246\n",
            "Epoch 8/1000, Train Loss: 7.6368, Val Loss: 7.8318\n",
            "Epoch 9/1000, Train Loss: 7.6334, Val Loss: 7.8230\n",
            "Epoch 10/1000, Train Loss: 7.6362, Val Loss: 7.8307\n",
            "Epoch 11/1000, Train Loss: 7.6376, Val Loss: 7.8241\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8195\n",
            "Training with LR=0.001, Hidden=64, Dropout=0.1, Scale Factor=1.0\n",
            "Combination 22/81\n",
            "Epoch 1/1000, Train Loss: 5.5495, Val Loss: 5.6857\n",
            "Epoch 2/1000, Train Loss: 5.5464, Val Loss: 5.6739\n",
            "Epoch 3/1000, Train Loss: 5.5450, Val Loss: 5.6685\n",
            "Epoch 4/1000, Train Loss: 5.5439, Val Loss: 5.6689\n",
            "Epoch 5/1000, Train Loss: 5.5357, Val Loss: 5.6749\n",
            "Epoch 6/1000, Train Loss: 5.5405, Val Loss: 5.6745\n",
            "Epoch 7/1000, Train Loss: 5.5432, Val Loss: 5.6698\n",
            "Epoch 8/1000, Train Loss: 5.5393, Val Loss: 5.6732\n",
            "Epoch 9/1000, Train Loss: 5.5426, Val Loss: 5.6705\n",
            "Epoch 10/1000, Train Loss: 5.5382, Val Loss: 5.6671\n",
            "Epoch 11/1000, Train Loss: 5.5428, Val Loss: 5.6681\n",
            "Epoch 12/1000, Train Loss: 5.5386, Val Loss: 5.6675\n",
            "Epoch 13/1000, Train Loss: 5.5348, Val Loss: 5.6651\n",
            "Epoch 14/1000, Train Loss: 5.5403, Val Loss: 5.6714\n",
            "Epoch 15/1000, Train Loss: 5.5376, Val Loss: 5.6622\n",
            "Epoch 16/1000, Train Loss: 5.5404, Val Loss: 5.6641\n",
            "Epoch 17/1000, Train Loss: 5.5389, Val Loss: 5.6622\n",
            "Epoch 18/1000, Train Loss: 5.5384, Val Loss: 5.6648\n",
            "Epoch 19/1000, Train Loss: 5.5369, Val Loss: 5.6690\n",
            "Epoch 20/1000, Train Loss: 5.5424, Val Loss: 5.6705\n",
            "Epoch 21/1000, Train Loss: 5.5393, Val Loss: 5.6634\n",
            "Epoch 22/1000, Train Loss: 5.5356, Val Loss: 5.6660\n",
            "Epoch 23/1000, Train Loss: 5.5372, Val Loss: 5.6692\n",
            "Epoch 24/1000, Train Loss: 5.5398, Val Loss: 5.6679\n",
            "Epoch 25/1000, Train Loss: 5.5360, Val Loss: 5.6812\n",
            "Early stopping triggered at epoch 25. Best Val Loss: 5.6622\n",
            "Epoch 1/1000, Train Loss: 5.5578, Val Loss: 5.6799\n",
            "Epoch 2/1000, Train Loss: 5.5527, Val Loss: 5.6729\n",
            "Epoch 3/1000, Train Loss: 5.5471, Val Loss: 5.6677\n",
            "Epoch 4/1000, Train Loss: 5.5437, Val Loss: 5.6704\n",
            "Epoch 5/1000, Train Loss: 5.5463, Val Loss: 5.6649\n",
            "Epoch 6/1000, Train Loss: 5.5453, Val Loss: 5.6752\n",
            "Epoch 7/1000, Train Loss: 5.5397, Val Loss: 5.6661\n",
            "Epoch 8/1000, Train Loss: 5.5477, Val Loss: 5.6655\n",
            "Epoch 9/1000, Train Loss: 5.5426, Val Loss: 5.6665\n",
            "Epoch 10/1000, Train Loss: 5.5378, Val Loss: 5.6716\n",
            "Epoch 11/1000, Train Loss: 5.5397, Val Loss: 5.6726\n",
            "Epoch 12/1000, Train Loss: 5.5376, Val Loss: 5.6723\n",
            "Epoch 13/1000, Train Loss: 5.5376, Val Loss: 5.6679\n",
            "Epoch 14/1000, Train Loss: 5.5419, Val Loss: 5.6652\n",
            "Epoch 15/1000, Train Loss: 5.5413, Val Loss: 5.6661\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 5.6649\n",
            "Epoch 1/1000, Train Loss: 5.5626, Val Loss: 5.6734\n",
            "Epoch 2/1000, Train Loss: 5.5498, Val Loss: 5.6795\n",
            "Epoch 3/1000, Train Loss: 5.5482, Val Loss: 5.6833\n",
            "Epoch 4/1000, Train Loss: 5.5489, Val Loss: 5.6670\n",
            "Epoch 5/1000, Train Loss: 5.5441, Val Loss: 5.6823\n",
            "Epoch 6/1000, Train Loss: 5.5401, Val Loss: 5.6693\n",
            "Epoch 7/1000, Train Loss: 5.5505, Val Loss: 5.6768\n",
            "Epoch 8/1000, Train Loss: 5.5413, Val Loss: 5.6740\n",
            "Epoch 9/1000, Train Loss: 5.5417, Val Loss: 5.6679\n",
            "Epoch 10/1000, Train Loss: 5.5405, Val Loss: 5.6662\n",
            "Epoch 11/1000, Train Loss: 5.5432, Val Loss: 5.6716\n",
            "Epoch 12/1000, Train Loss: 5.5362, Val Loss: 5.6706\n",
            "Epoch 13/1000, Train Loss: 5.5440, Val Loss: 5.6775\n",
            "Epoch 14/1000, Train Loss: 5.5404, Val Loss: 5.6625\n",
            "Epoch 15/1000, Train Loss: 5.5413, Val Loss: 5.6698\n",
            "Epoch 16/1000, Train Loss: 5.5394, Val Loss: 5.6723\n",
            "Epoch 17/1000, Train Loss: 5.5386, Val Loss: 5.6732\n",
            "Epoch 18/1000, Train Loss: 5.5374, Val Loss: 5.6707\n",
            "Epoch 19/1000, Train Loss: 5.5437, Val Loss: 5.6664\n",
            "Epoch 20/1000, Train Loss: 5.5348, Val Loss: 5.6681\n",
            "Epoch 21/1000, Train Loss: 5.5354, Val Loss: 5.6689\n",
            "Epoch 22/1000, Train Loss: 5.5369, Val Loss: 5.6703\n",
            "Epoch 23/1000, Train Loss: 5.5355, Val Loss: 5.6687\n",
            "Epoch 24/1000, Train Loss: 5.5357, Val Loss: 5.6659\n",
            "Early stopping triggered at epoch 24. Best Val Loss: 5.6625\n",
            "Training with LR=0.001, Hidden=64, Dropout=0.1, Scale Factor=1.25\n",
            "Combination 23/81\n",
            "Epoch 1/1000, Train Loss: 6.6174, Val Loss: 6.7579\n",
            "Epoch 2/1000, Train Loss: 6.6112, Val Loss: 6.7626\n",
            "Epoch 3/1000, Train Loss: 6.6185, Val Loss: 6.7565\n",
            "Epoch 4/1000, Train Loss: 6.6060, Val Loss: 6.7580\n",
            "Epoch 5/1000, Train Loss: 6.6078, Val Loss: 6.7619\n",
            "Epoch 6/1000, Train Loss: 6.6044, Val Loss: 6.7654\n",
            "Epoch 7/1000, Train Loss: 6.5968, Val Loss: 6.7675\n",
            "Epoch 8/1000, Train Loss: 6.6067, Val Loss: 6.7563\n",
            "Epoch 9/1000, Train Loss: 6.5980, Val Loss: 6.7587\n",
            "Epoch 10/1000, Train Loss: 6.6050, Val Loss: 6.7559\n",
            "Epoch 11/1000, Train Loss: 6.5997, Val Loss: 6.7576\n",
            "Epoch 12/1000, Train Loss: 6.6010, Val Loss: 6.7621\n",
            "Epoch 13/1000, Train Loss: 6.6019, Val Loss: 6.7574\n",
            "Epoch 14/1000, Train Loss: 6.5960, Val Loss: 6.7570\n",
            "Epoch 15/1000, Train Loss: 6.6045, Val Loss: 6.7623\n",
            "Epoch 16/1000, Train Loss: 6.6047, Val Loss: 6.7582\n",
            "Epoch 17/1000, Train Loss: 6.5976, Val Loss: 6.7592\n",
            "Epoch 18/1000, Train Loss: 6.5983, Val Loss: 6.7599\n",
            "Epoch 19/1000, Train Loss: 6.6020, Val Loss: 6.7584\n",
            "Epoch 20/1000, Train Loss: 6.5962, Val Loss: 6.7518\n",
            "Epoch 21/1000, Train Loss: 6.6038, Val Loss: 6.7570\n",
            "Epoch 22/1000, Train Loss: 6.5941, Val Loss: 6.7632\n",
            "Epoch 23/1000, Train Loss: 6.5951, Val Loss: 6.7483\n",
            "Epoch 24/1000, Train Loss: 6.5959, Val Loss: 6.7523\n",
            "Epoch 25/1000, Train Loss: 6.5957, Val Loss: 6.7575\n",
            "Epoch 26/1000, Train Loss: 6.6023, Val Loss: 6.7690\n",
            "Epoch 27/1000, Train Loss: 6.5973, Val Loss: 6.7693\n",
            "Epoch 28/1000, Train Loss: 6.5954, Val Loss: 6.7530\n",
            "Epoch 29/1000, Train Loss: 6.5920, Val Loss: 6.7732\n",
            "Epoch 30/1000, Train Loss: 6.5966, Val Loss: 6.7616\n",
            "Epoch 31/1000, Train Loss: 6.5942, Val Loss: 6.7589\n",
            "Epoch 32/1000, Train Loss: 6.5974, Val Loss: 6.7506\n",
            "Epoch 33/1000, Train Loss: 6.5979, Val Loss: 6.7569\n",
            "Early stopping triggered at epoch 33. Best Val Loss: 6.7483\n",
            "Epoch 1/1000, Train Loss: 6.6178, Val Loss: 6.7543\n",
            "Epoch 2/1000, Train Loss: 6.6065, Val Loss: 6.7662\n",
            "Epoch 3/1000, Train Loss: 6.6019, Val Loss: 6.7824\n",
            "Epoch 4/1000, Train Loss: 6.6086, Val Loss: 6.7603\n",
            "Epoch 5/1000, Train Loss: 6.6060, Val Loss: 6.7515\n",
            "Epoch 6/1000, Train Loss: 6.6023, Val Loss: 6.7546\n",
            "Epoch 7/1000, Train Loss: 6.5994, Val Loss: 6.7610\n",
            "Epoch 8/1000, Train Loss: 6.6034, Val Loss: 6.7610\n",
            "Epoch 9/1000, Train Loss: 6.5997, Val Loss: 6.7627\n",
            "Epoch 10/1000, Train Loss: 6.5997, Val Loss: 6.7654\n",
            "Epoch 11/1000, Train Loss: 6.6060, Val Loss: 6.7656\n",
            "Epoch 12/1000, Train Loss: 6.6005, Val Loss: 6.7620\n",
            "Epoch 13/1000, Train Loss: 6.5973, Val Loss: 6.7595\n",
            "Epoch 14/1000, Train Loss: 6.5946, Val Loss: 6.7537\n",
            "Epoch 15/1000, Train Loss: 6.5946, Val Loss: 6.7550\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 6.7515\n",
            "Epoch 1/1000, Train Loss: 6.6094, Val Loss: 6.7654\n",
            "Epoch 2/1000, Train Loss: 6.6059, Val Loss: 6.7627\n",
            "Epoch 3/1000, Train Loss: 6.6081, Val Loss: 6.7689\n",
            "Epoch 4/1000, Train Loss: 6.6024, Val Loss: 6.7551\n",
            "Epoch 5/1000, Train Loss: 6.6001, Val Loss: 6.7616\n",
            "Epoch 6/1000, Train Loss: 6.6008, Val Loss: 6.7541\n",
            "Epoch 7/1000, Train Loss: 6.6013, Val Loss: 6.7574\n",
            "Epoch 8/1000, Train Loss: 6.6006, Val Loss: 6.7633\n",
            "Epoch 9/1000, Train Loss: 6.6022, Val Loss: 6.7592\n",
            "Epoch 10/1000, Train Loss: 6.5982, Val Loss: 6.7614\n",
            "Epoch 11/1000, Train Loss: 6.5957, Val Loss: 6.7629\n",
            "Epoch 12/1000, Train Loss: 6.6017, Val Loss: 6.7619\n",
            "Epoch 13/1000, Train Loss: 6.6048, Val Loss: 6.7595\n",
            "Epoch 14/1000, Train Loss: 6.5971, Val Loss: 6.7535\n",
            "Epoch 15/1000, Train Loss: 6.5921, Val Loss: 6.7480\n",
            "Epoch 16/1000, Train Loss: 6.5956, Val Loss: 6.7485\n",
            "Epoch 17/1000, Train Loss: 6.5982, Val Loss: 6.7654\n",
            "Epoch 18/1000, Train Loss: 6.5948, Val Loss: 6.7573\n",
            "Epoch 19/1000, Train Loss: 6.5947, Val Loss: 6.7628\n",
            "Epoch 20/1000, Train Loss: 6.5960, Val Loss: 6.7590\n",
            "Epoch 21/1000, Train Loss: 6.5961, Val Loss: 6.7603\n",
            "Epoch 22/1000, Train Loss: 6.5965, Val Loss: 6.7566\n",
            "Epoch 23/1000, Train Loss: 6.5934, Val Loss: 6.7630\n",
            "Epoch 24/1000, Train Loss: 6.5971, Val Loss: 6.7561\n",
            "Epoch 25/1000, Train Loss: 6.5949, Val Loss: 6.7474\n",
            "Epoch 26/1000, Train Loss: 6.5929, Val Loss: 6.7656\n",
            "Epoch 27/1000, Train Loss: 6.5931, Val Loss: 6.7636\n",
            "Epoch 28/1000, Train Loss: 6.5897, Val Loss: 6.7610\n",
            "Epoch 29/1000, Train Loss: 6.5930, Val Loss: 6.7561\n",
            "Epoch 30/1000, Train Loss: 6.5889, Val Loss: 6.7633\n",
            "Epoch 31/1000, Train Loss: 6.5899, Val Loss: 6.7480\n",
            "Epoch 32/1000, Train Loss: 6.5952, Val Loss: 6.7519\n",
            "Epoch 33/1000, Train Loss: 6.5966, Val Loss: 6.7685\n",
            "Epoch 34/1000, Train Loss: 6.5900, Val Loss: 6.7544\n",
            "Epoch 35/1000, Train Loss: 6.5990, Val Loss: 6.7563\n",
            "Early stopping triggered at epoch 35. Best Val Loss: 6.7474\n",
            "Training with LR=0.001, Hidden=64, Dropout=0.1, Scale Factor=1.5\n",
            "Combination 24/81\n",
            "Epoch 1/1000, Train Loss: 7.6421, Val Loss: 7.8208\n",
            "Epoch 2/1000, Train Loss: 7.6414, Val Loss: 7.8413\n",
            "Epoch 3/1000, Train Loss: 7.6451, Val Loss: 7.8183\n",
            "Epoch 4/1000, Train Loss: 7.6402, Val Loss: 7.8260\n",
            "Epoch 5/1000, Train Loss: 7.6440, Val Loss: 7.8246\n",
            "Epoch 6/1000, Train Loss: 7.6324, Val Loss: 7.8384\n",
            "Epoch 7/1000, Train Loss: 7.6425, Val Loss: 7.8227\n",
            "Epoch 8/1000, Train Loss: 7.6310, Val Loss: 7.8495\n",
            "Epoch 9/1000, Train Loss: 7.6413, Val Loss: 7.8221\n",
            "Epoch 10/1000, Train Loss: 7.6391, Val Loss: 7.8261\n",
            "Epoch 11/1000, Train Loss: 7.6325, Val Loss: 7.8285\n",
            "Epoch 12/1000, Train Loss: 7.6427, Val Loss: 7.8234\n",
            "Epoch 13/1000, Train Loss: 7.6373, Val Loss: 7.8393\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 7.8183\n",
            "Epoch 1/1000, Train Loss: 7.6642, Val Loss: 7.8261\n",
            "Epoch 2/1000, Train Loss: 7.6457, Val Loss: 7.8288\n",
            "Epoch 3/1000, Train Loss: 7.6482, Val Loss: 7.8277\n",
            "Epoch 4/1000, Train Loss: 7.6393, Val Loss: 7.8208\n",
            "Epoch 5/1000, Train Loss: 7.6379, Val Loss: 7.8239\n",
            "Epoch 6/1000, Train Loss: 7.6382, Val Loss: 7.8216\n",
            "Epoch 7/1000, Train Loss: 7.6392, Val Loss: 7.8277\n",
            "Epoch 8/1000, Train Loss: 7.6357, Val Loss: 7.8262\n",
            "Epoch 9/1000, Train Loss: 7.6388, Val Loss: 7.8282\n",
            "Epoch 10/1000, Train Loss: 7.6383, Val Loss: 7.8271\n",
            "Epoch 11/1000, Train Loss: 7.6334, Val Loss: 7.8302\n",
            "Epoch 12/1000, Train Loss: 7.6370, Val Loss: 7.8242\n",
            "Epoch 13/1000, Train Loss: 7.6346, Val Loss: 7.8252\n",
            "Epoch 14/1000, Train Loss: 7.6310, Val Loss: 7.8229\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 7.8208\n",
            "Epoch 1/1000, Train Loss: 7.6646, Val Loss: 7.8200\n",
            "Epoch 2/1000, Train Loss: 7.6494, Val Loss: 7.8324\n",
            "Epoch 3/1000, Train Loss: 7.6426, Val Loss: 7.8201\n",
            "Epoch 4/1000, Train Loss: 7.6399, Val Loss: 7.8194\n",
            "Epoch 5/1000, Train Loss: 7.6389, Val Loss: 7.8203\n",
            "Epoch 6/1000, Train Loss: 7.6418, Val Loss: 7.8400\n",
            "Epoch 7/1000, Train Loss: 7.6394, Val Loss: 7.8239\n",
            "Epoch 8/1000, Train Loss: 7.6351, Val Loss: 7.8212\n",
            "Epoch 9/1000, Train Loss: 7.6437, Val Loss: 7.8188\n",
            "Epoch 10/1000, Train Loss: 7.6356, Val Loss: 7.8210\n",
            "Epoch 11/1000, Train Loss: 7.6350, Val Loss: 7.8245\n",
            "Epoch 12/1000, Train Loss: 7.6341, Val Loss: 7.8330\n",
            "Epoch 13/1000, Train Loss: 7.6345, Val Loss: 7.8302\n",
            "Epoch 14/1000, Train Loss: 7.6311, Val Loss: 7.8416\n",
            "Epoch 15/1000, Train Loss: 7.6313, Val Loss: 7.8240\n",
            "Epoch 16/1000, Train Loss: 7.6277, Val Loss: 7.8275\n",
            "Epoch 17/1000, Train Loss: 7.6330, Val Loss: 7.8236\n",
            "Epoch 18/1000, Train Loss: 7.6342, Val Loss: 7.8233\n",
            "Epoch 19/1000, Train Loss: 7.6354, Val Loss: 7.8241\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 7.8188\n",
            "Training with LR=0.001, Hidden=64, Dropout=0.2, Scale Factor=1.0\n",
            "Combination 25/81\n",
            "Epoch 1/1000, Train Loss: 5.5664, Val Loss: 5.6755\n",
            "Epoch 2/1000, Train Loss: 5.5491, Val Loss: 5.6739\n",
            "Epoch 3/1000, Train Loss: 5.5539, Val Loss: 5.6800\n",
            "Epoch 4/1000, Train Loss: 5.5475, Val Loss: 5.6895\n",
            "Epoch 5/1000, Train Loss: 5.5506, Val Loss: 5.6770\n",
            "Epoch 6/1000, Train Loss: 5.5389, Val Loss: 5.6792\n",
            "Epoch 7/1000, Train Loss: 5.5425, Val Loss: 5.6694\n",
            "Epoch 8/1000, Train Loss: 5.5436, Val Loss: 5.6680\n",
            "Epoch 9/1000, Train Loss: 5.5405, Val Loss: 5.6714\n",
            "Epoch 10/1000, Train Loss: 5.5370, Val Loss: 5.6777\n",
            "Epoch 11/1000, Train Loss: 5.5448, Val Loss: 5.6756\n",
            "Epoch 12/1000, Train Loss: 5.5392, Val Loss: 5.6673\n",
            "Epoch 13/1000, Train Loss: 5.5397, Val Loss: 5.6774\n",
            "Epoch 14/1000, Train Loss: 5.5386, Val Loss: 5.6835\n",
            "Epoch 15/1000, Train Loss: 5.5388, Val Loss: 5.6628\n",
            "Epoch 16/1000, Train Loss: 5.5435, Val Loss: 5.6689\n",
            "Epoch 17/1000, Train Loss: 5.5396, Val Loss: 5.6675\n",
            "Epoch 18/1000, Train Loss: 5.5401, Val Loss: 5.6634\n",
            "Epoch 19/1000, Train Loss: 5.5376, Val Loss: 5.6677\n",
            "Epoch 20/1000, Train Loss: 5.5385, Val Loss: 5.6731\n",
            "Epoch 21/1000, Train Loss: 5.5412, Val Loss: 5.6771\n",
            "Epoch 22/1000, Train Loss: 5.5382, Val Loss: 5.6646\n",
            "Epoch 23/1000, Train Loss: 5.5339, Val Loss: 5.6649\n",
            "Epoch 24/1000, Train Loss: 5.5365, Val Loss: 5.6647\n",
            "Epoch 25/1000, Train Loss: 5.5354, Val Loss: 5.6819\n",
            "Early stopping triggered at epoch 25. Best Val Loss: 5.6628\n",
            "Epoch 1/1000, Train Loss: 5.5686, Val Loss: 5.6718\n",
            "Epoch 2/1000, Train Loss: 5.5574, Val Loss: 5.6804\n",
            "Epoch 3/1000, Train Loss: 5.5541, Val Loss: 5.6946\n",
            "Epoch 4/1000, Train Loss: 5.5475, Val Loss: 5.6670\n",
            "Epoch 5/1000, Train Loss: 5.5450, Val Loss: 5.6817\n",
            "Epoch 6/1000, Train Loss: 5.5402, Val Loss: 5.6852\n",
            "Epoch 7/1000, Train Loss: 5.5509, Val Loss: 5.6820\n",
            "Epoch 8/1000, Train Loss: 5.5441, Val Loss: 5.6723\n",
            "Epoch 9/1000, Train Loss: 5.5444, Val Loss: 5.6679\n",
            "Epoch 10/1000, Train Loss: 5.5463, Val Loss: 5.6686\n",
            "Epoch 11/1000, Train Loss: 5.5410, Val Loss: 5.6817\n",
            "Epoch 12/1000, Train Loss: 5.5448, Val Loss: 5.6910\n",
            "Epoch 13/1000, Train Loss: 5.5398, Val Loss: 5.6670\n",
            "Epoch 14/1000, Train Loss: 5.5443, Val Loss: 5.6658\n",
            "Epoch 15/1000, Train Loss: 5.5373, Val Loss: 5.6662\n",
            "Epoch 16/1000, Train Loss: 5.5439, Val Loss: 5.6686\n",
            "Epoch 17/1000, Train Loss: 5.5350, Val Loss: 5.6744\n",
            "Epoch 18/1000, Train Loss: 5.5393, Val Loss: 5.6680\n",
            "Epoch 19/1000, Train Loss: 5.5400, Val Loss: 5.6642\n",
            "Epoch 20/1000, Train Loss: 5.5386, Val Loss: 5.6717\n",
            "Epoch 21/1000, Train Loss: 5.5409, Val Loss: 5.6638\n",
            "Epoch 22/1000, Train Loss: 5.5393, Val Loss: 5.6688\n",
            "Epoch 23/1000, Train Loss: 5.5422, Val Loss: 5.6718\n",
            "Epoch 24/1000, Train Loss: 5.5434, Val Loss: 5.6610\n",
            "Epoch 25/1000, Train Loss: 5.5404, Val Loss: 5.6661\n",
            "Epoch 26/1000, Train Loss: 5.5373, Val Loss: 5.6646\n",
            "Epoch 27/1000, Train Loss: 5.5391, Val Loss: 5.6710\n",
            "Epoch 28/1000, Train Loss: 5.5398, Val Loss: 5.6680\n",
            "Epoch 29/1000, Train Loss: 5.5383, Val Loss: 5.6689\n",
            "Epoch 30/1000, Train Loss: 5.5409, Val Loss: 5.6647\n",
            "Epoch 31/1000, Train Loss: 5.5423, Val Loss: 5.6642\n",
            "Epoch 32/1000, Train Loss: 5.5408, Val Loss: 5.6619\n",
            "Epoch 33/1000, Train Loss: 5.5377, Val Loss: 5.6708\n",
            "Epoch 34/1000, Train Loss: 5.5400, Val Loss: 5.6670\n",
            "Early stopping triggered at epoch 34. Best Val Loss: 5.6610\n",
            "Epoch 1/1000, Train Loss: 5.5678, Val Loss: 5.6742\n",
            "Epoch 2/1000, Train Loss: 5.5559, Val Loss: 5.6726\n",
            "Epoch 3/1000, Train Loss: 5.5511, Val Loss: 5.6685\n",
            "Epoch 4/1000, Train Loss: 5.5434, Val Loss: 5.6673\n",
            "Epoch 5/1000, Train Loss: 5.5428, Val Loss: 5.6660\n",
            "Epoch 6/1000, Train Loss: 5.5416, Val Loss: 5.6838\n",
            "Epoch 7/1000, Train Loss: 5.5421, Val Loss: 5.6696\n",
            "Epoch 8/1000, Train Loss: 5.5402, Val Loss: 5.6807\n",
            "Epoch 9/1000, Train Loss: 5.5419, Val Loss: 5.6682\n",
            "Epoch 10/1000, Train Loss: 5.5397, Val Loss: 5.6739\n",
            "Epoch 11/1000, Train Loss: 5.5452, Val Loss: 5.6710\n",
            "Epoch 12/1000, Train Loss: 5.5444, Val Loss: 5.6631\n",
            "Epoch 13/1000, Train Loss: 5.5402, Val Loss: 5.6691\n",
            "Epoch 14/1000, Train Loss: 5.5388, Val Loss: 5.6631\n",
            "Epoch 15/1000, Train Loss: 5.5448, Val Loss: 5.6684\n",
            "Epoch 16/1000, Train Loss: 5.5383, Val Loss: 5.6692\n",
            "Epoch 17/1000, Train Loss: 5.5396, Val Loss: 5.6635\n",
            "Epoch 18/1000, Train Loss: 5.5423, Val Loss: 5.6619\n",
            "Epoch 19/1000, Train Loss: 5.5406, Val Loss: 5.6660\n",
            "Epoch 20/1000, Train Loss: 5.5405, Val Loss: 5.6647\n",
            "Epoch 21/1000, Train Loss: 5.5425, Val Loss: 5.6637\n",
            "Epoch 22/1000, Train Loss: 5.5380, Val Loss: 5.6729\n",
            "Epoch 23/1000, Train Loss: 5.5462, Val Loss: 5.6748\n",
            "Epoch 24/1000, Train Loss: 5.5435, Val Loss: 5.6695\n",
            "Epoch 25/1000, Train Loss: 5.5415, Val Loss: 5.6716\n",
            "Epoch 26/1000, Train Loss: 5.5397, Val Loss: 5.6637\n",
            "Epoch 27/1000, Train Loss: 5.5389, Val Loss: 5.6684\n",
            "Epoch 28/1000, Train Loss: 5.5375, Val Loss: 5.6688\n",
            "Early stopping triggered at epoch 28. Best Val Loss: 5.6619\n",
            "Training with LR=0.001, Hidden=64, Dropout=0.2, Scale Factor=1.25\n",
            "Combination 26/81\n",
            "Epoch 1/1000, Train Loss: 6.6194, Val Loss: 6.7806\n",
            "Epoch 2/1000, Train Loss: 6.6143, Val Loss: 6.7604\n",
            "Epoch 3/1000, Train Loss: 6.6117, Val Loss: 6.7635\n",
            "Epoch 4/1000, Train Loss: 6.6101, Val Loss: 6.7568\n",
            "Epoch 5/1000, Train Loss: 6.6100, Val Loss: 6.7604\n",
            "Epoch 6/1000, Train Loss: 6.6102, Val Loss: 6.7601\n",
            "Epoch 7/1000, Train Loss: 6.6075, Val Loss: 6.7561\n",
            "Epoch 8/1000, Train Loss: 6.6011, Val Loss: 6.7685\n",
            "Epoch 9/1000, Train Loss: 6.6068, Val Loss: 6.7603\n",
            "Epoch 10/1000, Train Loss: 6.6012, Val Loss: 6.7566\n",
            "Epoch 11/1000, Train Loss: 6.6002, Val Loss: 6.7561\n",
            "Epoch 12/1000, Train Loss: 6.5983, Val Loss: 6.7634\n",
            "Epoch 13/1000, Train Loss: 6.6019, Val Loss: 6.7560\n",
            "Epoch 14/1000, Train Loss: 6.5953, Val Loss: 6.7584\n",
            "Epoch 15/1000, Train Loss: 6.5976, Val Loss: 6.7586\n",
            "Epoch 16/1000, Train Loss: 6.5969, Val Loss: 6.7510\n",
            "Epoch 17/1000, Train Loss: 6.6007, Val Loss: 6.7523\n",
            "Epoch 18/1000, Train Loss: 6.5937, Val Loss: 6.8184\n",
            "Epoch 19/1000, Train Loss: 6.5984, Val Loss: 6.7607\n",
            "Epoch 20/1000, Train Loss: 6.6005, Val Loss: 6.7632\n",
            "Epoch 21/1000, Train Loss: 6.5972, Val Loss: 6.7620\n",
            "Epoch 22/1000, Train Loss: 6.5993, Val Loss: 6.7529\n",
            "Epoch 23/1000, Train Loss: 6.5982, Val Loss: 6.7596\n",
            "Epoch 24/1000, Train Loss: 6.5993, Val Loss: 6.7605\n",
            "Epoch 25/1000, Train Loss: 6.5964, Val Loss: 6.7642\n",
            "Epoch 26/1000, Train Loss: 6.6033, Val Loss: 6.7556\n",
            "Early stopping triggered at epoch 26. Best Val Loss: 6.7510\n",
            "Epoch 1/1000, Train Loss: 6.6221, Val Loss: 6.7685\n",
            "Epoch 2/1000, Train Loss: 6.6147, Val Loss: 6.7683\n",
            "Epoch 3/1000, Train Loss: 6.6184, Val Loss: 6.7636\n",
            "Epoch 4/1000, Train Loss: 6.6158, Val Loss: 6.7636\n",
            "Epoch 5/1000, Train Loss: 6.6038, Val Loss: 6.7573\n",
            "Epoch 6/1000, Train Loss: 6.6083, Val Loss: 6.7862\n",
            "Epoch 7/1000, Train Loss: 6.6072, Val Loss: 6.7534\n",
            "Epoch 8/1000, Train Loss: 6.5966, Val Loss: 6.7595\n",
            "Epoch 9/1000, Train Loss: 6.6024, Val Loss: 6.7608\n",
            "Epoch 10/1000, Train Loss: 6.6039, Val Loss: 6.7636\n",
            "Epoch 11/1000, Train Loss: 6.6010, Val Loss: 6.7516\n",
            "Epoch 12/1000, Train Loss: 6.5970, Val Loss: 6.7643\n",
            "Epoch 13/1000, Train Loss: 6.6007, Val Loss: 6.7583\n",
            "Epoch 14/1000, Train Loss: 6.5931, Val Loss: 6.7639\n",
            "Epoch 15/1000, Train Loss: 6.5948, Val Loss: 6.7673\n",
            "Epoch 16/1000, Train Loss: 6.5957, Val Loss: 6.7694\n",
            "Epoch 17/1000, Train Loss: 6.5953, Val Loss: 6.7552\n",
            "Epoch 18/1000, Train Loss: 6.5964, Val Loss: 6.7624\n",
            "Epoch 19/1000, Train Loss: 6.6020, Val Loss: 6.7654\n",
            "Epoch 20/1000, Train Loss: 6.5952, Val Loss: 6.7602\n",
            "Epoch 21/1000, Train Loss: 6.5967, Val Loss: 6.7535\n",
            "Early stopping triggered at epoch 21. Best Val Loss: 6.7516\n",
            "Epoch 1/1000, Train Loss: 6.6167, Val Loss: 6.7859\n",
            "Epoch 2/1000, Train Loss: 6.6110, Val Loss: 6.7741\n",
            "Epoch 3/1000, Train Loss: 6.6033, Val Loss: 6.7634\n",
            "Epoch 4/1000, Train Loss: 6.6102, Val Loss: 6.7626\n",
            "Epoch 5/1000, Train Loss: 6.6016, Val Loss: 6.7673\n",
            "Epoch 6/1000, Train Loss: 6.6004, Val Loss: 6.7666\n",
            "Epoch 7/1000, Train Loss: 6.5994, Val Loss: 6.7629\n",
            "Epoch 8/1000, Train Loss: 6.5997, Val Loss: 6.7586\n",
            "Epoch 9/1000, Train Loss: 6.5947, Val Loss: 6.7614\n",
            "Epoch 10/1000, Train Loss: 6.5998, Val Loss: 6.7746\n",
            "Epoch 11/1000, Train Loss: 6.5975, Val Loss: 6.7819\n",
            "Epoch 12/1000, Train Loss: 6.5999, Val Loss: 6.7554\n",
            "Epoch 13/1000, Train Loss: 6.6002, Val Loss: 6.7591\n",
            "Epoch 14/1000, Train Loss: 6.6005, Val Loss: 6.7576\n",
            "Epoch 15/1000, Train Loss: 6.6029, Val Loss: 6.7557\n",
            "Epoch 16/1000, Train Loss: 6.5968, Val Loss: 6.7927\n",
            "Epoch 17/1000, Train Loss: 6.6045, Val Loss: 6.7627\n",
            "Epoch 18/1000, Train Loss: 6.6059, Val Loss: 6.7528\n",
            "Epoch 19/1000, Train Loss: 6.5985, Val Loss: 6.7598\n",
            "Epoch 20/1000, Train Loss: 6.5955, Val Loss: 6.7570\n",
            "Epoch 21/1000, Train Loss: 6.5953, Val Loss: 6.7550\n",
            "Epoch 22/1000, Train Loss: 6.5995, Val Loss: 6.7580\n",
            "Epoch 23/1000, Train Loss: 6.5966, Val Loss: 6.7921\n",
            "Epoch 24/1000, Train Loss: 6.6026, Val Loss: 6.7586\n",
            "Epoch 25/1000, Train Loss: 6.6000, Val Loss: 6.7554\n",
            "Epoch 26/1000, Train Loss: 6.6054, Val Loss: 6.7532\n",
            "Epoch 27/1000, Train Loss: 6.5915, Val Loss: 6.7587\n",
            "Epoch 28/1000, Train Loss: 6.6001, Val Loss: 6.7503\n",
            "Epoch 29/1000, Train Loss: 6.5992, Val Loss: 6.7671\n",
            "Epoch 30/1000, Train Loss: 6.5952, Val Loss: 6.7539\n",
            "Epoch 31/1000, Train Loss: 6.5975, Val Loss: 6.7499\n",
            "Epoch 32/1000, Train Loss: 6.6007, Val Loss: 6.7569\n",
            "Epoch 33/1000, Train Loss: 6.5996, Val Loss: 6.7600\n",
            "Epoch 34/1000, Train Loss: 6.5956, Val Loss: 6.7596\n",
            "Epoch 35/1000, Train Loss: 6.5992, Val Loss: 6.7595\n",
            "Epoch 36/1000, Train Loss: 6.5969, Val Loss: 6.7484\n",
            "Epoch 37/1000, Train Loss: 6.5957, Val Loss: 6.7568\n",
            "Epoch 38/1000, Train Loss: 6.5934, Val Loss: 6.7578\n",
            "Epoch 39/1000, Train Loss: 6.5936, Val Loss: 6.7618\n",
            "Epoch 40/1000, Train Loss: 6.5957, Val Loss: 6.7594\n",
            "Epoch 41/1000, Train Loss: 6.5960, Val Loss: 6.7630\n",
            "Epoch 42/1000, Train Loss: 6.5991, Val Loss: 6.7605\n",
            "Epoch 43/1000, Train Loss: 6.5997, Val Loss: 6.7559\n",
            "Epoch 44/1000, Train Loss: 6.5952, Val Loss: 6.7550\n",
            "Epoch 45/1000, Train Loss: 6.5972, Val Loss: 6.7565\n",
            "Epoch 46/1000, Train Loss: 6.5996, Val Loss: 6.7607\n",
            "Early stopping triggered at epoch 46. Best Val Loss: 6.7484\n",
            "Training with LR=0.001, Hidden=64, Dropout=0.2, Scale Factor=1.5\n",
            "Combination 27/81\n",
            "Epoch 1/1000, Train Loss: 7.6626, Val Loss: 7.8138\n",
            "Epoch 2/1000, Train Loss: 7.6446, Val Loss: 7.8350\n",
            "Epoch 3/1000, Train Loss: 7.6484, Val Loss: 7.8249\n",
            "Epoch 4/1000, Train Loss: 7.6471, Val Loss: 7.8345\n",
            "Epoch 5/1000, Train Loss: 7.6403, Val Loss: 7.8217\n",
            "Epoch 6/1000, Train Loss: 7.6391, Val Loss: 7.8402\n",
            "Epoch 7/1000, Train Loss: 7.6390, Val Loss: 7.8191\n",
            "Epoch 8/1000, Train Loss: 7.6446, Val Loss: 7.8257\n",
            "Epoch 9/1000, Train Loss: 7.6459, Val Loss: 7.8279\n",
            "Epoch 10/1000, Train Loss: 7.6419, Val Loss: 7.8260\n",
            "Epoch 11/1000, Train Loss: 7.6321, Val Loss: 7.8356\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8138\n",
            "Epoch 1/1000, Train Loss: 7.6699, Val Loss: 7.8252\n",
            "Epoch 2/1000, Train Loss: 7.6477, Val Loss: 7.8243\n",
            "Epoch 3/1000, Train Loss: 7.6520, Val Loss: 7.8390\n",
            "Epoch 4/1000, Train Loss: 7.6413, Val Loss: 7.8337\n",
            "Epoch 5/1000, Train Loss: 7.6478, Val Loss: 7.8299\n",
            "Epoch 6/1000, Train Loss: 7.6422, Val Loss: 7.8371\n",
            "Epoch 7/1000, Train Loss: 7.6388, Val Loss: 7.8222\n",
            "Epoch 8/1000, Train Loss: 7.6404, Val Loss: 7.8240\n",
            "Epoch 9/1000, Train Loss: 7.6345, Val Loss: 7.8273\n",
            "Epoch 10/1000, Train Loss: 7.6387, Val Loss: 7.8233\n",
            "Epoch 11/1000, Train Loss: 7.6296, Val Loss: 7.8249\n",
            "Epoch 12/1000, Train Loss: 7.6352, Val Loss: 7.8232\n",
            "Epoch 13/1000, Train Loss: 7.6327, Val Loss: 7.8496\n",
            "Epoch 14/1000, Train Loss: 7.6391, Val Loss: 7.8287\n",
            "Epoch 15/1000, Train Loss: 7.6401, Val Loss: 7.8278\n",
            "Epoch 16/1000, Train Loss: 7.6342, Val Loss: 7.8235\n",
            "Epoch 17/1000, Train Loss: 7.6346, Val Loss: 7.8324\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 7.8222\n",
            "Epoch 1/1000, Train Loss: 7.6648, Val Loss: 7.8272\n",
            "Epoch 2/1000, Train Loss: 7.6441, Val Loss: 7.8308\n",
            "Epoch 3/1000, Train Loss: 7.6473, Val Loss: 7.8211\n",
            "Epoch 4/1000, Train Loss: 7.6443, Val Loss: 7.8220\n",
            "Epoch 5/1000, Train Loss: 7.6453, Val Loss: 7.8318\n",
            "Epoch 6/1000, Train Loss: 7.6382, Val Loss: 7.8202\n",
            "Epoch 7/1000, Train Loss: 7.6387, Val Loss: 7.8222\n",
            "Epoch 8/1000, Train Loss: 7.6374, Val Loss: 7.8288\n",
            "Epoch 9/1000, Train Loss: 7.6382, Val Loss: 7.8251\n",
            "Epoch 10/1000, Train Loss: 7.6355, Val Loss: 7.8262\n",
            "Epoch 11/1000, Train Loss: 7.6387, Val Loss: 7.8437\n",
            "Epoch 12/1000, Train Loss: 7.6370, Val Loss: 7.8215\n",
            "Epoch 13/1000, Train Loss: 7.6356, Val Loss: 7.8214\n",
            "Epoch 14/1000, Train Loss: 7.6379, Val Loss: 7.8222\n",
            "Epoch 15/1000, Train Loss: 7.6332, Val Loss: 7.8265\n",
            "Epoch 16/1000, Train Loss: 7.6314, Val Loss: 7.8202\n",
            "Epoch 17/1000, Train Loss: 7.6397, Val Loss: 7.8245\n",
            "Epoch 18/1000, Train Loss: 7.6334, Val Loss: 7.8371\n",
            "Epoch 19/1000, Train Loss: 7.6362, Val Loss: 7.8210\n",
            "Epoch 20/1000, Train Loss: 7.6315, Val Loss: 7.8288\n",
            "Epoch 21/1000, Train Loss: 7.6367, Val Loss: 7.8193\n",
            "Epoch 22/1000, Train Loss: 7.6315, Val Loss: 7.8240\n",
            "Epoch 23/1000, Train Loss: 7.6323, Val Loss: 7.8195\n",
            "Epoch 24/1000, Train Loss: 7.6414, Val Loss: 7.8201\n",
            "Epoch 25/1000, Train Loss: 7.6322, Val Loss: 7.8330\n",
            "Epoch 26/1000, Train Loss: 7.6373, Val Loss: 7.8231\n",
            "Epoch 27/1000, Train Loss: 7.6348, Val Loss: 7.8210\n",
            "Epoch 28/1000, Train Loss: 7.6324, Val Loss: 7.8216\n",
            "Epoch 29/1000, Train Loss: 7.6315, Val Loss: 7.8239\n",
            "Epoch 30/1000, Train Loss: 7.6335, Val Loss: 7.8413\n",
            "Epoch 31/1000, Train Loss: 7.6330, Val Loss: 7.8319\n",
            "Early stopping triggered at epoch 31. Best Val Loss: 7.8193\n",
            "Training with LR=0.005, Hidden=16, Dropout=0.0, Scale Factor=1.0\n",
            "Combination 28/81\n",
            "Epoch 1/1000, Train Loss: 5.5658, Val Loss: 5.6906\n",
            "Epoch 2/1000, Train Loss: 5.5502, Val Loss: 5.6774\n",
            "Epoch 3/1000, Train Loss: 5.5489, Val Loss: 5.6719\n",
            "Epoch 4/1000, Train Loss: 5.5473, Val Loss: 5.6650\n",
            "Epoch 5/1000, Train Loss: 5.5403, Val Loss: 5.6662\n",
            "Epoch 6/1000, Train Loss: 5.5430, Val Loss: 5.6721\n",
            "Epoch 7/1000, Train Loss: 5.5445, Val Loss: 5.6709\n",
            "Epoch 8/1000, Train Loss: 5.5430, Val Loss: 5.6749\n",
            "Epoch 9/1000, Train Loss: 5.5416, Val Loss: 5.6790\n",
            "Epoch 10/1000, Train Loss: 5.5429, Val Loss: 5.6673\n",
            "Epoch 11/1000, Train Loss: 5.5414, Val Loss: 5.6783\n",
            "Epoch 12/1000, Train Loss: 5.5426, Val Loss: 5.6649\n",
            "Epoch 13/1000, Train Loss: 5.5446, Val Loss: 5.6671\n",
            "Epoch 14/1000, Train Loss: 5.5405, Val Loss: 5.6773\n",
            "Epoch 15/1000, Train Loss: 5.5394, Val Loss: 5.6711\n",
            "Epoch 16/1000, Train Loss: 5.5390, Val Loss: 5.6774\n",
            "Epoch 17/1000, Train Loss: 5.5425, Val Loss: 5.6669\n",
            "Epoch 18/1000, Train Loss: 5.5434, Val Loss: 5.6776\n",
            "Epoch 19/1000, Train Loss: 5.5423, Val Loss: 5.6657\n",
            "Epoch 20/1000, Train Loss: 5.5467, Val Loss: 5.6740\n",
            "Epoch 21/1000, Train Loss: 5.5421, Val Loss: 5.6659\n",
            "Epoch 22/1000, Train Loss: 5.5408, Val Loss: 5.6672\n",
            "Early stopping triggered at epoch 22. Best Val Loss: 5.6649\n",
            "Epoch 1/1000, Train Loss: 5.5510, Val Loss: 5.6755\n",
            "Epoch 2/1000, Train Loss: 5.5462, Val Loss: 5.6762\n",
            "Epoch 3/1000, Train Loss: 5.5426, Val Loss: 5.6724\n",
            "Epoch 4/1000, Train Loss: 5.5428, Val Loss: 5.6709\n",
            "Epoch 5/1000, Train Loss: 5.5406, Val Loss: 5.6614\n",
            "Epoch 6/1000, Train Loss: 5.5384, Val Loss: 5.6655\n",
            "Epoch 7/1000, Train Loss: 5.5432, Val Loss: 5.6776\n",
            "Epoch 8/1000, Train Loss: 5.5410, Val Loss: 5.6818\n",
            "Epoch 9/1000, Train Loss: 5.5395, Val Loss: 5.6626\n",
            "Epoch 10/1000, Train Loss: 5.5540, Val Loss: 5.6672\n",
            "Epoch 11/1000, Train Loss: 5.5512, Val Loss: 5.6696\n",
            "Epoch 12/1000, Train Loss: 5.5439, Val Loss: 5.6625\n",
            "Epoch 13/1000, Train Loss: 5.5410, Val Loss: 5.6769\n",
            "Epoch 14/1000, Train Loss: 5.5430, Val Loss: 5.6666\n",
            "Epoch 15/1000, Train Loss: 5.5380, Val Loss: 5.6600\n",
            "Epoch 16/1000, Train Loss: 5.5416, Val Loss: 5.6670\n",
            "Epoch 17/1000, Train Loss: 5.5449, Val Loss: 5.7590\n",
            "Epoch 18/1000, Train Loss: 5.5431, Val Loss: 5.6658\n",
            "Epoch 19/1000, Train Loss: 5.5368, Val Loss: 5.6701\n",
            "Epoch 20/1000, Train Loss: 5.5397, Val Loss: 5.6697\n",
            "Epoch 21/1000, Train Loss: 5.5383, Val Loss: 5.6754\n",
            "Epoch 22/1000, Train Loss: 5.5365, Val Loss: 5.6705\n",
            "Epoch 23/1000, Train Loss: 5.5390, Val Loss: 5.6662\n",
            "Epoch 24/1000, Train Loss: 5.5400, Val Loss: 5.6672\n",
            "Epoch 25/1000, Train Loss: 5.5380, Val Loss: 5.6683\n",
            "Early stopping triggered at epoch 25. Best Val Loss: 5.6600\n",
            "Epoch 1/1000, Train Loss: 5.5479, Val Loss: 5.6671\n",
            "Epoch 2/1000, Train Loss: 5.5491, Val Loss: 5.6728\n",
            "Epoch 3/1000, Train Loss: 5.5446, Val Loss: 5.6704\n",
            "Epoch 4/1000, Train Loss: 5.5416, Val Loss: 5.6790\n",
            "Epoch 5/1000, Train Loss: 5.5418, Val Loss: 5.6699\n",
            "Epoch 6/1000, Train Loss: 5.5465, Val Loss: 5.6672\n",
            "Epoch 7/1000, Train Loss: 5.5394, Val Loss: 5.6719\n",
            "Epoch 8/1000, Train Loss: 5.5413, Val Loss: 5.6701\n",
            "Epoch 9/1000, Train Loss: 5.5454, Val Loss: 5.6686\n",
            "Epoch 10/1000, Train Loss: 5.5438, Val Loss: 5.6688\n",
            "Epoch 11/1000, Train Loss: 5.5401, Val Loss: 5.6752\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 5.6671\n",
            "Training with LR=0.005, Hidden=16, Dropout=0.0, Scale Factor=1.25\n",
            "Combination 29/81\n",
            "Epoch 1/1000, Train Loss: 6.6057, Val Loss: 6.7550\n",
            "Epoch 2/1000, Train Loss: 6.6069, Val Loss: 6.7564\n",
            "Epoch 3/1000, Train Loss: 6.6030, Val Loss: 6.7565\n",
            "Epoch 4/1000, Train Loss: 6.6046, Val Loss: 6.7559\n",
            "Epoch 5/1000, Train Loss: 6.6037, Val Loss: 6.7624\n",
            "Epoch 6/1000, Train Loss: 6.6046, Val Loss: 6.7563\n",
            "Epoch 7/1000, Train Loss: 6.6013, Val Loss: 6.7528\n",
            "Epoch 8/1000, Train Loss: 6.6038, Val Loss: 6.7616\n",
            "Epoch 9/1000, Train Loss: 6.6018, Val Loss: 6.7552\n",
            "Epoch 10/1000, Train Loss: 6.5999, Val Loss: 6.7554\n",
            "Epoch 11/1000, Train Loss: 6.6031, Val Loss: 6.7602\n",
            "Epoch 12/1000, Train Loss: 6.5982, Val Loss: 6.7585\n",
            "Epoch 13/1000, Train Loss: 6.6065, Val Loss: 6.7571\n",
            "Epoch 14/1000, Train Loss: 6.5961, Val Loss: 6.7631\n",
            "Epoch 15/1000, Train Loss: 6.5990, Val Loss: 6.7608\n",
            "Epoch 16/1000, Train Loss: 6.5989, Val Loss: 6.8125\n",
            "Epoch 17/1000, Train Loss: 6.5992, Val Loss: 6.7560\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 6.7528\n",
            "Epoch 1/1000, Train Loss: 6.6106, Val Loss: 6.7650\n",
            "Epoch 2/1000, Train Loss: 6.6030, Val Loss: 6.7629\n",
            "Epoch 3/1000, Train Loss: 6.6044, Val Loss: 6.7653\n",
            "Epoch 4/1000, Train Loss: 6.5995, Val Loss: 6.7513\n",
            "Epoch 5/1000, Train Loss: 6.6058, Val Loss: 6.7486\n",
            "Epoch 6/1000, Train Loss: 6.6025, Val Loss: 6.7512\n",
            "Epoch 7/1000, Train Loss: 6.5980, Val Loss: 6.7478\n",
            "Epoch 8/1000, Train Loss: 6.6011, Val Loss: 6.7610\n",
            "Epoch 9/1000, Train Loss: 6.6026, Val Loss: 6.7497\n",
            "Epoch 10/1000, Train Loss: 6.5977, Val Loss: 6.7532\n",
            "Epoch 11/1000, Train Loss: 6.6028, Val Loss: 6.7610\n",
            "Epoch 12/1000, Train Loss: 6.6023, Val Loss: 6.7597\n",
            "Epoch 13/1000, Train Loss: 6.6015, Val Loss: 6.7592\n",
            "Epoch 14/1000, Train Loss: 6.6000, Val Loss: 6.7525\n",
            "Epoch 15/1000, Train Loss: 6.6013, Val Loss: 6.7675\n",
            "Epoch 16/1000, Train Loss: 6.6007, Val Loss: 6.7640\n",
            "Epoch 17/1000, Train Loss: 6.6033, Val Loss: 6.7710\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 6.7478\n",
            "Epoch 1/1000, Train Loss: 6.6037, Val Loss: 6.7610\n",
            "Epoch 2/1000, Train Loss: 6.6021, Val Loss: 6.7504\n",
            "Epoch 3/1000, Train Loss: 6.6018, Val Loss: 6.7774\n",
            "Epoch 4/1000, Train Loss: 6.6034, Val Loss: 6.7719\n",
            "Epoch 5/1000, Train Loss: 6.6059, Val Loss: 6.7594\n",
            "Epoch 6/1000, Train Loss: 6.6020, Val Loss: 6.7542\n",
            "Epoch 7/1000, Train Loss: 6.6007, Val Loss: 6.7580\n",
            "Epoch 8/1000, Train Loss: 6.6051, Val Loss: 6.7581\n",
            "Epoch 9/1000, Train Loss: 6.6015, Val Loss: 6.7597\n",
            "Epoch 10/1000, Train Loss: 6.6024, Val Loss: 6.7627\n",
            "Epoch 11/1000, Train Loss: 6.5971, Val Loss: 6.7563\n",
            "Epoch 12/1000, Train Loss: 6.5998, Val Loss: 6.7637\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 6.7504\n",
            "Training with LR=0.005, Hidden=16, Dropout=0.0, Scale Factor=1.5\n",
            "Combination 30/81\n",
            "Epoch 1/1000, Train Loss: 7.6699, Val Loss: 7.8272\n",
            "Epoch 2/1000, Train Loss: 7.6509, Val Loss: 7.8259\n",
            "Epoch 3/1000, Train Loss: 7.6351, Val Loss: 7.8282\n",
            "Epoch 4/1000, Train Loss: 7.6426, Val Loss: 7.8194\n",
            "Epoch 5/1000, Train Loss: 7.6411, Val Loss: 7.8248\n",
            "Epoch 6/1000, Train Loss: 7.6359, Val Loss: 7.8203\n",
            "Epoch 7/1000, Train Loss: 7.6388, Val Loss: 7.8275\n",
            "Epoch 8/1000, Train Loss: 7.6327, Val Loss: 7.8226\n",
            "Epoch 9/1000, Train Loss: 7.6368, Val Loss: 7.8205\n",
            "Epoch 10/1000, Train Loss: 7.6425, Val Loss: 7.8232\n",
            "Epoch 11/1000, Train Loss: 7.6359, Val Loss: 7.8297\n",
            "Epoch 12/1000, Train Loss: 7.6362, Val Loss: 7.8310\n",
            "Epoch 13/1000, Train Loss: 7.6325, Val Loss: 7.8247\n",
            "Epoch 14/1000, Train Loss: 7.6373, Val Loss: 7.8331\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 7.8194\n",
            "Epoch 1/1000, Train Loss: 7.6417, Val Loss: 7.8210\n",
            "Epoch 2/1000, Train Loss: 7.6409, Val Loss: 7.8220\n",
            "Epoch 3/1000, Train Loss: 7.6397, Val Loss: 7.8223\n",
            "Epoch 4/1000, Train Loss: 7.6399, Val Loss: 7.8252\n",
            "Epoch 5/1000, Train Loss: 7.6386, Val Loss: 7.8245\n",
            "Epoch 6/1000, Train Loss: 7.6402, Val Loss: 7.8288\n",
            "Epoch 7/1000, Train Loss: 7.6340, Val Loss: 7.8263\n",
            "Epoch 8/1000, Train Loss: 7.6331, Val Loss: 7.8349\n",
            "Epoch 9/1000, Train Loss: 7.6344, Val Loss: 7.8224\n",
            "Epoch 10/1000, Train Loss: 7.6353, Val Loss: 7.8306\n",
            "Epoch 11/1000, Train Loss: 7.6329, Val Loss: 7.8237\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8210\n",
            "Epoch 1/1000, Train Loss: 7.6667, Val Loss: 7.8554\n",
            "Epoch 2/1000, Train Loss: 7.6523, Val Loss: 7.8211\n",
            "Epoch 3/1000, Train Loss: 7.6515, Val Loss: 7.8336\n",
            "Epoch 4/1000, Train Loss: 7.6493, Val Loss: 7.8284\n",
            "Epoch 5/1000, Train Loss: 7.6419, Val Loss: 7.8174\n",
            "Epoch 6/1000, Train Loss: 7.6387, Val Loss: 7.8197\n",
            "Epoch 7/1000, Train Loss: 7.6418, Val Loss: 7.8236\n",
            "Epoch 8/1000, Train Loss: 7.6362, Val Loss: 7.8255\n",
            "Epoch 9/1000, Train Loss: 7.6393, Val Loss: 7.8324\n",
            "Epoch 10/1000, Train Loss: 7.6392, Val Loss: 7.8264\n",
            "Epoch 11/1000, Train Loss: 7.6347, Val Loss: 7.8256\n",
            "Epoch 12/1000, Train Loss: 7.6322, Val Loss: 7.8243\n",
            "Epoch 13/1000, Train Loss: 7.6361, Val Loss: 7.8900\n",
            "Epoch 14/1000, Train Loss: 7.6390, Val Loss: 7.8310\n",
            "Epoch 15/1000, Train Loss: 7.6367, Val Loss: 7.8228\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 7.8174\n",
            "Training with LR=0.005, Hidden=16, Dropout=0.1, Scale Factor=1.0\n",
            "Combination 31/81\n",
            "Epoch 1/1000, Train Loss: 5.5530, Val Loss: 5.6781\n",
            "Epoch 2/1000, Train Loss: 5.5469, Val Loss: 5.6810\n",
            "Epoch 3/1000, Train Loss: 5.5452, Val Loss: 5.6736\n",
            "Epoch 4/1000, Train Loss: 5.5425, Val Loss: 5.6778\n",
            "Epoch 5/1000, Train Loss: 5.5430, Val Loss: 5.6644\n",
            "Epoch 6/1000, Train Loss: 5.5430, Val Loss: 5.6654\n",
            "Epoch 7/1000, Train Loss: 5.5483, Val Loss: 5.6635\n",
            "Epoch 8/1000, Train Loss: 5.5381, Val Loss: 5.6777\n",
            "Epoch 9/1000, Train Loss: 5.5451, Val Loss: 5.6814\n",
            "Epoch 10/1000, Train Loss: 5.5461, Val Loss: 5.6743\n",
            "Epoch 11/1000, Train Loss: 5.5463, Val Loss: 5.6680\n",
            "Epoch 12/1000, Train Loss: 5.5444, Val Loss: 5.6709\n",
            "Epoch 13/1000, Train Loss: 5.5484, Val Loss: 5.6624\n",
            "Epoch 14/1000, Train Loss: 5.5523, Val Loss: 5.6659\n",
            "Epoch 15/1000, Train Loss: 5.5494, Val Loss: 5.6643\n",
            "Epoch 16/1000, Train Loss: 5.5501, Val Loss: 5.6671\n",
            "Epoch 17/1000, Train Loss: 5.5468, Val Loss: 5.6736\n",
            "Epoch 18/1000, Train Loss: 5.5410, Val Loss: 5.6668\n",
            "Epoch 19/1000, Train Loss: 5.5467, Val Loss: 5.6739\n",
            "Epoch 20/1000, Train Loss: 5.5468, Val Loss: 5.6670\n",
            "Epoch 21/1000, Train Loss: 5.5454, Val Loss: 5.6636\n",
            "Epoch 22/1000, Train Loss: 5.5419, Val Loss: 5.6734\n",
            "Epoch 23/1000, Train Loss: 5.5435, Val Loss: 5.6670\n",
            "Early stopping triggered at epoch 23. Best Val Loss: 5.6624\n",
            "Epoch 1/1000, Train Loss: 5.5632, Val Loss: 5.6631\n",
            "Epoch 2/1000, Train Loss: 5.5480, Val Loss: 5.6764\n",
            "Epoch 3/1000, Train Loss: 5.5453, Val Loss: 5.6711\n",
            "Epoch 4/1000, Train Loss: 5.5445, Val Loss: 5.6653\n",
            "Epoch 5/1000, Train Loss: 5.5459, Val Loss: 5.6694\n",
            "Epoch 6/1000, Train Loss: 5.5474, Val Loss: 5.6662\n",
            "Epoch 7/1000, Train Loss: 5.5434, Val Loss: 5.6864\n",
            "Epoch 8/1000, Train Loss: 5.5499, Val Loss: 5.6621\n",
            "Epoch 9/1000, Train Loss: 5.5504, Val Loss: 5.6654\n",
            "Epoch 10/1000, Train Loss: 5.5423, Val Loss: 5.6839\n",
            "Epoch 11/1000, Train Loss: 5.5424, Val Loss: 5.6828\n",
            "Epoch 12/1000, Train Loss: 5.5435, Val Loss: 5.6671\n",
            "Epoch 13/1000, Train Loss: 5.5484, Val Loss: 5.6692\n",
            "Epoch 14/1000, Train Loss: 5.5467, Val Loss: 5.6610\n",
            "Epoch 15/1000, Train Loss: 5.5449, Val Loss: 5.6618\n",
            "Epoch 16/1000, Train Loss: 5.5448, Val Loss: 5.6629\n",
            "Epoch 17/1000, Train Loss: 5.5435, Val Loss: 5.6644\n",
            "Epoch 18/1000, Train Loss: 5.5449, Val Loss: 5.6659\n",
            "Epoch 19/1000, Train Loss: 5.5441, Val Loss: 5.7374\n",
            "Epoch 20/1000, Train Loss: 5.5445, Val Loss: 5.6711\n",
            "Epoch 21/1000, Train Loss: 5.5418, Val Loss: 5.6684\n",
            "Epoch 22/1000, Train Loss: 5.5445, Val Loss: 5.6677\n",
            "Epoch 23/1000, Train Loss: 5.5409, Val Loss: 5.6661\n",
            "Epoch 24/1000, Train Loss: 5.5430, Val Loss: 5.6670\n",
            "Early stopping triggered at epoch 24. Best Val Loss: 5.6610\n",
            "Epoch 1/1000, Train Loss: 5.5577, Val Loss: 5.6704\n",
            "Epoch 2/1000, Train Loss: 5.5463, Val Loss: 5.6716\n",
            "Epoch 3/1000, Train Loss: 5.5414, Val Loss: 5.6745\n",
            "Epoch 4/1000, Train Loss: 5.5475, Val Loss: 5.6633\n",
            "Epoch 5/1000, Train Loss: 5.5422, Val Loss: 5.6692\n",
            "Epoch 6/1000, Train Loss: 5.5426, Val Loss: 5.6646\n",
            "Epoch 7/1000, Train Loss: 5.5447, Val Loss: 5.6664\n",
            "Epoch 8/1000, Train Loss: 5.5464, Val Loss: 5.6732\n",
            "Epoch 9/1000, Train Loss: 5.5479, Val Loss: 5.6876\n",
            "Epoch 10/1000, Train Loss: 5.5447, Val Loss: 5.6653\n",
            "Epoch 11/1000, Train Loss: 5.5418, Val Loss: 5.6758\n",
            "Epoch 12/1000, Train Loss: 5.5515, Val Loss: 5.6684\n",
            "Epoch 13/1000, Train Loss: 5.5428, Val Loss: 5.6684\n",
            "Epoch 14/1000, Train Loss: 5.5536, Val Loss: 5.6713\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 5.6633\n",
            "Training with LR=0.005, Hidden=16, Dropout=0.1, Scale Factor=1.25\n",
            "Combination 32/81\n",
            "Epoch 1/1000, Train Loss: 6.6108, Val Loss: 6.7524\n",
            "Epoch 2/1000, Train Loss: 6.6066, Val Loss: 6.7708\n",
            "Epoch 3/1000, Train Loss: 6.6070, Val Loss: 6.7589\n",
            "Epoch 4/1000, Train Loss: 6.6009, Val Loss: 6.7593\n",
            "Epoch 5/1000, Train Loss: 6.6047, Val Loss: 6.7651\n",
            "Epoch 6/1000, Train Loss: 6.6055, Val Loss: 6.7695\n",
            "Epoch 7/1000, Train Loss: 6.6015, Val Loss: 6.7610\n",
            "Epoch 8/1000, Train Loss: 6.6056, Val Loss: 6.7519\n",
            "Epoch 9/1000, Train Loss: 6.6053, Val Loss: 6.7667\n",
            "Epoch 10/1000, Train Loss: 6.6050, Val Loss: 6.7599\n",
            "Epoch 11/1000, Train Loss: 6.6058, Val Loss: 6.7552\n",
            "Epoch 12/1000, Train Loss: 6.6034, Val Loss: 6.7663\n",
            "Epoch 13/1000, Train Loss: 6.6038, Val Loss: 6.7485\n",
            "Epoch 14/1000, Train Loss: 6.6073, Val Loss: 6.7566\n",
            "Epoch 15/1000, Train Loss: 6.6005, Val Loss: 6.7552\n",
            "Epoch 16/1000, Train Loss: 6.6050, Val Loss: 6.7683\n",
            "Epoch 17/1000, Train Loss: 6.6016, Val Loss: 6.7573\n",
            "Epoch 18/1000, Train Loss: 6.6053, Val Loss: 6.7584\n",
            "Epoch 19/1000, Train Loss: 6.6042, Val Loss: 6.7569\n",
            "Epoch 20/1000, Train Loss: 6.6055, Val Loss: 6.7517\n",
            "Epoch 21/1000, Train Loss: 6.6062, Val Loss: 6.7537\n",
            "Epoch 22/1000, Train Loss: 6.6004, Val Loss: 6.7537\n",
            "Epoch 23/1000, Train Loss: 6.5990, Val Loss: 6.7528\n",
            "Early stopping triggered at epoch 23. Best Val Loss: 6.7485\n",
            "Epoch 1/1000, Train Loss: 6.6308, Val Loss: 6.7503\n",
            "Epoch 2/1000, Train Loss: 6.6027, Val Loss: 6.7515\n",
            "Epoch 3/1000, Train Loss: 6.6087, Val Loss: 6.7555\n",
            "Epoch 4/1000, Train Loss: 6.6059, Val Loss: 6.7499\n",
            "Epoch 5/1000, Train Loss: 6.6026, Val Loss: 6.7563\n",
            "Epoch 6/1000, Train Loss: 6.6019, Val Loss: 6.7564\n",
            "Epoch 7/1000, Train Loss: 6.6046, Val Loss: 6.7537\n",
            "Epoch 8/1000, Train Loss: 6.6008, Val Loss: 6.7507\n",
            "Epoch 9/1000, Train Loss: 6.6037, Val Loss: 6.8690\n",
            "Epoch 10/1000, Train Loss: 6.6031, Val Loss: 6.7517\n",
            "Epoch 11/1000, Train Loss: 6.6029, Val Loss: 6.7597\n",
            "Epoch 12/1000, Train Loss: 6.6002, Val Loss: 6.7778\n",
            "Epoch 13/1000, Train Loss: 6.5995, Val Loss: 6.7587\n",
            "Epoch 14/1000, Train Loss: 6.5965, Val Loss: 6.7533\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 6.7499\n",
            "Epoch 1/1000, Train Loss: 6.6388, Val Loss: 6.7608\n",
            "Epoch 2/1000, Train Loss: 6.6114, Val Loss: 6.7603\n",
            "Epoch 3/1000, Train Loss: 6.6018, Val Loss: 6.7623\n",
            "Epoch 4/1000, Train Loss: 6.6109, Val Loss: 6.7616\n",
            "Epoch 5/1000, Train Loss: 6.5999, Val Loss: 6.7628\n",
            "Epoch 6/1000, Train Loss: 6.6054, Val Loss: 6.7594\n",
            "Epoch 7/1000, Train Loss: 6.6025, Val Loss: 6.7694\n",
            "Epoch 8/1000, Train Loss: 6.5976, Val Loss: 6.7672\n",
            "Epoch 9/1000, Train Loss: 6.6052, Val Loss: 6.7633\n",
            "Epoch 10/1000, Train Loss: 6.6013, Val Loss: 6.7577\n",
            "Epoch 11/1000, Train Loss: 6.6007, Val Loss: 6.7670\n",
            "Epoch 12/1000, Train Loss: 6.6043, Val Loss: 6.7639\n",
            "Epoch 13/1000, Train Loss: 6.6033, Val Loss: 6.7504\n",
            "Epoch 14/1000, Train Loss: 6.6142, Val Loss: 6.7601\n",
            "Epoch 15/1000, Train Loss: 6.6047, Val Loss: 6.7548\n",
            "Epoch 16/1000, Train Loss: 6.6000, Val Loss: 6.7615\n",
            "Epoch 17/1000, Train Loss: 6.6120, Val Loss: 6.7757\n",
            "Epoch 18/1000, Train Loss: 6.6089, Val Loss: 6.7574\n",
            "Epoch 19/1000, Train Loss: 6.6052, Val Loss: 6.7587\n",
            "Epoch 20/1000, Train Loss: 6.6058, Val Loss: 6.7602\n",
            "Epoch 21/1000, Train Loss: 6.6032, Val Loss: 6.7605\n",
            "Epoch 22/1000, Train Loss: 6.6048, Val Loss: 6.7631\n",
            "Epoch 23/1000, Train Loss: 6.6030, Val Loss: 6.7645\n",
            "Early stopping triggered at epoch 23. Best Val Loss: 6.7504\n",
            "Training with LR=0.005, Hidden=16, Dropout=0.1, Scale Factor=1.5\n",
            "Combination 33/81\n",
            "Epoch 1/1000, Train Loss: 7.6638, Val Loss: 7.8196\n",
            "Epoch 2/1000, Train Loss: 7.6413, Val Loss: 7.8189\n",
            "Epoch 3/1000, Train Loss: 7.6358, Val Loss: 7.8228\n",
            "Epoch 4/1000, Train Loss: 7.6488, Val Loss: 7.8227\n",
            "Epoch 5/1000, Train Loss: 7.6391, Val Loss: 7.8278\n",
            "Epoch 6/1000, Train Loss: 7.6412, Val Loss: 7.8239\n",
            "Epoch 7/1000, Train Loss: 7.6377, Val Loss: 7.8240\n",
            "Epoch 8/1000, Train Loss: 7.6475, Val Loss: 7.8258\n",
            "Epoch 9/1000, Train Loss: 7.6351, Val Loss: 7.8224\n",
            "Epoch 10/1000, Train Loss: 7.6445, Val Loss: 7.8351\n",
            "Epoch 11/1000, Train Loss: 7.6381, Val Loss: 7.8417\n",
            "Epoch 12/1000, Train Loss: 7.6357, Val Loss: 7.8267\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 7.8189\n",
            "Epoch 1/1000, Train Loss: 7.6843, Val Loss: 7.8203\n",
            "Epoch 2/1000, Train Loss: 7.6548, Val Loss: 7.8230\n",
            "Epoch 3/1000, Train Loss: 7.6440, Val Loss: 7.8238\n",
            "Epoch 4/1000, Train Loss: 7.6414, Val Loss: 7.8519\n",
            "Epoch 5/1000, Train Loss: 7.6391, Val Loss: 7.8472\n",
            "Epoch 6/1000, Train Loss: 7.6419, Val Loss: 7.8211\n",
            "Epoch 7/1000, Train Loss: 7.6420, Val Loss: 7.8387\n",
            "Epoch 8/1000, Train Loss: 7.6374, Val Loss: 7.8211\n",
            "Epoch 9/1000, Train Loss: 7.6401, Val Loss: 7.8226\n",
            "Epoch 10/1000, Train Loss: 7.6425, Val Loss: 7.8377\n",
            "Epoch 11/1000, Train Loss: 7.6402, Val Loss: 7.8295\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8203\n",
            "Epoch 1/1000, Train Loss: 7.6657, Val Loss: 7.8229\n",
            "Epoch 2/1000, Train Loss: 7.6448, Val Loss: 7.8222\n",
            "Epoch 3/1000, Train Loss: 7.6396, Val Loss: 7.8212\n",
            "Epoch 4/1000, Train Loss: 7.6422, Val Loss: 7.8209\n",
            "Epoch 5/1000, Train Loss: 7.6327, Val Loss: 7.8262\n",
            "Epoch 6/1000, Train Loss: 7.6394, Val Loss: 7.8255\n",
            "Epoch 7/1000, Train Loss: 7.6348, Val Loss: 7.8207\n",
            "Epoch 8/1000, Train Loss: 7.6341, Val Loss: 7.8267\n",
            "Epoch 9/1000, Train Loss: 7.6346, Val Loss: 7.8276\n",
            "Epoch 10/1000, Train Loss: 7.6402, Val Loss: 7.8602\n",
            "Epoch 11/1000, Train Loss: 7.6409, Val Loss: 7.8424\n",
            "Epoch 12/1000, Train Loss: 7.6392, Val Loss: 7.8305\n",
            "Epoch 13/1000, Train Loss: 7.6435, Val Loss: 7.8228\n",
            "Epoch 14/1000, Train Loss: 7.6366, Val Loss: 7.8549\n",
            "Epoch 15/1000, Train Loss: 7.6463, Val Loss: 7.8331\n",
            "Epoch 16/1000, Train Loss: 7.6519, Val Loss: 7.8272\n",
            "Epoch 17/1000, Train Loss: 7.6412, Val Loss: 7.8293\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 7.8207\n",
            "Training with LR=0.005, Hidden=16, Dropout=0.2, Scale Factor=1.0\n",
            "Combination 34/81\n",
            "Epoch 1/1000, Train Loss: 5.5603, Val Loss: 5.6657\n",
            "Epoch 2/1000, Train Loss: 5.5505, Val Loss: 5.6632\n",
            "Epoch 3/1000, Train Loss: 5.5509, Val Loss: 5.9219\n",
            "Epoch 4/1000, Train Loss: 5.5497, Val Loss: 5.6669\n",
            "Epoch 5/1000, Train Loss: 5.5460, Val Loss: 5.6639\n",
            "Epoch 6/1000, Train Loss: 5.5501, Val Loss: 5.6643\n",
            "Epoch 7/1000, Train Loss: 5.5453, Val Loss: 5.6877\n",
            "Epoch 8/1000, Train Loss: 5.5519, Val Loss: 5.6687\n",
            "Epoch 9/1000, Train Loss: 5.5452, Val Loss: 5.6796\n",
            "Epoch 10/1000, Train Loss: 5.5519, Val Loss: 5.6672\n",
            "Epoch 11/1000, Train Loss: 5.5446, Val Loss: 5.6889\n",
            "Epoch 12/1000, Train Loss: 5.5536, Val Loss: 5.6652\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 5.6632\n",
            "Epoch 1/1000, Train Loss: 5.5537, Val Loss: 5.6667\n",
            "Epoch 2/1000, Train Loss: 5.5485, Val Loss: 5.6779\n",
            "Epoch 3/1000, Train Loss: 5.5484, Val Loss: 5.6623\n",
            "Epoch 4/1000, Train Loss: 5.5470, Val Loss: 5.6648\n",
            "Epoch 5/1000, Train Loss: 5.5453, Val Loss: 5.6706\n",
            "Epoch 6/1000, Train Loss: 5.5559, Val Loss: 5.6649\n",
            "Epoch 7/1000, Train Loss: 5.5449, Val Loss: 5.6724\n",
            "Epoch 8/1000, Train Loss: 5.5482, Val Loss: 5.6654\n",
            "Epoch 9/1000, Train Loss: 5.5467, Val Loss: 5.6665\n",
            "Epoch 10/1000, Train Loss: 5.5514, Val Loss: 5.6656\n",
            "Epoch 11/1000, Train Loss: 5.5394, Val Loss: 5.6670\n",
            "Epoch 12/1000, Train Loss: 5.5469, Val Loss: 5.6739\n",
            "Epoch 13/1000, Train Loss: 5.5540, Val Loss: 5.6657\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 5.6623\n",
            "Epoch 1/1000, Train Loss: 5.5478, Val Loss: 5.6704\n",
            "Epoch 2/1000, Train Loss: 5.5499, Val Loss: 5.6701\n",
            "Epoch 3/1000, Train Loss: 5.5497, Val Loss: 5.6637\n",
            "Epoch 4/1000, Train Loss: 5.5535, Val Loss: 5.6652\n",
            "Epoch 5/1000, Train Loss: 5.5518, Val Loss: 5.6640\n",
            "Epoch 6/1000, Train Loss: 5.5492, Val Loss: 5.6686\n",
            "Epoch 7/1000, Train Loss: 5.5449, Val Loss: 5.6789\n",
            "Epoch 8/1000, Train Loss: 5.5418, Val Loss: 5.6863\n",
            "Epoch 9/1000, Train Loss: 5.5464, Val Loss: 5.6714\n",
            "Epoch 10/1000, Train Loss: 5.5477, Val Loss: 5.6704\n",
            "Epoch 11/1000, Train Loss: 5.5522, Val Loss: 5.6667\n",
            "Epoch 12/1000, Train Loss: 5.5495, Val Loss: 5.6637\n",
            "Epoch 13/1000, Train Loss: 5.5450, Val Loss: 5.6703\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 5.6637\n",
            "Training with LR=0.005, Hidden=16, Dropout=0.2, Scale Factor=1.25\n",
            "Combination 35/81\n",
            "Epoch 1/1000, Train Loss: 6.6319, Val Loss: 6.7504\n",
            "Epoch 2/1000, Train Loss: 6.6116, Val Loss: 6.7486\n",
            "Epoch 3/1000, Train Loss: 6.6080, Val Loss: 6.7627\n",
            "Epoch 4/1000, Train Loss: 6.6109, Val Loss: 6.7503\n",
            "Epoch 5/1000, Train Loss: 6.6073, Val Loss: 6.7548\n",
            "Epoch 6/1000, Train Loss: 6.6072, Val Loss: 6.7605\n",
            "Epoch 7/1000, Train Loss: 6.6058, Val Loss: 6.7551\n",
            "Epoch 8/1000, Train Loss: 6.6017, Val Loss: 6.7567\n",
            "Epoch 9/1000, Train Loss: 6.6064, Val Loss: 6.7618\n",
            "Epoch 10/1000, Train Loss: 6.6082, Val Loss: 6.7610\n",
            "Epoch 11/1000, Train Loss: 6.5978, Val Loss: 6.7540\n",
            "Epoch 12/1000, Train Loss: 6.5995, Val Loss: 6.7646\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 6.7486\n",
            "Epoch 1/1000, Train Loss: 6.6492, Val Loss: 6.7638\n",
            "Epoch 2/1000, Train Loss: 6.6131, Val Loss: 6.7611\n",
            "Epoch 3/1000, Train Loss: 6.5982, Val Loss: 6.7658\n",
            "Epoch 4/1000, Train Loss: 6.6093, Val Loss: 6.7606\n",
            "Epoch 5/1000, Train Loss: 6.6089, Val Loss: 6.7552\n",
            "Epoch 6/1000, Train Loss: 6.6013, Val Loss: 6.7647\n",
            "Epoch 7/1000, Train Loss: 6.6042, Val Loss: 6.7512\n",
            "Epoch 8/1000, Train Loss: 6.6050, Val Loss: 6.7625\n",
            "Epoch 9/1000, Train Loss: 6.6081, Val Loss: 6.7524\n",
            "Epoch 10/1000, Train Loss: 6.6070, Val Loss: 6.7517\n",
            "Epoch 11/1000, Train Loss: 6.6043, Val Loss: 6.7564\n",
            "Epoch 12/1000, Train Loss: 6.6069, Val Loss: 6.7530\n",
            "Epoch 13/1000, Train Loss: 6.5974, Val Loss: 6.7605\n",
            "Epoch 14/1000, Train Loss: 6.6233, Val Loss: 6.7525\n",
            "Epoch 15/1000, Train Loss: 6.6048, Val Loss: 6.7648\n",
            "Epoch 16/1000, Train Loss: 6.6047, Val Loss: 6.7539\n",
            "Epoch 17/1000, Train Loss: 6.6024, Val Loss: 6.7539\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 6.7512\n",
            "Epoch 1/1000, Train Loss: 6.6225, Val Loss: 6.7660\n",
            "Epoch 2/1000, Train Loss: 6.6030, Val Loss: 6.7747\n",
            "Epoch 3/1000, Train Loss: 6.6069, Val Loss: 6.7733\n",
            "Epoch 4/1000, Train Loss: 6.6080, Val Loss: 6.7976\n",
            "Epoch 5/1000, Train Loss: 6.6023, Val Loss: 6.7672\n",
            "Epoch 6/1000, Train Loss: 6.6087, Val Loss: 6.7566\n",
            "Epoch 7/1000, Train Loss: 6.6037, Val Loss: 6.7585\n",
            "Epoch 8/1000, Train Loss: 6.6104, Val Loss: 6.7761\n",
            "Epoch 9/1000, Train Loss: 6.6096, Val Loss: 6.7697\n",
            "Epoch 10/1000, Train Loss: 6.6057, Val Loss: 6.7612\n",
            "Epoch 11/1000, Train Loss: 6.6066, Val Loss: 6.7621\n",
            "Epoch 12/1000, Train Loss: 6.6105, Val Loss: 6.7580\n",
            "Epoch 13/1000, Train Loss: 6.6012, Val Loss: 6.7627\n",
            "Epoch 14/1000, Train Loss: 6.6079, Val Loss: 6.7577\n",
            "Epoch 15/1000, Train Loss: 6.6078, Val Loss: 6.7473\n",
            "Epoch 16/1000, Train Loss: 6.6107, Val Loss: 6.7569\n",
            "Epoch 17/1000, Train Loss: 6.6060, Val Loss: 6.7686\n",
            "Epoch 18/1000, Train Loss: 6.6111, Val Loss: 6.7554\n",
            "Epoch 19/1000, Train Loss: 6.6109, Val Loss: 6.7491\n",
            "Epoch 20/1000, Train Loss: 6.6109, Val Loss: 6.7456\n",
            "Epoch 21/1000, Train Loss: 6.6067, Val Loss: 6.7447\n",
            "Epoch 22/1000, Train Loss: 6.6087, Val Loss: 6.7447\n",
            "Epoch 23/1000, Train Loss: 6.6064, Val Loss: 6.7451\n",
            "Epoch 24/1000, Train Loss: 6.6076, Val Loss: 6.7459\n",
            "Epoch 25/1000, Train Loss: 6.6097, Val Loss: 6.7481\n",
            "Epoch 26/1000, Train Loss: 6.6085, Val Loss: 6.7457\n",
            "Epoch 27/1000, Train Loss: 6.6091, Val Loss: 6.7456\n",
            "Epoch 28/1000, Train Loss: 6.6087, Val Loss: 6.7635\n",
            "Epoch 29/1000, Train Loss: 6.6120, Val Loss: 6.7499\n",
            "Epoch 30/1000, Train Loss: 6.6062, Val Loss: 6.7467\n",
            "Epoch 31/1000, Train Loss: 6.6060, Val Loss: 6.7755\n",
            "Epoch 32/1000, Train Loss: 6.5995, Val Loss: 6.7634\n",
            "Early stopping triggered at epoch 32. Best Val Loss: 6.7447\n",
            "Training with LR=0.005, Hidden=16, Dropout=0.2, Scale Factor=1.5\n",
            "Combination 36/81\n",
            "Epoch 1/1000, Train Loss: 7.6624, Val Loss: 7.8216\n",
            "Epoch 2/1000, Train Loss: 7.6457, Val Loss: 7.8256\n",
            "Epoch 3/1000, Train Loss: 7.6462, Val Loss: 7.8227\n",
            "Epoch 4/1000, Train Loss: 7.6425, Val Loss: 7.8166\n",
            "Epoch 5/1000, Train Loss: 7.6587, Val Loss: 7.8170\n",
            "Epoch 6/1000, Train Loss: 7.6445, Val Loss: 7.8146\n",
            "Epoch 7/1000, Train Loss: 7.6451, Val Loss: 7.8212\n",
            "Epoch 8/1000, Train Loss: 7.6457, Val Loss: 7.8291\n",
            "Epoch 9/1000, Train Loss: 7.6458, Val Loss: 7.8277\n",
            "Epoch 10/1000, Train Loss: 7.6422, Val Loss: 7.8271\n",
            "Epoch 11/1000, Train Loss: 7.6463, Val Loss: 7.8225\n",
            "Epoch 12/1000, Train Loss: 7.6420, Val Loss: 7.8314\n",
            "Epoch 13/1000, Train Loss: 7.6448, Val Loss: 7.8306\n",
            "Epoch 14/1000, Train Loss: 7.6489, Val Loss: 7.8252\n",
            "Epoch 15/1000, Train Loss: 7.6423, Val Loss: 7.8270\n",
            "Epoch 16/1000, Train Loss: 7.6380, Val Loss: 7.8260\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 7.8146\n",
            "Epoch 1/1000, Train Loss: 7.6600, Val Loss: 7.8203\n",
            "Epoch 2/1000, Train Loss: 7.6491, Val Loss: 7.8209\n",
            "Epoch 3/1000, Train Loss: 7.6435, Val Loss: 7.8238\n",
            "Epoch 4/1000, Train Loss: 7.6364, Val Loss: 7.8267\n",
            "Epoch 5/1000, Train Loss: 7.6411, Val Loss: 7.8201\n",
            "Epoch 6/1000, Train Loss: 7.6403, Val Loss: 7.8253\n",
            "Epoch 7/1000, Train Loss: 7.6358, Val Loss: 7.8258\n",
            "Epoch 8/1000, Train Loss: 7.6397, Val Loss: 7.8247\n",
            "Epoch 9/1000, Train Loss: 7.6464, Val Loss: 7.8325\n",
            "Epoch 10/1000, Train Loss: 7.6267, Val Loss: 7.8243\n",
            "Epoch 11/1000, Train Loss: 7.6394, Val Loss: 7.8237\n",
            "Epoch 12/1000, Train Loss: 7.6354, Val Loss: 7.8272\n",
            "Epoch 13/1000, Train Loss: 7.6480, Val Loss: 7.8289\n",
            "Epoch 14/1000, Train Loss: 7.6388, Val Loss: 7.8275\n",
            "Epoch 15/1000, Train Loss: 7.6314, Val Loss: 7.8258\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 7.8201\n",
            "Epoch 1/1000, Train Loss: 7.6696, Val Loss: 7.8342\n",
            "Epoch 2/1000, Train Loss: 7.6481, Val Loss: 7.8255\n",
            "Epoch 3/1000, Train Loss: 7.6500, Val Loss: 7.8250\n",
            "Epoch 4/1000, Train Loss: 7.6386, Val Loss: 7.8321\n",
            "Epoch 5/1000, Train Loss: 7.6422, Val Loss: 7.8338\n",
            "Epoch 6/1000, Train Loss: 7.6480, Val Loss: 7.8244\n",
            "Epoch 7/1000, Train Loss: 7.6447, Val Loss: 7.8283\n",
            "Epoch 8/1000, Train Loss: 7.6581, Val Loss: 7.8125\n",
            "Epoch 9/1000, Train Loss: 7.6494, Val Loss: 7.8255\n",
            "Epoch 10/1000, Train Loss: 7.6492, Val Loss: 7.8175\n",
            "Epoch 11/1000, Train Loss: 7.6422, Val Loss: 7.8260\n",
            "Epoch 12/1000, Train Loss: 7.6347, Val Loss: 7.8323\n",
            "Epoch 13/1000, Train Loss: 7.6433, Val Loss: 7.8169\n",
            "Epoch 14/1000, Train Loss: 7.6450, Val Loss: 7.8284\n",
            "Epoch 15/1000, Train Loss: 7.6324, Val Loss: 7.8204\n",
            "Epoch 16/1000, Train Loss: 7.6387, Val Loss: 7.8149\n",
            "Epoch 17/1000, Train Loss: 7.6417, Val Loss: 7.8188\n",
            "Epoch 18/1000, Train Loss: 7.6522, Val Loss: 7.8190\n",
            "Early stopping triggered at epoch 18. Best Val Loss: 7.8125\n",
            "Training with LR=0.005, Hidden=32, Dropout=0.0, Scale Factor=1.0\n",
            "Combination 37/81\n",
            "Epoch 1/1000, Train Loss: 5.5510, Val Loss: 5.6692\n",
            "Epoch 2/1000, Train Loss: 5.5462, Val Loss: 5.6630\n",
            "Epoch 3/1000, Train Loss: 5.5439, Val Loss: 5.6651\n",
            "Epoch 4/1000, Train Loss: 5.5460, Val Loss: 5.6670\n",
            "Epoch 5/1000, Train Loss: 5.5450, Val Loss: 5.6646\n",
            "Epoch 6/1000, Train Loss: 5.5424, Val Loss: 5.6693\n",
            "Epoch 7/1000, Train Loss: 5.5428, Val Loss: 5.6734\n",
            "Epoch 8/1000, Train Loss: 5.5440, Val Loss: 5.6711\n",
            "Epoch 9/1000, Train Loss: 5.5469, Val Loss: 5.6645\n",
            "Epoch 10/1000, Train Loss: 5.5462, Val Loss: 5.6650\n",
            "Epoch 11/1000, Train Loss: 5.5437, Val Loss: 5.6719\n",
            "Epoch 12/1000, Train Loss: 5.5400, Val Loss: 5.6815\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 5.6630\n",
            "Epoch 1/1000, Train Loss: 5.5561, Val Loss: 5.6678\n",
            "Epoch 2/1000, Train Loss: 5.5477, Val Loss: 5.6669\n",
            "Epoch 3/1000, Train Loss: 5.5456, Val Loss: 5.6726\n",
            "Epoch 4/1000, Train Loss: 5.5393, Val Loss: 5.6641\n",
            "Epoch 5/1000, Train Loss: 5.5480, Val Loss: 5.6696\n",
            "Epoch 6/1000, Train Loss: 5.5479, Val Loss: 5.6738\n",
            "Epoch 7/1000, Train Loss: 5.5434, Val Loss: 5.6655\n",
            "Epoch 8/1000, Train Loss: 5.5474, Val Loss: 5.6665\n",
            "Epoch 9/1000, Train Loss: 5.5409, Val Loss: 5.6865\n",
            "Epoch 10/1000, Train Loss: 5.5462, Val Loss: 5.6655\n",
            "Epoch 11/1000, Train Loss: 5.5475, Val Loss: 5.6702\n",
            "Epoch 12/1000, Train Loss: 5.5434, Val Loss: 5.6791\n",
            "Epoch 13/1000, Train Loss: 5.5455, Val Loss: 5.6653\n",
            "Epoch 14/1000, Train Loss: 5.5408, Val Loss: 5.6643\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 5.6641\n",
            "Epoch 1/1000, Train Loss: 5.5580, Val Loss: 5.6644\n",
            "Epoch 2/1000, Train Loss: 5.5447, Val Loss: 5.6692\n",
            "Epoch 3/1000, Train Loss: 5.5427, Val Loss: 5.6756\n",
            "Epoch 4/1000, Train Loss: 5.5466, Val Loss: 5.6653\n",
            "Epoch 5/1000, Train Loss: 5.5452, Val Loss: 5.7772\n",
            "Epoch 6/1000, Train Loss: 5.5437, Val Loss: 5.6626\n",
            "Epoch 7/1000, Train Loss: 5.5461, Val Loss: 5.6720\n",
            "Epoch 8/1000, Train Loss: 5.5477, Val Loss: 5.6671\n",
            "Epoch 9/1000, Train Loss: 5.5478, Val Loss: 5.6655\n",
            "Epoch 10/1000, Train Loss: 5.5435, Val Loss: 5.6738\n",
            "Epoch 11/1000, Train Loss: 5.5462, Val Loss: 5.6695\n",
            "Epoch 12/1000, Train Loss: 5.5454, Val Loss: 5.6657\n",
            "Epoch 13/1000, Train Loss: 5.5432, Val Loss: 5.6664\n",
            "Epoch 14/1000, Train Loss: 5.5439, Val Loss: 5.6739\n",
            "Epoch 15/1000, Train Loss: 5.5386, Val Loss: 5.6715\n",
            "Epoch 16/1000, Train Loss: 5.5437, Val Loss: 5.6693\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 5.6626\n",
            "Training with LR=0.005, Hidden=32, Dropout=0.0, Scale Factor=1.25\n",
            "Combination 38/81\n",
            "Epoch 1/1000, Train Loss: 6.6238, Val Loss: 6.7555\n",
            "Epoch 2/1000, Train Loss: 6.6037, Val Loss: 6.7639\n",
            "Epoch 3/1000, Train Loss: 6.6066, Val Loss: 6.7618\n",
            "Epoch 4/1000, Train Loss: 6.6062, Val Loss: 6.7588\n",
            "Epoch 5/1000, Train Loss: 6.6045, Val Loss: 6.7597\n",
            "Epoch 6/1000, Train Loss: 6.6026, Val Loss: 6.7560\n",
            "Epoch 7/1000, Train Loss: 6.6004, Val Loss: 6.7553\n",
            "Epoch 8/1000, Train Loss: 6.5993, Val Loss: 6.7810\n",
            "Epoch 9/1000, Train Loss: 6.5987, Val Loss: 6.7570\n",
            "Epoch 10/1000, Train Loss: 6.6029, Val Loss: 6.7597\n",
            "Epoch 11/1000, Train Loss: 6.5951, Val Loss: 6.7512\n",
            "Epoch 12/1000, Train Loss: 6.6004, Val Loss: 6.7584\n",
            "Epoch 13/1000, Train Loss: 6.6008, Val Loss: 6.7648\n",
            "Epoch 14/1000, Train Loss: 6.5979, Val Loss: 6.7583\n",
            "Epoch 15/1000, Train Loss: 6.5978, Val Loss: 6.7686\n",
            "Epoch 16/1000, Train Loss: 6.5995, Val Loss: 6.7643\n",
            "Epoch 17/1000, Train Loss: 6.6015, Val Loss: 6.7597\n",
            "Epoch 18/1000, Train Loss: 6.6014, Val Loss: 6.7591\n",
            "Epoch 19/1000, Train Loss: 6.6010, Val Loss: 6.7589\n",
            "Epoch 20/1000, Train Loss: 6.6034, Val Loss: 6.7589\n",
            "Epoch 21/1000, Train Loss: 6.6008, Val Loss: 6.7696\n",
            "Early stopping triggered at epoch 21. Best Val Loss: 6.7512\n",
            "Epoch 1/1000, Train Loss: 6.6093, Val Loss: 6.7829\n",
            "Epoch 2/1000, Train Loss: 6.6061, Val Loss: 6.7623\n",
            "Epoch 3/1000, Train Loss: 6.6062, Val Loss: 6.7617\n",
            "Epoch 4/1000, Train Loss: 6.6020, Val Loss: 6.7626\n",
            "Epoch 5/1000, Train Loss: 6.6019, Val Loss: 6.7709\n",
            "Epoch 6/1000, Train Loss: 6.5998, Val Loss: 6.7699\n",
            "Epoch 7/1000, Train Loss: 6.6074, Val Loss: 6.7689\n",
            "Epoch 8/1000, Train Loss: 6.6006, Val Loss: 6.7524\n",
            "Epoch 9/1000, Train Loss: 6.6026, Val Loss: 6.7599\n",
            "Epoch 10/1000, Train Loss: 6.6025, Val Loss: 6.7608\n",
            "Epoch 11/1000, Train Loss: 6.6024, Val Loss: 6.7587\n",
            "Epoch 12/1000, Train Loss: 6.5991, Val Loss: 6.7632\n",
            "Epoch 13/1000, Train Loss: 6.5987, Val Loss: 6.7760\n",
            "Epoch 14/1000, Train Loss: 6.6043, Val Loss: 6.7614\n",
            "Epoch 15/1000, Train Loss: 6.6000, Val Loss: 6.7559\n",
            "Epoch 16/1000, Train Loss: 6.5988, Val Loss: 6.7650\n",
            "Epoch 17/1000, Train Loss: 6.6022, Val Loss: 6.7659\n",
            "Epoch 18/1000, Train Loss: 6.5981, Val Loss: 6.7611\n",
            "Early stopping triggered at epoch 18. Best Val Loss: 6.7524\n",
            "Epoch 1/1000, Train Loss: 6.6237, Val Loss: 6.7661\n",
            "Epoch 2/1000, Train Loss: 6.6085, Val Loss: 6.7561\n",
            "Epoch 3/1000, Train Loss: 6.6080, Val Loss: 6.7531\n",
            "Epoch 4/1000, Train Loss: 6.6017, Val Loss: 6.7567\n",
            "Epoch 5/1000, Train Loss: 6.6005, Val Loss: 6.7515\n",
            "Epoch 6/1000, Train Loss: 6.6066, Val Loss: 6.7529\n",
            "Epoch 7/1000, Train Loss: 6.6027, Val Loss: 6.7599\n",
            "Epoch 8/1000, Train Loss: 6.6014, Val Loss: 6.7826\n",
            "Epoch 9/1000, Train Loss: 6.6072, Val Loss: 6.7487\n",
            "Epoch 10/1000, Train Loss: 6.6023, Val Loss: 6.7631\n",
            "Epoch 11/1000, Train Loss: 6.5996, Val Loss: 6.7760\n",
            "Epoch 12/1000, Train Loss: 6.6018, Val Loss: 6.7658\n",
            "Epoch 13/1000, Train Loss: 6.5953, Val Loss: 6.7893\n",
            "Epoch 14/1000, Train Loss: 6.5990, Val Loss: 6.7519\n",
            "Epoch 15/1000, Train Loss: 6.5978, Val Loss: 6.7594\n",
            "Epoch 16/1000, Train Loss: 6.5952, Val Loss: 6.7697\n",
            "Epoch 17/1000, Train Loss: 6.5988, Val Loss: 6.7537\n",
            "Epoch 18/1000, Train Loss: 6.5970, Val Loss: 6.7616\n",
            "Epoch 19/1000, Train Loss: 6.5977, Val Loss: 6.7858\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 6.7487\n",
            "Training with LR=0.005, Hidden=32, Dropout=0.0, Scale Factor=1.5\n",
            "Combination 39/81\n",
            "Epoch 1/1000, Train Loss: 7.6659, Val Loss: 7.8244\n",
            "Epoch 2/1000, Train Loss: 7.6511, Val Loss: 7.8234\n",
            "Epoch 3/1000, Train Loss: 7.6429, Val Loss: 7.8263\n",
            "Epoch 4/1000, Train Loss: 7.6453, Val Loss: 7.8211\n",
            "Epoch 5/1000, Train Loss: 7.6387, Val Loss: 7.8265\n",
            "Epoch 6/1000, Train Loss: 7.6386, Val Loss: 7.8195\n",
            "Epoch 7/1000, Train Loss: 7.6383, Val Loss: 7.8218\n",
            "Epoch 8/1000, Train Loss: 7.6372, Val Loss: 7.8808\n",
            "Epoch 9/1000, Train Loss: 7.6406, Val Loss: 7.8279\n",
            "Epoch 10/1000, Train Loss: 7.6390, Val Loss: 7.8274\n",
            "Epoch 11/1000, Train Loss: 7.6377, Val Loss: 7.8214\n",
            "Epoch 12/1000, Train Loss: 7.6359, Val Loss: 7.8268\n",
            "Epoch 13/1000, Train Loss: 7.6352, Val Loss: 7.8276\n",
            "Epoch 14/1000, Train Loss: 7.6393, Val Loss: 7.8245\n",
            "Epoch 15/1000, Train Loss: 7.6385, Val Loss: 7.8248\n",
            "Epoch 16/1000, Train Loss: 7.6351, Val Loss: 7.8229\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 7.8195\n",
            "Epoch 1/1000, Train Loss: 7.6543, Val Loss: 7.8200\n",
            "Epoch 2/1000, Train Loss: 7.6501, Val Loss: 7.8123\n",
            "Epoch 3/1000, Train Loss: 7.6394, Val Loss: 7.8260\n",
            "Epoch 4/1000, Train Loss: 7.6387, Val Loss: 7.8218\n",
            "Epoch 5/1000, Train Loss: 7.6387, Val Loss: 7.8244\n",
            "Epoch 6/1000, Train Loss: 7.6430, Val Loss: 7.8200\n",
            "Epoch 7/1000, Train Loss: 7.6302, Val Loss: 7.8368\n",
            "Epoch 8/1000, Train Loss: 7.6429, Val Loss: 7.8231\n",
            "Epoch 9/1000, Train Loss: 7.6397, Val Loss: 7.8403\n",
            "Epoch 10/1000, Train Loss: 7.6375, Val Loss: 7.8206\n",
            "Epoch 11/1000, Train Loss: 7.6369, Val Loss: 7.8296\n",
            "Epoch 12/1000, Train Loss: 7.6339, Val Loss: 7.8231\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 7.8123\n",
            "Epoch 1/1000, Train Loss: 7.6390, Val Loss: 7.8286\n",
            "Epoch 2/1000, Train Loss: 7.6397, Val Loss: 7.8268\n",
            "Epoch 3/1000, Train Loss: 7.6390, Val Loss: 7.8657\n",
            "Epoch 4/1000, Train Loss: 7.6428, Val Loss: 7.8312\n",
            "Epoch 5/1000, Train Loss: 7.6355, Val Loss: 7.8284\n",
            "Epoch 6/1000, Train Loss: 7.6440, Val Loss: 7.8220\n",
            "Epoch 7/1000, Train Loss: 7.6327, Val Loss: 7.8303\n",
            "Epoch 8/1000, Train Loss: 7.6365, Val Loss: 7.8347\n",
            "Epoch 9/1000, Train Loss: 7.6459, Val Loss: 7.8216\n",
            "Epoch 10/1000, Train Loss: 7.6399, Val Loss: 7.8251\n",
            "Epoch 11/1000, Train Loss: 7.6374, Val Loss: 7.8296\n",
            "Epoch 12/1000, Train Loss: 7.6371, Val Loss: 7.8270\n",
            "Epoch 13/1000, Train Loss: 7.6399, Val Loss: 7.8310\n",
            "Epoch 14/1000, Train Loss: 7.6364, Val Loss: 7.8235\n",
            "Epoch 15/1000, Train Loss: 7.6356, Val Loss: 7.8222\n",
            "Epoch 16/1000, Train Loss: 7.6393, Val Loss: 7.8230\n",
            "Epoch 17/1000, Train Loss: 7.6389, Val Loss: 7.8290\n",
            "Epoch 18/1000, Train Loss: 7.6337, Val Loss: 7.8238\n",
            "Epoch 19/1000, Train Loss: 7.6475, Val Loss: 7.8249\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 7.8216\n",
            "Training with LR=0.005, Hidden=32, Dropout=0.1, Scale Factor=1.0\n",
            "Combination 40/81\n",
            "Epoch 1/1000, Train Loss: 5.5475, Val Loss: 5.6688\n",
            "Epoch 2/1000, Train Loss: 5.5515, Val Loss: 5.6670\n",
            "Epoch 3/1000, Train Loss: 5.5430, Val Loss: 5.6672\n",
            "Epoch 4/1000, Train Loss: 5.5469, Val Loss: 5.6737\n",
            "Epoch 5/1000, Train Loss: 5.5559, Val Loss: 5.6673\n",
            "Epoch 6/1000, Train Loss: 5.5465, Val Loss: 5.6695\n",
            "Epoch 7/1000, Train Loss: 5.5476, Val Loss: 5.7109\n",
            "Epoch 8/1000, Train Loss: 5.5532, Val Loss: 5.6749\n",
            "Epoch 9/1000, Train Loss: 5.5492, Val Loss: 5.6716\n",
            "Epoch 10/1000, Train Loss: 5.5420, Val Loss: 5.6688\n",
            "Epoch 11/1000, Train Loss: 5.5455, Val Loss: 5.6736\n",
            "Epoch 12/1000, Train Loss: 5.5527, Val Loss: 5.6892\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 5.6670\n",
            "Epoch 1/1000, Train Loss: 5.5641, Val Loss: 5.6701\n",
            "Epoch 2/1000, Train Loss: 5.5478, Val Loss: 5.6871\n",
            "Epoch 3/1000, Train Loss: 5.5478, Val Loss: 5.6654\n",
            "Epoch 4/1000, Train Loss: 5.5447, Val Loss: 5.6658\n",
            "Epoch 5/1000, Train Loss: 5.5447, Val Loss: 5.6637\n",
            "Epoch 6/1000, Train Loss: 5.5430, Val Loss: 5.6686\n",
            "Epoch 7/1000, Train Loss: 5.5424, Val Loss: 5.6691\n",
            "Epoch 8/1000, Train Loss: 5.5453, Val Loss: 5.6682\n",
            "Epoch 9/1000, Train Loss: 5.5469, Val Loss: 5.6677\n",
            "Epoch 10/1000, Train Loss: 5.5439, Val Loss: 5.6713\n",
            "Epoch 11/1000, Train Loss: 5.5378, Val Loss: 5.6750\n",
            "Epoch 12/1000, Train Loss: 5.5473, Val Loss: 5.6684\n",
            "Epoch 13/1000, Train Loss: 5.5423, Val Loss: 5.6676\n",
            "Epoch 14/1000, Train Loss: 5.5404, Val Loss: 5.6934\n",
            "Epoch 15/1000, Train Loss: 5.5436, Val Loss: 5.6698\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 5.6637\n",
            "Epoch 1/1000, Train Loss: 5.5577, Val Loss: 5.6641\n",
            "Epoch 2/1000, Train Loss: 5.5425, Val Loss: 5.6961\n",
            "Epoch 3/1000, Train Loss: 5.5447, Val Loss: 5.6778\n",
            "Epoch 4/1000, Train Loss: 5.5450, Val Loss: 5.6678\n",
            "Epoch 5/1000, Train Loss: 5.5449, Val Loss: 5.6806\n",
            "Epoch 6/1000, Train Loss: 5.5494, Val Loss: 5.6621\n",
            "Epoch 7/1000, Train Loss: 5.5489, Val Loss: 5.6649\n",
            "Epoch 8/1000, Train Loss: 5.5455, Val Loss: 5.6644\n",
            "Epoch 9/1000, Train Loss: 5.5435, Val Loss: 5.6665\n",
            "Epoch 10/1000, Train Loss: 5.5493, Val Loss: 5.6741\n",
            "Epoch 11/1000, Train Loss: 5.5433, Val Loss: 5.6672\n",
            "Epoch 12/1000, Train Loss: 5.5469, Val Loss: 5.6660\n",
            "Epoch 13/1000, Train Loss: 5.5438, Val Loss: 5.6718\n",
            "Epoch 14/1000, Train Loss: 5.5425, Val Loss: 5.6685\n",
            "Epoch 15/1000, Train Loss: 5.5421, Val Loss: 5.6745\n",
            "Epoch 16/1000, Train Loss: 5.5463, Val Loss: 5.6705\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 5.6621\n",
            "Training with LR=0.005, Hidden=32, Dropout=0.1, Scale Factor=1.25\n",
            "Combination 41/81\n",
            "Epoch 1/1000, Train Loss: 6.6259, Val Loss: 6.7534\n",
            "Epoch 2/1000, Train Loss: 6.6040, Val Loss: 6.7478\n",
            "Epoch 3/1000, Train Loss: 6.6130, Val Loss: 6.7556\n",
            "Epoch 4/1000, Train Loss: 6.6077, Val Loss: 6.7585\n",
            "Epoch 5/1000, Train Loss: 6.6035, Val Loss: 6.7566\n",
            "Epoch 6/1000, Train Loss: 6.6102, Val Loss: 6.7665\n",
            "Epoch 7/1000, Train Loss: 6.6069, Val Loss: 6.7704\n",
            "Epoch 8/1000, Train Loss: 6.6080, Val Loss: 6.7487\n",
            "Epoch 9/1000, Train Loss: 6.6030, Val Loss: 6.7506\n",
            "Epoch 10/1000, Train Loss: 6.6029, Val Loss: 6.7531\n",
            "Epoch 11/1000, Train Loss: 6.6065, Val Loss: 6.7606\n",
            "Epoch 12/1000, Train Loss: 6.6057, Val Loss: 6.7520\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 6.7478\n",
            "Epoch 1/1000, Train Loss: 6.6341, Val Loss: 6.7722\n",
            "Epoch 2/1000, Train Loss: 6.6157, Val Loss: 6.7733\n",
            "Epoch 3/1000, Train Loss: 6.6082, Val Loss: 6.7970\n",
            "Epoch 4/1000, Train Loss: 6.6023, Val Loss: 6.7636\n",
            "Epoch 5/1000, Train Loss: 6.6100, Val Loss: 6.7638\n",
            "Epoch 6/1000, Train Loss: 6.6061, Val Loss: 6.7485\n",
            "Epoch 7/1000, Train Loss: 6.6080, Val Loss: 6.7642\n",
            "Epoch 8/1000, Train Loss: 6.6109, Val Loss: 6.7519\n",
            "Epoch 9/1000, Train Loss: 6.6100, Val Loss: 6.7705\n",
            "Epoch 10/1000, Train Loss: 6.6081, Val Loss: 6.7732\n",
            "Epoch 11/1000, Train Loss: 6.6001, Val Loss: 6.7550\n",
            "Epoch 12/1000, Train Loss: 6.6007, Val Loss: 6.7714\n",
            "Epoch 13/1000, Train Loss: 6.6058, Val Loss: 6.7607\n",
            "Epoch 14/1000, Train Loss: 6.5989, Val Loss: 6.7497\n",
            "Epoch 15/1000, Train Loss: 6.6066, Val Loss: 6.7544\n",
            "Epoch 16/1000, Train Loss: 6.6041, Val Loss: 6.7720\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 6.7485\n",
            "Epoch 1/1000, Train Loss: 6.6295, Val Loss: 6.7547\n",
            "Epoch 2/1000, Train Loss: 6.6144, Val Loss: 6.7692\n",
            "Epoch 3/1000, Train Loss: 6.6088, Val Loss: 6.7571\n",
            "Epoch 4/1000, Train Loss: 6.6039, Val Loss: 6.7490\n",
            "Epoch 5/1000, Train Loss: 6.6079, Val Loss: 6.7486\n",
            "Epoch 6/1000, Train Loss: 6.6063, Val Loss: 6.7593\n",
            "Epoch 7/1000, Train Loss: 6.6032, Val Loss: 6.7594\n",
            "Epoch 8/1000, Train Loss: 6.6026, Val Loss: 6.8229\n",
            "Epoch 9/1000, Train Loss: 6.6084, Val Loss: 6.7497\n",
            "Epoch 10/1000, Train Loss: 6.6060, Val Loss: 6.7512\n",
            "Epoch 11/1000, Train Loss: 6.6084, Val Loss: 6.7528\n",
            "Epoch 12/1000, Train Loss: 6.6011, Val Loss: 6.7562\n",
            "Epoch 13/1000, Train Loss: 6.6009, Val Loss: 6.7562\n",
            "Epoch 14/1000, Train Loss: 6.6024, Val Loss: 6.7562\n",
            "Epoch 15/1000, Train Loss: 6.6042, Val Loss: 6.7585\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 6.7486\n",
            "Training with LR=0.005, Hidden=32, Dropout=0.1, Scale Factor=1.5\n",
            "Combination 42/81\n",
            "Epoch 1/1000, Train Loss: 7.6563, Val Loss: 7.8331\n",
            "Epoch 2/1000, Train Loss: 7.6360, Val Loss: 7.8272\n",
            "Epoch 3/1000, Train Loss: 7.6498, Val Loss: 7.8252\n",
            "Epoch 4/1000, Train Loss: 7.6394, Val Loss: 7.8245\n",
            "Epoch 5/1000, Train Loss: 7.6349, Val Loss: 7.8486\n",
            "Epoch 6/1000, Train Loss: 7.6424, Val Loss: 7.8284\n",
            "Epoch 7/1000, Train Loss: 7.6436, Val Loss: 7.8322\n",
            "Epoch 8/1000, Train Loss: 7.6495, Val Loss: 7.8374\n",
            "Epoch 9/1000, Train Loss: 7.6401, Val Loss: 7.8263\n",
            "Epoch 10/1000, Train Loss: 7.6440, Val Loss: 7.8368\n",
            "Epoch 11/1000, Train Loss: 7.6419, Val Loss: 7.8239\n",
            "Epoch 12/1000, Train Loss: 7.6432, Val Loss: 7.8269\n",
            "Epoch 13/1000, Train Loss: 7.6390, Val Loss: 7.8317\n",
            "Epoch 14/1000, Train Loss: 7.6364, Val Loss: 7.8337\n",
            "Epoch 15/1000, Train Loss: 7.6407, Val Loss: 7.8328\n",
            "Epoch 16/1000, Train Loss: 7.6428, Val Loss: 7.8344\n",
            "Epoch 17/1000, Train Loss: 7.6394, Val Loss: 7.8296\n",
            "Epoch 18/1000, Train Loss: 7.6333, Val Loss: 7.8294\n",
            "Epoch 19/1000, Train Loss: 7.6441, Val Loss: 7.8224\n",
            "Epoch 20/1000, Train Loss: 7.6371, Val Loss: 7.8273\n",
            "Epoch 21/1000, Train Loss: 7.6471, Val Loss: 7.8245\n",
            "Epoch 22/1000, Train Loss: 7.6387, Val Loss: 7.8232\n",
            "Epoch 23/1000, Train Loss: 7.6389, Val Loss: 7.8576\n",
            "Epoch 24/1000, Train Loss: 7.6424, Val Loss: 7.8205\n",
            "Epoch 25/1000, Train Loss: 7.6479, Val Loss: 7.8221\n",
            "Epoch 26/1000, Train Loss: 7.6456, Val Loss: 7.8285\n",
            "Epoch 27/1000, Train Loss: 7.6459, Val Loss: 7.8322\n",
            "Epoch 28/1000, Train Loss: 7.6476, Val Loss: 7.8244\n",
            "Epoch 29/1000, Train Loss: 7.6363, Val Loss: 7.8253\n",
            "Epoch 30/1000, Train Loss: 7.6397, Val Loss: 7.8284\n",
            "Epoch 31/1000, Train Loss: 7.6370, Val Loss: 7.8421\n",
            "Epoch 32/1000, Train Loss: 7.6435, Val Loss: 7.8281\n",
            "Epoch 33/1000, Train Loss: 7.6400, Val Loss: 7.8269\n",
            "Epoch 34/1000, Train Loss: 7.6326, Val Loss: 7.8237\n",
            "Early stopping triggered at epoch 34. Best Val Loss: 7.8205\n",
            "Epoch 1/1000, Train Loss: 7.6650, Val Loss: 7.8274\n",
            "Epoch 2/1000, Train Loss: 7.6435, Val Loss: 7.8215\n",
            "Epoch 3/1000, Train Loss: 7.6439, Val Loss: 7.8257\n",
            "Epoch 4/1000, Train Loss: 7.6443, Val Loss: 7.8260\n",
            "Epoch 5/1000, Train Loss: 7.6427, Val Loss: 7.8221\n",
            "Epoch 6/1000, Train Loss: 7.6527, Val Loss: 7.8201\n",
            "Epoch 7/1000, Train Loss: 7.6461, Val Loss: 7.8240\n",
            "Epoch 8/1000, Train Loss: 7.6364, Val Loss: 7.8265\n",
            "Epoch 9/1000, Train Loss: 7.6382, Val Loss: 7.8237\n",
            "Epoch 10/1000, Train Loss: 7.6546, Val Loss: 7.8234\n",
            "Epoch 11/1000, Train Loss: 7.6472, Val Loss: 7.8181\n",
            "Epoch 12/1000, Train Loss: 7.6442, Val Loss: 7.8243\n",
            "Epoch 13/1000, Train Loss: 7.6441, Val Loss: 7.8223\n",
            "Epoch 14/1000, Train Loss: 7.6425, Val Loss: 7.8236\n",
            "Epoch 15/1000, Train Loss: 7.6424, Val Loss: 7.8271\n",
            "Epoch 16/1000, Train Loss: 7.6465, Val Loss: 7.8251\n",
            "Epoch 17/1000, Train Loss: 7.6467, Val Loss: 7.8228\n",
            "Epoch 18/1000, Train Loss: 7.6357, Val Loss: 7.8237\n",
            "Epoch 19/1000, Train Loss: 7.6476, Val Loss: 7.8200\n",
            "Epoch 20/1000, Train Loss: 7.6358, Val Loss: 7.8188\n",
            "Epoch 21/1000, Train Loss: 7.6374, Val Loss: 7.8298\n",
            "Early stopping triggered at epoch 21. Best Val Loss: 7.8181\n",
            "Epoch 1/1000, Train Loss: 7.6721, Val Loss: 7.8218\n",
            "Epoch 2/1000, Train Loss: 7.6434, Val Loss: 7.8209\n",
            "Epoch 3/1000, Train Loss: 7.6500, Val Loss: 7.8271\n",
            "Epoch 4/1000, Train Loss: 7.6399, Val Loss: 7.8254\n",
            "Epoch 5/1000, Train Loss: 7.6382, Val Loss: 7.8261\n",
            "Epoch 6/1000, Train Loss: 7.6458, Val Loss: 7.8287\n",
            "Epoch 7/1000, Train Loss: 7.6430, Val Loss: 7.8143\n",
            "Epoch 8/1000, Train Loss: 7.6486, Val Loss: 7.8301\n",
            "Epoch 9/1000, Train Loss: 7.6567, Val Loss: 7.8243\n",
            "Epoch 10/1000, Train Loss: 7.6424, Val Loss: 7.8304\n",
            "Epoch 11/1000, Train Loss: 7.6433, Val Loss: 7.8297\n",
            "Epoch 12/1000, Train Loss: 7.6448, Val Loss: 7.8296\n",
            "Epoch 13/1000, Train Loss: 7.6480, Val Loss: 7.8349\n",
            "Epoch 14/1000, Train Loss: 7.6472, Val Loss: 7.8225\n",
            "Epoch 15/1000, Train Loss: 7.6451, Val Loss: 7.8249\n",
            "Epoch 16/1000, Train Loss: 7.6455, Val Loss: 7.8284\n",
            "Epoch 17/1000, Train Loss: 7.6400, Val Loss: 7.8251\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 7.8143\n",
            "Training with LR=0.005, Hidden=32, Dropout=0.2, Scale Factor=1.0\n",
            "Combination 43/81\n",
            "Epoch 1/1000, Train Loss: 5.5626, Val Loss: 5.6801\n",
            "Epoch 2/1000, Train Loss: 5.5570, Val Loss: 5.6680\n",
            "Epoch 3/1000, Train Loss: 5.5496, Val Loss: 5.6653\n",
            "Epoch 4/1000, Train Loss: 5.5467, Val Loss: 5.6638\n",
            "Epoch 5/1000, Train Loss: 5.5506, Val Loss: 5.6688\n",
            "Epoch 6/1000, Train Loss: 5.5547, Val Loss: 5.6710\n",
            "Epoch 7/1000, Train Loss: 5.5429, Val Loss: 5.6729\n",
            "Epoch 8/1000, Train Loss: 5.5521, Val Loss: 5.6693\n",
            "Epoch 9/1000, Train Loss: 5.5448, Val Loss: 5.6686\n",
            "Epoch 10/1000, Train Loss: 5.5445, Val Loss: 5.6721\n",
            "Epoch 11/1000, Train Loss: 5.5448, Val Loss: 5.7150\n",
            "Epoch 12/1000, Train Loss: 5.5521, Val Loss: 5.6620\n",
            "Epoch 13/1000, Train Loss: 5.5492, Val Loss: 5.6708\n",
            "Epoch 14/1000, Train Loss: 5.5434, Val Loss: 5.6687\n",
            "Epoch 15/1000, Train Loss: 5.5521, Val Loss: 5.6670\n",
            "Epoch 16/1000, Train Loss: 5.5480, Val Loss: 5.6835\n",
            "Epoch 17/1000, Train Loss: 5.5473, Val Loss: 5.6621\n",
            "Epoch 18/1000, Train Loss: 5.5498, Val Loss: 5.6665\n",
            "Epoch 19/1000, Train Loss: 5.5476, Val Loss: 5.6626\n",
            "Epoch 20/1000, Train Loss: 5.5409, Val Loss: 5.6665\n",
            "Epoch 21/1000, Train Loss: 5.5451, Val Loss: 5.6712\n",
            "Epoch 22/1000, Train Loss: 5.5515, Val Loss: 5.6630\n",
            "Early stopping triggered at epoch 22. Best Val Loss: 5.6620\n",
            "Epoch 1/1000, Train Loss: 5.5733, Val Loss: 5.6901\n",
            "Epoch 2/1000, Train Loss: 5.5559, Val Loss: 5.6852\n",
            "Epoch 3/1000, Train Loss: 5.5531, Val Loss: 5.6679\n",
            "Epoch 4/1000, Train Loss: 5.5540, Val Loss: 5.6766\n",
            "Epoch 5/1000, Train Loss: 5.5560, Val Loss: 5.6714\n",
            "Epoch 6/1000, Train Loss: 5.5512, Val Loss: 5.6676\n",
            "Epoch 7/1000, Train Loss: 5.5521, Val Loss: 5.6673\n",
            "Epoch 8/1000, Train Loss: 5.5491, Val Loss: 5.6806\n",
            "Epoch 9/1000, Train Loss: 5.5555, Val Loss: 5.6739\n",
            "Epoch 10/1000, Train Loss: 5.5484, Val Loss: 5.6650\n",
            "Epoch 11/1000, Train Loss: 5.5532, Val Loss: 5.6986\n",
            "Epoch 12/1000, Train Loss: 5.5510, Val Loss: 5.6942\n",
            "Epoch 13/1000, Train Loss: 5.5560, Val Loss: 5.6954\n",
            "Epoch 14/1000, Train Loss: 5.5499, Val Loss: 5.6629\n",
            "Epoch 15/1000, Train Loss: 5.5561, Val Loss: 5.6792\n",
            "Epoch 16/1000, Train Loss: 5.5501, Val Loss: 5.6864\n",
            "Epoch 17/1000, Train Loss: 5.5492, Val Loss: 5.6703\n",
            "Epoch 18/1000, Train Loss: 5.5412, Val Loss: 5.6615\n",
            "Epoch 19/1000, Train Loss: 5.5528, Val Loss: 5.6605\n",
            "Epoch 20/1000, Train Loss: 5.5488, Val Loss: 5.6692\n",
            "Epoch 21/1000, Train Loss: 5.5423, Val Loss: 5.6634\n",
            "Epoch 22/1000, Train Loss: 5.5441, Val Loss: 5.6728\n",
            "Epoch 23/1000, Train Loss: 5.5463, Val Loss: 5.6636\n",
            "Epoch 24/1000, Train Loss: 5.5495, Val Loss: 5.6655\n",
            "Epoch 25/1000, Train Loss: 5.5488, Val Loss: 5.6630\n",
            "Epoch 26/1000, Train Loss: 5.5450, Val Loss: 5.6739\n",
            "Epoch 27/1000, Train Loss: 5.5432, Val Loss: 5.6713\n",
            "Epoch 28/1000, Train Loss: 5.5429, Val Loss: 5.6717\n",
            "Epoch 29/1000, Train Loss: 5.5440, Val Loss: 5.6971\n",
            "Early stopping triggered at epoch 29. Best Val Loss: 5.6605\n",
            "Epoch 1/1000, Train Loss: 5.5622, Val Loss: 5.6809\n",
            "Epoch 2/1000, Train Loss: 5.5501, Val Loss: 5.6715\n",
            "Epoch 3/1000, Train Loss: 5.5494, Val Loss: 5.6688\n",
            "Epoch 4/1000, Train Loss: 5.5536, Val Loss: 5.6687\n",
            "Epoch 5/1000, Train Loss: 5.5488, Val Loss: 5.6667\n",
            "Epoch 6/1000, Train Loss: 5.5518, Val Loss: 5.6642\n",
            "Epoch 7/1000, Train Loss: 5.5512, Val Loss: 5.6764\n",
            "Epoch 8/1000, Train Loss: 5.5550, Val Loss: 5.6725\n",
            "Epoch 9/1000, Train Loss: 5.5471, Val Loss: 5.6833\n",
            "Epoch 10/1000, Train Loss: 5.5512, Val Loss: 5.6834\n",
            "Epoch 11/1000, Train Loss: 5.5498, Val Loss: 5.6756\n",
            "Epoch 12/1000, Train Loss: 5.5477, Val Loss: 5.6821\n",
            "Epoch 13/1000, Train Loss: 5.5512, Val Loss: 5.6893\n",
            "Epoch 14/1000, Train Loss: 5.5503, Val Loss: 5.6752\n",
            "Epoch 15/1000, Train Loss: 5.5534, Val Loss: 5.6644\n",
            "Epoch 16/1000, Train Loss: 5.5500, Val Loss: 5.6708\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 5.6642\n",
            "Training with LR=0.005, Hidden=32, Dropout=0.2, Scale Factor=1.25\n",
            "Combination 44/81\n",
            "Epoch 1/1000, Train Loss: 6.6431, Val Loss: 6.7569\n",
            "Epoch 2/1000, Train Loss: 6.6062, Val Loss: 6.7553\n",
            "Epoch 3/1000, Train Loss: 6.6088, Val Loss: 6.7634\n",
            "Epoch 4/1000, Train Loss: 6.6016, Val Loss: 6.7507\n",
            "Epoch 5/1000, Train Loss: 6.6038, Val Loss: 6.7633\n",
            "Epoch 6/1000, Train Loss: 6.5991, Val Loss: 6.7549\n",
            "Epoch 7/1000, Train Loss: 6.6160, Val Loss: 6.7522\n",
            "Epoch 8/1000, Train Loss: 6.6069, Val Loss: 6.7546\n",
            "Epoch 9/1000, Train Loss: 6.6066, Val Loss: 6.7657\n",
            "Epoch 10/1000, Train Loss: 6.6057, Val Loss: 6.7562\n",
            "Epoch 11/1000, Train Loss: 6.6072, Val Loss: 6.7557\n",
            "Epoch 12/1000, Train Loss: 6.6077, Val Loss: 6.7533\n",
            "Epoch 13/1000, Train Loss: 6.6074, Val Loss: 6.7534\n",
            "Epoch 14/1000, Train Loss: 6.6053, Val Loss: 6.7503\n",
            "Epoch 15/1000, Train Loss: 6.6067, Val Loss: 6.7558\n",
            "Epoch 16/1000, Train Loss: 6.6058, Val Loss: 6.7615\n",
            "Epoch 17/1000, Train Loss: 6.6026, Val Loss: 6.7525\n",
            "Epoch 18/1000, Train Loss: 6.6110, Val Loss: 6.7584\n",
            "Epoch 19/1000, Train Loss: 6.5988, Val Loss: 6.7601\n",
            "Epoch 20/1000, Train Loss: 6.6069, Val Loss: 6.7550\n",
            "Epoch 21/1000, Train Loss: 6.6021, Val Loss: 6.7558\n",
            "Epoch 22/1000, Train Loss: 6.6124, Val Loss: 6.7562\n",
            "Epoch 23/1000, Train Loss: 6.6062, Val Loss: 6.7558\n",
            "Epoch 24/1000, Train Loss: 6.6046, Val Loss: 6.7656\n",
            "Early stopping triggered at epoch 24. Best Val Loss: 6.7503\n",
            "Epoch 1/1000, Train Loss: 6.6442, Val Loss: 6.7624\n",
            "Epoch 2/1000, Train Loss: 6.6088, Val Loss: 6.7661\n",
            "Epoch 3/1000, Train Loss: 6.6090, Val Loss: 6.7560\n",
            "Epoch 4/1000, Train Loss: 6.6087, Val Loss: 6.7658\n",
            "Epoch 5/1000, Train Loss: 6.6067, Val Loss: 6.8020\n",
            "Epoch 6/1000, Train Loss: 6.6126, Val Loss: 6.7881\n",
            "Epoch 7/1000, Train Loss: 6.6116, Val Loss: 6.7738\n",
            "Epoch 8/1000, Train Loss: 6.6121, Val Loss: 6.7662\n",
            "Epoch 9/1000, Train Loss: 6.5989, Val Loss: 6.7765\n",
            "Epoch 10/1000, Train Loss: 6.6109, Val Loss: 6.7769\n",
            "Epoch 11/1000, Train Loss: 6.6037, Val Loss: 6.7564\n",
            "Epoch 12/1000, Train Loss: 6.6090, Val Loss: 6.7595\n",
            "Epoch 13/1000, Train Loss: 6.6068, Val Loss: 6.7649\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 6.7560\n",
            "Epoch 1/1000, Train Loss: 6.6241, Val Loss: 6.7504\n",
            "Epoch 2/1000, Train Loss: 6.6055, Val Loss: 6.7554\n",
            "Epoch 3/1000, Train Loss: 6.6091, Val Loss: 6.7495\n",
            "Epoch 4/1000, Train Loss: 6.6183, Val Loss: 6.7515\n",
            "Epoch 5/1000, Train Loss: 6.6099, Val Loss: 6.7631\n",
            "Epoch 6/1000, Train Loss: 6.6070, Val Loss: 6.7720\n",
            "Epoch 7/1000, Train Loss: 6.6119, Val Loss: 6.7686\n",
            "Epoch 8/1000, Train Loss: 6.6060, Val Loss: 6.7581\n",
            "Epoch 9/1000, Train Loss: 6.6105, Val Loss: 6.7694\n",
            "Epoch 10/1000, Train Loss: 6.6100, Val Loss: 6.7634\n",
            "Epoch 11/1000, Train Loss: 6.6064, Val Loss: 6.7550\n",
            "Epoch 12/1000, Train Loss: 6.6052, Val Loss: 6.7503\n",
            "Epoch 13/1000, Train Loss: 6.6032, Val Loss: 6.7941\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 6.7495\n",
            "Training with LR=0.005, Hidden=32, Dropout=0.2, Scale Factor=1.5\n",
            "Combination 45/81\n",
            "Epoch 1/1000, Train Loss: 7.6673, Val Loss: 7.8302\n",
            "Epoch 2/1000, Train Loss: 7.6491, Val Loss: 7.8279\n",
            "Epoch 3/1000, Train Loss: 7.6438, Val Loss: 7.8365\n",
            "Epoch 4/1000, Train Loss: 7.6506, Val Loss: 7.8234\n",
            "Epoch 5/1000, Train Loss: 7.6454, Val Loss: 7.8364\n",
            "Epoch 6/1000, Train Loss: 7.6444, Val Loss: 7.8323\n",
            "Epoch 7/1000, Train Loss: 7.6433, Val Loss: 7.8333\n",
            "Epoch 8/1000, Train Loss: 7.6481, Val Loss: 7.8338\n",
            "Epoch 9/1000, Train Loss: 7.6475, Val Loss: 7.8284\n",
            "Epoch 10/1000, Train Loss: 7.6387, Val Loss: 7.8261\n",
            "Epoch 11/1000, Train Loss: 7.6473, Val Loss: 7.8520\n",
            "Epoch 12/1000, Train Loss: 7.6454, Val Loss: 7.8227\n",
            "Epoch 13/1000, Train Loss: 7.6393, Val Loss: 7.8458\n",
            "Epoch 14/1000, Train Loss: 7.6477, Val Loss: 7.8259\n",
            "Epoch 15/1000, Train Loss: 7.6416, Val Loss: 7.8219\n",
            "Epoch 16/1000, Train Loss: 7.6355, Val Loss: 7.8238\n",
            "Epoch 17/1000, Train Loss: 7.6360, Val Loss: 7.8304\n",
            "Epoch 18/1000, Train Loss: 7.6471, Val Loss: 7.8272\n",
            "Epoch 19/1000, Train Loss: 7.6393, Val Loss: 7.8255\n",
            "Epoch 20/1000, Train Loss: 7.6434, Val Loss: 7.8456\n",
            "Epoch 21/1000, Train Loss: 7.6451, Val Loss: 7.8173\n",
            "Epoch 22/1000, Train Loss: 7.6377, Val Loss: 7.8153\n",
            "Epoch 23/1000, Train Loss: 7.6435, Val Loss: 7.8220\n",
            "Epoch 24/1000, Train Loss: 7.6349, Val Loss: 7.8273\n",
            "Epoch 25/1000, Train Loss: 7.6497, Val Loss: 7.8274\n",
            "Epoch 26/1000, Train Loss: 7.6408, Val Loss: 7.8391\n",
            "Epoch 27/1000, Train Loss: 7.6384, Val Loss: 7.8274\n",
            "Epoch 28/1000, Train Loss: 7.6369, Val Loss: 7.8297\n",
            "Epoch 29/1000, Train Loss: 7.6361, Val Loss: 7.8206\n",
            "Epoch 30/1000, Train Loss: 7.6388, Val Loss: 7.8280\n",
            "Epoch 31/1000, Train Loss: 7.6465, Val Loss: 7.8272\n",
            "Epoch 32/1000, Train Loss: 7.6452, Val Loss: 7.8196\n",
            "Early stopping triggered at epoch 32. Best Val Loss: 7.8153\n",
            "Epoch 1/1000, Train Loss: 7.6697, Val Loss: 7.8301\n",
            "Epoch 2/1000, Train Loss: 7.6470, Val Loss: 7.8259\n",
            "Epoch 3/1000, Train Loss: 7.6368, Val Loss: 7.8357\n",
            "Epoch 4/1000, Train Loss: 7.6408, Val Loss: 7.8227\n",
            "Epoch 5/1000, Train Loss: 7.6414, Val Loss: 7.8227\n",
            "Epoch 6/1000, Train Loss: 7.6403, Val Loss: 7.8335\n",
            "Epoch 7/1000, Train Loss: 7.6423, Val Loss: 7.8288\n",
            "Epoch 8/1000, Train Loss: 7.6396, Val Loss: 7.8311\n",
            "Epoch 9/1000, Train Loss: 7.6424, Val Loss: 7.8326\n",
            "Epoch 10/1000, Train Loss: 7.6396, Val Loss: 7.8326\n",
            "Epoch 11/1000, Train Loss: 7.6470, Val Loss: 7.8233\n",
            "Epoch 12/1000, Train Loss: 7.6432, Val Loss: 7.8326\n",
            "Epoch 13/1000, Train Loss: 7.6405, Val Loss: 7.8205\n",
            "Epoch 14/1000, Train Loss: 7.6427, Val Loss: 7.8230\n",
            "Epoch 15/1000, Train Loss: 7.6397, Val Loss: 7.8249\n",
            "Epoch 16/1000, Train Loss: 7.6375, Val Loss: 7.8188\n",
            "Epoch 17/1000, Train Loss: 7.6432, Val Loss: 7.8286\n",
            "Epoch 18/1000, Train Loss: 7.6397, Val Loss: 7.8307\n",
            "Epoch 19/1000, Train Loss: 7.6526, Val Loss: 7.8208\n",
            "Epoch 20/1000, Train Loss: 7.6350, Val Loss: 7.8193\n",
            "Epoch 21/1000, Train Loss: 7.6301, Val Loss: 7.8253\n",
            "Epoch 22/1000, Train Loss: 7.6482, Val Loss: 7.8213\n",
            "Epoch 23/1000, Train Loss: 7.6477, Val Loss: 7.8304\n",
            "Epoch 24/1000, Train Loss: 7.6401, Val Loss: 7.8286\n",
            "Epoch 25/1000, Train Loss: 7.6455, Val Loss: 7.8305\n",
            "Epoch 26/1000, Train Loss: 7.6329, Val Loss: 7.8210\n",
            "Early stopping triggered at epoch 26. Best Val Loss: 7.8188\n",
            "Epoch 1/1000, Train Loss: 7.6631, Val Loss: 7.8143\n",
            "Epoch 2/1000, Train Loss: 7.6478, Val Loss: 7.8132\n",
            "Epoch 3/1000, Train Loss: 7.6502, Val Loss: 7.8267\n",
            "Epoch 4/1000, Train Loss: 7.6630, Val Loss: 7.8231\n",
            "Epoch 5/1000, Train Loss: 7.6450, Val Loss: 7.8278\n",
            "Epoch 6/1000, Train Loss: 7.6432, Val Loss: 7.8246\n",
            "Epoch 7/1000, Train Loss: 7.6385, Val Loss: 7.8314\n",
            "Epoch 8/1000, Train Loss: 7.6463, Val Loss: 7.8174\n",
            "Epoch 9/1000, Train Loss: 7.6593, Val Loss: 7.8167\n",
            "Epoch 10/1000, Train Loss: 7.6400, Val Loss: 7.8231\n",
            "Epoch 11/1000, Train Loss: 7.6423, Val Loss: 7.8228\n",
            "Epoch 12/1000, Train Loss: 7.6468, Val Loss: 7.8728\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 7.8132\n",
            "Training with LR=0.005, Hidden=64, Dropout=0.0, Scale Factor=1.0\n",
            "Combination 46/81\n",
            "Epoch 1/1000, Train Loss: 5.5736, Val Loss: 5.6905\n",
            "Epoch 2/1000, Train Loss: 5.5496, Val Loss: 5.6799\n",
            "Epoch 3/1000, Train Loss: 5.5443, Val Loss: 5.6990\n",
            "Epoch 4/1000, Train Loss: 5.5465, Val Loss: 5.6702\n",
            "Epoch 5/1000, Train Loss: 5.5490, Val Loss: 5.6663\n",
            "Epoch 6/1000, Train Loss: 5.5461, Val Loss: 5.6708\n",
            "Epoch 7/1000, Train Loss: 5.5437, Val Loss: 5.6950\n",
            "Epoch 8/1000, Train Loss: 5.5490, Val Loss: 5.6749\n",
            "Epoch 9/1000, Train Loss: 5.5474, Val Loss: 5.6702\n",
            "Epoch 10/1000, Train Loss: 5.5429, Val Loss: 5.6669\n",
            "Epoch 11/1000, Train Loss: 5.5477, Val Loss: 5.6628\n",
            "Epoch 12/1000, Train Loss: 5.5424, Val Loss: 5.6661\n",
            "Epoch 13/1000, Train Loss: 5.5483, Val Loss: 5.6652\n",
            "Epoch 14/1000, Train Loss: 5.5494, Val Loss: 5.6843\n",
            "Epoch 15/1000, Train Loss: 5.5509, Val Loss: 5.6748\n",
            "Epoch 16/1000, Train Loss: 5.5417, Val Loss: 5.7054\n",
            "Epoch 17/1000, Train Loss: 5.5482, Val Loss: 5.6991\n",
            "Epoch 18/1000, Train Loss: 5.5440, Val Loss: 5.6723\n",
            "Epoch 19/1000, Train Loss: 5.5475, Val Loss: 5.6982\n",
            "Epoch 20/1000, Train Loss: 5.5469, Val Loss: 5.6747\n",
            "Epoch 21/1000, Train Loss: 5.5487, Val Loss: 5.6658\n",
            "Early stopping triggered at epoch 21. Best Val Loss: 5.6628\n",
            "Epoch 1/1000, Train Loss: 5.5555, Val Loss: 5.6742\n",
            "Epoch 2/1000, Train Loss: 5.5470, Val Loss: 5.6700\n",
            "Epoch 3/1000, Train Loss: 5.5455, Val Loss: 5.6737\n",
            "Epoch 4/1000, Train Loss: 5.5443, Val Loss: 5.6659\n",
            "Epoch 5/1000, Train Loss: 5.5457, Val Loss: 5.6666\n",
            "Epoch 6/1000, Train Loss: 5.5506, Val Loss: 5.6650\n",
            "Epoch 7/1000, Train Loss: 5.5450, Val Loss: 5.6699\n",
            "Epoch 8/1000, Train Loss: 5.5470, Val Loss: 5.6824\n",
            "Epoch 9/1000, Train Loss: 5.5410, Val Loss: 5.6652\n",
            "Epoch 10/1000, Train Loss: 5.5439, Val Loss: 5.6743\n",
            "Epoch 11/1000, Train Loss: 5.5415, Val Loss: 5.6659\n",
            "Epoch 12/1000, Train Loss: 5.5442, Val Loss: 5.6805\n",
            "Epoch 13/1000, Train Loss: 5.5431, Val Loss: 5.6697\n",
            "Epoch 14/1000, Train Loss: 5.5461, Val Loss: 5.6672\n",
            "Epoch 15/1000, Train Loss: 5.5409, Val Loss: 5.6671\n",
            "Epoch 16/1000, Train Loss: 5.5400, Val Loss: 5.7215\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 5.6650\n",
            "Epoch 1/1000, Train Loss: 5.5678, Val Loss: 5.6803\n",
            "Epoch 2/1000, Train Loss: 5.5526, Val Loss: 5.6703\n",
            "Epoch 3/1000, Train Loss: 5.5474, Val Loss: 5.6710\n",
            "Epoch 4/1000, Train Loss: 5.5503, Val Loss: 5.6806\n",
            "Epoch 5/1000, Train Loss: 5.5538, Val Loss: 5.6768\n",
            "Epoch 6/1000, Train Loss: 5.5425, Val Loss: 5.7151\n",
            "Epoch 7/1000, Train Loss: 5.5462, Val Loss: 5.6755\n",
            "Epoch 8/1000, Train Loss: 5.5424, Val Loss: 5.6672\n",
            "Epoch 9/1000, Train Loss: 5.5448, Val Loss: 5.6703\n",
            "Epoch 10/1000, Train Loss: 5.5444, Val Loss: 5.6724\n",
            "Epoch 11/1000, Train Loss: 5.5471, Val Loss: 5.6671\n",
            "Epoch 12/1000, Train Loss: 5.5398, Val Loss: 5.6677\n",
            "Epoch 13/1000, Train Loss: 5.5470, Val Loss: 5.6846\n",
            "Epoch 14/1000, Train Loss: 5.5483, Val Loss: 5.6644\n",
            "Epoch 15/1000, Train Loss: 5.5439, Val Loss: 5.6697\n",
            "Epoch 16/1000, Train Loss: 5.5516, Val Loss: 5.6748\n",
            "Epoch 17/1000, Train Loss: 5.5519, Val Loss: 5.7481\n",
            "Epoch 18/1000, Train Loss: 5.5482, Val Loss: 5.6617\n",
            "Epoch 19/1000, Train Loss: 5.5514, Val Loss: 5.6768\n",
            "Epoch 20/1000, Train Loss: 5.5482, Val Loss: 5.6638\n",
            "Epoch 21/1000, Train Loss: 5.5473, Val Loss: 5.6639\n",
            "Epoch 22/1000, Train Loss: 5.5436, Val Loss: 5.6661\n",
            "Epoch 23/1000, Train Loss: 5.5446, Val Loss: 5.6613\n",
            "Epoch 24/1000, Train Loss: 5.5440, Val Loss: 5.6629\n",
            "Epoch 25/1000, Train Loss: 5.5438, Val Loss: 5.6767\n",
            "Epoch 26/1000, Train Loss: 5.5416, Val Loss: 5.6797\n",
            "Epoch 27/1000, Train Loss: 5.5450, Val Loss: 5.6709\n",
            "Epoch 28/1000, Train Loss: 5.5495, Val Loss: 5.6692\n",
            "Epoch 29/1000, Train Loss: 5.5445, Val Loss: 5.6861\n",
            "Epoch 30/1000, Train Loss: 5.5471, Val Loss: 5.6795\n",
            "Epoch 31/1000, Train Loss: 5.5490, Val Loss: 5.6742\n",
            "Epoch 32/1000, Train Loss: 5.5451, Val Loss: 5.6756\n",
            "Epoch 33/1000, Train Loss: 5.5418, Val Loss: 5.6758\n",
            "Early stopping triggered at epoch 33. Best Val Loss: 5.6613\n",
            "Training with LR=0.005, Hidden=64, Dropout=0.0, Scale Factor=1.25\n",
            "Combination 47/81\n",
            "Epoch 1/1000, Train Loss: 6.6340, Val Loss: 6.7613\n",
            "Epoch 2/1000, Train Loss: 6.6081, Val Loss: 6.9249\n",
            "Epoch 3/1000, Train Loss: 6.6025, Val Loss: 6.7547\n",
            "Epoch 4/1000, Train Loss: 6.6121, Val Loss: 6.7640\n",
            "Epoch 5/1000, Train Loss: 6.6047, Val Loss: 6.7514\n",
            "Epoch 6/1000, Train Loss: 6.6040, Val Loss: 6.7627\n",
            "Epoch 7/1000, Train Loss: 6.6077, Val Loss: 6.7563\n",
            "Epoch 8/1000, Train Loss: 6.6009, Val Loss: 6.7539\n",
            "Epoch 9/1000, Train Loss: 6.6086, Val Loss: 6.7583\n",
            "Epoch 10/1000, Train Loss: 6.6051, Val Loss: 6.7566\n",
            "Epoch 11/1000, Train Loss: 6.6042, Val Loss: 6.8058\n",
            "Epoch 12/1000, Train Loss: 6.6019, Val Loss: 6.7758\n",
            "Epoch 13/1000, Train Loss: 6.6012, Val Loss: 6.7569\n",
            "Epoch 14/1000, Train Loss: 6.6074, Val Loss: 6.7996\n",
            "Epoch 15/1000, Train Loss: 6.6029, Val Loss: 6.7579\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 6.7514\n",
            "Epoch 1/1000, Train Loss: 6.6233, Val Loss: 6.7897\n",
            "Epoch 2/1000, Train Loss: 6.6024, Val Loss: 6.7751\n",
            "Epoch 3/1000, Train Loss: 6.6081, Val Loss: 6.7700\n",
            "Epoch 4/1000, Train Loss: 6.6079, Val Loss: 6.7627\n",
            "Epoch 5/1000, Train Loss: 6.5983, Val Loss: 6.7593\n",
            "Epoch 6/1000, Train Loss: 6.6031, Val Loss: 6.7603\n",
            "Epoch 7/1000, Train Loss: 6.6138, Val Loss: 6.7577\n",
            "Epoch 8/1000, Train Loss: 6.5986, Val Loss: 6.7921\n",
            "Epoch 9/1000, Train Loss: 6.6068, Val Loss: 6.7685\n",
            "Epoch 10/1000, Train Loss: 6.5995, Val Loss: 6.7578\n",
            "Epoch 11/1000, Train Loss: 6.6017, Val Loss: 6.7609\n",
            "Epoch 12/1000, Train Loss: 6.6059, Val Loss: 6.7556\n",
            "Epoch 13/1000, Train Loss: 6.6059, Val Loss: 6.7587\n",
            "Epoch 14/1000, Train Loss: 6.6003, Val Loss: 6.7642\n",
            "Epoch 15/1000, Train Loss: 6.5985, Val Loss: 6.7784\n",
            "Epoch 16/1000, Train Loss: 6.6096, Val Loss: 6.7488\n",
            "Epoch 17/1000, Train Loss: 6.6021, Val Loss: 6.7765\n",
            "Epoch 18/1000, Train Loss: 6.6046, Val Loss: 6.7768\n",
            "Epoch 19/1000, Train Loss: 6.6041, Val Loss: 6.7556\n",
            "Epoch 20/1000, Train Loss: 6.6048, Val Loss: 6.7560\n",
            "Epoch 21/1000, Train Loss: 6.6022, Val Loss: 6.7586\n",
            "Epoch 22/1000, Train Loss: 6.6026, Val Loss: 6.7643\n",
            "Epoch 23/1000, Train Loss: 6.6031, Val Loss: 6.7589\n",
            "Epoch 24/1000, Train Loss: 6.5988, Val Loss: 6.7588\n",
            "Epoch 25/1000, Train Loss: 6.6032, Val Loss: 6.7597\n",
            "Epoch 26/1000, Train Loss: 6.6020, Val Loss: 6.7766\n",
            "Early stopping triggered at epoch 26. Best Val Loss: 6.7488\n",
            "Epoch 1/1000, Train Loss: 6.6337, Val Loss: 6.7473\n",
            "Epoch 2/1000, Train Loss: 6.6225, Val Loss: 6.7570\n",
            "Epoch 3/1000, Train Loss: 6.6041, Val Loss: 6.7617\n",
            "Epoch 4/1000, Train Loss: 6.6131, Val Loss: 6.7612\n",
            "Epoch 5/1000, Train Loss: 6.6037, Val Loss: 6.7656\n",
            "Epoch 6/1000, Train Loss: 6.6046, Val Loss: 6.7489\n",
            "Epoch 7/1000, Train Loss: 6.6041, Val Loss: 6.7588\n",
            "Epoch 8/1000, Train Loss: 6.6044, Val Loss: 6.7601\n",
            "Epoch 9/1000, Train Loss: 6.6028, Val Loss: 6.7878\n",
            "Epoch 10/1000, Train Loss: 6.6026, Val Loss: 6.7537\n",
            "Epoch 11/1000, Train Loss: 6.6003, Val Loss: 6.7700\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 6.7473\n",
            "Training with LR=0.005, Hidden=64, Dropout=0.0, Scale Factor=1.5\n",
            "Combination 48/81\n",
            "Epoch 1/1000, Train Loss: 7.6740, Val Loss: 7.8305\n",
            "Epoch 2/1000, Train Loss: 7.6493, Val Loss: 7.8325\n",
            "Epoch 3/1000, Train Loss: 7.6457, Val Loss: 7.8236\n",
            "Epoch 4/1000, Train Loss: 7.6425, Val Loss: 7.8321\n",
            "Epoch 5/1000, Train Loss: 7.6426, Val Loss: 7.8301\n",
            "Epoch 6/1000, Train Loss: 7.6434, Val Loss: 7.8182\n",
            "Epoch 7/1000, Train Loss: 7.6372, Val Loss: 7.8176\n",
            "Epoch 8/1000, Train Loss: 7.6470, Val Loss: 7.8221\n",
            "Epoch 9/1000, Train Loss: 7.6404, Val Loss: 7.8222\n",
            "Epoch 10/1000, Train Loss: 7.6376, Val Loss: 7.8226\n",
            "Epoch 11/1000, Train Loss: 7.6366, Val Loss: 7.8472\n",
            "Epoch 12/1000, Train Loss: 7.6359, Val Loss: 7.8348\n",
            "Epoch 13/1000, Train Loss: 7.6350, Val Loss: 7.8310\n",
            "Epoch 14/1000, Train Loss: 7.6414, Val Loss: 7.8278\n",
            "Epoch 15/1000, Train Loss: 7.6417, Val Loss: 7.8260\n",
            "Epoch 16/1000, Train Loss: 7.6391, Val Loss: 7.8259\n",
            "Epoch 17/1000, Train Loss: 7.6379, Val Loss: 7.8298\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 7.8176\n",
            "Epoch 1/1000, Train Loss: 7.6620, Val Loss: 7.8201\n",
            "Epoch 2/1000, Train Loss: 7.6478, Val Loss: 7.8289\n",
            "Epoch 3/1000, Train Loss: 7.6437, Val Loss: 7.8467\n",
            "Epoch 4/1000, Train Loss: 7.6449, Val Loss: 7.8230\n",
            "Epoch 5/1000, Train Loss: 7.6447, Val Loss: 7.8497\n",
            "Epoch 6/1000, Train Loss: 7.6461, Val Loss: 7.8246\n",
            "Epoch 7/1000, Train Loss: 7.6423, Val Loss: 7.8179\n",
            "Epoch 8/1000, Train Loss: 7.6400, Val Loss: 7.8241\n",
            "Epoch 9/1000, Train Loss: 7.6420, Val Loss: 7.8263\n",
            "Epoch 10/1000, Train Loss: 7.6389, Val Loss: 7.8223\n",
            "Epoch 11/1000, Train Loss: 7.6386, Val Loss: 7.8189\n",
            "Epoch 12/1000, Train Loss: 7.6355, Val Loss: 7.8569\n",
            "Epoch 13/1000, Train Loss: 7.6427, Val Loss: 7.8384\n",
            "Epoch 14/1000, Train Loss: 7.6490, Val Loss: 7.8330\n",
            "Epoch 15/1000, Train Loss: 7.6399, Val Loss: 7.8243\n",
            "Epoch 16/1000, Train Loss: 7.6390, Val Loss: 7.8252\n",
            "Epoch 17/1000, Train Loss: 7.6366, Val Loss: 7.8362\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 7.8179\n",
            "Epoch 1/1000, Train Loss: 7.6663, Val Loss: 7.8254\n",
            "Epoch 2/1000, Train Loss: 7.6498, Val Loss: 7.8293\n",
            "Epoch 3/1000, Train Loss: 7.6489, Val Loss: 7.8218\n",
            "Epoch 4/1000, Train Loss: 7.6395, Val Loss: 7.8222\n",
            "Epoch 5/1000, Train Loss: 7.6446, Val Loss: 7.8932\n",
            "Epoch 6/1000, Train Loss: 7.6613, Val Loss: 7.8229\n",
            "Epoch 7/1000, Train Loss: 7.6379, Val Loss: 7.8293\n",
            "Epoch 8/1000, Train Loss: 7.6447, Val Loss: 7.8254\n",
            "Epoch 9/1000, Train Loss: 7.6447, Val Loss: 7.8217\n",
            "Epoch 10/1000, Train Loss: 7.6408, Val Loss: 7.8366\n",
            "Epoch 11/1000, Train Loss: 7.6381, Val Loss: 7.8236\n",
            "Epoch 12/1000, Train Loss: 7.6410, Val Loss: 7.8285\n",
            "Epoch 13/1000, Train Loss: 7.6386, Val Loss: 7.8247\n",
            "Epoch 14/1000, Train Loss: 7.6395, Val Loss: 7.8252\n",
            "Epoch 15/1000, Train Loss: 7.6353, Val Loss: 7.8423\n",
            "Epoch 16/1000, Train Loss: 7.6384, Val Loss: 7.8793\n",
            "Epoch 17/1000, Train Loss: 7.6405, Val Loss: 7.8225\n",
            "Epoch 18/1000, Train Loss: 7.6409, Val Loss: 7.8535\n",
            "Epoch 19/1000, Train Loss: 7.6424, Val Loss: 7.8207\n",
            "Epoch 20/1000, Train Loss: 7.6399, Val Loss: 7.8741\n",
            "Epoch 21/1000, Train Loss: 7.6418, Val Loss: 7.8219\n",
            "Epoch 22/1000, Train Loss: 7.6395, Val Loss: 7.8240\n",
            "Epoch 23/1000, Train Loss: 7.6360, Val Loss: 7.8323\n",
            "Epoch 24/1000, Train Loss: 7.6426, Val Loss: 7.8352\n",
            "Epoch 25/1000, Train Loss: 7.6405, Val Loss: 7.8306\n",
            "Epoch 26/1000, Train Loss: 7.6396, Val Loss: 7.8279\n",
            "Epoch 27/1000, Train Loss: 7.6448, Val Loss: 7.8283\n",
            "Epoch 28/1000, Train Loss: 7.6401, Val Loss: 7.8307\n",
            "Epoch 29/1000, Train Loss: 7.6397, Val Loss: 7.8267\n",
            "Early stopping triggered at epoch 29. Best Val Loss: 7.8207\n",
            "Training with LR=0.005, Hidden=64, Dropout=0.1, Scale Factor=1.0\n",
            "Combination 49/81\n",
            "Epoch 1/1000, Train Loss: 5.5750, Val Loss: 5.6951\n",
            "Epoch 2/1000, Train Loss: 5.5511, Val Loss: 5.6665\n",
            "Epoch 3/1000, Train Loss: 5.5482, Val Loss: 5.6700\n",
            "Epoch 4/1000, Train Loss: 5.5500, Val Loss: 5.6752\n",
            "Epoch 5/1000, Train Loss: 5.5487, Val Loss: 5.6760\n",
            "Epoch 6/1000, Train Loss: 5.5483, Val Loss: 5.7253\n",
            "Epoch 7/1000, Train Loss: 5.5505, Val Loss: 5.6735\n",
            "Epoch 8/1000, Train Loss: 5.5427, Val Loss: 5.6674\n",
            "Epoch 9/1000, Train Loss: 5.5521, Val Loss: 5.6683\n",
            "Epoch 10/1000, Train Loss: 5.5492, Val Loss: 5.6650\n",
            "Epoch 11/1000, Train Loss: 5.5449, Val Loss: 5.6720\n",
            "Epoch 12/1000, Train Loss: 5.5485, Val Loss: 5.6706\n",
            "Epoch 13/1000, Train Loss: 5.5467, Val Loss: 5.6651\n",
            "Epoch 14/1000, Train Loss: 5.5478, Val Loss: 5.6768\n",
            "Epoch 15/1000, Train Loss: 5.5467, Val Loss: 5.6839\n",
            "Epoch 16/1000, Train Loss: 5.5499, Val Loss: 5.6640\n",
            "Epoch 17/1000, Train Loss: 5.5453, Val Loss: 5.6667\n",
            "Epoch 18/1000, Train Loss: 5.5424, Val Loss: 5.6812\n",
            "Epoch 19/1000, Train Loss: 5.5379, Val Loss: 5.6853\n",
            "Epoch 20/1000, Train Loss: 5.5459, Val Loss: 5.6861\n",
            "Epoch 21/1000, Train Loss: 5.5460, Val Loss: 5.6690\n",
            "Epoch 22/1000, Train Loss: 5.5501, Val Loss: 5.6702\n",
            "Epoch 23/1000, Train Loss: 5.5488, Val Loss: 5.6687\n",
            "Epoch 24/1000, Train Loss: 5.5496, Val Loss: 5.6691\n",
            "Epoch 25/1000, Train Loss: 5.5449, Val Loss: 5.6707\n",
            "Epoch 26/1000, Train Loss: 5.5412, Val Loss: 5.6769\n",
            "Early stopping triggered at epoch 26. Best Val Loss: 5.6640\n",
            "Epoch 1/1000, Train Loss: 5.5786, Val Loss: 5.6965\n",
            "Epoch 2/1000, Train Loss: 5.5510, Val Loss: 5.6736\n",
            "Epoch 3/1000, Train Loss: 5.5445, Val Loss: 5.6657\n",
            "Epoch 4/1000, Train Loss: 5.5495, Val Loss: 5.6655\n",
            "Epoch 5/1000, Train Loss: 5.5517, Val Loss: 5.7358\n",
            "Epoch 6/1000, Train Loss: 5.5515, Val Loss: 5.6848\n",
            "Epoch 7/1000, Train Loss: 5.5468, Val Loss: 5.6653\n",
            "Epoch 8/1000, Train Loss: 5.5499, Val Loss: 5.6750\n",
            "Epoch 9/1000, Train Loss: 5.5538, Val Loss: 5.6672\n",
            "Epoch 10/1000, Train Loss: 5.5533, Val Loss: 5.6747\n",
            "Epoch 11/1000, Train Loss: 5.5464, Val Loss: 5.6691\n",
            "Epoch 12/1000, Train Loss: 5.5460, Val Loss: 5.6707\n",
            "Epoch 13/1000, Train Loss: 5.5479, Val Loss: 5.6628\n",
            "Epoch 14/1000, Train Loss: 5.5481, Val Loss: 5.6725\n",
            "Epoch 15/1000, Train Loss: 5.5454, Val Loss: 5.6727\n",
            "Epoch 16/1000, Train Loss: 5.5490, Val Loss: 5.6646\n",
            "Epoch 17/1000, Train Loss: 5.5439, Val Loss: 5.6626\n",
            "Epoch 18/1000, Train Loss: 5.5476, Val Loss: 5.6678\n",
            "Epoch 19/1000, Train Loss: 5.5535, Val Loss: 5.6621\n",
            "Epoch 20/1000, Train Loss: 5.5449, Val Loss: 5.6687\n",
            "Epoch 21/1000, Train Loss: 5.5456, Val Loss: 5.6691\n",
            "Epoch 22/1000, Train Loss: 5.5456, Val Loss: 5.6616\n",
            "Epoch 23/1000, Train Loss: 5.5444, Val Loss: 5.6672\n",
            "Epoch 24/1000, Train Loss: 5.5483, Val Loss: 5.6666\n",
            "Epoch 25/1000, Train Loss: 5.5468, Val Loss: 5.6621\n",
            "Epoch 26/1000, Train Loss: 5.5479, Val Loss: 5.6620\n",
            "Epoch 27/1000, Train Loss: 5.5464, Val Loss: 5.6616\n",
            "Epoch 28/1000, Train Loss: 5.5458, Val Loss: 5.6666\n",
            "Epoch 29/1000, Train Loss: 5.5491, Val Loss: 5.6769\n",
            "Epoch 30/1000, Train Loss: 5.5394, Val Loss: 5.6813\n",
            "Epoch 31/1000, Train Loss: 5.5461, Val Loss: 5.6703\n",
            "Epoch 32/1000, Train Loss: 5.5432, Val Loss: 5.6704\n",
            "Early stopping triggered at epoch 32. Best Val Loss: 5.6616\n",
            "Epoch 1/1000, Train Loss: 5.5660, Val Loss: 5.6641\n",
            "Epoch 2/1000, Train Loss: 5.5510, Val Loss: 5.6668\n",
            "Epoch 3/1000, Train Loss: 5.5464, Val Loss: 5.6644\n",
            "Epoch 4/1000, Train Loss: 5.5472, Val Loss: 5.6635\n",
            "Epoch 5/1000, Train Loss: 5.5517, Val Loss: 5.6677\n",
            "Epoch 6/1000, Train Loss: 5.5480, Val Loss: 5.6664\n",
            "Epoch 7/1000, Train Loss: 5.5504, Val Loss: 5.6674\n",
            "Epoch 8/1000, Train Loss: 5.5428, Val Loss: 5.6748\n",
            "Epoch 9/1000, Train Loss: 5.5570, Val Loss: 5.6956\n",
            "Epoch 10/1000, Train Loss: 5.5471, Val Loss: 5.6693\n",
            "Epoch 11/1000, Train Loss: 5.5488, Val Loss: 5.6617\n",
            "Epoch 12/1000, Train Loss: 5.5454, Val Loss: 5.6715\n",
            "Epoch 13/1000, Train Loss: 5.5447, Val Loss: 5.6780\n",
            "Epoch 14/1000, Train Loss: 5.5459, Val Loss: 5.6938\n",
            "Epoch 15/1000, Train Loss: 5.5449, Val Loss: 5.6735\n",
            "Epoch 16/1000, Train Loss: 5.5419, Val Loss: 5.6675\n",
            "Epoch 17/1000, Train Loss: 5.5477, Val Loss: 5.6676\n",
            "Epoch 18/1000, Train Loss: 5.5466, Val Loss: 5.6703\n",
            "Epoch 19/1000, Train Loss: 5.5478, Val Loss: 5.6748\n",
            "Epoch 20/1000, Train Loss: 5.5518, Val Loss: 5.6680\n",
            "Epoch 21/1000, Train Loss: 5.5527, Val Loss: 5.6688\n",
            "Early stopping triggered at epoch 21. Best Val Loss: 5.6617\n",
            "Training with LR=0.005, Hidden=64, Dropout=0.1, Scale Factor=1.25\n",
            "Combination 50/81\n",
            "Epoch 1/1000, Train Loss: 6.6335, Val Loss: 6.7727\n",
            "Epoch 2/1000, Train Loss: 6.6084, Val Loss: 6.7741\n",
            "Epoch 3/1000, Train Loss: 6.6101, Val Loss: 6.7598\n",
            "Epoch 4/1000, Train Loss: 6.6038, Val Loss: 6.9035\n",
            "Epoch 5/1000, Train Loss: 6.6098, Val Loss: 6.7585\n",
            "Epoch 6/1000, Train Loss: 6.6064, Val Loss: 6.7600\n",
            "Epoch 7/1000, Train Loss: 6.6068, Val Loss: 6.7791\n",
            "Epoch 8/1000, Train Loss: 6.6130, Val Loss: 6.7854\n",
            "Epoch 9/1000, Train Loss: 6.6093, Val Loss: 6.7660\n",
            "Epoch 10/1000, Train Loss: 6.6101, Val Loss: 6.7554\n",
            "Epoch 11/1000, Train Loss: 6.6124, Val Loss: 6.7572\n",
            "Epoch 12/1000, Train Loss: 6.6081, Val Loss: 6.7761\n",
            "Epoch 13/1000, Train Loss: 6.6125, Val Loss: 6.7761\n",
            "Epoch 14/1000, Train Loss: 6.6217, Val Loss: 6.7786\n",
            "Epoch 15/1000, Train Loss: 6.6121, Val Loss: 6.7747\n",
            "Epoch 16/1000, Train Loss: 6.6114, Val Loss: 6.7729\n",
            "Epoch 17/1000, Train Loss: 6.6029, Val Loss: 6.7716\n",
            "Epoch 18/1000, Train Loss: 6.6078, Val Loss: 6.7764\n",
            "Epoch 19/1000, Train Loss: 6.6111, Val Loss: 6.7470\n",
            "Epoch 20/1000, Train Loss: 6.6110, Val Loss: 6.7475\n",
            "Epoch 21/1000, Train Loss: 6.6238, Val Loss: 6.7593\n",
            "Epoch 22/1000, Train Loss: 6.6053, Val Loss: 6.7537\n",
            "Epoch 23/1000, Train Loss: 6.6130, Val Loss: 6.7637\n",
            "Epoch 24/1000, Train Loss: 6.6020, Val Loss: 6.7610\n",
            "Epoch 25/1000, Train Loss: 6.6057, Val Loss: 6.7774\n",
            "Epoch 26/1000, Train Loss: 6.6041, Val Loss: 6.7618\n",
            "Epoch 27/1000, Train Loss: 6.6007, Val Loss: 6.7766\n",
            "Epoch 28/1000, Train Loss: 6.6080, Val Loss: 6.7719\n",
            "Epoch 29/1000, Train Loss: 6.6109, Val Loss: 6.7637\n",
            "Early stopping triggered at epoch 29. Best Val Loss: 6.7470\n",
            "Epoch 1/1000, Train Loss: 6.6255, Val Loss: 6.7693\n",
            "Epoch 2/1000, Train Loss: 6.6118, Val Loss: 6.7588\n",
            "Epoch 3/1000, Train Loss: 6.6102, Val Loss: 6.7511\n",
            "Epoch 4/1000, Train Loss: 6.6071, Val Loss: 6.7683\n",
            "Epoch 5/1000, Train Loss: 6.6076, Val Loss: 6.7844\n",
            "Epoch 6/1000, Train Loss: 6.6134, Val Loss: 6.7564\n",
            "Epoch 7/1000, Train Loss: 6.6125, Val Loss: 6.7584\n",
            "Epoch 8/1000, Train Loss: 6.6087, Val Loss: 6.7584\n",
            "Epoch 9/1000, Train Loss: 6.6034, Val Loss: 6.7512\n",
            "Epoch 10/1000, Train Loss: 6.6126, Val Loss: 6.7519\n",
            "Epoch 11/1000, Train Loss: 6.6069, Val Loss: 6.7543\n",
            "Epoch 12/1000, Train Loss: 6.6016, Val Loss: 6.7783\n",
            "Epoch 13/1000, Train Loss: 6.6040, Val Loss: 6.7569\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 6.7511\n",
            "Epoch 1/1000, Train Loss: 6.6387, Val Loss: 6.7647\n",
            "Epoch 2/1000, Train Loss: 6.6112, Val Loss: 6.7645\n",
            "Epoch 3/1000, Train Loss: 6.6081, Val Loss: 6.7674\n",
            "Epoch 4/1000, Train Loss: 6.6090, Val Loss: 6.7591\n",
            "Epoch 5/1000, Train Loss: 6.6137, Val Loss: 6.7648\n",
            "Epoch 6/1000, Train Loss: 6.6060, Val Loss: 6.7819\n",
            "Epoch 7/1000, Train Loss: 6.6095, Val Loss: 6.7598\n",
            "Epoch 8/1000, Train Loss: 6.6082, Val Loss: 6.7659\n",
            "Epoch 9/1000, Train Loss: 6.6095, Val Loss: 6.7682\n",
            "Epoch 10/1000, Train Loss: 6.6070, Val Loss: 6.7686\n",
            "Epoch 11/1000, Train Loss: 6.6083, Val Loss: 6.7723\n",
            "Epoch 12/1000, Train Loss: 6.6040, Val Loss: 6.7709\n",
            "Epoch 13/1000, Train Loss: 6.6041, Val Loss: 6.7676\n",
            "Epoch 14/1000, Train Loss: 6.6205, Val Loss: 6.7637\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 6.7591\n",
            "Training with LR=0.005, Hidden=64, Dropout=0.1, Scale Factor=1.5\n",
            "Combination 51/81\n",
            "Epoch 1/1000, Train Loss: 7.6755, Val Loss: 7.8315\n",
            "Epoch 2/1000, Train Loss: 7.6588, Val Loss: 7.8380\n",
            "Epoch 3/1000, Train Loss: 7.6460, Val Loss: 7.8293\n",
            "Epoch 4/1000, Train Loss: 7.6561, Val Loss: 7.8265\n",
            "Epoch 5/1000, Train Loss: 7.6505, Val Loss: 7.8146\n",
            "Epoch 6/1000, Train Loss: 7.6500, Val Loss: 7.8185\n",
            "Epoch 7/1000, Train Loss: 7.6410, Val Loss: 7.8210\n",
            "Epoch 8/1000, Train Loss: 7.6460, Val Loss: 7.8258\n",
            "Epoch 9/1000, Train Loss: 7.6486, Val Loss: 7.8261\n",
            "Epoch 10/1000, Train Loss: 7.6494, Val Loss: 7.8261\n",
            "Epoch 11/1000, Train Loss: 7.6394, Val Loss: 7.8315\n",
            "Epoch 12/1000, Train Loss: 7.6439, Val Loss: 7.8280\n",
            "Epoch 13/1000, Train Loss: 7.6463, Val Loss: 7.8213\n",
            "Epoch 14/1000, Train Loss: 7.6558, Val Loss: 7.8178\n",
            "Epoch 15/1000, Train Loss: 7.6476, Val Loss: 7.8182\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 7.8146\n",
            "Epoch 1/1000, Train Loss: 7.6916, Val Loss: 7.8289\n",
            "Epoch 2/1000, Train Loss: 7.6499, Val Loss: 7.8236\n",
            "Epoch 3/1000, Train Loss: 7.6489, Val Loss: 7.8327\n",
            "Epoch 4/1000, Train Loss: 7.6493, Val Loss: 7.8208\n",
            "Epoch 5/1000, Train Loss: 7.6465, Val Loss: 7.8265\n",
            "Epoch 6/1000, Train Loss: 7.6571, Val Loss: 7.8282\n",
            "Epoch 7/1000, Train Loss: 7.6513, Val Loss: 7.8251\n",
            "Epoch 8/1000, Train Loss: 7.6421, Val Loss: 7.8226\n",
            "Epoch 9/1000, Train Loss: 7.6539, Val Loss: 7.8274\n",
            "Epoch 10/1000, Train Loss: 7.6454, Val Loss: 7.8347\n",
            "Epoch 11/1000, Train Loss: 7.6599, Val Loss: 7.8245\n",
            "Epoch 12/1000, Train Loss: 7.6514, Val Loss: 7.8280\n",
            "Epoch 13/1000, Train Loss: 7.6529, Val Loss: 7.8242\n",
            "Epoch 14/1000, Train Loss: 7.6460, Val Loss: 7.8293\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 7.8208\n",
            "Epoch 1/1000, Train Loss: 7.6760, Val Loss: 7.8250\n",
            "Epoch 2/1000, Train Loss: 7.6461, Val Loss: 7.8229\n",
            "Epoch 3/1000, Train Loss: 7.6525, Val Loss: 7.8176\n",
            "Epoch 4/1000, Train Loss: 7.6456, Val Loss: 7.8220\n",
            "Epoch 5/1000, Train Loss: 7.6523, Val Loss: 7.8307\n",
            "Epoch 6/1000, Train Loss: 7.6499, Val Loss: 7.8254\n",
            "Epoch 7/1000, Train Loss: 7.6540, Val Loss: 7.8256\n",
            "Epoch 8/1000, Train Loss: 7.6475, Val Loss: 7.8254\n",
            "Epoch 9/1000, Train Loss: 7.6457, Val Loss: 7.8305\n",
            "Epoch 10/1000, Train Loss: 7.6432, Val Loss: 7.8286\n",
            "Epoch 11/1000, Train Loss: 7.6395, Val Loss: 7.8538\n",
            "Epoch 12/1000, Train Loss: 7.6422, Val Loss: 7.8288\n",
            "Epoch 13/1000, Train Loss: 7.6556, Val Loss: 7.8262\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 7.8176\n",
            "Training with LR=0.005, Hidden=64, Dropout=0.2, Scale Factor=1.0\n",
            "Combination 52/81\n",
            "Epoch 1/1000, Train Loss: 5.5731, Val Loss: 5.6884\n",
            "Epoch 2/1000, Train Loss: 5.5492, Val Loss: 5.6878\n",
            "Epoch 3/1000, Train Loss: 5.5544, Val Loss: 5.6751\n",
            "Epoch 4/1000, Train Loss: 5.5565, Val Loss: 5.6723\n",
            "Epoch 5/1000, Train Loss: 5.5533, Val Loss: 5.6847\n",
            "Epoch 6/1000, Train Loss: 5.5516, Val Loss: 5.6696\n",
            "Epoch 7/1000, Train Loss: 5.5607, Val Loss: 5.6911\n",
            "Epoch 8/1000, Train Loss: 5.5537, Val Loss: 5.6704\n",
            "Epoch 9/1000, Train Loss: 5.5510, Val Loss: 5.6890\n",
            "Epoch 10/1000, Train Loss: 5.5540, Val Loss: 5.6650\n",
            "Epoch 11/1000, Train Loss: 5.5555, Val Loss: 5.6720\n",
            "Epoch 12/1000, Train Loss: 5.5528, Val Loss: 5.6638\n",
            "Epoch 13/1000, Train Loss: 5.5516, Val Loss: 5.6715\n",
            "Epoch 14/1000, Train Loss: 5.5503, Val Loss: 5.6671\n",
            "Epoch 15/1000, Train Loss: 5.5532, Val Loss: 5.6656\n",
            "Epoch 16/1000, Train Loss: 5.5463, Val Loss: 5.6681\n",
            "Epoch 17/1000, Train Loss: 5.5506, Val Loss: 5.6653\n",
            "Epoch 18/1000, Train Loss: 5.5477, Val Loss: 5.6721\n",
            "Epoch 19/1000, Train Loss: 5.5514, Val Loss: 5.6665\n",
            "Epoch 20/1000, Train Loss: 5.5505, Val Loss: 5.6669\n",
            "Epoch 21/1000, Train Loss: 5.5501, Val Loss: 5.6660\n",
            "Epoch 22/1000, Train Loss: 5.5495, Val Loss: 5.6726\n",
            "Early stopping triggered at epoch 22. Best Val Loss: 5.6638\n",
            "Epoch 1/1000, Train Loss: 5.5712, Val Loss: 5.6658\n",
            "Epoch 2/1000, Train Loss: 5.5532, Val Loss: 5.6641\n",
            "Epoch 3/1000, Train Loss: 5.5541, Val Loss: 5.6681\n",
            "Epoch 4/1000, Train Loss: 5.5444, Val Loss: 5.6733\n",
            "Epoch 5/1000, Train Loss: 5.5562, Val Loss: 5.6628\n",
            "Epoch 6/1000, Train Loss: 5.5548, Val Loss: 5.6801\n",
            "Epoch 7/1000, Train Loss: 5.5495, Val Loss: 5.6631\n",
            "Epoch 8/1000, Train Loss: 5.5463, Val Loss: 5.6739\n",
            "Epoch 9/1000, Train Loss: 5.5486, Val Loss: 5.6900\n",
            "Epoch 10/1000, Train Loss: 5.5521, Val Loss: 5.6715\n",
            "Epoch 11/1000, Train Loss: 5.5501, Val Loss: 5.6624\n",
            "Epoch 12/1000, Train Loss: 5.5524, Val Loss: 5.6996\n",
            "Epoch 13/1000, Train Loss: 5.5575, Val Loss: 5.6656\n",
            "Epoch 14/1000, Train Loss: 5.5506, Val Loss: 5.6689\n",
            "Epoch 15/1000, Train Loss: 5.5419, Val Loss: 5.6684\n",
            "Epoch 16/1000, Train Loss: 5.5491, Val Loss: 5.6659\n",
            "Epoch 17/1000, Train Loss: 5.5497, Val Loss: 5.6730\n",
            "Epoch 18/1000, Train Loss: 5.5502, Val Loss: 5.6685\n",
            "Epoch 19/1000, Train Loss: 5.5485, Val Loss: 5.6664\n",
            "Epoch 20/1000, Train Loss: 5.5550, Val Loss: 5.6657\n",
            "Epoch 21/1000, Train Loss: 5.5462, Val Loss: 5.6651\n",
            "Early stopping triggered at epoch 21. Best Val Loss: 5.6624\n",
            "Epoch 1/1000, Train Loss: 5.5825, Val Loss: 5.6678\n",
            "Epoch 2/1000, Train Loss: 5.5562, Val Loss: 5.6643\n",
            "Epoch 3/1000, Train Loss: 5.5519, Val Loss: 5.6702\n",
            "Epoch 4/1000, Train Loss: 5.5602, Val Loss: 5.6965\n",
            "Epoch 5/1000, Train Loss: 5.5626, Val Loss: 5.6683\n",
            "Epoch 6/1000, Train Loss: 5.5496, Val Loss: 5.6657\n",
            "Epoch 7/1000, Train Loss: 5.5448, Val Loss: 5.6784\n",
            "Epoch 8/1000, Train Loss: 5.5515, Val Loss: 5.6648\n",
            "Epoch 9/1000, Train Loss: 5.5489, Val Loss: 5.6662\n",
            "Epoch 10/1000, Train Loss: 5.5513, Val Loss: 5.6665\n",
            "Epoch 11/1000, Train Loss: 5.5495, Val Loss: 5.6644\n",
            "Epoch 12/1000, Train Loss: 5.5473, Val Loss: 5.6654\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 5.6643\n",
            "Training with LR=0.005, Hidden=64, Dropout=0.2, Scale Factor=1.25\n",
            "Combination 53/81\n",
            "Epoch 1/1000, Train Loss: 6.6423, Val Loss: 6.7772\n",
            "Epoch 2/1000, Train Loss: 6.6248, Val Loss: 6.7589\n",
            "Epoch 3/1000, Train Loss: 6.6110, Val Loss: 6.7502\n",
            "Epoch 4/1000, Train Loss: 6.6135, Val Loss: 6.7553\n",
            "Epoch 5/1000, Train Loss: 6.6198, Val Loss: 6.7582\n",
            "Epoch 6/1000, Train Loss: 6.6076, Val Loss: 6.7724\n",
            "Epoch 7/1000, Train Loss: 6.6118, Val Loss: 6.7597\n",
            "Epoch 8/1000, Train Loss: 6.6036, Val Loss: 6.7703\n",
            "Epoch 9/1000, Train Loss: 6.6188, Val Loss: 6.7730\n",
            "Epoch 10/1000, Train Loss: 6.6123, Val Loss: 6.7489\n",
            "Epoch 11/1000, Train Loss: 6.6095, Val Loss: 6.7693\n",
            "Epoch 12/1000, Train Loss: 6.6218, Val Loss: 6.7541\n",
            "Epoch 13/1000, Train Loss: 6.6160, Val Loss: 6.7507\n",
            "Epoch 14/1000, Train Loss: 6.6060, Val Loss: 6.7557\n",
            "Epoch 15/1000, Train Loss: 6.6073, Val Loss: 6.7677\n",
            "Epoch 16/1000, Train Loss: 6.6128, Val Loss: 6.7602\n",
            "Epoch 17/1000, Train Loss: 6.6174, Val Loss: 6.7756\n",
            "Epoch 18/1000, Train Loss: 6.6117, Val Loss: 6.7662\n",
            "Epoch 19/1000, Train Loss: 6.6114, Val Loss: 6.7684\n",
            "Epoch 20/1000, Train Loss: 6.6089, Val Loss: 6.7686\n",
            "Early stopping triggered at epoch 20. Best Val Loss: 6.7489\n",
            "Epoch 1/1000, Train Loss: 6.6325, Val Loss: 6.7700\n",
            "Epoch 2/1000, Train Loss: 6.6178, Val Loss: 6.7503\n",
            "Epoch 3/1000, Train Loss: 6.6189, Val Loss: 6.7547\n",
            "Epoch 4/1000, Train Loss: 6.6176, Val Loss: 6.7552\n",
            "Epoch 5/1000, Train Loss: 6.6213, Val Loss: 6.7587\n",
            "Epoch 6/1000, Train Loss: 6.6124, Val Loss: 6.7617\n",
            "Epoch 7/1000, Train Loss: 6.6123, Val Loss: 6.7717\n",
            "Epoch 8/1000, Train Loss: 6.6114, Val Loss: 6.7722\n",
            "Epoch 9/1000, Train Loss: 6.6106, Val Loss: 6.7629\n",
            "Epoch 10/1000, Train Loss: 6.6164, Val Loss: 6.7686\n",
            "Epoch 11/1000, Train Loss: 6.6149, Val Loss: 6.7624\n",
            "Epoch 12/1000, Train Loss: 6.6202, Val Loss: 6.7638\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 6.7503\n",
            "Epoch 1/1000, Train Loss: 6.6358, Val Loss: 6.7958\n",
            "Epoch 2/1000, Train Loss: 6.6153, Val Loss: 6.7718\n",
            "Epoch 3/1000, Train Loss: 6.6076, Val Loss: 6.7633\n",
            "Epoch 4/1000, Train Loss: 6.6174, Val Loss: 6.7626\n",
            "Epoch 5/1000, Train Loss: 6.6087, Val Loss: 6.7763\n",
            "Epoch 6/1000, Train Loss: 6.6306, Val Loss: 6.7654\n",
            "Epoch 7/1000, Train Loss: 6.6122, Val Loss: 6.7549\n",
            "Epoch 8/1000, Train Loss: 6.6108, Val Loss: 6.7611\n",
            "Epoch 9/1000, Train Loss: 6.6094, Val Loss: 6.7787\n",
            "Epoch 10/1000, Train Loss: 6.6120, Val Loss: 6.7627\n",
            "Epoch 11/1000, Train Loss: 6.6111, Val Loss: 6.7627\n",
            "Epoch 12/1000, Train Loss: 6.6065, Val Loss: 6.7616\n",
            "Epoch 13/1000, Train Loss: 6.6133, Val Loss: 6.7605\n",
            "Epoch 14/1000, Train Loss: 6.6123, Val Loss: 6.7733\n",
            "Epoch 15/1000, Train Loss: 6.6105, Val Loss: 6.7711\n",
            "Epoch 16/1000, Train Loss: 6.6178, Val Loss: 6.7618\n",
            "Epoch 17/1000, Train Loss: 6.6012, Val Loss: 6.7710\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 6.7549\n",
            "Training with LR=0.005, Hidden=64, Dropout=0.2, Scale Factor=1.5\n",
            "Combination 54/81\n",
            "Epoch 1/1000, Train Loss: 7.7011, Val Loss: 7.8257\n",
            "Epoch 2/1000, Train Loss: 7.6497, Val Loss: 7.8288\n",
            "Epoch 3/1000, Train Loss: 7.6496, Val Loss: 7.8274\n",
            "Epoch 4/1000, Train Loss: 7.6529, Val Loss: 7.8274\n",
            "Epoch 5/1000, Train Loss: 7.6471, Val Loss: 7.8255\n",
            "Epoch 6/1000, Train Loss: 7.6518, Val Loss: 7.8313\n",
            "Epoch 7/1000, Train Loss: 7.6451, Val Loss: 7.8309\n",
            "Epoch 8/1000, Train Loss: 7.6646, Val Loss: 7.8310\n",
            "Epoch 9/1000, Train Loss: 7.6623, Val Loss: 7.8390\n",
            "Epoch 10/1000, Train Loss: 7.6472, Val Loss: 7.8279\n",
            "Epoch 11/1000, Train Loss: 7.6468, Val Loss: 7.8253\n",
            "Epoch 12/1000, Train Loss: 7.6501, Val Loss: 7.8254\n",
            "Epoch 13/1000, Train Loss: 7.6500, Val Loss: 7.8308\n",
            "Epoch 14/1000, Train Loss: 7.6533, Val Loss: 7.8262\n",
            "Epoch 15/1000, Train Loss: 7.6474, Val Loss: 7.8185\n",
            "Epoch 16/1000, Train Loss: 7.6429, Val Loss: 7.8447\n",
            "Epoch 17/1000, Train Loss: 7.6468, Val Loss: 7.8403\n",
            "Epoch 18/1000, Train Loss: 7.6525, Val Loss: 7.8267\n",
            "Epoch 19/1000, Train Loss: 7.6414, Val Loss: 7.8271\n",
            "Epoch 20/1000, Train Loss: 7.6449, Val Loss: 7.8295\n",
            "Epoch 21/1000, Train Loss: 7.6468, Val Loss: 7.8404\n",
            "Epoch 22/1000, Train Loss: 7.6480, Val Loss: 7.8230\n",
            "Epoch 23/1000, Train Loss: 7.6576, Val Loss: 7.8276\n",
            "Epoch 24/1000, Train Loss: 7.6441, Val Loss: 7.8364\n",
            "Epoch 25/1000, Train Loss: 7.6409, Val Loss: 7.8241\n",
            "Early stopping triggered at epoch 25. Best Val Loss: 7.8185\n",
            "Epoch 1/1000, Train Loss: 7.6878, Val Loss: 7.8185\n",
            "Epoch 2/1000, Train Loss: 7.6566, Val Loss: 7.8288\n",
            "Epoch 3/1000, Train Loss: 7.6624, Val Loss: 7.8213\n",
            "Epoch 4/1000, Train Loss: 7.6512, Val Loss: 7.8296\n",
            "Epoch 5/1000, Train Loss: 7.6493, Val Loss: 7.8294\n",
            "Epoch 6/1000, Train Loss: 7.6492, Val Loss: 7.8260\n",
            "Epoch 7/1000, Train Loss: 7.6598, Val Loss: 7.8265\n",
            "Epoch 8/1000, Train Loss: 7.6565, Val Loss: 7.8258\n",
            "Epoch 9/1000, Train Loss: 7.6430, Val Loss: 7.8327\n",
            "Epoch 10/1000, Train Loss: 7.6405, Val Loss: 7.8288\n",
            "Epoch 11/1000, Train Loss: 7.6463, Val Loss: 7.8243\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8185\n",
            "Epoch 1/1000, Train Loss: 7.6799, Val Loss: 7.8318\n",
            "Epoch 2/1000, Train Loss: 7.6525, Val Loss: 7.8315\n",
            "Epoch 3/1000, Train Loss: 7.6459, Val Loss: 7.8345\n",
            "Epoch 4/1000, Train Loss: 7.6662, Val Loss: 7.8449\n",
            "Epoch 5/1000, Train Loss: 7.6443, Val Loss: 7.8309\n",
            "Epoch 6/1000, Train Loss: 7.6437, Val Loss: 7.8267\n",
            "Epoch 7/1000, Train Loss: 7.6519, Val Loss: 7.8301\n",
            "Epoch 8/1000, Train Loss: 7.6426, Val Loss: 7.8257\n",
            "Epoch 9/1000, Train Loss: 7.6469, Val Loss: 7.8323\n",
            "Epoch 10/1000, Train Loss: 7.6440, Val Loss: 7.8346\n",
            "Epoch 11/1000, Train Loss: 7.6565, Val Loss: 7.8169\n",
            "Epoch 12/1000, Train Loss: 7.6470, Val Loss: 7.8297\n",
            "Epoch 13/1000, Train Loss: 7.6427, Val Loss: 7.8269\n",
            "Epoch 14/1000, Train Loss: 7.6508, Val Loss: 7.8357\n",
            "Epoch 15/1000, Train Loss: 7.6396, Val Loss: 7.8319\n",
            "Epoch 16/1000, Train Loss: 7.6512, Val Loss: 7.8269\n",
            "Epoch 17/1000, Train Loss: 7.6460, Val Loss: 7.8287\n",
            "Epoch 18/1000, Train Loss: 7.6440, Val Loss: 7.8254\n",
            "Epoch 19/1000, Train Loss: 7.6486, Val Loss: 7.8532\n",
            "Epoch 20/1000, Train Loss: 7.6367, Val Loss: 7.8268\n",
            "Epoch 21/1000, Train Loss: 7.6400, Val Loss: 7.8251\n",
            "Early stopping triggered at epoch 21. Best Val Loss: 7.8169\n",
            "Training with LR=0.01, Hidden=16, Dropout=0.0, Scale Factor=1.0\n",
            "Combination 55/81\n",
            "Epoch 1/1000, Train Loss: 5.5614, Val Loss: 5.6795\n",
            "Epoch 2/1000, Train Loss: 5.5541, Val Loss: 5.6641\n",
            "Epoch 3/1000, Train Loss: 5.5515, Val Loss: 5.6670\n",
            "Epoch 4/1000, Train Loss: 5.5444, Val Loss: 5.6691\n",
            "Epoch 5/1000, Train Loss: 5.5443, Val Loss: 5.6714\n",
            "Epoch 6/1000, Train Loss: 5.5541, Val Loss: 5.6854\n",
            "Epoch 7/1000, Train Loss: 5.5460, Val Loss: 5.7610\n",
            "Epoch 8/1000, Train Loss: 5.5426, Val Loss: 5.6704\n",
            "Epoch 9/1000, Train Loss: 5.5428, Val Loss: 5.6861\n",
            "Epoch 10/1000, Train Loss: 5.5448, Val Loss: 5.6846\n",
            "Epoch 11/1000, Train Loss: 5.5437, Val Loss: 5.6684\n",
            "Epoch 12/1000, Train Loss: 5.5425, Val Loss: 5.6750\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 5.6641\n",
            "Epoch 1/1000, Train Loss: 5.5561, Val Loss: 5.7366\n",
            "Epoch 2/1000, Train Loss: 5.5498, Val Loss: 5.7356\n",
            "Epoch 3/1000, Train Loss: 5.5463, Val Loss: 5.6709\n",
            "Epoch 4/1000, Train Loss: 5.5524, Val Loss: 5.6717\n",
            "Epoch 5/1000, Train Loss: 5.5525, Val Loss: 5.6782\n",
            "Epoch 6/1000, Train Loss: 5.5450, Val Loss: 5.6867\n",
            "Epoch 7/1000, Train Loss: 5.5501, Val Loss: 5.6660\n",
            "Epoch 8/1000, Train Loss: 5.5511, Val Loss: 5.6647\n",
            "Epoch 9/1000, Train Loss: 5.5508, Val Loss: 5.6651\n",
            "Epoch 10/1000, Train Loss: 5.5473, Val Loss: 5.6656\n",
            "Epoch 11/1000, Train Loss: 5.5429, Val Loss: 5.6721\n",
            "Epoch 12/1000, Train Loss: 5.5427, Val Loss: 5.6698\n",
            "Epoch 13/1000, Train Loss: 5.5460, Val Loss: 5.6793\n",
            "Epoch 14/1000, Train Loss: 5.5416, Val Loss: 5.6680\n",
            "Epoch 15/1000, Train Loss: 5.5430, Val Loss: 5.6728\n",
            "Epoch 16/1000, Train Loss: 5.5396, Val Loss: 5.6675\n",
            "Epoch 17/1000, Train Loss: 5.5437, Val Loss: 5.6673\n",
            "Epoch 18/1000, Train Loss: 5.5419, Val Loss: 5.6727\n",
            "Early stopping triggered at epoch 18. Best Val Loss: 5.6647\n",
            "Epoch 1/1000, Train Loss: 5.5629, Val Loss: 5.6674\n",
            "Epoch 2/1000, Train Loss: 5.5501, Val Loss: 5.6725\n",
            "Epoch 3/1000, Train Loss: 5.5541, Val Loss: 5.6696\n",
            "Epoch 4/1000, Train Loss: 5.5481, Val Loss: 5.6810\n",
            "Epoch 5/1000, Train Loss: 5.5479, Val Loss: 5.6739\n",
            "Epoch 6/1000, Train Loss: 5.5442, Val Loss: 5.6692\n",
            "Epoch 7/1000, Train Loss: 5.5448, Val Loss: 5.6886\n",
            "Epoch 8/1000, Train Loss: 5.5489, Val Loss: 5.6751\n",
            "Epoch 9/1000, Train Loss: 5.5473, Val Loss: 5.6728\n",
            "Epoch 10/1000, Train Loss: 5.5421, Val Loss: 5.6821\n",
            "Epoch 11/1000, Train Loss: 5.5498, Val Loss: 5.6701\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 5.6674\n",
            "Training with LR=0.01, Hidden=16, Dropout=0.0, Scale Factor=1.25\n",
            "Combination 56/81\n",
            "Epoch 1/1000, Train Loss: 6.6252, Val Loss: 6.7708\n",
            "Epoch 2/1000, Train Loss: 6.6072, Val Loss: 6.7664\n",
            "Epoch 3/1000, Train Loss: 6.6045, Val Loss: 6.7731\n",
            "Epoch 4/1000, Train Loss: 6.6062, Val Loss: 6.7530\n",
            "Epoch 5/1000, Train Loss: 6.6031, Val Loss: 6.8168\n",
            "Epoch 6/1000, Train Loss: 6.6086, Val Loss: 6.7855\n",
            "Epoch 7/1000, Train Loss: 6.6048, Val Loss: 6.7514\n",
            "Epoch 8/1000, Train Loss: 6.6030, Val Loss: 6.7746\n",
            "Epoch 9/1000, Train Loss: 6.6033, Val Loss: 6.7460\n",
            "Epoch 10/1000, Train Loss: 6.6082, Val Loss: 6.7608\n",
            "Epoch 11/1000, Train Loss: 6.6035, Val Loss: 6.7713\n",
            "Epoch 12/1000, Train Loss: 6.6043, Val Loss: 6.7838\n",
            "Epoch 13/1000, Train Loss: 6.6077, Val Loss: 6.7527\n",
            "Epoch 14/1000, Train Loss: 6.6293, Val Loss: 6.7539\n",
            "Epoch 15/1000, Train Loss: 6.6150, Val Loss: 6.7605\n",
            "Epoch 16/1000, Train Loss: 6.6116, Val Loss: 6.7468\n",
            "Epoch 17/1000, Train Loss: 6.6021, Val Loss: 6.7595\n",
            "Epoch 18/1000, Train Loss: 6.6014, Val Loss: 6.7593\n",
            "Epoch 19/1000, Train Loss: 6.6082, Val Loss: 6.7526\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 6.7460\n",
            "Epoch 1/1000, Train Loss: 6.6185, Val Loss: 6.7704\n",
            "Epoch 2/1000, Train Loss: 6.6081, Val Loss: 6.7597\n",
            "Epoch 3/1000, Train Loss: 6.6101, Val Loss: 6.7666\n",
            "Epoch 4/1000, Train Loss: 6.6064, Val Loss: 6.7662\n",
            "Epoch 5/1000, Train Loss: 6.6060, Val Loss: 6.7715\n",
            "Epoch 6/1000, Train Loss: 6.6049, Val Loss: 6.7680\n",
            "Epoch 7/1000, Train Loss: 6.6238, Val Loss: 6.7673\n",
            "Epoch 8/1000, Train Loss: 6.6116, Val Loss: 6.7703\n",
            "Epoch 9/1000, Train Loss: 6.6109, Val Loss: 6.7688\n",
            "Epoch 10/1000, Train Loss: 6.6077, Val Loss: 6.7694\n",
            "Epoch 11/1000, Train Loss: 6.6143, Val Loss: 6.7607\n",
            "Epoch 12/1000, Train Loss: 6.6054, Val Loss: 6.7642\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 6.7597\n",
            "Epoch 1/1000, Train Loss: 6.6116, Val Loss: 6.9550\n",
            "Epoch 2/1000, Train Loss: 6.6046, Val Loss: 6.7579\n",
            "Epoch 3/1000, Train Loss: 6.6054, Val Loss: 6.7588\n",
            "Epoch 4/1000, Train Loss: 6.6061, Val Loss: 6.8062\n",
            "Epoch 5/1000, Train Loss: 6.6085, Val Loss: 6.7774\n",
            "Epoch 6/1000, Train Loss: 6.6042, Val Loss: 6.7762\n",
            "Epoch 7/1000, Train Loss: 6.6050, Val Loss: 6.7618\n",
            "Epoch 8/1000, Train Loss: 6.6021, Val Loss: 6.7635\n",
            "Epoch 9/1000, Train Loss: 6.6048, Val Loss: 6.7544\n",
            "Epoch 10/1000, Train Loss: 6.6019, Val Loss: 6.7582\n",
            "Epoch 11/1000, Train Loss: 6.6063, Val Loss: 6.7601\n",
            "Epoch 12/1000, Train Loss: 6.6029, Val Loss: 6.7609\n",
            "Epoch 13/1000, Train Loss: 6.6185, Val Loss: 6.7688\n",
            "Epoch 14/1000, Train Loss: 6.6107, Val Loss: 6.7518\n",
            "Epoch 15/1000, Train Loss: 6.6029, Val Loss: 6.7504\n",
            "Epoch 16/1000, Train Loss: 6.6038, Val Loss: 6.7533\n",
            "Epoch 17/1000, Train Loss: 6.6065, Val Loss: 6.7557\n",
            "Epoch 18/1000, Train Loss: 6.6039, Val Loss: 6.7532\n",
            "Epoch 19/1000, Train Loss: 6.6008, Val Loss: 6.7526\n",
            "Epoch 20/1000, Train Loss: 6.6019, Val Loss: 6.7631\n",
            "Epoch 21/1000, Train Loss: 6.6091, Val Loss: 6.7533\n",
            "Epoch 22/1000, Train Loss: 6.6007, Val Loss: 6.7487\n",
            "Epoch 23/1000, Train Loss: 6.6025, Val Loss: 6.7536\n",
            "Epoch 24/1000, Train Loss: 6.6028, Val Loss: 6.7645\n",
            "Epoch 25/1000, Train Loss: 6.6013, Val Loss: 6.7529\n",
            "Epoch 26/1000, Train Loss: 6.6042, Val Loss: 6.7610\n",
            "Epoch 27/1000, Train Loss: 6.5970, Val Loss: 6.7559\n",
            "Epoch 28/1000, Train Loss: 6.6014, Val Loss: 6.7559\n",
            "Epoch 29/1000, Train Loss: 6.6058, Val Loss: 6.7547\n",
            "Epoch 30/1000, Train Loss: 6.6005, Val Loss: 6.7557\n",
            "Epoch 31/1000, Train Loss: 6.6000, Val Loss: 6.7577\n",
            "Epoch 32/1000, Train Loss: 6.6022, Val Loss: 6.7525\n",
            "Early stopping triggered at epoch 32. Best Val Loss: 6.7487\n",
            "Training with LR=0.01, Hidden=16, Dropout=0.0, Scale Factor=1.5\n",
            "Combination 57/81\n",
            "Epoch 1/1000, Train Loss: 7.6695, Val Loss: 7.8434\n",
            "Epoch 2/1000, Train Loss: 7.6568, Val Loss: 7.8262\n",
            "Epoch 3/1000, Train Loss: 7.6512, Val Loss: 7.8266\n",
            "Epoch 4/1000, Train Loss: 7.6425, Val Loss: 7.8279\n",
            "Epoch 5/1000, Train Loss: 7.6526, Val Loss: 7.8197\n",
            "Epoch 6/1000, Train Loss: 7.6540, Val Loss: 7.8240\n",
            "Epoch 7/1000, Train Loss: 7.6421, Val Loss: 7.8299\n",
            "Epoch 8/1000, Train Loss: 7.6500, Val Loss: 7.8187\n",
            "Epoch 9/1000, Train Loss: 7.6525, Val Loss: 7.8279\n",
            "Epoch 10/1000, Train Loss: 7.6415, Val Loss: 7.8237\n",
            "Epoch 11/1000, Train Loss: 7.6446, Val Loss: 7.8330\n",
            "Epoch 12/1000, Train Loss: 7.6491, Val Loss: 7.8237\n",
            "Epoch 13/1000, Train Loss: 7.6456, Val Loss: 7.8493\n",
            "Epoch 14/1000, Train Loss: 7.6491, Val Loss: 7.8370\n",
            "Epoch 15/1000, Train Loss: 7.6362, Val Loss: 7.8428\n",
            "Epoch 16/1000, Train Loss: 7.6420, Val Loss: 7.8231\n",
            "Epoch 17/1000, Train Loss: 7.6443, Val Loss: 7.8196\n",
            "Epoch 18/1000, Train Loss: 7.6415, Val Loss: 7.8227\n",
            "Early stopping triggered at epoch 18. Best Val Loss: 7.8187\n",
            "Epoch 1/1000, Train Loss: 7.6783, Val Loss: 7.8715\n",
            "Epoch 2/1000, Train Loss: 7.6471, Val Loss: 7.8201\n",
            "Epoch 3/1000, Train Loss: 7.6546, Val Loss: 7.8213\n",
            "Epoch 4/1000, Train Loss: 7.6456, Val Loss: 7.8335\n",
            "Epoch 5/1000, Train Loss: 7.6418, Val Loss: 7.8172\n",
            "Epoch 6/1000, Train Loss: 7.6478, Val Loss: 7.8281\n",
            "Epoch 7/1000, Train Loss: 7.6453, Val Loss: 7.8250\n",
            "Epoch 8/1000, Train Loss: 7.6505, Val Loss: 7.8330\n",
            "Epoch 9/1000, Train Loss: 7.6423, Val Loss: 7.9789\n",
            "Epoch 10/1000, Train Loss: 7.6447, Val Loss: 7.8326\n",
            "Epoch 11/1000, Train Loss: 7.6353, Val Loss: 7.8419\n",
            "Epoch 12/1000, Train Loss: 7.6414, Val Loss: 7.8250\n",
            "Epoch 13/1000, Train Loss: 7.6391, Val Loss: 7.8328\n",
            "Epoch 14/1000, Train Loss: 7.6473, Val Loss: 7.8350\n",
            "Epoch 15/1000, Train Loss: 7.6463, Val Loss: 7.8252\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 7.8172\n",
            "Epoch 1/1000, Train Loss: 7.6616, Val Loss: 7.8307\n",
            "Epoch 2/1000, Train Loss: 7.6481, Val Loss: 7.8239\n",
            "Epoch 3/1000, Train Loss: 7.6471, Val Loss: 7.8271\n",
            "Epoch 4/1000, Train Loss: 7.6407, Val Loss: 7.8237\n",
            "Epoch 5/1000, Train Loss: 7.6527, Val Loss: 7.8155\n",
            "Epoch 6/1000, Train Loss: 7.6466, Val Loss: 7.8419\n",
            "Epoch 7/1000, Train Loss: 7.6417, Val Loss: 7.8122\n",
            "Epoch 8/1000, Train Loss: 7.6477, Val Loss: 7.8104\n",
            "Epoch 9/1000, Train Loss: 7.6442, Val Loss: 7.8206\n",
            "Epoch 10/1000, Train Loss: 7.6411, Val Loss: 7.8226\n",
            "Epoch 11/1000, Train Loss: 7.6402, Val Loss: 7.8153\n",
            "Epoch 12/1000, Train Loss: 7.6365, Val Loss: 7.8193\n",
            "Epoch 13/1000, Train Loss: 7.6464, Val Loss: 7.8139\n",
            "Epoch 14/1000, Train Loss: 7.6414, Val Loss: 7.8452\n",
            "Epoch 15/1000, Train Loss: 7.6435, Val Loss: 7.8345\n",
            "Epoch 16/1000, Train Loss: 7.6421, Val Loss: 7.8277\n",
            "Epoch 17/1000, Train Loss: 7.6369, Val Loss: 7.8243\n",
            "Epoch 18/1000, Train Loss: 7.6422, Val Loss: 7.8156\n",
            "Early stopping triggered at epoch 18. Best Val Loss: 7.8104\n",
            "Training with LR=0.01, Hidden=16, Dropout=0.1, Scale Factor=1.0\n",
            "Combination 58/81\n",
            "Epoch 1/1000, Train Loss: 5.5593, Val Loss: 5.6786\n",
            "Epoch 2/1000, Train Loss: 5.5490, Val Loss: 5.6683\n",
            "Epoch 3/1000, Train Loss: 5.5495, Val Loss: 5.6754\n",
            "Epoch 4/1000, Train Loss: 5.5502, Val Loss: 5.6700\n",
            "Epoch 5/1000, Train Loss: 5.5507, Val Loss: 5.6708\n",
            "Epoch 6/1000, Train Loss: 5.5536, Val Loss: 5.6668\n",
            "Epoch 7/1000, Train Loss: 5.5491, Val Loss: 5.6667\n",
            "Epoch 8/1000, Train Loss: 5.5461, Val Loss: 5.6808\n",
            "Epoch 9/1000, Train Loss: 5.5545, Val Loss: 5.6677\n",
            "Epoch 10/1000, Train Loss: 5.5641, Val Loss: 5.6664\n",
            "Epoch 11/1000, Train Loss: 5.5575, Val Loss: 5.6755\n",
            "Epoch 12/1000, Train Loss: 5.5500, Val Loss: 5.6715\n",
            "Epoch 13/1000, Train Loss: 5.5445, Val Loss: 5.6709\n",
            "Epoch 14/1000, Train Loss: 5.5470, Val Loss: 5.6704\n",
            "Epoch 15/1000, Train Loss: 5.5490, Val Loss: 5.6765\n",
            "Epoch 16/1000, Train Loss: 5.5454, Val Loss: 5.6831\n",
            "Epoch 17/1000, Train Loss: 5.5527, Val Loss: 5.6925\n",
            "Epoch 18/1000, Train Loss: 5.5506, Val Loss: 5.6725\n",
            "Epoch 19/1000, Train Loss: 5.5563, Val Loss: 5.6683\n",
            "Epoch 20/1000, Train Loss: 5.5684, Val Loss: 5.7080\n",
            "Early stopping triggered at epoch 20. Best Val Loss: 5.6664\n",
            "Epoch 1/1000, Train Loss: 5.5637, Val Loss: 5.6733\n",
            "Epoch 2/1000, Train Loss: 5.5474, Val Loss: 5.6787\n",
            "Epoch 3/1000, Train Loss: 5.5548, Val Loss: 5.6747\n",
            "Epoch 4/1000, Train Loss: 5.5513, Val Loss: 5.6664\n",
            "Epoch 5/1000, Train Loss: 5.5438, Val Loss: 5.6728\n",
            "Epoch 6/1000, Train Loss: 5.5556, Val Loss: 5.6646\n",
            "Epoch 7/1000, Train Loss: 5.5450, Val Loss: 5.6681\n",
            "Epoch 8/1000, Train Loss: 5.5491, Val Loss: 5.6675\n",
            "Epoch 9/1000, Train Loss: 5.5532, Val Loss: 5.6655\n",
            "Epoch 10/1000, Train Loss: 5.5457, Val Loss: 5.6719\n",
            "Epoch 11/1000, Train Loss: 5.5517, Val Loss: 5.6719\n",
            "Epoch 12/1000, Train Loss: 5.5492, Val Loss: 5.6711\n",
            "Epoch 13/1000, Train Loss: 5.5507, Val Loss: 5.7909\n",
            "Epoch 14/1000, Train Loss: 5.5439, Val Loss: 5.7165\n",
            "Epoch 15/1000, Train Loss: 5.5445, Val Loss: 5.6723\n",
            "Epoch 16/1000, Train Loss: 5.5503, Val Loss: 5.6771\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 5.6646\n",
            "Epoch 1/1000, Train Loss: 5.5580, Val Loss: 5.6885\n",
            "Epoch 2/1000, Train Loss: 5.5570, Val Loss: 5.6766\n",
            "Epoch 3/1000, Train Loss: 5.5515, Val Loss: 5.6693\n",
            "Epoch 4/1000, Train Loss: 5.5461, Val Loss: 5.6854\n",
            "Epoch 5/1000, Train Loss: 5.5536, Val Loss: 5.6673\n",
            "Epoch 6/1000, Train Loss: 5.5555, Val Loss: 5.6626\n",
            "Epoch 7/1000, Train Loss: 5.5564, Val Loss: 5.6689\n",
            "Epoch 8/1000, Train Loss: 5.5497, Val Loss: 5.6674\n",
            "Epoch 9/1000, Train Loss: 5.5482, Val Loss: 5.6630\n",
            "Epoch 10/1000, Train Loss: 5.5465, Val Loss: 5.6610\n",
            "Epoch 11/1000, Train Loss: 5.5527, Val Loss: 5.6780\n",
            "Epoch 12/1000, Train Loss: 5.5480, Val Loss: 5.6614\n",
            "Epoch 13/1000, Train Loss: 5.5516, Val Loss: 5.6608\n",
            "Epoch 14/1000, Train Loss: 5.5491, Val Loss: 5.6618\n",
            "Epoch 15/1000, Train Loss: 5.5491, Val Loss: 5.6606\n",
            "Epoch 16/1000, Train Loss: 5.5479, Val Loss: 5.6817\n",
            "Epoch 17/1000, Train Loss: 5.5453, Val Loss: 5.6624\n",
            "Epoch 18/1000, Train Loss: 5.5460, Val Loss: 5.6645\n",
            "Epoch 19/1000, Train Loss: 5.5497, Val Loss: 5.6607\n",
            "Epoch 20/1000, Train Loss: 5.5498, Val Loss: 5.6630\n",
            "Epoch 21/1000, Train Loss: 5.5505, Val Loss: 5.6707\n",
            "Epoch 22/1000, Train Loss: 5.5447, Val Loss: 5.6631\n",
            "Epoch 23/1000, Train Loss: 5.5453, Val Loss: 5.6612\n",
            "Epoch 24/1000, Train Loss: 5.5495, Val Loss: 5.6607\n",
            "Epoch 25/1000, Train Loss: 5.5492, Val Loss: 5.6632\n",
            "Early stopping triggered at epoch 25. Best Val Loss: 5.6606\n",
            "Training with LR=0.01, Hidden=16, Dropout=0.1, Scale Factor=1.25\n",
            "Combination 59/81\n",
            "Epoch 1/1000, Train Loss: 6.6347, Val Loss: 6.7574\n",
            "Epoch 2/1000, Train Loss: 6.6126, Val Loss: 6.7690\n",
            "Epoch 3/1000, Train Loss: 6.6170, Val Loss: 6.7495\n",
            "Epoch 4/1000, Train Loss: 6.6025, Val Loss: 6.7674\n",
            "Epoch 5/1000, Train Loss: 6.6086, Val Loss: 6.7631\n",
            "Epoch 6/1000, Train Loss: 6.6069, Val Loss: 6.7558\n",
            "Epoch 7/1000, Train Loss: 6.6062, Val Loss: 6.7552\n",
            "Epoch 8/1000, Train Loss: 6.6175, Val Loss: 6.7713\n",
            "Epoch 9/1000, Train Loss: 6.6145, Val Loss: 6.7521\n",
            "Epoch 10/1000, Train Loss: 6.6251, Val Loss: 6.7529\n",
            "Epoch 11/1000, Train Loss: 6.6104, Val Loss: 6.7586\n",
            "Epoch 12/1000, Train Loss: 6.6085, Val Loss: 6.8042\n",
            "Epoch 13/1000, Train Loss: 6.6085, Val Loss: 6.7621\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 6.7495\n",
            "Epoch 1/1000, Train Loss: 6.6179, Val Loss: 6.7728\n",
            "Epoch 2/1000, Train Loss: 6.6235, Val Loss: 6.7610\n",
            "Epoch 3/1000, Train Loss: 6.6218, Val Loss: 6.7465\n",
            "Epoch 4/1000, Train Loss: 6.6189, Val Loss: 6.7497\n",
            "Epoch 5/1000, Train Loss: 6.6062, Val Loss: 6.7527\n",
            "Epoch 6/1000, Train Loss: 6.6044, Val Loss: 6.7535\n",
            "Epoch 7/1000, Train Loss: 6.6144, Val Loss: 6.7554\n",
            "Epoch 8/1000, Train Loss: 6.6131, Val Loss: 6.7546\n",
            "Epoch 9/1000, Train Loss: 6.6053, Val Loss: 6.7524\n",
            "Epoch 10/1000, Train Loss: 6.6161, Val Loss: 6.7549\n",
            "Epoch 11/1000, Train Loss: 6.6104, Val Loss: 6.7631\n",
            "Epoch 12/1000, Train Loss: 6.6039, Val Loss: 6.7587\n",
            "Epoch 13/1000, Train Loss: 6.6080, Val Loss: 6.7605\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 6.7465\n",
            "Epoch 1/1000, Train Loss: 6.6260, Val Loss: 6.7534\n",
            "Epoch 2/1000, Train Loss: 6.6154, Val Loss: 6.7605\n",
            "Epoch 3/1000, Train Loss: 6.6137, Val Loss: 6.7674\n",
            "Epoch 4/1000, Train Loss: 6.6124, Val Loss: 6.7719\n",
            "Epoch 5/1000, Train Loss: 6.6091, Val Loss: 6.7592\n",
            "Epoch 6/1000, Train Loss: 6.6207, Val Loss: 6.7697\n",
            "Epoch 7/1000, Train Loss: 6.6144, Val Loss: 6.7657\n",
            "Epoch 8/1000, Train Loss: 6.6065, Val Loss: 6.7848\n",
            "Epoch 9/1000, Train Loss: 6.6141, Val Loss: 6.7730\n",
            "Epoch 10/1000, Train Loss: 6.6162, Val Loss: 6.7770\n",
            "Epoch 11/1000, Train Loss: 6.6131, Val Loss: 6.7781\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 6.7534\n",
            "Training with LR=0.01, Hidden=16, Dropout=0.1, Scale Factor=1.5\n",
            "Combination 60/81\n",
            "Epoch 1/1000, Train Loss: 7.6533, Val Loss: 7.8172\n",
            "Epoch 2/1000, Train Loss: 7.6755, Val Loss: 7.8137\n",
            "Epoch 3/1000, Train Loss: 7.6574, Val Loss: 7.8425\n",
            "Epoch 4/1000, Train Loss: 7.6408, Val Loss: 7.8188\n",
            "Epoch 5/1000, Train Loss: 7.6553, Val Loss: 7.8175\n",
            "Epoch 6/1000, Train Loss: 7.6451, Val Loss: 7.8253\n",
            "Epoch 7/1000, Train Loss: 7.6439, Val Loss: 7.8273\n",
            "Epoch 8/1000, Train Loss: 7.6414, Val Loss: 7.8210\n",
            "Epoch 9/1000, Train Loss: 7.6398, Val Loss: 7.8249\n",
            "Epoch 10/1000, Train Loss: 7.6864, Val Loss: 7.8209\n",
            "Epoch 11/1000, Train Loss: 7.6371, Val Loss: 7.8333\n",
            "Epoch 12/1000, Train Loss: 7.6441, Val Loss: 7.8213\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 7.8137\n",
            "Epoch 1/1000, Train Loss: 7.6860, Val Loss: 7.8639\n",
            "Epoch 2/1000, Train Loss: 7.6548, Val Loss: 7.8177\n",
            "Epoch 3/1000, Train Loss: 7.6465, Val Loss: 7.8185\n",
            "Epoch 4/1000, Train Loss: 7.6482, Val Loss: 7.8277\n",
            "Epoch 5/1000, Train Loss: 7.6545, Val Loss: 7.8183\n",
            "Epoch 6/1000, Train Loss: 7.6581, Val Loss: 7.8128\n",
            "Epoch 7/1000, Train Loss: 7.6481, Val Loss: 7.8240\n",
            "Epoch 8/1000, Train Loss: 7.6502, Val Loss: 7.8240\n",
            "Epoch 9/1000, Train Loss: 7.6490, Val Loss: 7.8304\n",
            "Epoch 10/1000, Train Loss: 7.6476, Val Loss: 7.8216\n",
            "Epoch 11/1000, Train Loss: 7.6491, Val Loss: 7.8150\n",
            "Epoch 12/1000, Train Loss: 7.6382, Val Loss: 7.8197\n",
            "Epoch 13/1000, Train Loss: 7.6439, Val Loss: 7.8290\n",
            "Epoch 14/1000, Train Loss: 7.6451, Val Loss: 7.8226\n",
            "Epoch 15/1000, Train Loss: 7.6503, Val Loss: 7.8320\n",
            "Epoch 16/1000, Train Loss: 7.6465, Val Loss: 7.8225\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 7.8128\n",
            "Epoch 1/1000, Train Loss: 7.6550, Val Loss: 7.8278\n",
            "Epoch 2/1000, Train Loss: 7.6501, Val Loss: 7.8216\n",
            "Epoch 3/1000, Train Loss: 7.6468, Val Loss: 7.8263\n",
            "Epoch 4/1000, Train Loss: 7.6533, Val Loss: 7.8256\n",
            "Epoch 5/1000, Train Loss: 7.6434, Val Loss: 7.8292\n",
            "Epoch 6/1000, Train Loss: 7.6543, Val Loss: 7.8230\n",
            "Epoch 7/1000, Train Loss: 7.6434, Val Loss: 7.8325\n",
            "Epoch 8/1000, Train Loss: 7.6500, Val Loss: 7.8300\n",
            "Epoch 9/1000, Train Loss: 7.6460, Val Loss: 7.8285\n",
            "Epoch 10/1000, Train Loss: 7.6443, Val Loss: 7.8263\n",
            "Epoch 11/1000, Train Loss: 7.7405, Val Loss: 7.8351\n",
            "Epoch 12/1000, Train Loss: 7.6510, Val Loss: 7.8289\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 7.8216\n",
            "Training with LR=0.01, Hidden=16, Dropout=0.2, Scale Factor=1.0\n",
            "Combination 61/81\n",
            "Epoch 1/1000, Train Loss: 5.5629, Val Loss: 5.6754\n",
            "Epoch 2/1000, Train Loss: 5.5515, Val Loss: 5.7239\n",
            "Epoch 3/1000, Train Loss: 5.5624, Val Loss: 5.6656\n",
            "Epoch 4/1000, Train Loss: 5.5576, Val Loss: 5.6653\n",
            "Epoch 5/1000, Train Loss: 5.5543, Val Loss: 5.6749\n",
            "Epoch 6/1000, Train Loss: 5.5444, Val Loss: 5.6805\n",
            "Epoch 7/1000, Train Loss: 5.5456, Val Loss: 5.6704\n",
            "Epoch 8/1000, Train Loss: 5.5509, Val Loss: 5.6668\n",
            "Epoch 9/1000, Train Loss: 5.5558, Val Loss: 5.6637\n",
            "Epoch 10/1000, Train Loss: 5.5477, Val Loss: 5.6666\n",
            "Epoch 11/1000, Train Loss: 5.5551, Val Loss: 5.6693\n",
            "Epoch 12/1000, Train Loss: 5.5406, Val Loss: 5.6643\n",
            "Epoch 13/1000, Train Loss: 5.5519, Val Loss: 5.6670\n",
            "Epoch 14/1000, Train Loss: 5.5509, Val Loss: 5.6683\n",
            "Epoch 15/1000, Train Loss: 5.5418, Val Loss: 5.6692\n",
            "Epoch 16/1000, Train Loss: 5.5464, Val Loss: 5.6819\n",
            "Epoch 17/1000, Train Loss: 5.5454, Val Loss: 5.6748\n",
            "Epoch 18/1000, Train Loss: 5.5473, Val Loss: 5.6672\n",
            "Epoch 19/1000, Train Loss: 5.5454, Val Loss: 5.6810\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 5.6637\n",
            "Epoch 1/1000, Train Loss: 5.5686, Val Loss: 5.6740\n",
            "Epoch 2/1000, Train Loss: 5.5611, Val Loss: 5.6680\n",
            "Epoch 3/1000, Train Loss: 5.5589, Val Loss: 5.6716\n",
            "Epoch 4/1000, Train Loss: 5.5616, Val Loss: 5.6614\n",
            "Epoch 5/1000, Train Loss: 5.5492, Val Loss: 5.6796\n",
            "Epoch 6/1000, Train Loss: 5.5497, Val Loss: 5.6611\n",
            "Epoch 7/1000, Train Loss: 5.5503, Val Loss: 5.6611\n",
            "Epoch 8/1000, Train Loss: 5.5568, Val Loss: 5.6778\n",
            "Epoch 9/1000, Train Loss: 5.5515, Val Loss: 5.6679\n",
            "Epoch 10/1000, Train Loss: 5.5573, Val Loss: 5.6626\n",
            "Epoch 11/1000, Train Loss: 5.5456, Val Loss: 5.6619\n",
            "Epoch 12/1000, Train Loss: 5.5505, Val Loss: 5.6906\n",
            "Epoch 13/1000, Train Loss: 5.5503, Val Loss: 5.6656\n",
            "Epoch 14/1000, Train Loss: 5.5486, Val Loss: 5.6610\n",
            "Epoch 15/1000, Train Loss: 5.5532, Val Loss: 5.6748\n",
            "Epoch 16/1000, Train Loss: 5.5502, Val Loss: 5.6649\n",
            "Epoch 17/1000, Train Loss: 5.5570, Val Loss: 5.6640\n",
            "Epoch 18/1000, Train Loss: 5.5530, Val Loss: 5.6720\n",
            "Epoch 19/1000, Train Loss: 5.5500, Val Loss: 5.6681\n",
            "Epoch 20/1000, Train Loss: 5.5512, Val Loss: 5.6982\n",
            "Epoch 21/1000, Train Loss: 5.5545, Val Loss: 5.6812\n",
            "Epoch 22/1000, Train Loss: 5.5566, Val Loss: 5.6617\n",
            "Epoch 23/1000, Train Loss: 5.5459, Val Loss: 5.6640\n",
            "Epoch 24/1000, Train Loss: 5.5544, Val Loss: 5.6611\n",
            "Early stopping triggered at epoch 24. Best Val Loss: 5.6610\n",
            "Epoch 1/1000, Train Loss: 5.5617, Val Loss: 5.6661\n",
            "Epoch 2/1000, Train Loss: 5.5595, Val Loss: 5.6725\n",
            "Epoch 3/1000, Train Loss: 5.5588, Val Loss: 5.6630\n",
            "Epoch 4/1000, Train Loss: 5.5498, Val Loss: 5.6656\n",
            "Epoch 5/1000, Train Loss: 5.5502, Val Loss: 5.6637\n",
            "Epoch 6/1000, Train Loss: 5.5496, Val Loss: 5.6636\n",
            "Epoch 7/1000, Train Loss: 5.5545, Val Loss: 5.6627\n",
            "Epoch 8/1000, Train Loss: 5.5535, Val Loss: 5.6623\n",
            "Epoch 9/1000, Train Loss: 5.5479, Val Loss: 5.6699\n",
            "Epoch 10/1000, Train Loss: 5.5436, Val Loss: 5.6772\n",
            "Epoch 11/1000, Train Loss: 5.5442, Val Loss: 5.6639\n",
            "Epoch 12/1000, Train Loss: 5.5462, Val Loss: 5.6774\n",
            "Epoch 13/1000, Train Loss: 5.5524, Val Loss: 5.6666\n",
            "Epoch 14/1000, Train Loss: 5.5490, Val Loss: 5.6645\n",
            "Epoch 15/1000, Train Loss: 5.5523, Val Loss: 5.6635\n",
            "Epoch 16/1000, Train Loss: 5.5478, Val Loss: 5.6776\n",
            "Epoch 17/1000, Train Loss: 5.5502, Val Loss: 5.6624\n",
            "Epoch 18/1000, Train Loss: 5.5504, Val Loss: 5.6625\n",
            "Early stopping triggered at epoch 18. Best Val Loss: 5.6623\n",
            "Training with LR=0.01, Hidden=16, Dropout=0.2, Scale Factor=1.25\n",
            "Combination 62/81\n",
            "Epoch 1/1000, Train Loss: 6.6252, Val Loss: 6.7686\n",
            "Epoch 2/1000, Train Loss: 6.6134, Val Loss: 6.7585\n",
            "Epoch 3/1000, Train Loss: 6.6221, Val Loss: 6.7529\n",
            "Epoch 4/1000, Train Loss: 6.6062, Val Loss: 6.7664\n",
            "Epoch 5/1000, Train Loss: 6.6131, Val Loss: 6.7490\n",
            "Epoch 6/1000, Train Loss: 6.6187, Val Loss: 6.7527\n",
            "Epoch 7/1000, Train Loss: 6.6057, Val Loss: 6.7497\n",
            "Epoch 8/1000, Train Loss: 6.6107, Val Loss: 6.7527\n",
            "Epoch 9/1000, Train Loss: 6.6036, Val Loss: 6.7540\n",
            "Epoch 10/1000, Train Loss: 6.6088, Val Loss: 6.7713\n",
            "Epoch 11/1000, Train Loss: 6.6148, Val Loss: 6.7603\n",
            "Epoch 12/1000, Train Loss: 6.6113, Val Loss: 6.7701\n",
            "Epoch 13/1000, Train Loss: 6.6052, Val Loss: 6.7672\n",
            "Epoch 14/1000, Train Loss: 6.6185, Val Loss: 6.7620\n",
            "Epoch 15/1000, Train Loss: 6.6068, Val Loss: 6.7585\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 6.7490\n",
            "Epoch 1/1000, Train Loss: 6.6345, Val Loss: 6.7697\n",
            "Epoch 2/1000, Train Loss: 6.6050, Val Loss: 6.7724\n",
            "Epoch 3/1000, Train Loss: 6.6127, Val Loss: 6.7522\n",
            "Epoch 4/1000, Train Loss: 6.6231, Val Loss: 6.7489\n",
            "Epoch 5/1000, Train Loss: 6.6201, Val Loss: 6.7606\n",
            "Epoch 6/1000, Train Loss: 6.6101, Val Loss: 6.7660\n",
            "Epoch 7/1000, Train Loss: 6.6159, Val Loss: 6.7538\n",
            "Epoch 8/1000, Train Loss: 6.6175, Val Loss: 6.7548\n",
            "Epoch 9/1000, Train Loss: 6.6078, Val Loss: 6.7527\n",
            "Epoch 10/1000, Train Loss: 6.6124, Val Loss: 6.7554\n",
            "Epoch 11/1000, Train Loss: 6.6087, Val Loss: 6.7540\n",
            "Epoch 12/1000, Train Loss: 6.6057, Val Loss: 6.7643\n",
            "Epoch 13/1000, Train Loss: 6.6094, Val Loss: 6.7584\n",
            "Epoch 14/1000, Train Loss: 6.6063, Val Loss: 6.7642\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 6.7489\n",
            "Epoch 1/1000, Train Loss: 6.6170, Val Loss: 6.7699\n",
            "Epoch 2/1000, Train Loss: 6.6160, Val Loss: 6.7682\n",
            "Epoch 3/1000, Train Loss: 6.6169, Val Loss: 6.7496\n",
            "Epoch 4/1000, Train Loss: 6.6142, Val Loss: 6.7765\n",
            "Epoch 5/1000, Train Loss: 6.6197, Val Loss: 6.7757\n",
            "Epoch 6/1000, Train Loss: 6.6092, Val Loss: 6.7720\n",
            "Epoch 7/1000, Train Loss: 6.6142, Val Loss: 6.7699\n",
            "Epoch 8/1000, Train Loss: 6.6122, Val Loss: 6.7702\n",
            "Epoch 9/1000, Train Loss: 6.6166, Val Loss: 6.7619\n",
            "Epoch 10/1000, Train Loss: 6.6059, Val Loss: 6.7650\n",
            "Epoch 11/1000, Train Loss: 6.6069, Val Loss: 6.7643\n",
            "Epoch 12/1000, Train Loss: 6.6045, Val Loss: 6.7620\n",
            "Epoch 13/1000, Train Loss: 6.6120, Val Loss: 6.7569\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 6.7496\n",
            "Training with LR=0.01, Hidden=16, Dropout=0.2, Scale Factor=1.5\n",
            "Combination 63/81\n",
            "Epoch 1/1000, Train Loss: 7.6691, Val Loss: 7.8159\n",
            "Epoch 2/1000, Train Loss: 7.6524, Val Loss: 7.8176\n",
            "Epoch 3/1000, Train Loss: 7.6491, Val Loss: 7.8285\n",
            "Epoch 4/1000, Train Loss: 7.6450, Val Loss: 7.8188\n",
            "Epoch 5/1000, Train Loss: 7.6555, Val Loss: 7.8195\n",
            "Epoch 6/1000, Train Loss: 7.6532, Val Loss: 7.8245\n",
            "Epoch 7/1000, Train Loss: 7.6451, Val Loss: 7.8217\n",
            "Epoch 8/1000, Train Loss: 7.6434, Val Loss: 7.8226\n",
            "Epoch 9/1000, Train Loss: 7.6483, Val Loss: 7.8198\n",
            "Epoch 10/1000, Train Loss: 7.6470, Val Loss: 7.8208\n",
            "Epoch 11/1000, Train Loss: 7.6416, Val Loss: 7.8211\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8159\n",
            "Epoch 1/1000, Train Loss: 7.6752, Val Loss: 7.8132\n",
            "Epoch 2/1000, Train Loss: 7.6493, Val Loss: 7.8149\n",
            "Epoch 3/1000, Train Loss: 7.6478, Val Loss: 7.8178\n",
            "Epoch 4/1000, Train Loss: 7.6453, Val Loss: 7.8221\n",
            "Epoch 5/1000, Train Loss: 7.6542, Val Loss: 7.8206\n",
            "Epoch 6/1000, Train Loss: 7.6598, Val Loss: 7.8217\n",
            "Epoch 7/1000, Train Loss: 7.6446, Val Loss: 7.8316\n",
            "Epoch 8/1000, Train Loss: 7.6485, Val Loss: 7.8273\n",
            "Epoch 9/1000, Train Loss: 7.6487, Val Loss: 7.8219\n",
            "Epoch 10/1000, Train Loss: 7.6467, Val Loss: 7.8263\n",
            "Epoch 11/1000, Train Loss: 7.6738, Val Loss: 7.8255\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8132\n",
            "Epoch 1/1000, Train Loss: 7.6756, Val Loss: 7.8328\n",
            "Epoch 2/1000, Train Loss: 7.6549, Val Loss: 7.8191\n",
            "Epoch 3/1000, Train Loss: 7.6603, Val Loss: 7.8153\n",
            "Epoch 4/1000, Train Loss: 7.6703, Val Loss: 7.8228\n",
            "Epoch 5/1000, Train Loss: 7.6569, Val Loss: 7.8239\n",
            "Epoch 6/1000, Train Loss: 7.6533, Val Loss: 7.8306\n",
            "Epoch 7/1000, Train Loss: 7.6497, Val Loss: 7.8286\n",
            "Epoch 8/1000, Train Loss: 7.6550, Val Loss: 7.8271\n",
            "Epoch 9/1000, Train Loss: 7.6609, Val Loss: 7.8373\n",
            "Epoch 10/1000, Train Loss: 7.6408, Val Loss: 7.8274\n",
            "Epoch 11/1000, Train Loss: 7.6549, Val Loss: 7.8324\n",
            "Epoch 12/1000, Train Loss: 7.6498, Val Loss: 7.8275\n",
            "Epoch 13/1000, Train Loss: 7.6780, Val Loss: 7.8317\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 7.8153\n",
            "Training with LR=0.01, Hidden=32, Dropout=0.0, Scale Factor=1.0\n",
            "Combination 64/81\n",
            "Epoch 1/1000, Train Loss: 5.5621, Val Loss: 5.6718\n",
            "Epoch 2/1000, Train Loss: 5.5581, Val Loss: 5.6710\n",
            "Epoch 3/1000, Train Loss: 5.5468, Val Loss: 5.6815\n",
            "Epoch 4/1000, Train Loss: 5.5519, Val Loss: 5.6679\n",
            "Epoch 5/1000, Train Loss: 5.5510, Val Loss: 5.6740\n",
            "Epoch 6/1000, Train Loss: 5.5512, Val Loss: 5.6908\n",
            "Epoch 7/1000, Train Loss: 5.5478, Val Loss: 5.6673\n",
            "Epoch 8/1000, Train Loss: 5.5528, Val Loss: 5.6773\n",
            "Epoch 9/1000, Train Loss: 5.5551, Val Loss: 5.6675\n",
            "Epoch 10/1000, Train Loss: 5.5484, Val Loss: 5.6668\n",
            "Epoch 11/1000, Train Loss: 5.5461, Val Loss: 5.6702\n",
            "Epoch 12/1000, Train Loss: 5.5469, Val Loss: 5.6682\n",
            "Epoch 13/1000, Train Loss: 5.5499, Val Loss: 5.6671\n",
            "Epoch 14/1000, Train Loss: 5.5485, Val Loss: 5.6691\n",
            "Epoch 15/1000, Train Loss: 5.5469, Val Loss: 5.6696\n",
            "Epoch 16/1000, Train Loss: 5.5470, Val Loss: 5.6663\n",
            "Epoch 17/1000, Train Loss: 5.5449, Val Loss: 5.6685\n",
            "Epoch 18/1000, Train Loss: 5.5484, Val Loss: 5.6729\n",
            "Epoch 19/1000, Train Loss: 5.5413, Val Loss: 5.6719\n",
            "Epoch 20/1000, Train Loss: 5.5549, Val Loss: 5.6695\n",
            "Epoch 21/1000, Train Loss: 5.5453, Val Loss: 5.6726\n",
            "Epoch 22/1000, Train Loss: 5.5428, Val Loss: 5.6997\n",
            "Epoch 23/1000, Train Loss: 5.5467, Val Loss: 5.6748\n",
            "Epoch 24/1000, Train Loss: 5.5444, Val Loss: 5.6645\n",
            "Epoch 25/1000, Train Loss: 5.5601, Val Loss: 5.6733\n",
            "Epoch 26/1000, Train Loss: 5.5506, Val Loss: 5.6685\n",
            "Epoch 27/1000, Train Loss: 5.5437, Val Loss: 5.6816\n",
            "Epoch 28/1000, Train Loss: 5.5489, Val Loss: 5.6639\n",
            "Epoch 29/1000, Train Loss: 5.5463, Val Loss: 5.6788\n",
            "Epoch 30/1000, Train Loss: 5.5452, Val Loss: 5.6653\n",
            "Epoch 31/1000, Train Loss: 5.5455, Val Loss: 5.6732\n",
            "Epoch 32/1000, Train Loss: 5.5610, Val Loss: 5.6673\n",
            "Epoch 33/1000, Train Loss: 5.5546, Val Loss: 5.6686\n",
            "Epoch 34/1000, Train Loss: 5.5511, Val Loss: 5.6703\n",
            "Epoch 35/1000, Train Loss: 5.5438, Val Loss: 5.6670\n",
            "Epoch 36/1000, Train Loss: 5.5479, Val Loss: 5.6701\n",
            "Epoch 37/1000, Train Loss: 5.5430, Val Loss: 5.6771\n",
            "Epoch 38/1000, Train Loss: 5.5400, Val Loss: 5.6673\n",
            "Early stopping triggered at epoch 38. Best Val Loss: 5.6639\n",
            "Epoch 1/1000, Train Loss: 5.5644, Val Loss: 5.7042\n",
            "Epoch 2/1000, Train Loss: 5.5517, Val Loss: 5.6851\n",
            "Epoch 3/1000, Train Loss: 5.5524, Val Loss: 5.6744\n",
            "Epoch 4/1000, Train Loss: 5.5490, Val Loss: 5.6738\n",
            "Epoch 5/1000, Train Loss: 5.5475, Val Loss: 5.6671\n",
            "Epoch 6/1000, Train Loss: 5.5482, Val Loss: 5.6710\n",
            "Epoch 7/1000, Train Loss: 5.5499, Val Loss: 5.6656\n",
            "Epoch 8/1000, Train Loss: 5.5514, Val Loss: 5.6680\n",
            "Epoch 9/1000, Train Loss: 5.5482, Val Loss: 5.6978\n",
            "Epoch 10/1000, Train Loss: 5.5508, Val Loss: 5.7093\n",
            "Epoch 11/1000, Train Loss: 5.5530, Val Loss: 5.6735\n",
            "Epoch 12/1000, Train Loss: 5.5431, Val Loss: 5.6695\n",
            "Epoch 13/1000, Train Loss: 5.5447, Val Loss: 5.6860\n",
            "Epoch 14/1000, Train Loss: 5.5464, Val Loss: 5.6693\n",
            "Epoch 15/1000, Train Loss: 5.5449, Val Loss: 5.6669\n",
            "Epoch 16/1000, Train Loss: 5.5421, Val Loss: 5.6786\n",
            "Epoch 17/1000, Train Loss: 5.5455, Val Loss: 5.6716\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 5.6656\n",
            "Epoch 1/1000, Train Loss: 5.5672, Val Loss: 5.6722\n",
            "Epoch 2/1000, Train Loss: 5.5557, Val Loss: 5.6653\n",
            "Epoch 3/1000, Train Loss: 5.5502, Val Loss: 5.6814\n",
            "Epoch 4/1000, Train Loss: 5.5480, Val Loss: 5.6730\n",
            "Epoch 5/1000, Train Loss: 5.5566, Val Loss: 5.6691\n",
            "Epoch 6/1000, Train Loss: 5.5481, Val Loss: 5.6641\n",
            "Epoch 7/1000, Train Loss: 5.5554, Val Loss: 5.6609\n",
            "Epoch 8/1000, Train Loss: 5.5491, Val Loss: 5.6747\n",
            "Epoch 9/1000, Train Loss: 5.5489, Val Loss: 5.6626\n",
            "Epoch 10/1000, Train Loss: 5.5544, Val Loss: 5.6628\n",
            "Epoch 11/1000, Train Loss: 5.5451, Val Loss: 5.6649\n",
            "Epoch 12/1000, Train Loss: 5.5436, Val Loss: 5.6749\n",
            "Epoch 13/1000, Train Loss: 5.5417, Val Loss: 5.6637\n",
            "Epoch 14/1000, Train Loss: 5.5435, Val Loss: 5.7002\n",
            "Epoch 15/1000, Train Loss: 5.5435, Val Loss: 5.6841\n",
            "Epoch 16/1000, Train Loss: 5.5483, Val Loss: 5.6692\n",
            "Epoch 17/1000, Train Loss: 5.5433, Val Loss: 5.6768\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 5.6609\n",
            "Training with LR=0.01, Hidden=32, Dropout=0.0, Scale Factor=1.25\n",
            "Combination 65/81\n",
            "Epoch 1/1000, Train Loss: 6.6223, Val Loss: 6.7513\n",
            "Epoch 2/1000, Train Loss: 6.6069, Val Loss: 6.7596\n",
            "Epoch 3/1000, Train Loss: 6.6124, Val Loss: 6.7707\n",
            "Epoch 4/1000, Train Loss: 6.6292, Val Loss: 6.7829\n",
            "Epoch 5/1000, Train Loss: 6.6154, Val Loss: 6.7560\n",
            "Epoch 6/1000, Train Loss: 6.6108, Val Loss: 6.7678\n",
            "Epoch 7/1000, Train Loss: 6.6092, Val Loss: 6.7675\n",
            "Epoch 8/1000, Train Loss: 6.6110, Val Loss: 6.7983\n",
            "Epoch 9/1000, Train Loss: 6.6094, Val Loss: 6.7843\n",
            "Epoch 10/1000, Train Loss: 6.6066, Val Loss: 6.7842\n",
            "Epoch 11/1000, Train Loss: 6.6078, Val Loss: 6.7516\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 6.7513\n",
            "Epoch 1/1000, Train Loss: 6.6260, Val Loss: 6.7685\n",
            "Epoch 2/1000, Train Loss: 6.6134, Val Loss: 6.7523\n",
            "Epoch 3/1000, Train Loss: 6.6280, Val Loss: 6.7523\n",
            "Epoch 4/1000, Train Loss: 6.6213, Val Loss: 6.7556\n",
            "Epoch 5/1000, Train Loss: 6.6101, Val Loss: 6.7665\n",
            "Epoch 6/1000, Train Loss: 6.6063, Val Loss: 6.7786\n",
            "Epoch 7/1000, Train Loss: 6.6158, Val Loss: 6.7607\n",
            "Epoch 8/1000, Train Loss: 6.6061, Val Loss: 6.7584\n",
            "Epoch 9/1000, Train Loss: 6.6098, Val Loss: 6.7539\n",
            "Epoch 10/1000, Train Loss: 6.6052, Val Loss: 6.8349\n",
            "Epoch 11/1000, Train Loss: 6.6013, Val Loss: 6.7566\n",
            "Epoch 12/1000, Train Loss: 6.6076, Val Loss: 6.7563\n",
            "Epoch 13/1000, Train Loss: 6.6058, Val Loss: 6.7582\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 6.7523\n",
            "Epoch 1/1000, Train Loss: 6.6309, Val Loss: 6.7529\n",
            "Epoch 2/1000, Train Loss: 6.6187, Val Loss: 6.7523\n",
            "Epoch 3/1000, Train Loss: 6.6187, Val Loss: 6.7489\n",
            "Epoch 4/1000, Train Loss: 6.6081, Val Loss: 6.7503\n",
            "Epoch 5/1000, Train Loss: 6.6086, Val Loss: 6.7841\n",
            "Epoch 6/1000, Train Loss: 6.6143, Val Loss: 6.7567\n",
            "Epoch 7/1000, Train Loss: 6.6061, Val Loss: 6.7541\n",
            "Epoch 8/1000, Train Loss: 6.6067, Val Loss: 6.7568\n",
            "Epoch 9/1000, Train Loss: 6.6154, Val Loss: 6.7748\n",
            "Epoch 10/1000, Train Loss: 6.5988, Val Loss: 6.7740\n",
            "Epoch 11/1000, Train Loss: 6.5987, Val Loss: 6.7598\n",
            "Epoch 12/1000, Train Loss: 6.6073, Val Loss: 6.7512\n",
            "Epoch 13/1000, Train Loss: 6.5997, Val Loss: 6.7704\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 6.7489\n",
            "Training with LR=0.01, Hidden=32, Dropout=0.0, Scale Factor=1.5\n",
            "Combination 66/81\n",
            "Epoch 1/1000, Train Loss: 7.6590, Val Loss: 7.8270\n",
            "Epoch 2/1000, Train Loss: 7.6513, Val Loss: 7.8264\n",
            "Epoch 3/1000, Train Loss: 7.6440, Val Loss: 7.8260\n",
            "Epoch 4/1000, Train Loss: 7.6532, Val Loss: 7.8321\n",
            "Epoch 5/1000, Train Loss: 7.6543, Val Loss: 7.8200\n",
            "Epoch 6/1000, Train Loss: 7.6716, Val Loss: 8.3683\n",
            "Epoch 7/1000, Train Loss: 7.6493, Val Loss: 7.8219\n",
            "Epoch 8/1000, Train Loss: 7.6379, Val Loss: 7.8226\n",
            "Epoch 9/1000, Train Loss: 7.6470, Val Loss: 7.8210\n",
            "Epoch 10/1000, Train Loss: 7.6428, Val Loss: 7.8260\n",
            "Epoch 11/1000, Train Loss: 7.6532, Val Loss: 7.8236\n",
            "Epoch 12/1000, Train Loss: 7.6455, Val Loss: 7.8226\n",
            "Epoch 13/1000, Train Loss: 7.6476, Val Loss: 7.8255\n",
            "Epoch 14/1000, Train Loss: 7.6438, Val Loss: 7.8228\n",
            "Epoch 15/1000, Train Loss: 7.6440, Val Loss: 7.8221\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 7.8200\n",
            "Epoch 1/1000, Train Loss: 7.6726, Val Loss: 7.8251\n",
            "Epoch 2/1000, Train Loss: 7.6618, Val Loss: 7.8185\n",
            "Epoch 3/1000, Train Loss: 7.6506, Val Loss: 7.8187\n",
            "Epoch 4/1000, Train Loss: 7.6477, Val Loss: 7.8165\n",
            "Epoch 5/1000, Train Loss: 7.6435, Val Loss: 7.8203\n",
            "Epoch 6/1000, Train Loss: 7.6412, Val Loss: 7.8171\n",
            "Epoch 7/1000, Train Loss: 7.6452, Val Loss: 7.8304\n",
            "Epoch 8/1000, Train Loss: 7.6389, Val Loss: 7.8206\n",
            "Epoch 9/1000, Train Loss: 7.6478, Val Loss: 7.8277\n",
            "Epoch 10/1000, Train Loss: 7.6406, Val Loss: 7.8217\n",
            "Epoch 11/1000, Train Loss: 7.6391, Val Loss: 7.8275\n",
            "Epoch 12/1000, Train Loss: 7.6469, Val Loss: 7.8245\n",
            "Epoch 13/1000, Train Loss: 7.6473, Val Loss: 7.8201\n",
            "Epoch 14/1000, Train Loss: 7.6609, Val Loss: 7.8209\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 7.8165\n",
            "Epoch 1/1000, Train Loss: 7.6667, Val Loss: 7.8231\n",
            "Epoch 2/1000, Train Loss: 7.6469, Val Loss: 7.8260\n",
            "Epoch 3/1000, Train Loss: 7.6638, Val Loss: 7.8150\n",
            "Epoch 4/1000, Train Loss: 7.6461, Val Loss: 7.8302\n",
            "Epoch 5/1000, Train Loss: 7.6448, Val Loss: 7.8252\n",
            "Epoch 6/1000, Train Loss: 7.6443, Val Loss: 7.8315\n",
            "Epoch 7/1000, Train Loss: 7.6594, Val Loss: 7.8215\n",
            "Epoch 8/1000, Train Loss: 7.6509, Val Loss: 7.8272\n",
            "Epoch 9/1000, Train Loss: 7.6499, Val Loss: 7.8305\n",
            "Epoch 10/1000, Train Loss: 7.6471, Val Loss: 7.8130\n",
            "Epoch 11/1000, Train Loss: 7.6500, Val Loss: 7.8221\n",
            "Epoch 12/1000, Train Loss: 7.6476, Val Loss: 7.8221\n",
            "Epoch 13/1000, Train Loss: 7.6403, Val Loss: 7.8350\n",
            "Epoch 14/1000, Train Loss: 7.6464, Val Loss: 7.8142\n",
            "Epoch 15/1000, Train Loss: 7.6396, Val Loss: 7.8201\n",
            "Epoch 16/1000, Train Loss: 7.6413, Val Loss: 7.8440\n",
            "Epoch 17/1000, Train Loss: 7.6397, Val Loss: 7.8213\n",
            "Epoch 18/1000, Train Loss: 7.6452, Val Loss: 7.8258\n",
            "Epoch 19/1000, Train Loss: 7.6386, Val Loss: 7.8334\n",
            "Epoch 20/1000, Train Loss: 7.6405, Val Loss: 7.8190\n",
            "Early stopping triggered at epoch 20. Best Val Loss: 7.8130\n",
            "Training with LR=0.01, Hidden=32, Dropout=0.1, Scale Factor=1.0\n",
            "Combination 67/81\n",
            "Epoch 1/1000, Train Loss: 5.5687, Val Loss: 5.6879\n",
            "Epoch 2/1000, Train Loss: 5.5554, Val Loss: 5.6736\n",
            "Epoch 3/1000, Train Loss: 5.5569, Val Loss: 5.6644\n",
            "Epoch 4/1000, Train Loss: 5.5555, Val Loss: 5.6612\n",
            "Epoch 5/1000, Train Loss: 5.5495, Val Loss: 5.6636\n",
            "Epoch 6/1000, Train Loss: 5.5494, Val Loss: 5.6616\n",
            "Epoch 7/1000, Train Loss: 5.5595, Val Loss: 5.6833\n",
            "Epoch 8/1000, Train Loss: 5.5517, Val Loss: 5.6635\n",
            "Epoch 9/1000, Train Loss: 5.5567, Val Loss: 5.6609\n",
            "Epoch 10/1000, Train Loss: 5.5562, Val Loss: 5.6662\n",
            "Epoch 11/1000, Train Loss: 5.5602, Val Loss: 5.7087\n",
            "Epoch 12/1000, Train Loss: 5.5541, Val Loss: 5.6754\n",
            "Epoch 13/1000, Train Loss: 5.5509, Val Loss: 5.6692\n",
            "Epoch 14/1000, Train Loss: 5.5469, Val Loss: 5.6648\n",
            "Epoch 15/1000, Train Loss: 5.5529, Val Loss: 5.6716\n",
            "Epoch 16/1000, Train Loss: 5.5500, Val Loss: 5.6920\n",
            "Epoch 17/1000, Train Loss: 5.5493, Val Loss: 5.6618\n",
            "Epoch 18/1000, Train Loss: 5.5474, Val Loss: 5.6607\n",
            "Epoch 19/1000, Train Loss: 5.5528, Val Loss: 5.6641\n",
            "Epoch 20/1000, Train Loss: 5.5506, Val Loss: 5.6623\n",
            "Epoch 21/1000, Train Loss: 5.5488, Val Loss: 5.6733\n",
            "Epoch 22/1000, Train Loss: 5.5515, Val Loss: 5.6648\n",
            "Epoch 23/1000, Train Loss: 5.5511, Val Loss: 5.6611\n",
            "Epoch 24/1000, Train Loss: 5.5505, Val Loss: 5.6637\n",
            "Epoch 25/1000, Train Loss: 5.5536, Val Loss: 5.6739\n",
            "Epoch 26/1000, Train Loss: 5.5483, Val Loss: 5.6607\n",
            "Epoch 27/1000, Train Loss: 5.5569, Val Loss: 5.6626\n",
            "Epoch 28/1000, Train Loss: 5.5538, Val Loss: 5.6788\n",
            "Epoch 29/1000, Train Loss: 5.5540, Val Loss: 5.6690\n",
            "Epoch 30/1000, Train Loss: 5.5511, Val Loss: 5.6760\n",
            "Epoch 31/1000, Train Loss: 5.5507, Val Loss: 5.6634\n",
            "Epoch 32/1000, Train Loss: 5.5626, Val Loss: 5.6913\n",
            "Epoch 33/1000, Train Loss: 5.5531, Val Loss: 5.6659\n",
            "Epoch 34/1000, Train Loss: 5.5546, Val Loss: 5.6611\n",
            "Epoch 35/1000, Train Loss: 5.5546, Val Loss: 5.6622\n",
            "Epoch 36/1000, Train Loss: 5.5481, Val Loss: 5.6703\n",
            "Early stopping triggered at epoch 36. Best Val Loss: 5.6607\n",
            "Epoch 1/1000, Train Loss: 5.5728, Val Loss: 5.6708\n",
            "Epoch 2/1000, Train Loss: 5.5514, Val Loss: 5.6733\n",
            "Epoch 3/1000, Train Loss: 5.5587, Val Loss: 5.6780\n",
            "Epoch 4/1000, Train Loss: 5.5503, Val Loss: 5.6677\n",
            "Epoch 5/1000, Train Loss: 5.5530, Val Loss: 5.6845\n",
            "Epoch 6/1000, Train Loss: 5.5491, Val Loss: 5.6696\n",
            "Epoch 7/1000, Train Loss: 5.5576, Val Loss: 5.6887\n",
            "Epoch 8/1000, Train Loss: 5.5491, Val Loss: 5.6850\n",
            "Epoch 9/1000, Train Loss: 5.5507, Val Loss: 5.6755\n",
            "Epoch 10/1000, Train Loss: 5.5548, Val Loss: 5.6836\n",
            "Epoch 11/1000, Train Loss: 5.5499, Val Loss: 5.7516\n",
            "Epoch 12/1000, Train Loss: 5.5482, Val Loss: 5.6678\n",
            "Epoch 13/1000, Train Loss: 5.5539, Val Loss: 5.6752\n",
            "Epoch 14/1000, Train Loss: 5.5530, Val Loss: 5.6860\n",
            "Early stopping triggered at epoch 14. Best Val Loss: 5.6677\n",
            "Epoch 1/1000, Train Loss: 5.5676, Val Loss: 5.6662\n",
            "Epoch 2/1000, Train Loss: 5.5527, Val Loss: 5.6672\n",
            "Epoch 3/1000, Train Loss: 5.5510, Val Loss: 5.6660\n",
            "Epoch 4/1000, Train Loss: 5.5569, Val Loss: 5.6779\n",
            "Epoch 5/1000, Train Loss: 5.5513, Val Loss: 5.6848\n",
            "Epoch 6/1000, Train Loss: 5.5571, Val Loss: 5.6762\n",
            "Epoch 7/1000, Train Loss: 5.5471, Val Loss: 5.7035\n",
            "Epoch 8/1000, Train Loss: 5.5562, Val Loss: 5.6965\n",
            "Epoch 9/1000, Train Loss: 5.5483, Val Loss: 5.7097\n",
            "Epoch 10/1000, Train Loss: 5.5509, Val Loss: 5.6680\n",
            "Epoch 11/1000, Train Loss: 5.5538, Val Loss: 5.6713\n",
            "Epoch 12/1000, Train Loss: 5.5471, Val Loss: 5.6683\n",
            "Epoch 13/1000, Train Loss: 5.5420, Val Loss: 5.6715\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 5.6660\n",
            "Training with LR=0.01, Hidden=32, Dropout=0.1, Scale Factor=1.25\n",
            "Combination 68/81\n",
            "Epoch 1/1000, Train Loss: 6.6502, Val Loss: 6.7579\n",
            "Epoch 2/1000, Train Loss: 6.6210, Val Loss: 6.7660\n",
            "Epoch 3/1000, Train Loss: 6.6092, Val Loss: 6.7747\n",
            "Epoch 4/1000, Train Loss: 6.6067, Val Loss: 6.7705\n",
            "Epoch 5/1000, Train Loss: 6.6101, Val Loss: 6.7732\n",
            "Epoch 6/1000, Train Loss: 6.6112, Val Loss: 6.7719\n",
            "Epoch 7/1000, Train Loss: 6.6168, Val Loss: 6.7574\n",
            "Epoch 8/1000, Train Loss: 6.6082, Val Loss: 6.7506\n",
            "Epoch 9/1000, Train Loss: 6.6097, Val Loss: 6.7571\n",
            "Epoch 10/1000, Train Loss: 6.6127, Val Loss: 6.7559\n",
            "Epoch 11/1000, Train Loss: 6.6098, Val Loss: 6.7689\n",
            "Epoch 12/1000, Train Loss: 6.6097, Val Loss: 6.7583\n",
            "Epoch 13/1000, Train Loss: 6.6154, Val Loss: 6.7569\n",
            "Epoch 14/1000, Train Loss: 6.6064, Val Loss: 6.7735\n",
            "Epoch 15/1000, Train Loss: 6.6198, Val Loss: 6.7658\n",
            "Epoch 16/1000, Train Loss: 6.6053, Val Loss: 6.7632\n",
            "Epoch 17/1000, Train Loss: 6.6083, Val Loss: 6.7632\n",
            "Epoch 18/1000, Train Loss: 6.6165, Val Loss: 6.7643\n",
            "Early stopping triggered at epoch 18. Best Val Loss: 6.7506\n",
            "Epoch 1/1000, Train Loss: 6.6304, Val Loss: 6.7950\n",
            "Epoch 2/1000, Train Loss: 6.6186, Val Loss: 6.7624\n",
            "Epoch 3/1000, Train Loss: 6.6221, Val Loss: 6.7612\n",
            "Epoch 4/1000, Train Loss: 6.6194, Val Loss: 6.7705\n",
            "Epoch 5/1000, Train Loss: 6.6141, Val Loss: 6.7700\n",
            "Epoch 6/1000, Train Loss: 6.6172, Val Loss: 6.7753\n",
            "Epoch 7/1000, Train Loss: 6.6111, Val Loss: 6.7608\n",
            "Epoch 8/1000, Train Loss: 6.6096, Val Loss: 6.7510\n",
            "Epoch 9/1000, Train Loss: 6.6125, Val Loss: 6.7861\n",
            "Epoch 10/1000, Train Loss: 6.6130, Val Loss: 6.7519\n",
            "Epoch 11/1000, Train Loss: 6.6168, Val Loss: 6.7987\n",
            "Epoch 12/1000, Train Loss: 6.6137, Val Loss: 6.7743\n",
            "Epoch 13/1000, Train Loss: 6.6098, Val Loss: 6.7582\n",
            "Epoch 14/1000, Train Loss: 6.6108, Val Loss: 6.7755\n",
            "Epoch 15/1000, Train Loss: 6.6064, Val Loss: 6.7684\n",
            "Epoch 16/1000, Train Loss: 6.6091, Val Loss: 6.7646\n",
            "Epoch 17/1000, Train Loss: 6.6086, Val Loss: 6.7471\n",
            "Epoch 18/1000, Train Loss: 6.6211, Val Loss: 6.7830\n",
            "Epoch 19/1000, Train Loss: 6.6180, Val Loss: 6.7762\n",
            "Epoch 20/1000, Train Loss: 6.6298, Val Loss: 6.7802\n",
            "Epoch 21/1000, Train Loss: 6.7183, Val Loss: 6.7762\n",
            "Epoch 22/1000, Train Loss: 6.6165, Val Loss: 6.7730\n",
            "Epoch 23/1000, Train Loss: 6.6052, Val Loss: 6.7732\n",
            "Epoch 24/1000, Train Loss: 6.6017, Val Loss: 6.7737\n",
            "Epoch 25/1000, Train Loss: 6.6148, Val Loss: 6.7702\n",
            "Epoch 26/1000, Train Loss: 6.6067, Val Loss: 6.7684\n",
            "Epoch 27/1000, Train Loss: 6.6034, Val Loss: 6.7741\n",
            "Early stopping triggered at epoch 27. Best Val Loss: 6.7471\n",
            "Epoch 1/1000, Train Loss: 6.6166, Val Loss: 6.7749\n",
            "Epoch 2/1000, Train Loss: 6.6272, Val Loss: 6.7488\n",
            "Epoch 3/1000, Train Loss: 6.6083, Val Loss: 6.7516\n",
            "Epoch 4/1000, Train Loss: 6.6099, Val Loss: 6.7543\n",
            "Epoch 5/1000, Train Loss: 6.6121, Val Loss: 6.7578\n",
            "Epoch 6/1000, Train Loss: 6.6113, Val Loss: 6.7596\n",
            "Epoch 7/1000, Train Loss: 6.6077, Val Loss: 6.7546\n",
            "Epoch 8/1000, Train Loss: 6.6145, Val Loss: 6.7531\n",
            "Epoch 9/1000, Train Loss: 6.6090, Val Loss: 6.7546\n",
            "Epoch 10/1000, Train Loss: 6.6149, Val Loss: 6.7808\n",
            "Epoch 11/1000, Train Loss: 6.6203, Val Loss: 6.7730\n",
            "Epoch 12/1000, Train Loss: 6.6146, Val Loss: 6.7785\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 6.7488\n",
            "Training with LR=0.01, Hidden=32, Dropout=0.1, Scale Factor=1.5\n",
            "Combination 69/81\n",
            "Epoch 1/1000, Train Loss: 7.6804, Val Loss: 7.8152\n",
            "Epoch 2/1000, Train Loss: 7.6495, Val Loss: 7.8330\n",
            "Epoch 3/1000, Train Loss: 7.6512, Val Loss: 7.8417\n",
            "Epoch 4/1000, Train Loss: 7.6494, Val Loss: 7.8230\n",
            "Epoch 5/1000, Train Loss: 7.6467, Val Loss: 7.8315\n",
            "Epoch 6/1000, Train Loss: 7.6702, Val Loss: 7.8169\n",
            "Epoch 7/1000, Train Loss: 7.6563, Val Loss: 7.8227\n",
            "Epoch 8/1000, Train Loss: 7.6456, Val Loss: 7.8287\n",
            "Epoch 9/1000, Train Loss: 7.6445, Val Loss: 7.8382\n",
            "Epoch 10/1000, Train Loss: 7.6397, Val Loss: 7.8243\n",
            "Epoch 11/1000, Train Loss: 7.6530, Val Loss: 7.8345\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8152\n",
            "Epoch 1/1000, Train Loss: 7.6576, Val Loss: 7.8236\n",
            "Epoch 2/1000, Train Loss: 7.6600, Val Loss: 7.8244\n",
            "Epoch 3/1000, Train Loss: 7.6477, Val Loss: 7.8242\n",
            "Epoch 4/1000, Train Loss: 7.6431, Val Loss: 7.8305\n",
            "Epoch 5/1000, Train Loss: 7.6493, Val Loss: 7.8217\n",
            "Epoch 6/1000, Train Loss: 7.6445, Val Loss: 7.8330\n",
            "Epoch 7/1000, Train Loss: 7.6523, Val Loss: 7.8104\n",
            "Epoch 8/1000, Train Loss: 7.6465, Val Loss: 7.8095\n",
            "Epoch 9/1000, Train Loss: 7.6445, Val Loss: 7.8177\n",
            "Epoch 10/1000, Train Loss: 7.6629, Val Loss: 7.8096\n",
            "Epoch 11/1000, Train Loss: 7.6407, Val Loss: 7.8109\n",
            "Epoch 12/1000, Train Loss: 7.6410, Val Loss: 7.8217\n",
            "Epoch 13/1000, Train Loss: 7.6488, Val Loss: 7.8227\n",
            "Epoch 14/1000, Train Loss: 7.6607, Val Loss: 7.8142\n",
            "Epoch 15/1000, Train Loss: 7.6592, Val Loss: 7.8323\n",
            "Epoch 16/1000, Train Loss: 7.6523, Val Loss: 7.8114\n",
            "Epoch 17/1000, Train Loss: 7.6504, Val Loss: 7.8116\n",
            "Epoch 18/1000, Train Loss: 7.6594, Val Loss: 7.8244\n",
            "Early stopping triggered at epoch 18. Best Val Loss: 7.8095\n",
            "Epoch 1/1000, Train Loss: 7.6789, Val Loss: 7.8353\n",
            "Epoch 2/1000, Train Loss: 7.6537, Val Loss: 7.8200\n",
            "Epoch 3/1000, Train Loss: 7.6580, Val Loss: 7.8140\n",
            "Epoch 4/1000, Train Loss: 7.6496, Val Loss: 7.8161\n",
            "Epoch 5/1000, Train Loss: 7.6474, Val Loss: 7.8241\n",
            "Epoch 6/1000, Train Loss: 7.6536, Val Loss: 7.8271\n",
            "Epoch 7/1000, Train Loss: 7.6513, Val Loss: 7.8188\n",
            "Epoch 8/1000, Train Loss: 7.6467, Val Loss: 7.8163\n",
            "Epoch 9/1000, Train Loss: 7.6509, Val Loss: 7.8219\n",
            "Epoch 10/1000, Train Loss: 7.6720, Val Loss: 7.8274\n",
            "Epoch 11/1000, Train Loss: 7.6495, Val Loss: 7.8290\n",
            "Epoch 12/1000, Train Loss: 7.6443, Val Loss: 7.8153\n",
            "Epoch 13/1000, Train Loss: 7.6437, Val Loss: 7.8364\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 7.8140\n",
            "Training with LR=0.01, Hidden=32, Dropout=0.2, Scale Factor=1.0\n",
            "Combination 70/81\n",
            "Epoch 1/1000, Train Loss: 5.5647, Val Loss: 5.6750\n",
            "Epoch 2/1000, Train Loss: 5.5658, Val Loss: 5.7307\n",
            "Epoch 3/1000, Train Loss: 5.5635, Val Loss: 5.6975\n",
            "Epoch 4/1000, Train Loss: 5.5560, Val Loss: 5.6657\n",
            "Epoch 5/1000, Train Loss: 5.5622, Val Loss: 5.6693\n",
            "Epoch 6/1000, Train Loss: 5.5586, Val Loss: 5.6711\n",
            "Epoch 7/1000, Train Loss: 5.5522, Val Loss: 5.6695\n",
            "Epoch 8/1000, Train Loss: 5.5558, Val Loss: 5.6680\n",
            "Epoch 9/1000, Train Loss: 5.5510, Val Loss: 5.6649\n",
            "Epoch 10/1000, Train Loss: 5.5534, Val Loss: 5.6763\n",
            "Epoch 11/1000, Train Loss: 5.5538, Val Loss: 5.6755\n",
            "Epoch 12/1000, Train Loss: 5.5547, Val Loss: 5.6737\n",
            "Epoch 13/1000, Train Loss: 5.5520, Val Loss: 5.6691\n",
            "Epoch 14/1000, Train Loss: 5.5499, Val Loss: 5.6711\n",
            "Epoch 15/1000, Train Loss: 5.5606, Val Loss: 5.6739\n",
            "Epoch 16/1000, Train Loss: 5.5484, Val Loss: 5.6658\n",
            "Epoch 17/1000, Train Loss: 5.5564, Val Loss: 5.6695\n",
            "Epoch 18/1000, Train Loss: 5.5447, Val Loss: 5.6687\n",
            "Epoch 19/1000, Train Loss: 5.5514, Val Loss: 5.6709\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 5.6649\n",
            "Epoch 1/1000, Train Loss: 5.5697, Val Loss: 5.6663\n",
            "Epoch 2/1000, Train Loss: 5.5628, Val Loss: 5.6652\n",
            "Epoch 3/1000, Train Loss: 5.5497, Val Loss: 5.6684\n",
            "Epoch 4/1000, Train Loss: 5.5469, Val Loss: 5.6711\n",
            "Epoch 5/1000, Train Loss: 5.5688, Val Loss: 5.6704\n",
            "Epoch 6/1000, Train Loss: 5.5582, Val Loss: 5.6662\n",
            "Epoch 7/1000, Train Loss: 5.5498, Val Loss: 5.6675\n",
            "Epoch 8/1000, Train Loss: 5.5485, Val Loss: 5.6672\n",
            "Epoch 9/1000, Train Loss: 5.5500, Val Loss: 5.6686\n",
            "Epoch 10/1000, Train Loss: 5.5461, Val Loss: 5.6707\n",
            "Epoch 11/1000, Train Loss: 5.5569, Val Loss: 5.6692\n",
            "Epoch 12/1000, Train Loss: 5.5507, Val Loss: 5.6704\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 5.6652\n",
            "Epoch 1/1000, Train Loss: 5.5733, Val Loss: 5.6670\n",
            "Epoch 2/1000, Train Loss: 5.5631, Val Loss: 5.6955\n",
            "Epoch 3/1000, Train Loss: 5.5567, Val Loss: 5.6616\n",
            "Epoch 4/1000, Train Loss: 5.5550, Val Loss: 5.6700\n",
            "Epoch 5/1000, Train Loss: 5.5539, Val Loss: 5.6637\n",
            "Epoch 6/1000, Train Loss: 5.5636, Val Loss: 5.6821\n",
            "Epoch 7/1000, Train Loss: 5.5536, Val Loss: 5.6808\n",
            "Epoch 8/1000, Train Loss: 5.5603, Val Loss: 5.6627\n",
            "Epoch 9/1000, Train Loss: 5.5576, Val Loss: 5.6657\n",
            "Epoch 10/1000, Train Loss: 5.5466, Val Loss: 5.6628\n",
            "Epoch 11/1000, Train Loss: 5.5636, Val Loss: 5.6684\n",
            "Epoch 12/1000, Train Loss: 5.5679, Val Loss: 5.6770\n",
            "Epoch 13/1000, Train Loss: 5.5532, Val Loss: 5.6667\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 5.6616\n",
            "Training with LR=0.01, Hidden=32, Dropout=0.2, Scale Factor=1.25\n",
            "Combination 71/81\n",
            "Epoch 1/1000, Train Loss: 6.6439, Val Loss: 6.7723\n",
            "Epoch 2/1000, Train Loss: 6.6183, Val Loss: 6.7875\n",
            "Epoch 3/1000, Train Loss: 6.6198, Val Loss: 6.7751\n",
            "Epoch 4/1000, Train Loss: 6.6168, Val Loss: 6.7993\n",
            "Epoch 5/1000, Train Loss: 6.6170, Val Loss: 6.7640\n",
            "Epoch 6/1000, Train Loss: 6.6098, Val Loss: 6.7766\n",
            "Epoch 7/1000, Train Loss: 6.6067, Val Loss: 6.7786\n",
            "Epoch 8/1000, Train Loss: 6.6192, Val Loss: 6.7731\n",
            "Epoch 9/1000, Train Loss: 6.6032, Val Loss: 6.7753\n",
            "Epoch 10/1000, Train Loss: 6.6304, Val Loss: 6.7773\n",
            "Epoch 11/1000, Train Loss: 6.6175, Val Loss: 6.7601\n",
            "Epoch 12/1000, Train Loss: 6.6288, Val Loss: 6.7707\n",
            "Epoch 13/1000, Train Loss: 6.6236, Val Loss: 6.7724\n",
            "Epoch 14/1000, Train Loss: 6.6056, Val Loss: 6.7801\n",
            "Epoch 15/1000, Train Loss: 6.6020, Val Loss: 6.7727\n",
            "Epoch 16/1000, Train Loss: 6.6129, Val Loss: 6.7537\n",
            "Epoch 17/1000, Train Loss: 6.6215, Val Loss: 6.7960\n",
            "Epoch 18/1000, Train Loss: 6.6102, Val Loss: 6.7736\n",
            "Epoch 19/1000, Train Loss: 6.6166, Val Loss: 6.7590\n",
            "Epoch 20/1000, Train Loss: 6.6092, Val Loss: 6.7923\n",
            "Epoch 21/1000, Train Loss: 6.6236, Val Loss: 6.7779\n",
            "Epoch 22/1000, Train Loss: 6.6176, Val Loss: 6.7606\n",
            "Epoch 23/1000, Train Loss: 6.6070, Val Loss: 6.7701\n",
            "Epoch 24/1000, Train Loss: 6.6500, Val Loss: 6.7628\n",
            "Epoch 25/1000, Train Loss: 6.6066, Val Loss: 6.7625\n",
            "Epoch 26/1000, Train Loss: 6.6040, Val Loss: 6.7608\n",
            "Early stopping triggered at epoch 26. Best Val Loss: 6.7537\n",
            "Epoch 1/1000, Train Loss: 6.6225, Val Loss: 6.7615\n",
            "Epoch 2/1000, Train Loss: 6.6094, Val Loss: 6.7784\n",
            "Epoch 3/1000, Train Loss: 6.6185, Val Loss: 6.7777\n",
            "Epoch 4/1000, Train Loss: 6.6217, Val Loss: 6.7851\n",
            "Epoch 5/1000, Train Loss: 6.6156, Val Loss: 6.7720\n",
            "Epoch 6/1000, Train Loss: 6.6174, Val Loss: 6.7634\n",
            "Epoch 7/1000, Train Loss: 6.6182, Val Loss: 6.7449\n",
            "Epoch 8/1000, Train Loss: 6.6215, Val Loss: 6.7504\n",
            "Epoch 9/1000, Train Loss: 6.6512, Val Loss: 6.7504\n",
            "Epoch 10/1000, Train Loss: 6.6191, Val Loss: 6.7886\n",
            "Epoch 11/1000, Train Loss: 6.6288, Val Loss: 6.7611\n",
            "Epoch 12/1000, Train Loss: 6.6147, Val Loss: 6.7719\n",
            "Epoch 13/1000, Train Loss: 6.6170, Val Loss: 6.7452\n",
            "Epoch 14/1000, Train Loss: 6.6150, Val Loss: 6.7727\n",
            "Epoch 15/1000, Train Loss: 6.6168, Val Loss: 6.7488\n",
            "Epoch 16/1000, Train Loss: 6.6154, Val Loss: 6.7772\n",
            "Epoch 17/1000, Train Loss: 6.6148, Val Loss: 6.7449\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 6.7449\n",
            "Epoch 1/1000, Train Loss: 6.6382, Val Loss: 6.7584\n",
            "Epoch 2/1000, Train Loss: 6.6188, Val Loss: 6.7554\n",
            "Epoch 3/1000, Train Loss: 6.6145, Val Loss: 6.7502\n",
            "Epoch 4/1000, Train Loss: 6.6214, Val Loss: 6.7524\n",
            "Epoch 5/1000, Train Loss: 6.6071, Val Loss: 6.7591\n",
            "Epoch 6/1000, Train Loss: 6.6121, Val Loss: 6.7718\n",
            "Epoch 7/1000, Train Loss: 6.6147, Val Loss: 6.7713\n",
            "Epoch 8/1000, Train Loss: 6.6132, Val Loss: 6.7622\n",
            "Epoch 9/1000, Train Loss: 6.6097, Val Loss: 6.7524\n",
            "Epoch 10/1000, Train Loss: 6.6249, Val Loss: 6.7611\n",
            "Epoch 11/1000, Train Loss: 6.6131, Val Loss: 6.7827\n",
            "Epoch 12/1000, Train Loss: 6.6128, Val Loss: 7.6371\n",
            "Epoch 13/1000, Train Loss: 6.6192, Val Loss: 6.7590\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 6.7502\n",
            "Training with LR=0.01, Hidden=32, Dropout=0.2, Scale Factor=1.5\n",
            "Combination 72/81\n",
            "Epoch 1/1000, Train Loss: 7.6850, Val Loss: 9.1642\n",
            "Epoch 2/1000, Train Loss: 7.6706, Val Loss: 7.8241\n",
            "Epoch 3/1000, Train Loss: 7.6472, Val Loss: 7.8368\n",
            "Epoch 4/1000, Train Loss: 7.6516, Val Loss: 7.8316\n",
            "Epoch 5/1000, Train Loss: 7.6703, Val Loss: 7.8232\n",
            "Epoch 6/1000, Train Loss: 7.6637, Val Loss: 7.8311\n",
            "Epoch 7/1000, Train Loss: 7.6504, Val Loss: 7.8282\n",
            "Epoch 8/1000, Train Loss: 7.6658, Val Loss: 7.8289\n",
            "Epoch 9/1000, Train Loss: 7.6895, Val Loss: 7.8268\n",
            "Epoch 10/1000, Train Loss: 7.6530, Val Loss: 7.8229\n",
            "Epoch 11/1000, Train Loss: 7.6531, Val Loss: 7.8585\n",
            "Epoch 12/1000, Train Loss: 7.6541, Val Loss: 7.8341\n",
            "Epoch 13/1000, Train Loss: 7.6444, Val Loss: 7.8255\n",
            "Epoch 14/1000, Train Loss: 7.6545, Val Loss: 7.8326\n",
            "Epoch 15/1000, Train Loss: 7.6571, Val Loss: 7.8251\n",
            "Epoch 16/1000, Train Loss: 7.6634, Val Loss: 7.8222\n",
            "Epoch 17/1000, Train Loss: 7.6481, Val Loss: 7.8310\n",
            "Epoch 18/1000, Train Loss: 7.6401, Val Loss: 7.8240\n",
            "Epoch 19/1000, Train Loss: 7.6517, Val Loss: 7.8278\n",
            "Epoch 20/1000, Train Loss: 7.6432, Val Loss: 7.8336\n",
            "Epoch 21/1000, Train Loss: 7.6481, Val Loss: 7.8270\n",
            "Epoch 22/1000, Train Loss: 7.6395, Val Loss: 7.8258\n",
            "Epoch 23/1000, Train Loss: 7.6475, Val Loss: 7.8393\n",
            "Epoch 24/1000, Train Loss: 7.6438, Val Loss: 7.8287\n",
            "Epoch 25/1000, Train Loss: 7.6459, Val Loss: 7.8277\n",
            "Epoch 26/1000, Train Loss: 7.6424, Val Loss: 7.8403\n",
            "Early stopping triggered at epoch 26. Best Val Loss: 7.8222\n",
            "Epoch 1/1000, Train Loss: 7.6800, Val Loss: 7.8298\n",
            "Epoch 2/1000, Train Loss: 7.6611, Val Loss: 7.8280\n",
            "Epoch 3/1000, Train Loss: 7.6575, Val Loss: 7.8274\n",
            "Epoch 4/1000, Train Loss: 7.6537, Val Loss: 7.8215\n",
            "Epoch 5/1000, Train Loss: 7.6627, Val Loss: 7.8229\n",
            "Epoch 6/1000, Train Loss: 7.6810, Val Loss: 7.8207\n",
            "Epoch 7/1000, Train Loss: 7.8513, Val Loss: 7.8235\n",
            "Epoch 8/1000, Train Loss: 7.6519, Val Loss: 7.8395\n",
            "Epoch 9/1000, Train Loss: 7.6496, Val Loss: 7.8377\n",
            "Epoch 10/1000, Train Loss: 7.6562, Val Loss: 7.8259\n",
            "Epoch 11/1000, Train Loss: 7.6461, Val Loss: 7.8216\n",
            "Epoch 12/1000, Train Loss: 7.6489, Val Loss: 7.8415\n",
            "Epoch 13/1000, Train Loss: 7.6473, Val Loss: 7.8302\n",
            "Epoch 14/1000, Train Loss: 7.6435, Val Loss: 7.8249\n",
            "Epoch 15/1000, Train Loss: 7.6516, Val Loss: 7.8234\n",
            "Epoch 16/1000, Train Loss: 7.6473, Val Loss: 7.8319\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 7.8207\n",
            "Epoch 1/1000, Train Loss: 7.6877, Val Loss: 7.8223\n",
            "Epoch 2/1000, Train Loss: 7.6455, Val Loss: 7.8573\n",
            "Epoch 3/1000, Train Loss: 7.6671, Val Loss: 7.8311\n",
            "Epoch 4/1000, Train Loss: 7.6641, Val Loss: 7.8336\n",
            "Epoch 5/1000, Train Loss: 7.6490, Val Loss: 7.8237\n",
            "Epoch 6/1000, Train Loss: 7.6501, Val Loss: 7.8247\n",
            "Epoch 7/1000, Train Loss: 7.6640, Val Loss: 7.8232\n",
            "Epoch 8/1000, Train Loss: 7.6619, Val Loss: 7.8185\n",
            "Epoch 9/1000, Train Loss: 7.6461, Val Loss: 7.8308\n",
            "Epoch 10/1000, Train Loss: 7.6549, Val Loss: 7.8159\n",
            "Epoch 11/1000, Train Loss: 7.6894, Val Loss: 7.8177\n",
            "Epoch 12/1000, Train Loss: 7.6811, Val Loss: 7.8241\n",
            "Epoch 13/1000, Train Loss: 7.6590, Val Loss: 7.8209\n",
            "Epoch 14/1000, Train Loss: 7.6439, Val Loss: 7.8222\n",
            "Epoch 15/1000, Train Loss: 7.6604, Val Loss: 7.8288\n",
            "Epoch 16/1000, Train Loss: 7.6753, Val Loss: 9.1506\n",
            "Epoch 17/1000, Train Loss: 7.6662, Val Loss: 7.8253\n",
            "Epoch 18/1000, Train Loss: 7.6591, Val Loss: 7.8273\n",
            "Epoch 19/1000, Train Loss: 7.6759, Val Loss: 7.8435\n",
            "Epoch 20/1000, Train Loss: 7.6436, Val Loss: 7.8175\n",
            "Early stopping triggered at epoch 20. Best Val Loss: 7.8159\n",
            "Training with LR=0.01, Hidden=64, Dropout=0.0, Scale Factor=1.0\n",
            "Combination 73/81\n",
            "Epoch 1/1000, Train Loss: 5.5670, Val Loss: 5.6676\n",
            "Epoch 2/1000, Train Loss: 5.5522, Val Loss: 5.6659\n",
            "Epoch 3/1000, Train Loss: 5.5510, Val Loss: 5.6720\n",
            "Epoch 4/1000, Train Loss: 5.5486, Val Loss: 5.6961\n",
            "Epoch 5/1000, Train Loss: 5.5647, Val Loss: 5.6634\n",
            "Epoch 6/1000, Train Loss: 5.5475, Val Loss: 5.6704\n",
            "Epoch 7/1000, Train Loss: 5.5479, Val Loss: 5.6762\n",
            "Epoch 8/1000, Train Loss: 5.5481, Val Loss: 5.6697\n",
            "Epoch 9/1000, Train Loss: 5.5594, Val Loss: 5.6745\n",
            "Epoch 10/1000, Train Loss: 5.5550, Val Loss: 5.6846\n",
            "Epoch 11/1000, Train Loss: 5.5498, Val Loss: 5.6769\n",
            "Epoch 12/1000, Train Loss: 5.5469, Val Loss: 5.6868\n",
            "Epoch 13/1000, Train Loss: 5.5463, Val Loss: 5.6707\n",
            "Epoch 14/1000, Train Loss: 5.5713, Val Loss: 5.6978\n",
            "Epoch 15/1000, Train Loss: 5.5610, Val Loss: 5.6796\n",
            "Early stopping triggered at epoch 15. Best Val Loss: 5.6634\n",
            "Epoch 1/1000, Train Loss: 5.5728, Val Loss: 5.6755\n",
            "Epoch 2/1000, Train Loss: 5.5572, Val Loss: 5.6715\n",
            "Epoch 3/1000, Train Loss: 5.5565, Val Loss: 5.6888\n",
            "Epoch 4/1000, Train Loss: 5.5529, Val Loss: 5.6627\n",
            "Epoch 5/1000, Train Loss: 5.5477, Val Loss: 5.6625\n",
            "Epoch 6/1000, Train Loss: 5.5516, Val Loss: 5.6623\n",
            "Epoch 7/1000, Train Loss: 5.5760, Val Loss: 5.7076\n",
            "Epoch 8/1000, Train Loss: 5.5611, Val Loss: 5.6618\n",
            "Epoch 9/1000, Train Loss: 5.5515, Val Loss: 5.6623\n",
            "Epoch 10/1000, Train Loss: 5.5459, Val Loss: 5.6702\n",
            "Epoch 11/1000, Train Loss: 5.5466, Val Loss: 5.6863\n",
            "Epoch 12/1000, Train Loss: 5.5513, Val Loss: 5.6634\n",
            "Epoch 13/1000, Train Loss: 5.5773, Val Loss: 5.6634\n",
            "Epoch 14/1000, Train Loss: 5.5540, Val Loss: 5.7058\n",
            "Epoch 15/1000, Train Loss: 5.5514, Val Loss: 5.6676\n",
            "Epoch 16/1000, Train Loss: 5.5510, Val Loss: 5.6647\n",
            "Epoch 17/1000, Train Loss: 5.5466, Val Loss: 5.6687\n",
            "Epoch 18/1000, Train Loss: 5.5727, Val Loss: 5.7169\n",
            "Early stopping triggered at epoch 18. Best Val Loss: 5.6618\n",
            "Epoch 1/1000, Train Loss: 5.5730, Val Loss: 5.7030\n",
            "Epoch 2/1000, Train Loss: 5.5543, Val Loss: 5.6731\n",
            "Epoch 3/1000, Train Loss: 5.5507, Val Loss: 5.6766\n",
            "Epoch 4/1000, Train Loss: 5.5648, Val Loss: 5.6643\n",
            "Epoch 5/1000, Train Loss: 5.5533, Val Loss: 5.6655\n",
            "Epoch 6/1000, Train Loss: 5.5616, Val Loss: 5.6615\n",
            "Epoch 7/1000, Train Loss: 5.5478, Val Loss: 5.6655\n",
            "Epoch 8/1000, Train Loss: 5.5507, Val Loss: 5.6802\n",
            "Epoch 9/1000, Train Loss: 5.5669, Val Loss: 5.6663\n",
            "Epoch 10/1000, Train Loss: 5.5557, Val Loss: 5.6703\n",
            "Epoch 11/1000, Train Loss: 5.5540, Val Loss: 5.7288\n",
            "Epoch 12/1000, Train Loss: 5.5581, Val Loss: 5.7148\n",
            "Epoch 13/1000, Train Loss: 5.5574, Val Loss: 5.6960\n",
            "Epoch 14/1000, Train Loss: 5.5462, Val Loss: 5.6638\n",
            "Epoch 15/1000, Train Loss: 5.5555, Val Loss: 5.6670\n",
            "Epoch 16/1000, Train Loss: 5.5591, Val Loss: 5.6623\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 5.6615\n",
            "Training with LR=0.01, Hidden=64, Dropout=0.0, Scale Factor=1.25\n",
            "Combination 74/81\n",
            "Epoch 1/1000, Train Loss: 6.6220, Val Loss: 6.7876\n",
            "Epoch 2/1000, Train Loss: 6.6284, Val Loss: 6.9844\n",
            "Epoch 3/1000, Train Loss: 6.6688, Val Loss: 6.7730\n",
            "Epoch 4/1000, Train Loss: 6.6107, Val Loss: 6.7993\n",
            "Epoch 5/1000, Train Loss: 6.6175, Val Loss: 6.7773\n",
            "Epoch 6/1000, Train Loss: 6.6177, Val Loss: 6.7828\n",
            "Epoch 7/1000, Train Loss: 6.6112, Val Loss: 6.7711\n",
            "Epoch 8/1000, Train Loss: 6.6096, Val Loss: 6.7745\n",
            "Epoch 9/1000, Train Loss: 6.6145, Val Loss: 7.0364\n",
            "Epoch 10/1000, Train Loss: 6.6115, Val Loss: 6.7690\n",
            "Epoch 11/1000, Train Loss: 6.6344, Val Loss: 6.7774\n",
            "Epoch 12/1000, Train Loss: 6.6205, Val Loss: 6.7667\n",
            "Epoch 13/1000, Train Loss: 6.6115, Val Loss: 6.7727\n",
            "Epoch 14/1000, Train Loss: 6.6216, Val Loss: 6.7733\n",
            "Epoch 15/1000, Train Loss: 6.6120, Val Loss: 6.7885\n",
            "Epoch 16/1000, Train Loss: 6.6121, Val Loss: 6.7885\n",
            "Epoch 17/1000, Train Loss: 6.6153, Val Loss: 6.7738\n",
            "Epoch 18/1000, Train Loss: 6.6239, Val Loss: 6.7739\n",
            "Epoch 19/1000, Train Loss: 6.6100, Val Loss: 6.7437\n",
            "Epoch 20/1000, Train Loss: 6.6137, Val Loss: 6.7508\n",
            "Epoch 21/1000, Train Loss: 6.6136, Val Loss: 6.7462\n",
            "Epoch 22/1000, Train Loss: 6.6588, Val Loss: 6.7487\n",
            "Epoch 23/1000, Train Loss: 6.6038, Val Loss: 6.7453\n",
            "Epoch 24/1000, Train Loss: 6.6156, Val Loss: 6.7493\n",
            "Epoch 25/1000, Train Loss: 6.6125, Val Loss: 6.7459\n",
            "Epoch 26/1000, Train Loss: 6.6044, Val Loss: 6.7472\n",
            "Epoch 27/1000, Train Loss: 6.6081, Val Loss: 6.7521\n",
            "Epoch 28/1000, Train Loss: 6.6114, Val Loss: 6.7558\n",
            "Epoch 29/1000, Train Loss: 6.6057, Val Loss: 6.8064\n",
            "Early stopping triggered at epoch 29. Best Val Loss: 6.7437\n",
            "Epoch 1/1000, Train Loss: 6.6210, Val Loss: 6.7733\n",
            "Epoch 2/1000, Train Loss: 6.6222, Val Loss: 6.7648\n",
            "Epoch 3/1000, Train Loss: 6.6170, Val Loss: 6.7745\n",
            "Epoch 4/1000, Train Loss: 6.6118, Val Loss: 6.7698\n",
            "Epoch 5/1000, Train Loss: 6.6208, Val Loss: 6.7581\n",
            "Epoch 6/1000, Train Loss: 6.6161, Val Loss: 6.8028\n",
            "Epoch 7/1000, Train Loss: 6.6181, Val Loss: 6.7716\n",
            "Epoch 8/1000, Train Loss: 6.6174, Val Loss: 6.7683\n",
            "Epoch 9/1000, Train Loss: 6.6125, Val Loss: 6.7918\n",
            "Epoch 10/1000, Train Loss: 6.6139, Val Loss: 6.7920\n",
            "Epoch 11/1000, Train Loss: 6.6237, Val Loss: 6.7524\n",
            "Epoch 12/1000, Train Loss: 6.6123, Val Loss: 6.7457\n",
            "Epoch 13/1000, Train Loss: 6.6144, Val Loss: 6.7770\n",
            "Epoch 14/1000, Train Loss: 6.6078, Val Loss: 6.7486\n",
            "Epoch 15/1000, Train Loss: 6.6147, Val Loss: 6.7699\n",
            "Epoch 16/1000, Train Loss: 6.6080, Val Loss: 6.7660\n",
            "Epoch 17/1000, Train Loss: 6.6245, Val Loss: 6.7572\n",
            "Epoch 18/1000, Train Loss: 6.5972, Val Loss: 6.7560\n",
            "Epoch 19/1000, Train Loss: 6.6062, Val Loss: 6.7575\n",
            "Epoch 20/1000, Train Loss: 6.6053, Val Loss: 6.7665\n",
            "Epoch 21/1000, Train Loss: 6.6039, Val Loss: 6.7619\n",
            "Epoch 22/1000, Train Loss: 6.5997, Val Loss: 6.7781\n",
            "Early stopping triggered at epoch 22. Best Val Loss: 6.7457\n",
            "Epoch 1/1000, Train Loss: 6.6254, Val Loss: 6.7869\n",
            "Epoch 2/1000, Train Loss: 6.6239, Val Loss: 6.7574\n",
            "Epoch 3/1000, Train Loss: 6.6092, Val Loss: 6.7565\n",
            "Epoch 4/1000, Train Loss: 6.6100, Val Loss: 6.7512\n",
            "Epoch 5/1000, Train Loss: 6.6387, Val Loss: 6.7735\n",
            "Epoch 6/1000, Train Loss: 6.6155, Val Loss: 6.7955\n",
            "Epoch 7/1000, Train Loss: 6.6215, Val Loss: 6.7743\n",
            "Epoch 8/1000, Train Loss: 6.6142, Val Loss: 6.7846\n",
            "Epoch 9/1000, Train Loss: 6.6244, Val Loss: 6.7483\n",
            "Epoch 10/1000, Train Loss: 6.6153, Val Loss: 6.7832\n",
            "Epoch 11/1000, Train Loss: 6.6129, Val Loss: 6.7796\n",
            "Epoch 12/1000, Train Loss: 6.6164, Val Loss: 7.1263\n",
            "Epoch 13/1000, Train Loss: 6.6104, Val Loss: 6.7608\n",
            "Epoch 14/1000, Train Loss: 6.6122, Val Loss: 6.8292\n",
            "Epoch 15/1000, Train Loss: 6.6030, Val Loss: 6.8082\n",
            "Epoch 16/1000, Train Loss: 6.6100, Val Loss: 6.7874\n",
            "Epoch 17/1000, Train Loss: 6.6148, Val Loss: 6.7640\n",
            "Epoch 18/1000, Train Loss: 6.6114, Val Loss: 6.7864\n",
            "Epoch 19/1000, Train Loss: 6.6191, Val Loss: 6.7777\n",
            "Early stopping triggered at epoch 19. Best Val Loss: 6.7483\n",
            "Training with LR=0.01, Hidden=64, Dropout=0.0, Scale Factor=1.5\n",
            "Combination 75/81\n",
            "Epoch 1/1000, Train Loss: 7.7148, Val Loss: 7.8240\n",
            "Epoch 2/1000, Train Loss: 7.6528, Val Loss: 7.8149\n",
            "Epoch 3/1000, Train Loss: 7.6414, Val Loss: 7.8547\n",
            "Epoch 4/1000, Train Loss: 7.6552, Val Loss: 7.8238\n",
            "Epoch 5/1000, Train Loss: 7.6460, Val Loss: 7.8258\n",
            "Epoch 6/1000, Train Loss: 7.6450, Val Loss: 7.8350\n",
            "Epoch 7/1000, Train Loss: 7.6419, Val Loss: 7.8214\n",
            "Epoch 8/1000, Train Loss: 7.6466, Val Loss: 7.8199\n",
            "Epoch 9/1000, Train Loss: 7.6703, Val Loss: 7.8522\n",
            "Epoch 10/1000, Train Loss: 7.6463, Val Loss: 7.9151\n",
            "Epoch 11/1000, Train Loss: 7.6469, Val Loss: 7.8268\n",
            "Epoch 12/1000, Train Loss: 7.6439, Val Loss: 7.8219\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 7.8149\n",
            "Epoch 1/1000, Train Loss: 7.6860, Val Loss: 7.8326\n",
            "Epoch 2/1000, Train Loss: 7.6556, Val Loss: 7.8227\n",
            "Epoch 3/1000, Train Loss: 7.6574, Val Loss: 7.8268\n",
            "Epoch 4/1000, Train Loss: 7.6482, Val Loss: 7.8366\n",
            "Epoch 5/1000, Train Loss: 7.6492, Val Loss: 7.8299\n",
            "Epoch 6/1000, Train Loss: 7.6448, Val Loss: 7.8264\n",
            "Epoch 7/1000, Train Loss: 7.6556, Val Loss: 7.8115\n",
            "Epoch 8/1000, Train Loss: 7.6451, Val Loss: 7.8315\n",
            "Epoch 9/1000, Train Loss: 7.6448, Val Loss: 7.8327\n",
            "Epoch 10/1000, Train Loss: 7.6532, Val Loss: 8.0864\n",
            "Epoch 11/1000, Train Loss: 7.6518, Val Loss: 7.8244\n",
            "Epoch 12/1000, Train Loss: 7.6450, Val Loss: 7.8395\n",
            "Epoch 13/1000, Train Loss: 7.6429, Val Loss: 7.8253\n",
            "Epoch 14/1000, Train Loss: 7.6516, Val Loss: 7.8240\n",
            "Epoch 15/1000, Train Loss: 7.6633, Val Loss: 7.8239\n",
            "Epoch 16/1000, Train Loss: 7.6417, Val Loss: 8.0066\n",
            "Epoch 17/1000, Train Loss: 7.6495, Val Loss: 7.8354\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 7.8115\n",
            "Epoch 1/1000, Train Loss: 7.6809, Val Loss: 7.8327\n",
            "Epoch 2/1000, Train Loss: 7.6644, Val Loss: 7.8323\n",
            "Epoch 3/1000, Train Loss: 7.6476, Val Loss: 7.8377\n",
            "Epoch 4/1000, Train Loss: 7.6618, Val Loss: 7.8239\n",
            "Epoch 5/1000, Train Loss: 7.6472, Val Loss: 7.8483\n",
            "Epoch 6/1000, Train Loss: 7.6577, Val Loss: 7.8168\n",
            "Epoch 7/1000, Train Loss: 7.6493, Val Loss: 7.8282\n",
            "Epoch 8/1000, Train Loss: 7.6431, Val Loss: 7.8563\n",
            "Epoch 9/1000, Train Loss: 7.7946, Val Loss: 7.9787\n",
            "Epoch 10/1000, Train Loss: 7.7566, Val Loss: 7.8211\n",
            "Epoch 11/1000, Train Loss: 7.6494, Val Loss: 7.8293\n",
            "Epoch 12/1000, Train Loss: 7.6383, Val Loss: 7.8285\n",
            "Epoch 13/1000, Train Loss: 7.6489, Val Loss: 7.8284\n",
            "Epoch 14/1000, Train Loss: 7.6392, Val Loss: 7.8257\n",
            "Epoch 15/1000, Train Loss: 7.6384, Val Loss: 7.8326\n",
            "Epoch 16/1000, Train Loss: 7.6572, Val Loss: 7.8296\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 7.8168\n",
            "Training with LR=0.01, Hidden=64, Dropout=0.1, Scale Factor=1.0\n",
            "Combination 76/81\n",
            "Epoch 1/1000, Train Loss: 5.5705, Val Loss: 5.7198\n",
            "Epoch 2/1000, Train Loss: 5.5583, Val Loss: 5.6719\n",
            "Epoch 3/1000, Train Loss: 5.5712, Val Loss: 5.6662\n",
            "Epoch 4/1000, Train Loss: 5.5525, Val Loss: 5.6729\n",
            "Epoch 5/1000, Train Loss: 5.5552, Val Loss: 5.6720\n",
            "Epoch 6/1000, Train Loss: 5.5524, Val Loss: 5.6735\n",
            "Epoch 7/1000, Train Loss: 5.5512, Val Loss: 5.6705\n",
            "Epoch 8/1000, Train Loss: 5.5516, Val Loss: 5.6783\n",
            "Epoch 9/1000, Train Loss: 5.5592, Val Loss: 5.6842\n",
            "Epoch 10/1000, Train Loss: 5.5509, Val Loss: 5.6653\n",
            "Epoch 11/1000, Train Loss: 5.5519, Val Loss: 5.6805\n",
            "Epoch 12/1000, Train Loss: 5.5505, Val Loss: 5.6748\n",
            "Epoch 13/1000, Train Loss: 5.5661, Val Loss: 5.6822\n",
            "Epoch 14/1000, Train Loss: 5.5617, Val Loss: 5.6680\n",
            "Epoch 15/1000, Train Loss: 5.5514, Val Loss: 5.6688\n",
            "Epoch 16/1000, Train Loss: 5.5559, Val Loss: 5.6687\n",
            "Epoch 17/1000, Train Loss: 5.5524, Val Loss: 5.6629\n",
            "Epoch 18/1000, Train Loss: 5.5519, Val Loss: 5.6654\n",
            "Epoch 19/1000, Train Loss: 5.5572, Val Loss: 5.6721\n",
            "Epoch 20/1000, Train Loss: 5.5487, Val Loss: 5.6651\n",
            "Epoch 21/1000, Train Loss: 5.5607, Val Loss: 5.6672\n",
            "Epoch 22/1000, Train Loss: 5.5545, Val Loss: 5.6677\n",
            "Epoch 23/1000, Train Loss: 5.5525, Val Loss: 5.6649\n",
            "Epoch 24/1000, Train Loss: 5.5555, Val Loss: 5.6732\n",
            "Epoch 25/1000, Train Loss: 5.5543, Val Loss: 5.6635\n",
            "Epoch 26/1000, Train Loss: 5.5504, Val Loss: 5.6778\n",
            "Epoch 27/1000, Train Loss: 5.5519, Val Loss: 5.6637\n",
            "Early stopping triggered at epoch 27. Best Val Loss: 5.6629\n",
            "Epoch 1/1000, Train Loss: 5.5785, Val Loss: 5.6659\n",
            "Epoch 2/1000, Train Loss: 5.5542, Val Loss: 5.6818\n",
            "Epoch 3/1000, Train Loss: 5.5592, Val Loss: 5.7101\n",
            "Epoch 4/1000, Train Loss: 5.5552, Val Loss: 5.6631\n",
            "Epoch 5/1000, Train Loss: 5.5639, Val Loss: 5.6636\n",
            "Epoch 6/1000, Train Loss: 5.5643, Val Loss: 5.6889\n",
            "Epoch 7/1000, Train Loss: 5.5570, Val Loss: 5.6769\n",
            "Epoch 8/1000, Train Loss: 5.5611, Val Loss: 5.6615\n",
            "Epoch 9/1000, Train Loss: 5.5503, Val Loss: 5.6706\n",
            "Epoch 10/1000, Train Loss: 5.5527, Val Loss: 5.6638\n",
            "Epoch 11/1000, Train Loss: 5.5549, Val Loss: 5.6708\n",
            "Epoch 12/1000, Train Loss: 5.5557, Val Loss: 5.6637\n",
            "Epoch 13/1000, Train Loss: 5.5639, Val Loss: 5.6693\n",
            "Epoch 14/1000, Train Loss: 5.5563, Val Loss: 5.6665\n",
            "Epoch 15/1000, Train Loss: 5.5500, Val Loss: 5.6684\n",
            "Epoch 16/1000, Train Loss: 5.5533, Val Loss: 5.6612\n",
            "Epoch 17/1000, Train Loss: 5.5584, Val Loss: 5.6699\n",
            "Epoch 18/1000, Train Loss: 5.5585, Val Loss: 5.6683\n",
            "Epoch 19/1000, Train Loss: 5.5511, Val Loss: 5.6625\n",
            "Epoch 20/1000, Train Loss: 5.5498, Val Loss: 5.6721\n",
            "Epoch 21/1000, Train Loss: 5.5684, Val Loss: 5.6623\n",
            "Epoch 22/1000, Train Loss: 5.5525, Val Loss: 5.6674\n",
            "Epoch 23/1000, Train Loss: 5.5527, Val Loss: 5.6620\n",
            "Epoch 24/1000, Train Loss: 5.5515, Val Loss: 5.6680\n",
            "Epoch 25/1000, Train Loss: 5.5525, Val Loss: 5.6628\n",
            "Epoch 26/1000, Train Loss: 5.5636, Val Loss: 5.6711\n",
            "Early stopping triggered at epoch 26. Best Val Loss: 5.6612\n",
            "Epoch 1/1000, Train Loss: 5.5753, Val Loss: 5.6784\n",
            "Epoch 2/1000, Train Loss: 5.5554, Val Loss: 5.6778\n",
            "Epoch 3/1000, Train Loss: 5.5605, Val Loss: 5.6641\n",
            "Epoch 4/1000, Train Loss: 5.5589, Val Loss: 5.6650\n",
            "Epoch 5/1000, Train Loss: 5.5506, Val Loss: 5.7291\n",
            "Epoch 6/1000, Train Loss: 5.5538, Val Loss: 5.6896\n",
            "Epoch 7/1000, Train Loss: 5.5603, Val Loss: 5.6648\n",
            "Epoch 8/1000, Train Loss: 5.5592, Val Loss: 5.6699\n",
            "Epoch 9/1000, Train Loss: 5.5586, Val Loss: 5.6622\n",
            "Epoch 10/1000, Train Loss: 5.5543, Val Loss: 5.6676\n",
            "Epoch 11/1000, Train Loss: 5.5614, Val Loss: 5.6699\n",
            "Epoch 12/1000, Train Loss: 5.5539, Val Loss: 5.6805\n",
            "Epoch 13/1000, Train Loss: 5.5568, Val Loss: 5.6617\n",
            "Epoch 14/1000, Train Loss: 5.5578, Val Loss: 5.6882\n",
            "Epoch 15/1000, Train Loss: 5.5566, Val Loss: 5.6624\n",
            "Epoch 16/1000, Train Loss: 5.5593, Val Loss: 5.7030\n",
            "Epoch 17/1000, Train Loss: 5.5627, Val Loss: 5.6666\n",
            "Epoch 18/1000, Train Loss: 5.5560, Val Loss: 5.6632\n",
            "Epoch 19/1000, Train Loss: 5.5619, Val Loss: 5.6741\n",
            "Epoch 20/1000, Train Loss: 5.5502, Val Loss: 5.6903\n",
            "Epoch 21/1000, Train Loss: 5.5468, Val Loss: 5.6770\n",
            "Epoch 22/1000, Train Loss: 5.5541, Val Loss: 5.6714\n",
            "Epoch 23/1000, Train Loss: 5.5472, Val Loss: 5.6731\n",
            "Early stopping triggered at epoch 23. Best Val Loss: 5.6617\n",
            "Training with LR=0.01, Hidden=64, Dropout=0.1, Scale Factor=1.25\n",
            "Combination 77/81\n",
            "Epoch 1/1000, Train Loss: 6.6416, Val Loss: 6.7508\n",
            "Epoch 2/1000, Train Loss: 6.6189, Val Loss: 6.7658\n",
            "Epoch 3/1000, Train Loss: 6.6019, Val Loss: 6.7637\n",
            "Epoch 4/1000, Train Loss: 6.6133, Val Loss: 6.7652\n",
            "Epoch 5/1000, Train Loss: 6.6198, Val Loss: 6.7703\n",
            "Epoch 6/1000, Train Loss: 6.6163, Val Loss: 6.7762\n",
            "Epoch 7/1000, Train Loss: 6.6228, Val Loss: 6.7761\n",
            "Epoch 8/1000, Train Loss: 6.6149, Val Loss: 6.7775\n",
            "Epoch 9/1000, Train Loss: 6.6257, Val Loss: 6.7635\n",
            "Epoch 10/1000, Train Loss: 6.6116, Val Loss: 6.7614\n",
            "Epoch 11/1000, Train Loss: 6.6183, Val Loss: 6.7514\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 6.7508\n",
            "Epoch 1/1000, Train Loss: 6.6446, Val Loss: 6.7791\n",
            "Epoch 2/1000, Train Loss: 6.6193, Val Loss: 6.7814\n",
            "Epoch 3/1000, Train Loss: 6.6159, Val Loss: 6.7807\n",
            "Epoch 4/1000, Train Loss: 6.6147, Val Loss: 6.7984\n",
            "Epoch 5/1000, Train Loss: 6.6293, Val Loss: 6.7743\n",
            "Epoch 6/1000, Train Loss: 6.6165, Val Loss: 6.7713\n",
            "Epoch 7/1000, Train Loss: 6.6193, Val Loss: 6.7806\n",
            "Epoch 8/1000, Train Loss: 6.6242, Val Loss: 6.7837\n",
            "Epoch 9/1000, Train Loss: 6.6189, Val Loss: 6.7814\n",
            "Epoch 10/1000, Train Loss: 6.6827, Val Loss: 6.7916\n",
            "Epoch 11/1000, Train Loss: 6.6097, Val Loss: 6.7672\n",
            "Epoch 12/1000, Train Loss: 6.6111, Val Loss: 6.7682\n",
            "Epoch 13/1000, Train Loss: 6.6217, Val Loss: 6.7757\n",
            "Epoch 14/1000, Train Loss: 6.6182, Val Loss: 6.7755\n",
            "Epoch 15/1000, Train Loss: 6.6198, Val Loss: 6.7746\n",
            "Epoch 16/1000, Train Loss: 6.6159, Val Loss: 6.8012\n",
            "Epoch 17/1000, Train Loss: 6.6196, Val Loss: 6.7875\n",
            "Epoch 18/1000, Train Loss: 6.6252, Val Loss: 6.7766\n",
            "Epoch 19/1000, Train Loss: 6.6287, Val Loss: 6.7719\n",
            "Epoch 20/1000, Train Loss: 6.6139, Val Loss: 6.7464\n",
            "Epoch 21/1000, Train Loss: 6.6183, Val Loss: 6.7567\n",
            "Epoch 22/1000, Train Loss: 6.6237, Val Loss: 6.7559\n",
            "Epoch 23/1000, Train Loss: 6.6051, Val Loss: 6.7562\n",
            "Epoch 24/1000, Train Loss: 6.6082, Val Loss: 6.7514\n",
            "Epoch 25/1000, Train Loss: 6.6100, Val Loss: 6.7677\n",
            "Epoch 26/1000, Train Loss: 6.6171, Val Loss: 6.7700\n",
            "Epoch 27/1000, Train Loss: 6.6178, Val Loss: 6.7640\n",
            "Epoch 28/1000, Train Loss: 6.6082, Val Loss: 6.7614\n",
            "Epoch 29/1000, Train Loss: 6.6129, Val Loss: 6.7666\n",
            "Epoch 30/1000, Train Loss: 6.6137, Val Loss: 6.7848\n",
            "Early stopping triggered at epoch 30. Best Val Loss: 6.7464\n",
            "Epoch 1/1000, Train Loss: 6.6365, Val Loss: 6.7737\n",
            "Epoch 2/1000, Train Loss: 6.6256, Val Loss: 6.7615\n",
            "Epoch 3/1000, Train Loss: 6.6187, Val Loss: 6.7824\n",
            "Epoch 4/1000, Train Loss: 6.6186, Val Loss: 7.4498\n",
            "Epoch 5/1000, Train Loss: 6.6128, Val Loss: 6.7888\n",
            "Epoch 6/1000, Train Loss: 6.6245, Val Loss: 6.7744\n",
            "Epoch 7/1000, Train Loss: 6.6262, Val Loss: 6.7479\n",
            "Epoch 8/1000, Train Loss: 6.6184, Val Loss: 6.7465\n",
            "Epoch 9/1000, Train Loss: 6.6131, Val Loss: 6.7470\n",
            "Epoch 10/1000, Train Loss: 6.6155, Val Loss: 6.7691\n",
            "Epoch 11/1000, Train Loss: 6.6529, Val Loss: 6.7533\n",
            "Epoch 12/1000, Train Loss: 6.6184, Val Loss: 6.7525\n",
            "Epoch 13/1000, Train Loss: 6.6074, Val Loss: 6.7590\n",
            "Epoch 14/1000, Train Loss: 6.5995, Val Loss: 6.7616\n",
            "Epoch 15/1000, Train Loss: 6.6051, Val Loss: 6.7742\n",
            "Epoch 16/1000, Train Loss: 6.6090, Val Loss: 7.0126\n",
            "Epoch 17/1000, Train Loss: 6.6129, Val Loss: 6.7876\n",
            "Epoch 18/1000, Train Loss: 6.6124, Val Loss: 6.7578\n",
            "Early stopping triggered at epoch 18. Best Val Loss: 6.7465\n",
            "Training with LR=0.01, Hidden=64, Dropout=0.1, Scale Factor=1.5\n",
            "Combination 78/81\n",
            "Epoch 1/1000, Train Loss: 7.6837, Val Loss: 7.8470\n",
            "Epoch 2/1000, Train Loss: 7.6779, Val Loss: 7.8332\n",
            "Epoch 3/1000, Train Loss: 7.6787, Val Loss: 7.8263\n",
            "Epoch 4/1000, Train Loss: 7.6509, Val Loss: 7.8272\n",
            "Epoch 5/1000, Train Loss: 7.6564, Val Loss: 7.8344\n",
            "Epoch 6/1000, Train Loss: 7.6769, Val Loss: 7.8326\n",
            "Epoch 7/1000, Train Loss: 7.6684, Val Loss: 7.8095\n",
            "Epoch 8/1000, Train Loss: 7.6497, Val Loss: 7.8313\n",
            "Epoch 9/1000, Train Loss: 7.6525, Val Loss: 7.8277\n",
            "Epoch 10/1000, Train Loss: 7.6621, Val Loss: 7.8123\n",
            "Epoch 11/1000, Train Loss: 7.6619, Val Loss: 7.9150\n",
            "Epoch 12/1000, Train Loss: 7.7773, Val Loss: 7.8231\n",
            "Epoch 13/1000, Train Loss: 7.6487, Val Loss: 7.8227\n",
            "Epoch 14/1000, Train Loss: 7.6497, Val Loss: 8.0267\n",
            "Epoch 15/1000, Train Loss: 7.6626, Val Loss: 7.8192\n",
            "Epoch 16/1000, Train Loss: 7.6460, Val Loss: 7.8189\n",
            "Epoch 17/1000, Train Loss: 7.6652, Val Loss: 7.8266\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 7.8095\n",
            "Epoch 1/1000, Train Loss: 7.8220, Val Loss: 7.9759\n",
            "Epoch 2/1000, Train Loss: 8.2422, Val Loss: 7.9759\n",
            "Epoch 3/1000, Train Loss: 7.8801, Val Loss: 7.9759\n",
            "Epoch 4/1000, Train Loss: 7.8595, Val Loss: 7.9759\n",
            "Epoch 5/1000, Train Loss: 7.8681, Val Loss: 7.9759\n",
            "Epoch 6/1000, Train Loss: 7.8288, Val Loss: 7.9759\n",
            "Epoch 7/1000, Train Loss: 9.4685, Val Loss: 11.9692\n",
            "Epoch 8/1000, Train Loss: 11.7915, Val Loss: 11.9692\n",
            "Epoch 9/1000, Train Loss: 11.7845, Val Loss: 11.9692\n",
            "Epoch 10/1000, Train Loss: 11.7838, Val Loss: 11.9692\n",
            "Epoch 11/1000, Train Loss: 11.7874, Val Loss: 11.9692\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.9759\n",
            "Epoch 1/1000, Train Loss: 7.7087, Val Loss: 7.8186\n",
            "Epoch 2/1000, Train Loss: 7.6636, Val Loss: 7.8277\n",
            "Epoch 3/1000, Train Loss: 7.6607, Val Loss: 7.8311\n",
            "Epoch 4/1000, Train Loss: 7.6525, Val Loss: 7.8515\n",
            "Epoch 5/1000, Train Loss: 7.6584, Val Loss: 7.8473\n",
            "Epoch 6/1000, Train Loss: 7.6675, Val Loss: 7.8319\n",
            "Epoch 7/1000, Train Loss: 7.6578, Val Loss: 7.8482\n",
            "Epoch 8/1000, Train Loss: 7.6702, Val Loss: 7.8262\n",
            "Epoch 9/1000, Train Loss: 7.6867, Val Loss: 7.8245\n",
            "Epoch 10/1000, Train Loss: 7.6468, Val Loss: 7.8227\n",
            "Epoch 11/1000, Train Loss: 7.6661, Val Loss: 7.8239\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8186\n",
            "Training with LR=0.01, Hidden=64, Dropout=0.2, Scale Factor=1.0\n",
            "Combination 79/81\n",
            "Epoch 1/1000, Train Loss: 5.6089, Val Loss: 5.6992\n",
            "Epoch 2/1000, Train Loss: 5.5640, Val Loss: 5.6754\n",
            "Epoch 3/1000, Train Loss: 5.5582, Val Loss: 5.6711\n",
            "Epoch 4/1000, Train Loss: 5.5606, Val Loss: 5.6616\n",
            "Epoch 5/1000, Train Loss: 5.5599, Val Loss: 5.6644\n",
            "Epoch 6/1000, Train Loss: 5.5612, Val Loss: 5.6697\n",
            "Epoch 7/1000, Train Loss: 5.5699, Val Loss: 5.7066\n",
            "Epoch 8/1000, Train Loss: 5.5580, Val Loss: 5.6994\n",
            "Epoch 9/1000, Train Loss: 5.5604, Val Loss: 5.6909\n",
            "Epoch 10/1000, Train Loss: 5.5590, Val Loss: 5.6618\n",
            "Epoch 11/1000, Train Loss: 5.5531, Val Loss: 5.6772\n",
            "Epoch 12/1000, Train Loss: 5.5578, Val Loss: 5.6719\n",
            "Epoch 13/1000, Train Loss: 5.5600, Val Loss: 5.6624\n",
            "Epoch 14/1000, Train Loss: 5.5571, Val Loss: 5.6612\n",
            "Epoch 15/1000, Train Loss: 5.5549, Val Loss: 5.6642\n",
            "Epoch 16/1000, Train Loss: 5.5533, Val Loss: 5.6632\n",
            "Epoch 17/1000, Train Loss: 5.5587, Val Loss: 5.6643\n",
            "Epoch 18/1000, Train Loss: 5.5545, Val Loss: 5.6894\n",
            "Epoch 19/1000, Train Loss: 5.5639, Val Loss: 5.6650\n",
            "Epoch 20/1000, Train Loss: 5.5528, Val Loss: 5.6656\n",
            "Epoch 21/1000, Train Loss: 5.5632, Val Loss: 5.6692\n",
            "Epoch 22/1000, Train Loss: 5.5581, Val Loss: 5.6716\n",
            "Epoch 23/1000, Train Loss: 5.5551, Val Loss: 5.6669\n",
            "Epoch 24/1000, Train Loss: 5.5499, Val Loss: 5.6643\n",
            "Early stopping triggered at epoch 24. Best Val Loss: 5.6612\n",
            "Epoch 1/1000, Train Loss: 5.5760, Val Loss: 5.6802\n",
            "Epoch 2/1000, Train Loss: 5.5662, Val Loss: 5.6665\n",
            "Epoch 3/1000, Train Loss: 5.5577, Val Loss: 5.6647\n",
            "Epoch 4/1000, Train Loss: 5.5707, Val Loss: 5.7006\n",
            "Epoch 5/1000, Train Loss: 5.5644, Val Loss: 5.6849\n",
            "Epoch 6/1000, Train Loss: 5.5696, Val Loss: 5.6659\n",
            "Epoch 7/1000, Train Loss: 5.5597, Val Loss: 5.6673\n",
            "Epoch 8/1000, Train Loss: 5.5841, Val Loss: 5.6661\n",
            "Epoch 9/1000, Train Loss: 5.5518, Val Loss: 5.6878\n",
            "Epoch 10/1000, Train Loss: 5.5589, Val Loss: 5.6759\n",
            "Epoch 11/1000, Train Loss: 5.5557, Val Loss: 5.6789\n",
            "Epoch 12/1000, Train Loss: 5.5576, Val Loss: 5.6770\n",
            "Epoch 13/1000, Train Loss: 5.5614, Val Loss: 5.6937\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 5.6647\n",
            "Epoch 1/1000, Train Loss: 5.5905, Val Loss: 5.6993\n",
            "Epoch 2/1000, Train Loss: 5.5532, Val Loss: 5.7103\n",
            "Epoch 3/1000, Train Loss: 5.5667, Val Loss: 5.7022\n",
            "Epoch 4/1000, Train Loss: 5.5767, Val Loss: 5.6709\n",
            "Epoch 5/1000, Train Loss: 5.5612, Val Loss: 5.6612\n",
            "Epoch 6/1000, Train Loss: 5.5522, Val Loss: 5.6603\n",
            "Epoch 7/1000, Train Loss: 5.5621, Val Loss: 5.6695\n",
            "Epoch 8/1000, Train Loss: 5.5735, Val Loss: 5.6677\n",
            "Epoch 9/1000, Train Loss: 5.5717, Val Loss: 5.6970\n",
            "Epoch 10/1000, Train Loss: 5.5527, Val Loss: 5.7096\n",
            "Epoch 11/1000, Train Loss: 5.5785, Val Loss: 5.6959\n",
            "Epoch 12/1000, Train Loss: 5.5958, Val Loss: 5.6777\n",
            "Epoch 13/1000, Train Loss: 5.5619, Val Loss: 5.6810\n",
            "Epoch 14/1000, Train Loss: 5.5572, Val Loss: 5.6724\n",
            "Epoch 15/1000, Train Loss: 5.5621, Val Loss: 5.6793\n",
            "Epoch 16/1000, Train Loss: 5.5489, Val Loss: 5.6663\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 5.6603\n",
            "Training with LR=0.01, Hidden=64, Dropout=0.2, Scale Factor=1.25\n",
            "Combination 80/81\n",
            "Epoch 1/1000, Train Loss: 6.6390, Val Loss: 6.7669\n",
            "Epoch 2/1000, Train Loss: 6.6412, Val Loss: 6.7715\n",
            "Epoch 3/1000, Train Loss: 6.6233, Val Loss: 6.7763\n",
            "Epoch 4/1000, Train Loss: 6.6250, Val Loss: 6.7652\n",
            "Epoch 5/1000, Train Loss: 6.6213, Val Loss: 6.8184\n",
            "Epoch 6/1000, Train Loss: 6.6415, Val Loss: 6.7553\n",
            "Epoch 7/1000, Train Loss: 6.6298, Val Loss: 6.7852\n",
            "Epoch 8/1000, Train Loss: 6.6261, Val Loss: 6.7841\n",
            "Epoch 9/1000, Train Loss: 6.6177, Val Loss: 6.7771\n",
            "Epoch 10/1000, Train Loss: 6.6137, Val Loss: 6.7807\n",
            "Epoch 11/1000, Train Loss: 6.6272, Val Loss: 6.7846\n",
            "Epoch 12/1000, Train Loss: 6.6259, Val Loss: 6.7870\n",
            "Epoch 13/1000, Train Loss: 6.6147, Val Loss: 6.7660\n",
            "Epoch 14/1000, Train Loss: 6.6341, Val Loss: 6.7737\n",
            "Epoch 15/1000, Train Loss: 6.6139, Val Loss: 6.7716\n",
            "Epoch 16/1000, Train Loss: 6.6188, Val Loss: 6.7616\n",
            "Early stopping triggered at epoch 16. Best Val Loss: 6.7553\n",
            "Epoch 1/1000, Train Loss: 6.6350, Val Loss: 6.7757\n",
            "Epoch 2/1000, Train Loss: 6.6322, Val Loss: 6.7570\n",
            "Epoch 3/1000, Train Loss: 6.6307, Val Loss: 6.7583\n",
            "Epoch 4/1000, Train Loss: 6.6317, Val Loss: 6.7545\n",
            "Epoch 5/1000, Train Loss: 6.6203, Val Loss: 6.7821\n",
            "Epoch 6/1000, Train Loss: 6.6657, Val Loss: 6.7684\n",
            "Epoch 7/1000, Train Loss: 6.6521, Val Loss: 6.7541\n",
            "Epoch 8/1000, Train Loss: 6.6218, Val Loss: 6.7684\n",
            "Epoch 9/1000, Train Loss: 6.6122, Val Loss: 6.7736\n",
            "Epoch 10/1000, Train Loss: 6.6163, Val Loss: 6.7636\n",
            "Epoch 11/1000, Train Loss: 6.6520, Val Loss: 6.7578\n",
            "Epoch 12/1000, Train Loss: 6.6107, Val Loss: 6.7930\n",
            "Epoch 13/1000, Train Loss: 6.6264, Val Loss: 6.7639\n",
            "Epoch 14/1000, Train Loss: 6.6171, Val Loss: 6.7671\n",
            "Epoch 15/1000, Train Loss: 6.6208, Val Loss: 6.7787\n",
            "Epoch 16/1000, Train Loss: 6.6072, Val Loss: 6.7929\n",
            "Epoch 17/1000, Train Loss: 6.6294, Val Loss: 6.7635\n",
            "Early stopping triggered at epoch 17. Best Val Loss: 6.7541\n",
            "Epoch 1/1000, Train Loss: 6.6497, Val Loss: 6.8700\n",
            "Epoch 2/1000, Train Loss: 6.6286, Val Loss: 6.7771\n",
            "Epoch 3/1000, Train Loss: 6.6292, Val Loss: 6.7642\n",
            "Epoch 4/1000, Train Loss: 6.6285, Val Loss: 6.7960\n",
            "Epoch 5/1000, Train Loss: 6.6170, Val Loss: 6.7872\n",
            "Epoch 6/1000, Train Loss: 6.6161, Val Loss: 6.7815\n",
            "Epoch 7/1000, Train Loss: 6.6328, Val Loss: 6.7764\n",
            "Epoch 8/1000, Train Loss: 6.6580, Val Loss: 6.7695\n",
            "Epoch 9/1000, Train Loss: 6.6278, Val Loss: 6.7753\n",
            "Epoch 10/1000, Train Loss: 6.6305, Val Loss: 6.7720\n",
            "Epoch 11/1000, Train Loss: 6.6417, Val Loss: 6.7896\n",
            "Epoch 12/1000, Train Loss: 6.6171, Val Loss: 6.8156\n",
            "Epoch 13/1000, Train Loss: 6.6245, Val Loss: 6.7781\n",
            "Early stopping triggered at epoch 13. Best Val Loss: 6.7642\n",
            "Training with LR=0.01, Hidden=64, Dropout=0.2, Scale Factor=1.5\n",
            "Combination 81/81\n",
            "Epoch 1/1000, Train Loss: 7.7204, Val Loss: 7.8214\n",
            "Epoch 2/1000, Train Loss: 7.6514, Val Loss: 7.8224\n",
            "Epoch 3/1000, Train Loss: 7.6451, Val Loss: 7.8420\n",
            "Epoch 4/1000, Train Loss: 7.7788, Val Loss: 7.8298\n",
            "Epoch 5/1000, Train Loss: 7.6844, Val Loss: 7.8264\n",
            "Epoch 6/1000, Train Loss: 7.6636, Val Loss: 7.8253\n",
            "Epoch 7/1000, Train Loss: 7.6521, Val Loss: 7.8275\n",
            "Epoch 8/1000, Train Loss: 7.6908, Val Loss: 7.8257\n",
            "Epoch 9/1000, Train Loss: 7.6554, Val Loss: 7.8245\n",
            "Epoch 10/1000, Train Loss: 7.6581, Val Loss: 7.8299\n",
            "Epoch 11/1000, Train Loss: 7.6452, Val Loss: 7.8926\n",
            "Early stopping triggered at epoch 11. Best Val Loss: 7.8214\n",
            "Epoch 1/1000, Train Loss: 7.7733, Val Loss: 7.8613\n",
            "Epoch 2/1000, Train Loss: 7.6637, Val Loss: 7.8155\n",
            "Epoch 3/1000, Train Loss: 7.6624, Val Loss: 7.8273\n",
            "Epoch 4/1000, Train Loss: 7.6554, Val Loss: 7.8274\n",
            "Epoch 5/1000, Train Loss: 7.6932, Val Loss: 7.8290\n",
            "Epoch 6/1000, Train Loss: 7.6557, Val Loss: 7.8258\n",
            "Epoch 7/1000, Train Loss: 7.6471, Val Loss: 7.8260\n",
            "Epoch 8/1000, Train Loss: 7.7293, Val Loss: 7.8244\n",
            "Epoch 9/1000, Train Loss: 7.6673, Val Loss: 7.8260\n",
            "Epoch 10/1000, Train Loss: 7.6654, Val Loss: 7.8348\n",
            "Epoch 11/1000, Train Loss: 7.6873, Val Loss: 7.8178\n",
            "Epoch 12/1000, Train Loss: 7.6552, Val Loss: 7.8232\n",
            "Early stopping triggered at epoch 12. Best Val Loss: 7.8155\n",
            "Epoch 1/1000, Train Loss: 7.6865, Val Loss: 7.8235\n",
            "Epoch 2/1000, Train Loss: 7.6573, Val Loss: 7.8291\n",
            "Epoch 3/1000, Train Loss: 7.6536, Val Loss: 7.8291\n",
            "Epoch 4/1000, Train Loss: 7.6625, Val Loss: 7.8378\n",
            "Epoch 5/1000, Train Loss: 7.6927, Val Loss: 7.8266\n",
            "Epoch 6/1000, Train Loss: 7.6627, Val Loss: 7.8286\n",
            "Epoch 7/1000, Train Loss: 7.6643, Val Loss: 7.8260\n",
            "Epoch 8/1000, Train Loss: 7.6597, Val Loss: 7.8287\n",
            "Epoch 9/1000, Train Loss: 7.6487, Val Loss: 7.8228\n",
            "Epoch 10/1000, Train Loss: 7.6794, Val Loss: 7.8179\n",
            "Epoch 11/1000, Train Loss: 7.6592, Val Loss: 7.8251\n",
            "Epoch 12/1000, Train Loss: 7.6510, Val Loss: 7.8190\n",
            "Epoch 13/1000, Train Loss: 7.6709, Val Loss: 7.8307\n",
            "Epoch 14/1000, Train Loss: 7.6476, Val Loss: 7.8308\n",
            "Epoch 15/1000, Train Loss: 7.6632, Val Loss: 7.8263\n",
            "Epoch 16/1000, Train Loss: 7.9608, Val Loss: 7.8215\n",
            "Epoch 17/1000, Train Loss: 7.7567, Val Loss: 7.8236\n",
            "Epoch 18/1000, Train Loss: 7.6439, Val Loss: 7.8325\n",
            "Epoch 19/1000, Train Loss: 7.6598, Val Loss: 7.8257\n",
            "Epoch 20/1000, Train Loss: 7.6811, Val Loss: 7.8246\n",
            "Early stopping triggered at epoch 20. Best Val Loss: 7.8179\n",
            "Hyperparameter tuning complete. Results saved to hyperparameter_tuning_results.csv\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Define hyperparameter grid\n",
        "learning_rates = [0.001, 0.005, 0.01]\n",
        "hidden_sizes = [16, 32, 64]\n",
        "dropouts = [0.0,0.1, 0.2]\n",
        "scale_factors = [1.0, 1.25, 1.5]\n",
        "\n",
        "\n",
        "# Number of runs per configuration\n",
        "num_runs = 3\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "total_combinations = len(learning_rates) * len(hidden_sizes) * len(dropouts) * len(scale_factors)\n",
        "count = 0\n",
        "# Iterate over hyperparameter combinations\n",
        "for lr, hidden_size, dropout, scale_factor in itertools.product(learning_rates, hidden_sizes, dropouts, scale_factors):\n",
        "    print(f\"Training with LR={lr}, Hidden={hidden_size}, Dropout={dropout}, Scale Factor={scale_factor}\")\n",
        "    print(f\"Combination {count+1}/{total_combinations}\")\n",
        "    count += 1\n",
        "\n",
        "    # Store results for multiple runs\n",
        "    run_results = []\n",
        "    for _ in range(num_runs):\n",
        "        model = train_bias_predictor(\n",
        "            reconstructed_train_tensor, bias_train_tensor,\n",
        "            reconstructed_val_tensor, bias_val_tensor,\n",
        "            epochs=1000, learning_rate=lr, patience=10,\n",
        "            input_dim=1, hidden_size=hidden_size, dropout=dropout, scale_factor=scale_factor\n",
        "        )\n",
        "        model.eval()\n",
        "        run_results.append(evaluate_model(model, reconstructed_test_tensor, y_pred_test, ground_truth_test))\n",
        "\n",
        "    # Compute averages across runs\n",
        "    avg_results = [sum(x) / num_runs for x in zip(*run_results)]\n",
        "\n",
        "    # Store in results list\n",
        "    results.append([lr, hidden_size, dropout, scale_factor] + avg_results)\n",
        "\n",
        "# Convert to DataFrame\n",
        "columns = [\"Learning Rate\", \"Hidden Size\", \"Dropout\", \"Scale Factor\",\n",
        "           \"Original Accuracy\", \"Corrected Accuracy\",\n",
        "           \"Original Recall\", \"Original Precision\", \"Original F1 Score\",\n",
        "           \"Corrected Recall\", \"Corrected Precision\", \"Corrected F1 Score\"]\n",
        "df_results = pd.DataFrame(results, columns=columns)\n",
        "\n",
        "# Save to CSV\n",
        "df_results.to_csv(\"hyperparameter_tuning_results.csv\", index=False)\n",
        "print(\"Hyperparameter tuning complete. Results saved to hyperparameter_tuning_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sort by corrected F1 score\n",
        "df_results = df_results.sort_values(by='Corrected F1 Score', ascending=False)\n",
        "df_results.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "_OF5nFm9RXkt",
        "outputId": "559eeb2e-1833-45a6-c267-9e4e504c18e6"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Learning Rate  Hidden Size  Dropout  Scale Factor  Original Accuracy  \\\n",
              "7           0.001           16      0.2          1.25           0.973333   \n",
              "4           0.001           16      0.1          1.25           0.973333   \n",
              "10          0.001           32      0.0          1.25           0.973333   \n",
              "46          0.005           64      0.0          1.25           0.973333   \n",
              "47          0.005           64      0.0          1.50           0.973333   \n",
              "61          0.010           16      0.2          1.25           0.973333   \n",
              "34          0.005           16      0.2          1.25           0.973333   \n",
              "40          0.005           32      0.1          1.25           0.973333   \n",
              "49          0.005           64      0.1          1.25           0.973333   \n",
              "37          0.005           32      0.0          1.25           0.973333   \n",
              "\n",
              "    Corrected Accuracy  Original Recall  Original Precision  \\\n",
              "7             0.974067         0.669983            0.997531   \n",
              "4             0.974044         0.669983            0.997531   \n",
              "10            0.974044         0.669983            0.997531   \n",
              "46            0.974044         0.669983            0.997531   \n",
              "47            0.974000         0.669983            0.997531   \n",
              "61            0.974044         0.669983            0.997531   \n",
              "34            0.974044         0.669983            0.997531   \n",
              "40            0.974022         0.669983            0.997531   \n",
              "49            0.974000         0.669983            0.997531   \n",
              "37            0.974022         0.669983            0.997531   \n",
              "\n",
              "    Original F1 Score  Corrected Recall  Corrected Precision  \\\n",
              "7            0.801587          0.680763             0.995153   \n",
              "4            0.801587          0.681039             0.994351   \n",
              "10           0.801587          0.681039             0.994354   \n",
              "46           0.801587          0.680763             0.994752   \n",
              "47           0.801587          0.681592             0.992755   \n",
              "61           0.801587          0.680210             0.995552   \n",
              "34           0.801587          0.680210             0.995551   \n",
              "40           0.801587          0.680763             0.994354   \n",
              "49           0.801587          0.681316             0.993151   \n",
              "37           0.801587          0.680486             0.994751   \n",
              "\n",
              "    Corrected F1 Score  \n",
              "7             0.808469  \n",
              "4             0.808399  \n",
              "10            0.808399  \n",
              "46            0.808336  \n",
              "47            0.808260  \n",
              "61            0.808210  \n",
              "34            0.808210  \n",
              "40            0.808204  \n",
              "49            0.808197  \n",
              "37            0.808140  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3228c088-fd2e-4b68-8b1e-028a461ead4f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Learning Rate</th>\n",
              "      <th>Hidden Size</th>\n",
              "      <th>Dropout</th>\n",
              "      <th>Scale Factor</th>\n",
              "      <th>Original Accuracy</th>\n",
              "      <th>Corrected Accuracy</th>\n",
              "      <th>Original Recall</th>\n",
              "      <th>Original Precision</th>\n",
              "      <th>Original F1 Score</th>\n",
              "      <th>Corrected Recall</th>\n",
              "      <th>Corrected Precision</th>\n",
              "      <th>Corrected F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.001</td>\n",
              "      <td>16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.973333</td>\n",
              "      <td>0.974067</td>\n",
              "      <td>0.669983</td>\n",
              "      <td>0.997531</td>\n",
              "      <td>0.801587</td>\n",
              "      <td>0.680763</td>\n",
              "      <td>0.995153</td>\n",
              "      <td>0.808469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.001</td>\n",
              "      <td>16</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.973333</td>\n",
              "      <td>0.974044</td>\n",
              "      <td>0.669983</td>\n",
              "      <td>0.997531</td>\n",
              "      <td>0.801587</td>\n",
              "      <td>0.681039</td>\n",
              "      <td>0.994351</td>\n",
              "      <td>0.808399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.973333</td>\n",
              "      <td>0.974044</td>\n",
              "      <td>0.669983</td>\n",
              "      <td>0.997531</td>\n",
              "      <td>0.801587</td>\n",
              "      <td>0.681039</td>\n",
              "      <td>0.994354</td>\n",
              "      <td>0.808399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.005</td>\n",
              "      <td>64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.973333</td>\n",
              "      <td>0.974044</td>\n",
              "      <td>0.669983</td>\n",
              "      <td>0.997531</td>\n",
              "      <td>0.801587</td>\n",
              "      <td>0.680763</td>\n",
              "      <td>0.994752</td>\n",
              "      <td>0.808336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.005</td>\n",
              "      <td>64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.973333</td>\n",
              "      <td>0.974000</td>\n",
              "      <td>0.669983</td>\n",
              "      <td>0.997531</td>\n",
              "      <td>0.801587</td>\n",
              "      <td>0.681592</td>\n",
              "      <td>0.992755</td>\n",
              "      <td>0.808260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>0.010</td>\n",
              "      <td>16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.973333</td>\n",
              "      <td>0.974044</td>\n",
              "      <td>0.669983</td>\n",
              "      <td>0.997531</td>\n",
              "      <td>0.801587</td>\n",
              "      <td>0.680210</td>\n",
              "      <td>0.995552</td>\n",
              "      <td>0.808210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.005</td>\n",
              "      <td>16</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.973333</td>\n",
              "      <td>0.974044</td>\n",
              "      <td>0.669983</td>\n",
              "      <td>0.997531</td>\n",
              "      <td>0.801587</td>\n",
              "      <td>0.680210</td>\n",
              "      <td>0.995551</td>\n",
              "      <td>0.808210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.005</td>\n",
              "      <td>32</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.973333</td>\n",
              "      <td>0.974022</td>\n",
              "      <td>0.669983</td>\n",
              "      <td>0.997531</td>\n",
              "      <td>0.801587</td>\n",
              "      <td>0.680763</td>\n",
              "      <td>0.994354</td>\n",
              "      <td>0.808204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.005</td>\n",
              "      <td>64</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.973333</td>\n",
              "      <td>0.974000</td>\n",
              "      <td>0.669983</td>\n",
              "      <td>0.997531</td>\n",
              "      <td>0.801587</td>\n",
              "      <td>0.681316</td>\n",
              "      <td>0.993151</td>\n",
              "      <td>0.808197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.005</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.973333</td>\n",
              "      <td>0.974022</td>\n",
              "      <td>0.669983</td>\n",
              "      <td>0.997531</td>\n",
              "      <td>0.801587</td>\n",
              "      <td>0.680486</td>\n",
              "      <td>0.994751</td>\n",
              "      <td>0.808140</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3228c088-fd2e-4b68-8b1e-028a461ead4f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3228c088-fd2e-4b68-8b1e-028a461ead4f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3228c088-fd2e-4b68-8b1e-028a461ead4f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ccd236d9-c079-4d08-9cd2-98418793914d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ccd236d9-c079-4d08-9cd2-98418793914d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ccd236d9-c079-4d08-9cd2-98418793914d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 81,\n  \"fields\": [\n    {\n      \"column\": \"Learning Rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0037047267105685413,\n        \"min\": 0.001,\n        \"max\": 0.01,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.001,\n          0.005,\n          0.01\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hidden Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 16,\n        \"max\": 64,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          16,\n          32,\n          64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dropout\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08215838362577495,\n        \"min\": 0.0,\n        \"max\": 0.2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.2,\n          0.1,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Scale Factor\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2053959590644373,\n        \"min\": 1.0,\n        \"max\": 1.5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.25,\n          1.5,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.351421106883819e-16,\n        \"min\": 0.9733333333333333,\n        \"max\": 0.9733333333333333,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9733333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.961670868468695e-05,\n        \"min\": 0.9736444444444444,\n        \"max\": 0.9740666666666667,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.9738888888888889\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.6699834162520729,\n        \"max\": 0.6699834162520729,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6699834162520729\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.117140368961273e-15,\n        \"min\": 0.997530864197531,\n        \"max\": 0.997530864197531,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.997530864197531\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Original F1 Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.937122951690182e-16,\n        \"min\": 0.8015873015873017,\n        \"max\": 0.8015873015873017,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.8015873015873017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0018598585264399882,\n        \"min\": 0.6763405196241017,\n        \"max\": 0.6832504145936982,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          0.6793808734107242\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0024165674623359127,\n        \"min\": 0.9880319283996594,\n        \"max\": 0.9963518395460594,\n        \"num_unique_values\": 74,\n        \"samples\": [\n          0.992754588550366\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Corrected F1 Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0007668275567284438,\n        \"min\": 0.8049060814042126,\n        \"max\": 0.8084686107940083,\n        \"num_unique_values\": 74,\n        \"samples\": [\n          0.8082597172646753\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}