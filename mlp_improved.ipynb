{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('diabetes_prediction_dataset.csv')\n",
    "df_encoded = pd.get_dummies(df, columns=['gender', 'smoking_history'], drop_first=True)\n",
    "testDF = df_encoded.sample(frac=1).reset_index(drop=True)\n",
    "x_unscaled = testDF.drop(['diabetes'], axis=1)\n",
    "y = testDF['diabetes']\n",
    "\n",
    "# Normalize the data\n",
    "numerical_columns = x_unscaled.select_dtypes(include=np.number).columns\n",
    "boolean_columns = x_unscaled.select_dtypes(include=bool).columns\n",
    "scaler = StandardScaler()\n",
    "temp = pd.DataFrame(scaler.fit_transform(x_unscaled[numerical_columns]), columns=numerical_columns)\n",
    "x_scaled = pd.concat([temp, x_unscaled[boolean_columns]], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder model with reduced complexity and dropout\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, encoding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Define the Decoder model with reduced complexity and dropout\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoding_dim, input_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_16888/3703475411.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(encoder_model_file))\n",
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_16888/3703475411.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load(decoder_model_file))\n"
     ]
    }
   ],
   "source": [
    "# Define input dimensions\n",
    "input_dim = x_train.shape[1]\n",
    "encoding_dim = 8\n",
    "\n",
    "# Load the trained autoencoder models\n",
    "encoder_model_file = './models/encoder2.pth'\n",
    "decoder_model_file = './models/decoder2.pth'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(input_dim, encoding_dim).to(device)\n",
    "decoder = Decoder(encoding_dim, input_dim).to(device)\n",
    "encoder.load_state_dict(torch.load(encoder_model_file))\n",
    "decoder.load_state_dict(torch.load(decoder_model_file))\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "autoencoder = Autoencoder(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values below 0.5: 18868\n",
      "Values above 0.5: 1132\n",
      "Original MLP Accuracy: 97.16%\n",
      "Total correct predictions: 19432\n",
      "Total wrong predictions: 568\n",
      "\n",
      "True Negatives: 18308\n",
      "False Positives: 8\n",
      "False Negatives: 560\n",
      "True Positives: 1124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_16888/1955608925.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  original_mlp.load_state_dict(torch.load(mlp_model_file))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Define the original MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        # self.model = nn.Sequential(\n",
    "        #     nn.Linear(input_dim, 128),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(128, 64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(64, 32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(32, 16),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(16, 1)\n",
    "        # )\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Load the original MLP model\n",
    "mlp_model_file = './models/mlp_model_resampled_0.1V2.pth'\n",
    "input_dim = x_train.shape[1]\n",
    "original_mlp = MLP(input_dim).to(device)\n",
    "original_mlp.load_state_dict(torch.load(mlp_model_file))\n",
    "original_mlp.eval()\n",
    "\n",
    "# Evaluate the original MLP model\n",
    "with torch.no_grad():\n",
    "    x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
    "    y_pred_original = original_mlp(x_test_tensor).cpu().numpy().flatten()\n",
    "    below_0_5 = np.sum(y_pred_original < 0.5)\n",
    "    above_0_5 = np.sum(y_pred_original >= 0.5)\n",
    "    print(f\"Values below 0.5: {below_0_5}\")\n",
    "    print(f\"Values above 0.5: {above_0_5}\")\n",
    "    y_pred_original = (y_pred_original > 0.5).astype(int)  # Convert to binary predictions\n",
    "\n",
    "# Calculate accuracy\n",
    "original_accuracy = accuracy_score(y_test, y_pred_original)\n",
    "print(f\"Original MLP Accuracy: {original_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix_original = confusion_matrix(y_test, y_pred_original)\n",
    "\n",
    "# Extract values from confusion matrix\n",
    "tn, fp, fn, tp = conf_matrix_original.ravel()\n",
    "\n",
    "print(f\"Total correct predictions: {tn + tp}\")\n",
    "print(f\"Total wrong predictions: {fp + fn}\\n\")\n",
    "print(f\"True Negatives: {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Positives: {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple error correction function by adding a bias (no learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F  # For activation functions\n",
    "\n",
    "def error_correction_function(mlp, autoencoder, x, bias_factor):\n",
    "    # Ensure all computations happen on the correct device\n",
    "    with torch.no_grad():\n",
    "        # Calculate reconstruction error using the autoencoder\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        reconstructed = autoencoder(x_tensor).cpu().numpy()\n",
    "    reconstruction_error = np.mean(np.square(x - reconstructed), axis=1)\n",
    "    threshold = np.mean(reconstruction_error)\n",
    "    \n",
    "    # Make predictions with the MLP model\n",
    "    with torch.no_grad():\n",
    "        x_test_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        y_pred_proba = mlp(x_test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    # Adjust predictions based on reconstruction error with adaptive bias factor\n",
    "    y_pred_proba_corrected = y_pred_proba.copy()\n",
    "    high_error_indices = np.where(reconstruction_error > threshold)[0]\n",
    "    print(\"Number of high error indices: \", len(high_error_indices))\n",
    "    for idx in high_error_indices:\n",
    "        adaptive_bias = bias_factor * (reconstruction_error[idx] / threshold)\n",
    "        if y_pred_proba[idx] < 0.5:\n",
    "            y_pred_proba_corrected[idx] += adaptive_bias\n",
    "        else:\n",
    "            y_pred_proba_corrected[idx] -= adaptive_bias\n",
    "    # y_pred_proba_corrected[high_error_indices] += bias_factor  # Apply bias factor\n",
    "    \n",
    "    initial_zeros = np.sum(y_pred_proba[high_error_indices] < 0.5)\n",
    "    initial_ones = np.sum(y_pred_proba[high_error_indices] >= 0.5)\n",
    "    print(f\"Initial predictions of 0: {initial_zeros}\")\n",
    "    print(f\"Initial predictions of 1: {initial_ones}\")\n",
    "    changed_predictions = np.sum((y_pred_proba[high_error_indices] > 0.5).astype(int) != (y_pred_proba_corrected[high_error_indices] > 0.5).astype(int))\n",
    "    print(f\"Number of changed predictions: {changed_predictions}\")\n",
    "    \n",
    "    y_pred_proba_corrected = np.clip(y_pred_proba_corrected, 0, 1)  # Ensure probabilities are in [0, 1]\n",
    "\n",
    "    # Convert corrected probabilities to binary predictions\n",
    "    y_pred_corrected = (y_pred_proba_corrected > 0.5).astype(int)\n",
    "    \n",
    "    return y_pred_corrected, reconstruction_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of high error indices:  4322\n",
      "Initial predictions of 0: 3466\n",
      "Initial predictions of 1: 856\n",
      "Number of changed predictions: 391\n"
     ]
    }
   ],
   "source": [
    "y_pred_corrected, reconstruction_error = error_correction_function(original_mlp, autoencoder, x_test.values, bias_factor=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Accuracy: 95.80%\n",
      "Corrected - True Negatives: 18146\n",
      "Corrected - False Positives: 170\n",
      "Corrected - False Negatives: 671\n",
      "Corrected - True Positives: 1013\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Calculate accuracy\n",
    "corrected_accuracy = accuracy_score(y_test, y_pred_corrected)\n",
    "print(f\"Corrected Accuracy: {corrected_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix_corrected = confusion_matrix(y_test, y_pred_corrected)\n",
    "\n",
    "# Print confusion matrix results\n",
    "tn, fp, fn, tp = conf_matrix_corrected.ravel()\n",
    "print(f\"Corrected - True Negatives: {tn}\")\n",
    "print(f\"Corrected - False Positives: {fp}\")\n",
    "print(f\"Corrected - False Negatives: {fn}\")\n",
    "print(f\"Corrected - True Positives: {tp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRYING OUT ERROR-CORRECTING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum reconstruction error:  -0.35256809419790586\n",
      "Maximum reconstruction error:  63.480175282524044\n",
      "Minimum bias:  -0.663520872592926\n",
      "Maximum bias:  0.5115594565868378\n",
      "new best accuracy:  0.81205\n",
      "new best accuracy:  0.8701\n",
      "new best accuracy:  0.88865\n",
      "new best accuracy:  0.9299\n",
      "new best accuracy:  0.95165\n",
      "Best Corrected Accuracy: 95.17%\n",
      "True Positives:  827\n",
      "True Negatives:  18206\n",
      "False Positives:  110\n",
      "False Negatives:  857\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the Bias Predictor Model\n",
    "class BiasPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiasPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 64)  # Reduced neurons\n",
    "        self.dropout1 = nn.Dropout(0.2)  # Lower dropout rate\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)  # Output layer for bias correction\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "# Prepare the training data for the bias predictor\n",
    "def prepare_bias_data(reconstruction_errors, original_probs, ground_truth):\n",
    "    # Use reconstruction errors as the only input feature\n",
    "    features = reconstruction_errors.reshape(-1, 1)\n",
    "    # print(features)\n",
    "    print(\"Minimum reconstruction error: \", np.min(features))\n",
    "    print(\"Maximum reconstruction error: \", np.max(features))\n",
    "    # Compute target biases\n",
    "    target_bias = ground_truth - original_probs\n",
    "    print(\"Minimum bias: \", np.min(target_bias))\n",
    "    print(\"Maximum bias: \", np.max(target_bias))\n",
    "    return torch.tensor(features, dtype=torch.float32), torch.tensor(target_bias, dtype=torch.float32)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the Bias Predictor\n",
    "def train_bias_predictor(features, target_bias, epochs=100, learning_rate=0.001):\n",
    "    model = BiasPredictor().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features).squeeze()\n",
    "        loss = criterion(predictions, target_bias)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "# Train the Bias Predictor with Class Weights\n",
    "# def train_bias_predictor_with_weights(features, target_bias, ground_truth, epochs=100, learning_rate=0.001):\n",
    "def train_bias_predictor_with_weights(features, target_bias, ground_truth, epochs=100, learning_rate=0.001, weight_positive_scale=1.0, weight_negative_scale=1.0):\n",
    "    model = BiasPredictor().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Define class weights\n",
    "    num_positive = np.sum(ground_truth == 1)\n",
    "    num_negative = np.sum(ground_truth == 0)\n",
    "    total = len(ground_truth)\n",
    "\n",
    "    weight_positive = (total / (2 * num_positive)) * weight_positive_scale\n",
    "    weight_negative = (total / (2 * num_negative)) * weight_negative_scale \n",
    "\n",
    "    # Convert weights to tensors\n",
    "    class_weights = torch.tensor([weight_negative, weight_positive], dtype=torch.float32).to(device)\n",
    "    criterion = nn.MSELoss(reduction='none')  # Use 'none' to apply weights manually\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features).squeeze()\n",
    "        \n",
    "        # Calculate weighted loss\n",
    "        loss = criterion(predictions, target_bias)\n",
    "        weights = torch.where(\n",
    "            target_bias > 0,  # Assign weights based on ground truth class\n",
    "            class_weights[1],  # Positive class weight\n",
    "            class_weights[0]   # Negative class weight\n",
    "        )\n",
    "        weighted_loss = torch.mean(loss * weights)\n",
    "        \n",
    "        weighted_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (epoch + 1) % 10 == 0:\n",
    "        #     print(f\"Epoch {epoch+1}/{epochs}, Loss: {weighted_loss.item()}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "# Apply bias correction during inference\n",
    "def apply_bias_correction(bias_predictor, reconstruction_errors, original_probs):\n",
    "    # Create feature tensor and move it to the correct device\n",
    "    features = torch.tensor(reconstruction_errors.reshape(-1, 1), dtype=torch.float32).to(next(bias_predictor.parameters()).device)\n",
    "    with torch.no_grad():\n",
    "        bias_correction = bias_predictor(features).squeeze().cpu().numpy()\n",
    "    # Apply the correction and clip probabilities to [0, 1]\n",
    "    corrected_probs = np.clip(original_probs + bias_correction, 0, 1)\n",
    "    return corrected_probs\n",
    "\n",
    "x_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "with torch.no_grad():\n",
    "    reconstructed = autoencoder(x_tensor).cpu().numpy()\n",
    "reconstruction_errors = np.mean(np.square(x_test.values - reconstructed), axis=1)\n",
    "\n",
    "# Normalizing the reconstruction errors\n",
    "mean_error = np.mean(reconstruction_errors)\n",
    "std_error = np.std(reconstruction_errors)\n",
    "reconstruction_errors_normalized = (reconstruction_errors - mean_error) / std_error\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
    "    logits = original_mlp(x_test_tensor)\n",
    "    y_pred_original = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "    \n",
    "original_probs = y_pred_original\n",
    "ground_truth = y_test.values\n",
    "\n",
    "\n",
    "# Prepare the data for training\n",
    "features, target_bias = prepare_bias_data(reconstruction_errors_normalized, original_probs, ground_truth)\n",
    "\n",
    "# Move tensors to the same device as the model\n",
    "features, target_bias = features.to(device), target_bias.to(device)\n",
    "\n",
    "# Train the bias predictor\n",
    "# bias_predictor = train_bias_predictor(features, target_bias)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_conf_matrix = None\n",
    "for pos in np.arange(0.1, 1.1, 0.05):\n",
    "    for neg in np.arange(0.1, 1.1, 0.05):\n",
    "        \n",
    "        # Train the bias predictor with weights\n",
    "        bias_predictor = train_bias_predictor_with_weights(features, target_bias, ground_truth, weight_positive_scale=pos, weight_negative_scale=neg)\n",
    "\n",
    "        # Apply bias correction during inference\n",
    "        corrected_probs = apply_bias_correction(bias_predictor, reconstruction_errors_normalized, original_probs)\n",
    "\n",
    "        # Convert probabilities to binary predictions\n",
    "        corrected_predictions = (corrected_probs > 0.5).astype(int)\n",
    "        # Calculate accuracy\n",
    "        corrected_accuracy = accuracy_score(y_test, corrected_predictions)\n",
    "        # print(f\"Corrected Accuracy for positive weight {pos:.2f} and negative weight {neg:.2f }: {corrected_accuracy * 100:.2f}%\")\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        conf_matrix_corrected = confusion_matrix(y_test, corrected_predictions)\n",
    "\n",
    "        if corrected_accuracy > best_accuracy:\n",
    "            print(\"new best accuracy: \", corrected_accuracy)\n",
    "            best_accuracy = corrected_accuracy\n",
    "            best_conf_matrix = conf_matrix_corrected\n",
    "\n",
    "# Print the best accuracy and corresponding confusion matrix\n",
    "print(f\"Best Corrected Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "tn, fp, fn, tp = best_conf_matrix.ravel()\n",
    "print(\"True Positives: \", tp)\n",
    "print(\"True Negatives: \", tn)\n",
    "print(\"False Positives: \", fp)\n",
    "print(\"False Negatives: \", fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Accuracy: 77.62%\n",
      "Corrected - True Negatives: 14240\n",
      "Corrected - False Positives: 4076\n",
      "Corrected - False Negatives: 400\n",
      "Corrected - True Positives: 1284\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "corrected_accuracy = accuracy_score(y_test, corrected_predictions)\n",
    "print(f\"Corrected Accuracy: {corrected_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix_corrected = confusion_matrix(y_test, corrected_predictions)\n",
    "\n",
    "# Print confusion matrix results\n",
    "tn, fp, fn, tp = conf_matrix_corrected.ravel()\n",
    "print(f\"Corrected - True Negatives: {tn}\")\n",
    "print(f\"Corrected - False Positives: {fp}\")\n",
    "print(f\"Corrected - False Negatives: {fn}\")\n",
    "print(f\"Corrected - True Positives: {tp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ground truth values equal to one: 1684\n",
      "Number of ground truth values equal to zero: 18316\n"
     ]
    }
   ],
   "source": [
    "num_ground_truth_ones = np.sum(ground_truth == 1)\n",
    "print(f\"Number of ground truth values equal to one: {num_ground_truth_ones}\")\n",
    "\n",
    "num_ground_truth_zeroes = np.sum(ground_truth == 0)\n",
    "print(f\"Number of ground truth values equal to zero: {num_ground_truth_zeroes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
