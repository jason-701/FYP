{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, encoding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoding_dim, input_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ComplexLSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "#         super(ComplexLSTMModel, self).__init__()\n",
    "#         # LSTM Layer: Bidirectional for richer context\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "#                             batch_first=True, dropout=dropout_rate, \n",
    "#                             bidirectional=True)\n",
    "#         # Fully connected layers for added complexity\n",
    "#         self.fc1 = nn.Linear(hidden_size * 2, hidden_size)  # Bidirectional LSTM doubles hidden size\n",
    "#         self.dropout1 = nn.Dropout(dropout_rate)\n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "#         self.dropout2 = nn.Dropout(dropout_rate)\n",
    "#         self.fc3 = nn.Linear(hidden_size // 2, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # LSTM expects input of shape (batch_size, sequence_length, input_size)\n",
    "#         out, _ = self.lstm(x)\n",
    "#         # Use the output of the last time step\n",
    "#         out = out[:, -1, :]  # Shape: (batch_size, hidden_size * 2)\n",
    "#         out = self.fc1(out)  # First fully connected layer\n",
    "#         out = self.dropout1(out)\n",
    "#         out = torch.relu(out)  # Nonlinear activation\n",
    "#         out = self.fc2(out)  # Second fully connected layer\n",
    "#         out = self.dropout2(out)\n",
    "#         out = torch.relu(out)  # Nonlinear activation\n",
    "#         out = self.fc3(out)  # Output layer\n",
    "#         return out\n",
    "\n",
    "\n",
    "class ComplexLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate):\n",
    "        super(ComplexLSTMModel, self).__init__()\n",
    "        # Unidirectional LSTM for sequential stock price prediction\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                            batch_first=True, dropout=dropout_rate)  # Removed bidirectional=True\n",
    "        \n",
    "        # Fully connected layers for added complexity\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, hidden_size // 4)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_size // 4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM expects input of shape (batch_size, sequence_length, input_size)\n",
    "        out, _ = self.lstm(x)\n",
    "        # Use the output of the last time step\n",
    "        out = out[:, -1, :]  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        out = torch.relu(self.fc1(out))  # First fully connected layer\n",
    "        out = self.dropout1(out)\n",
    "        out = torch.relu(self.fc2(out))  # Second fully connected layer\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc3(out)  # Output layer (no activation for regression)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, target, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X_seq.append(data[i:i + seq_length])\n",
    "        y_seq.append(target[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "sequence_length = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8020, 11)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('SPY.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow; font-size: 40px;\">Original LSTM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest Date: 1993-01-29 00:00:00\n",
      "Latest Date: 2024-12-04 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Convert the \"Date\" column to datetime\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], format='%Y/%m/%d')\n",
    "\n",
    "# Sort the data by date in ascending order\n",
    "df = df.sort_values(by=\"Date\")\n",
    "\n",
    "# Get the earliest and latest dates\n",
    "earliest_date = df[\"Date\"].min()\n",
    "latest_date = df[\"Date\"].max()\n",
    "\n",
    "print(\"Earliest Date:\", earliest_date)\n",
    "print(\"Latest Date:\", latest_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Target\" not in df.columns:\n",
    "    # df[\"Target\"] = (df[\"Close\"].shift(-1) > df[\"Close\"]).astype(int)\n",
    "    df[\"Target\"] = df[\"Close\"].shift(-1)\n",
    "\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df[\"Date\"]\n",
    "X = df.drop(columns=[\"Date\", \"Target\"])\n",
    "y = df[\"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]  \n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X[feature_columns]), columns=feature_columns, index=X.index)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, y.values, sequence_length)\n",
    "\n",
    "# Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_26320/258380665.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  original_lstm.load_state_dict(torch.load(LSTM_model_file))\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "# input_size = X_train.shape[2]  # Number of features\n",
    "# hidden_size = 32\n",
    "# num_layers = 2\n",
    "# dropout_rate = 0.3\n",
    "# learning_rate = 0.01\n",
    "# num_epochs = 100\n",
    "# patience = 10\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[2]  # Number of features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "dropout_rate = 0.1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "patience = 20\n",
    "\n",
    "# Initialize the LSTM model\n",
    "# model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "original_lstm = ComplexLSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "\n",
    "# LSTM_model_file = './lstm_trials/lstm_uni.pth'\n",
    "LSTM_model_file = './lstm_trials/lstm_after_gridsearch.pth'\n",
    "\n",
    "original_lstm.load_state_dict(torch.load(LSTM_model_file))\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# original_lstm.eval()\n",
    "# test_loss = 0.0\n",
    "# with torch.no_grad():\n",
    "#     for X_batch, y_batch in test_loader:\n",
    "#         outputs = original_lstm(X_batch)\n",
    "#         loss = criterion(outputs, y_batch)\n",
    "#         test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "# test_loss /= len(test_loader.dataset)\n",
    "# print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# # Calculate Mean Absolute Error (MAE)\n",
    "# test_mae = 0.0\n",
    "# with torch.no_grad():\n",
    "#     for X_batch, y_batch in test_loader:\n",
    "#         outputs = original_lstm(X_batch)\n",
    "#         mae = torch.mean(torch.abs(outputs - y_batch))\n",
    "#         test_mae += mae.item() * X_batch.size(0)\n",
    "\n",
    "# test_mae /= len(test_loader.dataset)\n",
    "# print(f'Test MAE: {test_mae:.4f}')\n",
    "\n",
    "original_lstm.eval()\n",
    "\n",
    "X_seq_tensor = torch.tensor(X_seq, dtype=torch.float32)\n",
    "y_seq_tensor = torch.tensor(y_seq, dtype=torch.float32)\n",
    "outputs = original_lstm(X_seq_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error percentage: 2.5766585022211075\n",
      "Largest error percentage: 22.331877052783966\n",
      "Less than actual: 5045\n",
      "More than actual: 2965\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "largest_error = 0\n",
    "less_than_actual = 0\n",
    "more_than_actual = 0\n",
    "\n",
    "for true, pred in zip(y_seq, outputs):\n",
    "    percent = abs(true - pred) / true\n",
    "    if percent > largest_error:\n",
    "        largest_error = percent\n",
    "    error += percent\n",
    "    if pred < true:\n",
    "        less_than_actual += 1\n",
    "    else:\n",
    "        more_than_actual += 1\n",
    "error = error / len(y_seq)\n",
    "error = error.item()\n",
    "largest_error = largest_error.item()\n",
    "print(f\"Average error percentage: {error*100}\")\n",
    "print(f\"Largest error percentage: {largest_error*100}\")\n",
    "print(f\"Less than actual: {less_than_actual}\")\n",
    "print(f\"More than actual: {more_than_actual}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_seq, label='True Values')\n",
    "plt.plot(outputs.detach().numpy(), label='Predicted Values')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('True vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow; font-size: 40px;\">Autoencoder</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X[feature_columns]), columns=feature_columns, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_val = X_val.values\n",
    "X_test = X_test.values\n",
    "X_scaled = X_scaled.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model_file = './models/encoder.pth'\n",
    "decoder_model_file = './models/decoder.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_26320/3102932148.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(encoder_model_file))\n",
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_26320/3102932148.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load(decoder_model_file))\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(encoder_model_file) and os.path.exists(decoder_model_file):\n",
    "    print(\"Loading existing models...\")\n",
    "    encoder = Encoder(input_dim, encoding_dim).to(device)\n",
    "    decoder = Decoder(encoding_dim, input_dim).to(device)\n",
    "    encoder.load_state_dict(torch.load(encoder_model_file))\n",
    "    decoder.load_state_dict(torch.load(decoder_model_file))\n",
    "else:\n",
    "    print(\"Some models are missing\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=64, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = Autoencoder(encoder, decoder).to(device)\n",
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.000393898925208361\n",
      "Mean Absolute Error (MAE): 0.013584979295197346\n",
      "Reconstruction Error Percentage: 7.316429470988849\n"
     ]
    }
   ],
   "source": [
    "# Calculate reconstruction error on the train set\n",
    "with torch.no_grad():\n",
    "    reconstructed_test = autoencoder(torch.tensor(X_scaled, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = np.mean((X_scaled - reconstructed_test) ** 2)\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = np.mean(np.abs(X_scaled - reconstructed_test))\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "\n",
    "reconstruction_error_percentage = (\n",
    "    np.mean(np.abs(X_scaled - reconstructed_test) / (np.abs(X_scaled) + 1e-8), axis=1) * 100\n",
    ")\n",
    "reconstruction_error_percentage_mean = np.mean(reconstruction_error_percentage)\n",
    "print(f'Reconstruction Error Percentage: {reconstruction_error_percentage_mean}')\n",
    "\n",
    "def calculate_index_error_percentage(index, X, reconstructed_X):\n",
    "    print(np.mean(np.abs(X[index] - reconstructed_X[index]) / (np.abs(X[index]) + 1e-8)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points with reconstruction error percentage greater than the mean: 989\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points with reconstruction error percentage greater than the mean:\", np.sum(reconstruction_error_percentage > reconstruction_error_percentage_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow; font-size: 40px;\">Improving the LSTM using the autoencoder</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8010, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure outputs is detached from the computation graph and converted to numpy\n",
    "predicted_values = outputs.detach().numpy()\n",
    "\n",
    "predicted_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['predictions'] = np.nan\n",
    "\n",
    "# # Assign the predicted values starting at index 24\n",
    "# df.loc[25:, 'predictions'] = predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reconstruction percentage to a separate dataframe\n",
    "new_df = pd.DataFrame(reconstruction_error_percentage, columns=['reconstruction_error_percentage'], index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['target'] = df['Target']\n",
    "\n",
    "new_df['predictions'] = np.nan\n",
    "new_df.loc[9:, 'predictions'] = predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['bias'] = new_df['target'] - new_df['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiasPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 256)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.dropout4 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc5 = nn.Linear(32, 16)\n",
    "        self.fc6 = nn.Linear(16, 8)\n",
    "        self.fc7 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        return self.fc7(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bias_predictor(features, target_bias, val_features, val_target_bias, epochs=1000, learning_rate=0.0001, patience=10):\n",
    "    model = BiasPredictor().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss\n",
    "    patience_counter = 0  # Counter for early stopping\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features).squeeze()\n",
    "        loss = criterion(predictions, target_bias)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(val_features).squeeze()\n",
    "            val_loss = criterion(val_predictions, val_target_bias)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()  # Save the best model state\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Print progress\n",
    "        # if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "        # Stop training if patience is exceeded\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    # Load the best model state before returning\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = new_df['reconstruction_error_percentage'].values.reshape(-1, 1)\n",
    "X = new_df\n",
    "y = new_df['bias'].values\n",
    "\n",
    "if 'reconstruction_error_percentage_array' not in X.columns:\n",
    "    X['reconstruction_error_percentage_array'] = [[] for _ in range(len(X))]\n",
    "\n",
    "    arr = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        temp_array = []\n",
    "        for j in range(9):\n",
    "            temp_array.append(reconstruction_error_percentage[i+j+1])\n",
    "        # X.at[i, 'reconstruction_error_percentage_array'] = temp_array\n",
    "        arr.append(temp_array)\n",
    "\n",
    "    X['reconstruction_error_percentage_array'] = arr\n",
    "\n",
    "# train val test split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG\\AppData\\Local\\Temp/ipykernel_26320/2873719968.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bias_predictor.load_state_dict(torch.load(bias_predictor_path))\n"
     ]
    }
   ],
   "source": [
    "bias_predictor_path = './models/bias_predictor.pth'\n",
    "\n",
    "# X_train X_val and X_test are the dataframes that contains other columns. In the actual training process we just \n",
    "# want to use the reconstruction error percentage column, hence we will extract that column and convert it to a tensor\n",
    "\n",
    "if not os.path.exists(bias_predictor_path):\n",
    "    bias_predictor = train_bias_predictor(torch.tensor(X_train['reconstruction_error_percentage'].values.reshape(-1,1), dtype=torch.float32).to(device), \n",
    "                                          torch.tensor(y_train, dtype=torch.float32).to(device),\n",
    "                                          torch.tensor(X_val['reconstruction_error_percentage'].values.reshape(-1,1), dtype=torch.float32).to(device),\n",
    "                                          torch.tensor(y_val, dtype=torch.float32).to(device),\n",
    "                                          epochs=1000, learning_rate=0.0001, patience=30)\n",
    "else:\n",
    "    bias_predictor = BiasPredictor().to(device)\n",
    "    bias_predictor.load_state_dict(torch.load(bias_predictor_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['prediction_error_percentage'] = np.nan\n",
    "new_df['prediction_error_percentage'] = (abs(new_df['bias']) / new_df['target']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(X_test['reconstruction_error_percentage'].values.reshape(-1,1), dtype=torch.float32).to(device)\n",
    "predictions = bias_predictor(features).squeeze().cpu().detach().numpy()\n",
    "# get a dataframe with predictions and y_test\n",
    "predictions_df = pd.DataFrame({'predictions': predictions, 'target': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reconstruction_error_percentage</th>\n",
       "      <th>target</th>\n",
       "      <th>predictions</th>\n",
       "      <th>bias</th>\n",
       "      <th>reconstruction_error_percentage_array</th>\n",
       "      <th>prediction_error_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.888079</td>\n",
       "      <td>24.976170</td>\n",
       "      <td>24.021490</td>\n",
       "      <td>0.954679</td>\n",
       "      <td>[0.8914554698822016, 0.8894131969174759, 0.892...</td>\n",
       "      <td>3.822361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.885900</td>\n",
       "      <td>24.346083</td>\n",
       "      <td>24.042667</td>\n",
       "      <td>0.303415</td>\n",
       "      <td>[0.8894131969174759, 0.8926279864160259, 0.891...</td>\n",
       "      <td>1.246259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.890866</td>\n",
       "      <td>24.328583</td>\n",
       "      <td>24.039593</td>\n",
       "      <td>0.288990</td>\n",
       "      <td>[0.8926279864160259, 0.8917197775553967, 0.888...</td>\n",
       "      <td>1.187862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.890510</td>\n",
       "      <td>24.311090</td>\n",
       "      <td>23.921227</td>\n",
       "      <td>0.389864</td>\n",
       "      <td>[0.8917197775553967, 0.8887546261209618, 0.887...</td>\n",
       "      <td>1.603647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.895600</td>\n",
       "      <td>24.398588</td>\n",
       "      <td>23.752037</td>\n",
       "      <td>0.646551</td>\n",
       "      <td>[0.8887546261209618, 0.8875140032785958, 0.886...</td>\n",
       "      <td>2.649953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8014</th>\n",
       "      <td>3.770355</td>\n",
       "      <td>598.830017</td>\n",
       "      <td>595.398499</td>\n",
       "      <td>3.431519</td>\n",
       "      <td>[2.3713694567566943, 16.458029836360343, 2.276...</td>\n",
       "      <td>0.573037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8015</th>\n",
       "      <td>1.781509</td>\n",
       "      <td>602.549988</td>\n",
       "      <td>596.411682</td>\n",
       "      <td>6.138306</td>\n",
       "      <td>[16.458029836360343, 2.2764292166983733, 4.542...</td>\n",
       "      <td>1.018721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8016</th>\n",
       "      <td>1.437670</td>\n",
       "      <td>603.630005</td>\n",
       "      <td>597.702881</td>\n",
       "      <td>5.927124</td>\n",
       "      <td>[2.2764292166983733, 4.542149906403055, 4.2425...</td>\n",
       "      <td>0.981913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8017</th>\n",
       "      <td>1.594694</td>\n",
       "      <td>603.909973</td>\n",
       "      <td>598.967468</td>\n",
       "      <td>4.942505</td>\n",
       "      <td>[4.542149906403055, 4.242525669273216, 3.79664...</td>\n",
       "      <td>0.818417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8018</th>\n",
       "      <td>1.240919</td>\n",
       "      <td>607.659973</td>\n",
       "      <td>600.244263</td>\n",
       "      <td>7.415710</td>\n",
       "      <td>[4.242525669273216, 3.796641441120553, 2.49075...</td>\n",
       "      <td>1.220372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8010 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      reconstruction_error_percentage      target  predictions      bias  \\\n",
       "9                            0.888079   24.976170    24.021490  0.954679   \n",
       "10                           0.885900   24.346083    24.042667  0.303415   \n",
       "11                           0.890866   24.328583    24.039593  0.288990   \n",
       "12                           0.890510   24.311090    23.921227  0.389864   \n",
       "13                           0.895600   24.398588    23.752037  0.646551   \n",
       "...                               ...         ...          ...       ...   \n",
       "8014                         3.770355  598.830017   595.398499  3.431519   \n",
       "8015                         1.781509  602.549988   596.411682  6.138306   \n",
       "8016                         1.437670  603.630005   597.702881  5.927124   \n",
       "8017                         1.594694  603.909973   598.967468  4.942505   \n",
       "8018                         1.240919  607.659973   600.244263  7.415710   \n",
       "\n",
       "                  reconstruction_error_percentage_array  \\\n",
       "9     [0.8914554698822016, 0.8894131969174759, 0.892...   \n",
       "10    [0.8894131969174759, 0.8926279864160259, 0.891...   \n",
       "11    [0.8926279864160259, 0.8917197775553967, 0.888...   \n",
       "12    [0.8917197775553967, 0.8887546261209618, 0.887...   \n",
       "13    [0.8887546261209618, 0.8875140032785958, 0.886...   \n",
       "...                                                 ...   \n",
       "8014  [2.3713694567566943, 16.458029836360343, 2.276...   \n",
       "8015  [16.458029836360343, 2.2764292166983733, 4.542...   \n",
       "8016  [2.2764292166983733, 4.542149906403055, 4.2425...   \n",
       "8017  [4.542149906403055, 4.242525669273216, 3.79664...   \n",
       "8018  [4.242525669273216, 3.796641441120553, 2.49075...   \n",
       "\n",
       "      prediction_error_percentage  \n",
       "9                        3.822361  \n",
       "10                       1.246259  \n",
       "11                       1.187862  \n",
       "12                       1.603647  \n",
       "13                       2.649953  \n",
       "...                           ...  \n",
       "8014                     0.573037  \n",
       "8015                     1.018721  \n",
       "8016                     0.981913  \n",
       "8017                     0.818417  \n",
       "8018                     1.220372  \n",
       "\n",
       "[8010 rows x 6 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['predicted_bias_with_one_input'] = np.nan\n",
    "X_test['predicted_bias_with_one_input'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reconstruction_error_percentage</th>\n",
       "      <th>target</th>\n",
       "      <th>predictions</th>\n",
       "      <th>bias</th>\n",
       "      <th>reconstruction_error_percentage_array</th>\n",
       "      <th>predicted_bias_with_one_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.872467</td>\n",
       "      <td>30.429846</td>\n",
       "      <td>28.182663</td>\n",
       "      <td>2.247183</td>\n",
       "      <td>[0.865808988877958, 0.8668465829545656, 0.8671...</td>\n",
       "      <td>-2.417470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789</th>\n",
       "      <td>6.734052</td>\n",
       "      <td>307.836670</td>\n",
       "      <td>308.924255</td>\n",
       "      <td>-1.087585</td>\n",
       "      <td>[10.185029277702128, 2.2025611446663187, 5.732...</td>\n",
       "      <td>-0.227217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3110</th>\n",
       "      <td>1.485681</td>\n",
       "      <td>82.987839</td>\n",
       "      <td>82.817764</td>\n",
       "      <td>0.170074</td>\n",
       "      <td>[0.7517255375362865, 0.6664800255598702, 0.686...</td>\n",
       "      <td>-2.615610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7057</th>\n",
       "      <td>2.748172</td>\n",
       "      <td>369.807526</td>\n",
       "      <td>371.961212</td>\n",
       "      <td>-2.153687</td>\n",
       "      <td>[1.3816415978691088, 4.0989117322815245, 0.985...</td>\n",
       "      <td>-1.453516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>0.990742</td>\n",
       "      <td>67.861076</td>\n",
       "      <td>65.243706</td>\n",
       "      <td>2.617371</td>\n",
       "      <td>[1.2778963613711947, 0.7747250560136459, 1.508...</td>\n",
       "      <td>-2.497043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>1.913761</td>\n",
       "      <td>79.646431</td>\n",
       "      <td>76.761215</td>\n",
       "      <td>2.885216</td>\n",
       "      <td>[0.6348746987879306, 0.5525179639740401, 1.215...</td>\n",
       "      <td>-2.187044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>1.169535</td>\n",
       "      <td>86.870735</td>\n",
       "      <td>88.112328</td>\n",
       "      <td>-1.241592</td>\n",
       "      <td>[1.2430595625005405, 1.1607447280814434, 1.275...</td>\n",
       "      <td>-2.024758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>3.809520</td>\n",
       "      <td>205.387939</td>\n",
       "      <td>204.572617</td>\n",
       "      <td>0.815323</td>\n",
       "      <td>[9.967309715008783, 4.466418914873627, 14.0424...</td>\n",
       "      <td>-1.132917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>0.645578</td>\n",
       "      <td>52.037102</td>\n",
       "      <td>49.502167</td>\n",
       "      <td>2.534935</td>\n",
       "      <td>[0.6782854304440461, 0.656536332774514, 0.6494...</td>\n",
       "      <td>-1.865426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>6.540621</td>\n",
       "      <td>178.703690</td>\n",
       "      <td>179.721680</td>\n",
       "      <td>-1.017990</td>\n",
       "      <td>[3.3069675607945737, 7.530232964330011, 5.0975...</td>\n",
       "      <td>-0.369135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1602 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      reconstruction_error_percentage      target  predictions      bias  \\\n",
       "563                          0.872467   30.429846    28.182663  2.247183   \n",
       "6789                         6.734052  307.836670   308.924255 -1.087585   \n",
       "3110                         1.485681   82.987839    82.817764  0.170074   \n",
       "7057                         2.748172  369.807526   371.961212 -2.153687   \n",
       "2614                         0.990742   67.861076    65.243706  2.617371   \n",
       "...                               ...         ...          ...       ...   \n",
       "2964                         1.913761   79.646431    76.761215  2.885216   \n",
       "1975                         1.169535   86.870735    88.112328 -1.241592   \n",
       "6054                         3.809520  205.387939   204.572617  0.815323   \n",
       "1088                         0.645578   52.037102    49.502167  2.534935   \n",
       "5560                         6.540621  178.703690   179.721680 -1.017990   \n",
       "\n",
       "                  reconstruction_error_percentage_array  \\\n",
       "563   [0.865808988877958, 0.8668465829545656, 0.8671...   \n",
       "6789  [10.185029277702128, 2.2025611446663187, 5.732...   \n",
       "3110  [0.7517255375362865, 0.6664800255598702, 0.686...   \n",
       "7057  [1.3816415978691088, 4.0989117322815245, 0.985...   \n",
       "2614  [1.2778963613711947, 0.7747250560136459, 1.508...   \n",
       "...                                                 ...   \n",
       "2964  [0.6348746987879306, 0.5525179639740401, 1.215...   \n",
       "1975  [1.2430595625005405, 1.1607447280814434, 1.275...   \n",
       "6054  [9.967309715008783, 4.466418914873627, 14.0424...   \n",
       "1088  [0.6782854304440461, 0.656536332774514, 0.6494...   \n",
       "5560  [3.3069675607945737, 7.530232964330011, 5.0975...   \n",
       "\n",
       "      predicted_bias_with_one_input  \n",
       "563                       -2.417470  \n",
       "6789                      -0.227217  \n",
       "3110                      -2.615610  \n",
       "7057                      -1.453516  \n",
       "2614                      -2.497043  \n",
       "...                             ...  \n",
       "2964                      -2.187044  \n",
       "1975                      -2.024758  \n",
       "6054                      -1.132917  \n",
       "1088                      -1.865426  \n",
       "5560                      -0.369135  \n",
       "\n",
       "[1602 rows x 6 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['new_predictions_with_one_input'] = X_test['predictions'] + X_test['predicted_bias_with_one_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['original_error_percentage'] = (abs(X_test['bias']) / X_test['target']) * 100\n",
    "X_test['new_error_percentage_with_one_input'] = (abs(X_test['target'] - X_test['new_predictions_with_one_input']) / X_test['target']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_error_percentage</th>\n",
       "      <th>new_error_percentage_with_one_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1602.000000</td>\n",
       "      <td>1602.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.530712</td>\n",
       "      <td>4.649052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.374628</td>\n",
       "      <td>4.685974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.000587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.767609</td>\n",
       "      <td>1.060381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.662241</td>\n",
       "      <td>2.924778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.795965</td>\n",
       "      <td>6.535524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18.144303</td>\n",
       "      <td>21.099976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       original_error_percentage  new_error_percentage_with_one_input\n",
       "count                1602.000000                          1602.000000\n",
       "mean                    2.530712                             4.649052\n",
       "std                     2.374628                             4.685974\n",
       "min                     0.000122                             0.000587\n",
       "25%                     0.767609                             1.060381\n",
       "50%                     1.662241                             2.924778\n",
       "75%                     3.795965                             6.535524\n",
       "max                    18.144303                            21.099976"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[['original_error_percentage', 'new_error_percentage_with_one_input']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bias_predictor.state_dict(), bias_predictor_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: yellow; font-size: 40px;\">BREAKERRRRRRRRR</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiasPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        # self.dropout4 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        # self.dropout5 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc6 = nn.Linear(32, 16)\n",
    "        self.fc7 = nn.Linear(16, 8)\n",
    "        self.fc8 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        # x = self.dropout4(x)\n",
    "        \n",
    "        x = F.relu(self.bn5(self.fc5(x)))\n",
    "        # x = self.dropout5(x)\n",
    "        \n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        return self.fc8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 16.9468, Val Loss: 17.2544\n",
      "Epoch 2/1000, Loss: 16.9170, Val Loss: 17.2447\n",
      "Epoch 3/1000, Loss: 16.8993, Val Loss: 17.2348\n",
      "Epoch 4/1000, Loss: 16.8860, Val Loss: 17.2254\n",
      "Epoch 5/1000, Loss: 16.8737, Val Loss: 17.2156\n",
      "Epoch 6/1000, Loss: 16.8526, Val Loss: 17.2051\n",
      "Epoch 7/1000, Loss: 16.8147, Val Loss: 17.1945\n",
      "Epoch 8/1000, Loss: 16.8113, Val Loss: 17.1823\n",
      "Epoch 9/1000, Loss: 16.7535, Val Loss: 17.1690\n",
      "Epoch 10/1000, Loss: 16.7591, Val Loss: 17.1556\n",
      "Epoch 11/1000, Loss: 16.7399, Val Loss: 17.1375\n",
      "Epoch 12/1000, Loss: 16.7024, Val Loss: 17.1181\n",
      "Epoch 13/1000, Loss: 16.7027, Val Loss: 17.0987\n",
      "Epoch 14/1000, Loss: 16.6552, Val Loss: 17.0787\n",
      "Epoch 15/1000, Loss: 16.6274, Val Loss: 17.0588\n",
      "Epoch 16/1000, Loss: 16.6215, Val Loss: 17.0375\n",
      "Epoch 17/1000, Loss: 16.5791, Val Loss: 17.0161\n",
      "Epoch 18/1000, Loss: 16.5875, Val Loss: 16.9936\n",
      "Epoch 19/1000, Loss: 16.5729, Val Loss: 16.9717\n",
      "Epoch 20/1000, Loss: 16.5455, Val Loss: 16.9510\n",
      "Epoch 21/1000, Loss: 16.5343, Val Loss: 16.9344\n",
      "Epoch 22/1000, Loss: 16.5044, Val Loss: 16.9167\n",
      "Epoch 23/1000, Loss: 16.4997, Val Loss: 16.8995\n",
      "Epoch 24/1000, Loss: 16.4860, Val Loss: 16.8823\n",
      "Epoch 25/1000, Loss: 16.4713, Val Loss: 16.8651\n",
      "Epoch 26/1000, Loss: 16.4492, Val Loss: 16.8478\n",
      "Epoch 27/1000, Loss: 16.4345, Val Loss: 16.8315\n",
      "Epoch 28/1000, Loss: 16.4371, Val Loss: 16.8163\n",
      "Epoch 29/1000, Loss: 16.4189, Val Loss: 16.8022\n",
      "Epoch 30/1000, Loss: 16.3973, Val Loss: 16.7863\n",
      "Epoch 31/1000, Loss: 16.3752, Val Loss: 16.7699\n",
      "Epoch 32/1000, Loss: 16.3382, Val Loss: 16.7551\n",
      "Epoch 33/1000, Loss: 16.3465, Val Loss: 16.7437\n",
      "Epoch 34/1000, Loss: 16.3403, Val Loss: 16.7334\n",
      "Epoch 35/1000, Loss: 16.3413, Val Loss: 16.7227\n",
      "Epoch 36/1000, Loss: 16.3427, Val Loss: 16.7128\n",
      "Epoch 37/1000, Loss: 16.3058, Val Loss: 16.7047\n",
      "Epoch 38/1000, Loss: 16.2949, Val Loss: 16.6942\n",
      "Epoch 39/1000, Loss: 16.2841, Val Loss: 16.6826\n",
      "Epoch 40/1000, Loss: 16.2649, Val Loss: 16.6711\n",
      "Epoch 41/1000, Loss: 16.2912, Val Loss: 16.6613\n",
      "Epoch 42/1000, Loss: 16.2801, Val Loss: 16.6523\n",
      "Epoch 43/1000, Loss: 16.2342, Val Loss: 16.6446\n",
      "Epoch 44/1000, Loss: 16.2322, Val Loss: 16.6382\n",
      "Epoch 45/1000, Loss: 16.2894, Val Loss: 16.6373\n",
      "Epoch 46/1000, Loss: 16.2355, Val Loss: 16.6365\n",
      "Epoch 47/1000, Loss: 16.2007, Val Loss: 16.6331\n",
      "Epoch 48/1000, Loss: 16.1929, Val Loss: 16.6291\n",
      "Epoch 49/1000, Loss: 16.1822, Val Loss: 16.6237\n",
      "Epoch 50/1000, Loss: 16.2001, Val Loss: 16.6167\n",
      "Epoch 51/1000, Loss: 16.2022, Val Loss: 16.6092\n",
      "Epoch 52/1000, Loss: 16.1812, Val Loss: 16.5996\n",
      "Epoch 53/1000, Loss: 16.1658, Val Loss: 16.5916\n",
      "Epoch 54/1000, Loss: 16.1485, Val Loss: 16.5830\n",
      "Epoch 55/1000, Loss: 16.1638, Val Loss: 16.5751\n",
      "Epoch 56/1000, Loss: 16.1493, Val Loss: 16.5697\n",
      "Epoch 57/1000, Loss: 16.1768, Val Loss: 16.5632\n",
      "Epoch 58/1000, Loss: 16.1688, Val Loss: 16.5556\n",
      "Epoch 59/1000, Loss: 16.1541, Val Loss: 16.5468\n",
      "Epoch 60/1000, Loss: 16.0906, Val Loss: 16.5378\n",
      "Epoch 61/1000, Loss: 16.0853, Val Loss: 16.5308\n",
      "Epoch 62/1000, Loss: 16.0836, Val Loss: 16.5244\n",
      "Epoch 63/1000, Loss: 16.1239, Val Loss: 16.5201\n",
      "Epoch 64/1000, Loss: 16.0692, Val Loss: 16.5172\n",
      "Epoch 65/1000, Loss: 16.0607, Val Loss: 16.5134\n",
      "Epoch 66/1000, Loss: 16.0682, Val Loss: 16.5052\n",
      "Epoch 67/1000, Loss: 16.0328, Val Loss: 16.4973\n",
      "Epoch 68/1000, Loss: 16.0350, Val Loss: 16.4859\n",
      "Epoch 69/1000, Loss: 16.0435, Val Loss: 16.4691\n",
      "Epoch 70/1000, Loss: 16.0292, Val Loss: 16.4506\n",
      "Epoch 71/1000, Loss: 15.9732, Val Loss: 16.4267\n",
      "Epoch 72/1000, Loss: 16.0029, Val Loss: 16.4079\n",
      "Epoch 73/1000, Loss: 16.0445, Val Loss: 16.3941\n",
      "Epoch 74/1000, Loss: 15.9809, Val Loss: 16.3848\n",
      "Epoch 75/1000, Loss: 15.9717, Val Loss: 16.3791\n",
      "Epoch 76/1000, Loss: 15.9920, Val Loss: 16.3744\n",
      "Epoch 77/1000, Loss: 15.9903, Val Loss: 16.3606\n",
      "Epoch 78/1000, Loss: 15.9525, Val Loss: 16.3477\n",
      "Epoch 79/1000, Loss: 15.8984, Val Loss: 16.3367\n",
      "Epoch 80/1000, Loss: 15.9375, Val Loss: 16.3288\n",
      "Epoch 81/1000, Loss: 15.8806, Val Loss: 16.3280\n",
      "Epoch 82/1000, Loss: 15.9369, Val Loss: 16.3278\n",
      "Epoch 83/1000, Loss: 15.9195, Val Loss: 16.3239\n",
      "Epoch 84/1000, Loss: 15.8466, Val Loss: 16.3169\n",
      "Epoch 85/1000, Loss: 15.9176, Val Loss: 16.3055\n",
      "Epoch 86/1000, Loss: 15.9092, Val Loss: 16.2939\n",
      "Epoch 87/1000, Loss: 15.9273, Val Loss: 16.2871\n",
      "Epoch 88/1000, Loss: 15.9242, Val Loss: 16.2785\n",
      "Epoch 89/1000, Loss: 15.8651, Val Loss: 16.2699\n",
      "Epoch 90/1000, Loss: 15.9066, Val Loss: 16.2610\n",
      "Epoch 91/1000, Loss: 15.9718, Val Loss: 16.2490\n",
      "Epoch 92/1000, Loss: 15.9000, Val Loss: 16.2417\n",
      "Epoch 93/1000, Loss: 15.9224, Val Loss: 16.2395\n",
      "Epoch 94/1000, Loss: 15.8828, Val Loss: 16.2419\n",
      "Epoch 95/1000, Loss: 15.9358, Val Loss: 16.2442\n",
      "Epoch 96/1000, Loss: 15.9567, Val Loss: 16.2457\n",
      "Epoch 97/1000, Loss: 15.9032, Val Loss: 16.2483\n",
      "Epoch 98/1000, Loss: 15.9607, Val Loss: 16.2455\n",
      "Epoch 99/1000, Loss: 15.8641, Val Loss: 16.2430\n",
      "Epoch 100/1000, Loss: 15.8512, Val Loss: 16.2420\n",
      "Epoch 101/1000, Loss: 15.8682, Val Loss: 16.2407\n",
      "Epoch 102/1000, Loss: 15.9100, Val Loss: 16.2369\n",
      "Epoch 103/1000, Loss: 15.8570, Val Loss: 16.2349\n",
      "Epoch 104/1000, Loss: 15.8083, Val Loss: 16.2341\n",
      "Epoch 105/1000, Loss: 15.8537, Val Loss: 16.2291\n",
      "Epoch 106/1000, Loss: 15.8147, Val Loss: 16.2262\n",
      "Epoch 107/1000, Loss: 15.8527, Val Loss: 16.2277\n",
      "Epoch 108/1000, Loss: 15.8717, Val Loss: 16.2257\n",
      "Epoch 109/1000, Loss: 15.8330, Val Loss: 16.2231\n",
      "Epoch 110/1000, Loss: 15.8192, Val Loss: 16.2239\n",
      "Epoch 111/1000, Loss: 15.8851, Val Loss: 16.2226\n",
      "Epoch 112/1000, Loss: 15.8362, Val Loss: 16.2215\n",
      "Epoch 113/1000, Loss: 15.8796, Val Loss: 16.2192\n",
      "Epoch 114/1000, Loss: 15.8228, Val Loss: 16.2154\n",
      "Epoch 115/1000, Loss: 15.9288, Val Loss: 16.2139\n",
      "Epoch 116/1000, Loss: 15.9323, Val Loss: 16.2081\n",
      "Epoch 117/1000, Loss: 15.7333, Val Loss: 16.2065\n",
      "Epoch 118/1000, Loss: 15.8424, Val Loss: 16.2044\n",
      "Epoch 119/1000, Loss: 15.8003, Val Loss: 16.2094\n",
      "Epoch 120/1000, Loss: 15.8241, Val Loss: 16.2144\n",
      "Epoch 121/1000, Loss: 15.8360, Val Loss: 16.2150\n",
      "Epoch 122/1000, Loss: 15.7987, Val Loss: 16.2191\n",
      "Epoch 123/1000, Loss: 15.8641, Val Loss: 16.2207\n",
      "Epoch 124/1000, Loss: 15.8766, Val Loss: 16.2160\n",
      "Epoch 125/1000, Loss: 15.8136, Val Loss: 16.2125\n",
      "Epoch 126/1000, Loss: 15.7908, Val Loss: 16.2074\n",
      "Epoch 127/1000, Loss: 15.8436, Val Loss: 16.2033\n",
      "Epoch 128/1000, Loss: 15.8567, Val Loss: 16.2017\n",
      "Epoch 129/1000, Loss: 15.8974, Val Loss: 16.2006\n",
      "Epoch 130/1000, Loss: 15.8576, Val Loss: 16.2005\n",
      "Epoch 131/1000, Loss: 15.8498, Val Loss: 16.1932\n",
      "Epoch 132/1000, Loss: 15.7922, Val Loss: 16.1869\n",
      "Epoch 133/1000, Loss: 15.7974, Val Loss: 16.1837\n",
      "Epoch 134/1000, Loss: 15.8410, Val Loss: 16.1824\n",
      "Epoch 135/1000, Loss: 15.8386, Val Loss: 16.1852\n",
      "Epoch 136/1000, Loss: 15.8653, Val Loss: 16.1895\n",
      "Epoch 137/1000, Loss: 15.8791, Val Loss: 16.1947\n",
      "Epoch 138/1000, Loss: 15.7989, Val Loss: 16.2001\n",
      "Epoch 139/1000, Loss: 15.7933, Val Loss: 16.2033\n",
      "Epoch 140/1000, Loss: 15.9099, Val Loss: 16.1988\n",
      "Epoch 141/1000, Loss: 15.8530, Val Loss: 16.1926\n",
      "Epoch 142/1000, Loss: 15.8479, Val Loss: 16.1879\n",
      "Epoch 143/1000, Loss: 15.8028, Val Loss: 16.1812\n",
      "Epoch 144/1000, Loss: 15.8130, Val Loss: 16.1841\n",
      "Epoch 145/1000, Loss: 15.7738, Val Loss: 16.1906\n",
      "Epoch 146/1000, Loss: 15.7529, Val Loss: 16.2015\n",
      "Epoch 147/1000, Loss: 15.8687, Val Loss: 16.2081\n",
      "Epoch 148/1000, Loss: 15.8839, Val Loss: 16.2068\n",
      "Epoch 149/1000, Loss: 15.7940, Val Loss: 16.2029\n",
      "Epoch 150/1000, Loss: 15.8065, Val Loss: 16.1984\n",
      "Epoch 151/1000, Loss: 15.7093, Val Loss: 16.1939\n",
      "Epoch 152/1000, Loss: 15.7863, Val Loss: 16.1952\n",
      "Epoch 153/1000, Loss: 15.8698, Val Loss: 16.1993\n",
      "Epoch 154/1000, Loss: 15.7762, Val Loss: 16.2016\n",
      "Epoch 155/1000, Loss: 15.8161, Val Loss: 16.2030\n",
      "Epoch 156/1000, Loss: 15.7888, Val Loss: 16.2053\n",
      "Epoch 157/1000, Loss: 15.7174, Val Loss: 16.2053\n",
      "Epoch 158/1000, Loss: 15.8091, Val Loss: 16.1996\n",
      "Epoch 159/1000, Loss: 15.7949, Val Loss: 16.1895\n",
      "Epoch 160/1000, Loss: 15.7830, Val Loss: 16.1821\n",
      "Epoch 161/1000, Loss: 15.8391, Val Loss: 16.1761\n",
      "Epoch 162/1000, Loss: 15.8150, Val Loss: 16.1723\n",
      "Epoch 163/1000, Loss: 15.8271, Val Loss: 16.1805\n",
      "Epoch 164/1000, Loss: 15.7790, Val Loss: 16.1914\n",
      "Epoch 165/1000, Loss: 15.8651, Val Loss: 16.2043\n",
      "Epoch 166/1000, Loss: 15.7918, Val Loss: 16.2088\n",
      "Epoch 167/1000, Loss: 15.7285, Val Loss: 16.2088\n",
      "Epoch 168/1000, Loss: 15.7816, Val Loss: 16.2010\n",
      "Epoch 169/1000, Loss: 15.8460, Val Loss: 16.1881\n",
      "Epoch 170/1000, Loss: 15.7997, Val Loss: 16.1754\n",
      "Epoch 171/1000, Loss: 15.7704, Val Loss: 16.1666\n",
      "Epoch 172/1000, Loss: 15.7688, Val Loss: 16.1665\n",
      "Epoch 173/1000, Loss: 15.8017, Val Loss: 16.1708\n",
      "Epoch 174/1000, Loss: 15.8308, Val Loss: 16.1747\n",
      "Epoch 175/1000, Loss: 15.8056, Val Loss: 16.1796\n",
      "Epoch 176/1000, Loss: 15.7756, Val Loss: 16.1893\n",
      "Epoch 177/1000, Loss: 15.8167, Val Loss: 16.1993\n",
      "Epoch 178/1000, Loss: 15.8410, Val Loss: 16.2071\n",
      "Epoch 179/1000, Loss: 15.8239, Val Loss: 16.2029\n",
      "Epoch 180/1000, Loss: 15.8201, Val Loss: 16.1981\n",
      "Epoch 181/1000, Loss: 15.7929, Val Loss: 16.1954\n",
      "Epoch 182/1000, Loss: 15.7288, Val Loss: 16.2007\n",
      "Epoch 183/1000, Loss: 15.6939, Val Loss: 16.2152\n",
      "Epoch 184/1000, Loss: 15.7922, Val Loss: 16.2318\n",
      "Epoch 185/1000, Loss: 15.7262, Val Loss: 16.2412\n",
      "Epoch 186/1000, Loss: 15.7844, Val Loss: 16.2441\n",
      "Epoch 187/1000, Loss: 15.7739, Val Loss: 16.2422\n",
      "Epoch 188/1000, Loss: 15.7820, Val Loss: 16.2385\n",
      "Epoch 189/1000, Loss: 15.8091, Val Loss: 16.2259\n",
      "Epoch 190/1000, Loss: 15.7914, Val Loss: 16.2033\n",
      "Epoch 191/1000, Loss: 15.7429, Val Loss: 16.1813\n",
      "Epoch 192/1000, Loss: 15.7923, Val Loss: 16.1671\n",
      "Epoch 193/1000, Loss: 15.7989, Val Loss: 16.1625\n",
      "Epoch 194/1000, Loss: 15.7978, Val Loss: 16.1762\n",
      "Epoch 195/1000, Loss: 15.7611, Val Loss: 16.1893\n",
      "Epoch 196/1000, Loss: 15.7540, Val Loss: 16.1915\n",
      "Epoch 197/1000, Loss: 15.6961, Val Loss: 16.1924\n",
      "Epoch 198/1000, Loss: 15.8151, Val Loss: 16.2041\n",
      "Epoch 199/1000, Loss: 15.8113, Val Loss: 16.2169\n",
      "Epoch 200/1000, Loss: 15.8238, Val Loss: 16.2243\n",
      "Epoch 201/1000, Loss: 15.6603, Val Loss: 16.2170\n",
      "Epoch 202/1000, Loss: 15.7735, Val Loss: 16.2136\n",
      "Epoch 203/1000, Loss: 15.7733, Val Loss: 16.2119\n",
      "Epoch 204/1000, Loss: 15.7669, Val Loss: 16.2097\n",
      "Epoch 205/1000, Loss: 15.7525, Val Loss: 16.2270\n",
      "Epoch 206/1000, Loss: 15.7816, Val Loss: 16.2418\n",
      "Epoch 207/1000, Loss: 15.7300, Val Loss: 16.2476\n",
      "Epoch 208/1000, Loss: 15.8265, Val Loss: 16.2504\n",
      "Epoch 209/1000, Loss: 15.7729, Val Loss: 16.2407\n",
      "Epoch 210/1000, Loss: 15.7544, Val Loss: 16.2358\n",
      "Epoch 211/1000, Loss: 15.7737, Val Loss: 16.2410\n",
      "Epoch 212/1000, Loss: 15.7892, Val Loss: 16.2491\n",
      "Epoch 213/1000, Loss: 15.7202, Val Loss: 16.2500\n",
      "Epoch 214/1000, Loss: 15.7976, Val Loss: 16.2479\n",
      "Epoch 215/1000, Loss: 15.7353, Val Loss: 16.2344\n",
      "Epoch 216/1000, Loss: 15.7867, Val Loss: 16.2217\n",
      "Epoch 217/1000, Loss: 15.7583, Val Loss: 16.2067\n",
      "Epoch 218/1000, Loss: 15.8114, Val Loss: 16.1947\n",
      "Epoch 219/1000, Loss: 15.7502, Val Loss: 16.1793\n",
      "Epoch 220/1000, Loss: 15.8068, Val Loss: 16.1651\n",
      "Epoch 221/1000, Loss: 15.7993, Val Loss: 16.1559\n",
      "Epoch 222/1000, Loss: 15.7250, Val Loss: 16.1576\n",
      "Epoch 223/1000, Loss: 15.6906, Val Loss: 16.1672\n",
      "Epoch 224/1000, Loss: 15.6984, Val Loss: 16.1734\n",
      "Epoch 225/1000, Loss: 15.7612, Val Loss: 16.1861\n",
      "Epoch 226/1000, Loss: 15.7773, Val Loss: 16.1903\n",
      "Epoch 227/1000, Loss: 15.7383, Val Loss: 16.1887\n",
      "Epoch 228/1000, Loss: 15.7264, Val Loss: 16.1821\n",
      "Epoch 229/1000, Loss: 15.6634, Val Loss: 16.1762\n",
      "Epoch 230/1000, Loss: 15.7473, Val Loss: 16.1812\n",
      "Epoch 231/1000, Loss: 15.7136, Val Loss: 16.1812\n",
      "Epoch 232/1000, Loss: 15.7806, Val Loss: 16.1808\n",
      "Epoch 233/1000, Loss: 15.7560, Val Loss: 16.1795\n",
      "Epoch 234/1000, Loss: 15.6845, Val Loss: 16.1741\n",
      "Epoch 235/1000, Loss: 15.7759, Val Loss: 16.1643\n",
      "Epoch 236/1000, Loss: 15.6897, Val Loss: 16.1578\n",
      "Epoch 237/1000, Loss: 15.7113, Val Loss: 16.1516\n",
      "Epoch 238/1000, Loss: 15.7506, Val Loss: 16.1499\n",
      "Epoch 239/1000, Loss: 15.7201, Val Loss: 16.1508\n",
      "Epoch 240/1000, Loss: 15.7067, Val Loss: 16.1584\n",
      "Epoch 241/1000, Loss: 15.7357, Val Loss: 16.1664\n",
      "Epoch 242/1000, Loss: 15.7600, Val Loss: 16.1731\n",
      "Epoch 243/1000, Loss: 15.7206, Val Loss: 16.1678\n",
      "Epoch 244/1000, Loss: 15.7022, Val Loss: 16.1576\n",
      "Epoch 245/1000, Loss: 15.7455, Val Loss: 16.1504\n",
      "Epoch 246/1000, Loss: 15.6823, Val Loss: 16.1490\n",
      "Epoch 247/1000, Loss: 15.7164, Val Loss: 16.1539\n",
      "Epoch 248/1000, Loss: 15.6894, Val Loss: 16.1739\n",
      "Epoch 249/1000, Loss: 15.6971, Val Loss: 16.1893\n",
      "Epoch 250/1000, Loss: 15.6986, Val Loss: 16.1844\n",
      "Epoch 251/1000, Loss: 15.6886, Val Loss: 16.1861\n",
      "Epoch 252/1000, Loss: 15.7467, Val Loss: 16.1782\n",
      "Epoch 253/1000, Loss: 15.7602, Val Loss: 16.1749\n",
      "Epoch 254/1000, Loss: 15.7704, Val Loss: 16.1739\n",
      "Epoch 255/1000, Loss: 15.7259, Val Loss: 16.1691\n",
      "Epoch 256/1000, Loss: 15.6735, Val Loss: 16.1665\n",
      "Epoch 257/1000, Loss: 15.6797, Val Loss: 16.1666\n",
      "Epoch 258/1000, Loss: 15.7523, Val Loss: 16.1716\n",
      "Epoch 259/1000, Loss: 15.6946, Val Loss: 16.1782\n",
      "Epoch 260/1000, Loss: 15.8178, Val Loss: 16.1688\n",
      "Epoch 261/1000, Loss: 15.7471, Val Loss: 16.1595\n",
      "Epoch 262/1000, Loss: 15.7177, Val Loss: 16.1510\n",
      "Epoch 263/1000, Loss: 15.8019, Val Loss: 16.1446\n",
      "Epoch 264/1000, Loss: 15.7329, Val Loss: 16.1327\n",
      "Epoch 265/1000, Loss: 15.7603, Val Loss: 16.1271\n",
      "Epoch 266/1000, Loss: 15.6853, Val Loss: 16.1272\n",
      "Epoch 267/1000, Loss: 15.6792, Val Loss: 16.1300\n",
      "Epoch 268/1000, Loss: 15.7210, Val Loss: 16.1319\n",
      "Epoch 269/1000, Loss: 15.7324, Val Loss: 16.1332\n",
      "Epoch 270/1000, Loss: 15.7458, Val Loss: 16.1333\n",
      "Epoch 271/1000, Loss: 15.8011, Val Loss: 16.1294\n",
      "Epoch 272/1000, Loss: 15.7344, Val Loss: 16.1325\n",
      "Epoch 273/1000, Loss: 15.6871, Val Loss: 16.1392\n",
      "Epoch 274/1000, Loss: 15.6912, Val Loss: 16.1337\n",
      "Epoch 275/1000, Loss: 15.7403, Val Loss: 16.1259\n",
      "Epoch 276/1000, Loss: 15.7366, Val Loss: 16.1182\n",
      "Epoch 277/1000, Loss: 15.7842, Val Loss: 16.1058\n",
      "Epoch 278/1000, Loss: 15.6883, Val Loss: 16.1002\n",
      "Epoch 279/1000, Loss: 15.6899, Val Loss: 16.1012\n",
      "Epoch 280/1000, Loss: 15.7186, Val Loss: 16.1094\n",
      "Epoch 281/1000, Loss: 15.6409, Val Loss: 16.1185\n",
      "Epoch 282/1000, Loss: 15.6981, Val Loss: 16.1230\n",
      "Epoch 283/1000, Loss: 15.6963, Val Loss: 16.1208\n",
      "Epoch 284/1000, Loss: 15.6791, Val Loss: 16.1127\n",
      "Epoch 285/1000, Loss: 15.6556, Val Loss: 16.1082\n",
      "Epoch 286/1000, Loss: 15.6624, Val Loss: 16.1040\n",
      "Epoch 287/1000, Loss: 15.7315, Val Loss: 16.1025\n",
      "Epoch 288/1000, Loss: 15.7003, Val Loss: 16.1073\n",
      "Epoch 289/1000, Loss: 15.8143, Val Loss: 16.1126\n",
      "Epoch 290/1000, Loss: 15.7756, Val Loss: 16.1274\n",
      "Epoch 291/1000, Loss: 15.7394, Val Loss: 16.1418\n",
      "Epoch 292/1000, Loss: 15.7367, Val Loss: 16.1504\n",
      "Epoch 293/1000, Loss: 15.7211, Val Loss: 16.1447\n",
      "Epoch 294/1000, Loss: 15.6899, Val Loss: 16.1427\n",
      "Epoch 295/1000, Loss: 15.7576, Val Loss: 16.1321\n",
      "Epoch 296/1000, Loss: 15.7122, Val Loss: 16.1305\n",
      "Epoch 297/1000, Loss: 15.6558, Val Loss: 16.1293\n",
      "Epoch 298/1000, Loss: 15.7515, Val Loss: 16.1318\n",
      "Epoch 299/1000, Loss: 15.7581, Val Loss: 16.1373\n",
      "Epoch 300/1000, Loss: 15.7173, Val Loss: 16.1375\n",
      "Epoch 301/1000, Loss: 15.7060, Val Loss: 16.1407\n",
      "Epoch 302/1000, Loss: 15.7192, Val Loss: 16.1425\n",
      "Epoch 303/1000, Loss: 15.6793, Val Loss: 16.1465\n",
      "Epoch 304/1000, Loss: 15.7193, Val Loss: 16.1523\n",
      "Epoch 305/1000, Loss: 15.7004, Val Loss: 16.1511\n",
      "Epoch 306/1000, Loss: 15.6609, Val Loss: 16.1553\n",
      "Epoch 307/1000, Loss: 15.7236, Val Loss: 16.1479\n",
      "Epoch 308/1000, Loss: 15.6136, Val Loss: 16.1421\n",
      "Epoch 309/1000, Loss: 15.6845, Val Loss: 16.1341\n",
      "Epoch 310/1000, Loss: 15.7297, Val Loss: 16.1393\n",
      "Epoch 311/1000, Loss: 15.6980, Val Loss: 16.1497\n",
      "Epoch 312/1000, Loss: 15.6790, Val Loss: 16.1648\n",
      "Epoch 313/1000, Loss: 15.6516, Val Loss: 16.1863\n",
      "Epoch 314/1000, Loss: 15.7526, Val Loss: 16.2040\n",
      "Epoch 315/1000, Loss: 15.7007, Val Loss: 16.2075\n",
      "Epoch 316/1000, Loss: 15.6700, Val Loss: 16.2012\n",
      "Epoch 317/1000, Loss: 15.7253, Val Loss: 16.1850\n",
      "Epoch 318/1000, Loss: 15.6902, Val Loss: 16.1669\n",
      "Early stopping triggered at epoch 318. Best Val Loss: 16.1002\n",
      "Epoch 1/1000, Loss: 16.8349, Val Loss: 17.1091\n",
      "Epoch 2/1000, Loss: 16.8129, Val Loss: 17.1045\n",
      "Epoch 3/1000, Loss: 16.7984, Val Loss: 17.0994\n",
      "Epoch 4/1000, Loss: 16.7805, Val Loss: 17.0936\n",
      "Epoch 5/1000, Loss: 16.7608, Val Loss: 17.0872\n",
      "Epoch 6/1000, Loss: 16.7572, Val Loss: 17.0795\n",
      "Epoch 7/1000, Loss: 16.7453, Val Loss: 17.0730\n",
      "Epoch 8/1000, Loss: 16.7219, Val Loss: 17.0672\n",
      "Epoch 9/1000, Loss: 16.6972, Val Loss: 17.0609\n",
      "Epoch 10/1000, Loss: 16.6849, Val Loss: 17.0545\n",
      "Epoch 11/1000, Loss: 16.6721, Val Loss: 17.0466\n",
      "Epoch 12/1000, Loss: 16.6577, Val Loss: 17.0389\n",
      "Epoch 13/1000, Loss: 16.6385, Val Loss: 17.0314\n",
      "Epoch 14/1000, Loss: 16.6397, Val Loss: 17.0222\n",
      "Epoch 15/1000, Loss: 16.6180, Val Loss: 17.0122\n",
      "Epoch 16/1000, Loss: 16.6041, Val Loss: 17.0015\n",
      "Epoch 17/1000, Loss: 16.5738, Val Loss: 16.9903\n",
      "Epoch 18/1000, Loss: 16.5556, Val Loss: 16.9811\n",
      "Epoch 19/1000, Loss: 16.5565, Val Loss: 16.9720\n",
      "Epoch 20/1000, Loss: 16.5389, Val Loss: 16.9616\n",
      "Epoch 21/1000, Loss: 16.5216, Val Loss: 16.9497\n",
      "Epoch 22/1000, Loss: 16.5266, Val Loss: 16.9390\n",
      "Epoch 23/1000, Loss: 16.5248, Val Loss: 16.9287\n",
      "Epoch 24/1000, Loss: 16.4862, Val Loss: 16.9197\n",
      "Epoch 25/1000, Loss: 16.4915, Val Loss: 16.9092\n",
      "Epoch 26/1000, Loss: 16.4903, Val Loss: 16.8993\n",
      "Epoch 27/1000, Loss: 16.4935, Val Loss: 16.8923\n",
      "Epoch 28/1000, Loss: 16.4978, Val Loss: 16.8850\n",
      "Epoch 29/1000, Loss: 16.5501, Val Loss: 16.8859\n",
      "Epoch 30/1000, Loss: 16.4725, Val Loss: 16.8866\n",
      "Epoch 31/1000, Loss: 16.5181, Val Loss: 16.8873\n",
      "Epoch 32/1000, Loss: 16.4464, Val Loss: 16.8871\n",
      "Epoch 33/1000, Loss: 16.4844, Val Loss: 16.8879\n",
      "Epoch 34/1000, Loss: 16.4517, Val Loss: 16.8873\n",
      "Epoch 35/1000, Loss: 16.4575, Val Loss: 16.8861\n",
      "Epoch 36/1000, Loss: 16.4598, Val Loss: 16.8832\n",
      "Epoch 37/1000, Loss: 16.4458, Val Loss: 16.8802\n",
      "Epoch 38/1000, Loss: 16.4396, Val Loss: 16.8760\n",
      "Epoch 39/1000, Loss: 16.4548, Val Loss: 16.8699\n",
      "Epoch 40/1000, Loss: 16.4314, Val Loss: 16.8621\n",
      "Epoch 41/1000, Loss: 16.4467, Val Loss: 16.8597\n",
      "Epoch 42/1000, Loss: 16.4139, Val Loss: 16.8570\n",
      "Epoch 43/1000, Loss: 16.4553, Val Loss: 16.8551\n",
      "Epoch 44/1000, Loss: 16.4516, Val Loss: 16.8545\n",
      "Epoch 45/1000, Loss: 16.4285, Val Loss: 16.8544\n",
      "Epoch 46/1000, Loss: 16.4289, Val Loss: 16.8542\n",
      "Epoch 47/1000, Loss: 16.3937, Val Loss: 16.8536\n",
      "Epoch 48/1000, Loss: 16.4134, Val Loss: 16.8541\n",
      "Epoch 49/1000, Loss: 16.4474, Val Loss: 16.8528\n",
      "Epoch 50/1000, Loss: 16.3884, Val Loss: 16.8486\n",
      "Epoch 51/1000, Loss: 16.4223, Val Loss: 16.8465\n",
      "Epoch 52/1000, Loss: 16.4039, Val Loss: 16.8453\n",
      "Epoch 53/1000, Loss: 16.4193, Val Loss: 16.8469\n",
      "Epoch 54/1000, Loss: 16.4151, Val Loss: 16.8497\n",
      "Epoch 55/1000, Loss: 16.4257, Val Loss: 16.8507\n",
      "Epoch 56/1000, Loss: 16.4072, Val Loss: 16.8505\n",
      "Epoch 57/1000, Loss: 16.3766, Val Loss: 16.8495\n",
      "Epoch 58/1000, Loss: 16.3910, Val Loss: 16.8490\n",
      "Epoch 59/1000, Loss: 16.3843, Val Loss: 16.8476\n",
      "Epoch 60/1000, Loss: 16.3826, Val Loss: 16.8459\n",
      "Epoch 61/1000, Loss: 16.4200, Val Loss: 16.8444\n",
      "Epoch 62/1000, Loss: 16.4020, Val Loss: 16.8429\n",
      "Epoch 63/1000, Loss: 16.3866, Val Loss: 16.8414\n",
      "Epoch 64/1000, Loss: 16.3911, Val Loss: 16.8401\n",
      "Epoch 65/1000, Loss: 16.4069, Val Loss: 16.8387\n",
      "Epoch 66/1000, Loss: 16.3728, Val Loss: 16.8374\n",
      "Epoch 67/1000, Loss: 16.3554, Val Loss: 16.8355\n",
      "Epoch 68/1000, Loss: 16.4216, Val Loss: 16.8336\n",
      "Epoch 69/1000, Loss: 16.3596, Val Loss: 16.8312\n",
      "Epoch 70/1000, Loss: 16.3508, Val Loss: 16.8296\n",
      "Epoch 71/1000, Loss: 16.4211, Val Loss: 16.8277\n",
      "Epoch 72/1000, Loss: 16.3780, Val Loss: 16.8269\n",
      "Epoch 73/1000, Loss: 16.3928, Val Loss: 16.8257\n",
      "Epoch 74/1000, Loss: 16.3500, Val Loss: 16.8247\n",
      "Epoch 75/1000, Loss: 16.3796, Val Loss: 16.8237\n",
      "Epoch 76/1000, Loss: 16.3606, Val Loss: 16.8226\n",
      "Epoch 77/1000, Loss: 16.3729, Val Loss: 16.8217\n",
      "Epoch 78/1000, Loss: 16.3700, Val Loss: 16.8197\n",
      "Epoch 79/1000, Loss: 16.4184, Val Loss: 16.8181\n",
      "Epoch 80/1000, Loss: 16.3901, Val Loss: 16.8169\n",
      "Epoch 81/1000, Loss: 16.3814, Val Loss: 16.8151\n",
      "Epoch 82/1000, Loss: 16.3848, Val Loss: 16.8131\n",
      "Epoch 83/1000, Loss: 16.3638, Val Loss: 16.8117\n",
      "Epoch 84/1000, Loss: 16.3862, Val Loss: 16.8092\n",
      "Epoch 85/1000, Loss: 16.3588, Val Loss: 16.8078\n",
      "Epoch 86/1000, Loss: 16.3435, Val Loss: 16.8070\n",
      "Epoch 87/1000, Loss: 16.3930, Val Loss: 16.8061\n",
      "Epoch 88/1000, Loss: 16.3754, Val Loss: 16.8043\n",
      "Epoch 89/1000, Loss: 16.3379, Val Loss: 16.8029\n",
      "Epoch 90/1000, Loss: 16.3254, Val Loss: 16.8012\n",
      "Epoch 91/1000, Loss: 16.3523, Val Loss: 16.7996\n",
      "Epoch 92/1000, Loss: 16.3682, Val Loss: 16.7991\n",
      "Epoch 93/1000, Loss: 16.3960, Val Loss: 16.7974\n",
      "Epoch 94/1000, Loss: 16.3509, Val Loss: 16.7962\n",
      "Epoch 95/1000, Loss: 16.3469, Val Loss: 16.7940\n",
      "Epoch 96/1000, Loss: 16.3685, Val Loss: 16.7926\n",
      "Epoch 97/1000, Loss: 16.3306, Val Loss: 16.7903\n",
      "Epoch 98/1000, Loss: 16.3498, Val Loss: 16.7893\n",
      "Epoch 99/1000, Loss: 16.3272, Val Loss: 16.7877\n",
      "Epoch 100/1000, Loss: 16.3676, Val Loss: 16.7874\n",
      "Epoch 101/1000, Loss: 16.3492, Val Loss: 16.7859\n",
      "Epoch 102/1000, Loss: 16.3664, Val Loss: 16.7846\n",
      "Epoch 103/1000, Loss: 16.3856, Val Loss: 16.7827\n",
      "Epoch 104/1000, Loss: 16.3525, Val Loss: 16.7811\n",
      "Epoch 105/1000, Loss: 16.3258, Val Loss: 16.7794\n",
      "Epoch 106/1000, Loss: 16.3241, Val Loss: 16.7774\n",
      "Epoch 107/1000, Loss: 16.3636, Val Loss: 16.7772\n",
      "Epoch 108/1000, Loss: 16.3475, Val Loss: 16.7766\n",
      "Epoch 109/1000, Loss: 16.3304, Val Loss: 16.7757\n",
      "Epoch 110/1000, Loss: 16.3244, Val Loss: 16.7754\n",
      "Epoch 111/1000, Loss: 16.3428, Val Loss: 16.7738\n",
      "Epoch 112/1000, Loss: 16.3413, Val Loss: 16.7740\n",
      "Epoch 113/1000, Loss: 16.3384, Val Loss: 16.7741\n",
      "Epoch 114/1000, Loss: 16.3143, Val Loss: 16.7731\n",
      "Epoch 115/1000, Loss: 16.3542, Val Loss: 16.7723\n",
      "Epoch 116/1000, Loss: 16.3389, Val Loss: 16.7712\n",
      "Epoch 117/1000, Loss: 16.3759, Val Loss: 16.7694\n",
      "Epoch 118/1000, Loss: 16.3004, Val Loss: 16.7679\n",
      "Epoch 119/1000, Loss: 16.3625, Val Loss: 16.7651\n",
      "Epoch 120/1000, Loss: 16.3289, Val Loss: 16.7636\n",
      "Epoch 121/1000, Loss: 16.3351, Val Loss: 16.7599\n",
      "Epoch 122/1000, Loss: 16.3673, Val Loss: 16.7582\n",
      "Epoch 123/1000, Loss: 16.3185, Val Loss: 16.7564\n",
      "Epoch 124/1000, Loss: 16.3223, Val Loss: 16.7549\n",
      "Epoch 125/1000, Loss: 16.3099, Val Loss: 16.7538\n",
      "Epoch 126/1000, Loss: 16.3141, Val Loss: 16.7537\n",
      "Epoch 127/1000, Loss: 16.3159, Val Loss: 16.7533\n",
      "Epoch 128/1000, Loss: 16.3632, Val Loss: 16.7528\n",
      "Epoch 129/1000, Loss: 16.2884, Val Loss: 16.7527\n",
      "Epoch 130/1000, Loss: 16.2985, Val Loss: 16.7520\n",
      "Epoch 131/1000, Loss: 16.3480, Val Loss: 16.7510\n",
      "Epoch 132/1000, Loss: 16.2915, Val Loss: 16.7505\n",
      "Epoch 133/1000, Loss: 16.3267, Val Loss: 16.7493\n",
      "Epoch 134/1000, Loss: 16.3044, Val Loss: 16.7491\n",
      "Epoch 135/1000, Loss: 16.2880, Val Loss: 16.7490\n",
      "Epoch 136/1000, Loss: 16.3221, Val Loss: 16.7487\n",
      "Epoch 137/1000, Loss: 16.3073, Val Loss: 16.7469\n",
      "Epoch 138/1000, Loss: 16.3664, Val Loss: 16.7473\n",
      "Epoch 139/1000, Loss: 16.3361, Val Loss: 16.7453\n",
      "Epoch 140/1000, Loss: 16.2756, Val Loss: 16.7442\n",
      "Epoch 141/1000, Loss: 16.3338, Val Loss: 16.7421\n",
      "Epoch 142/1000, Loss: 16.3088, Val Loss: 16.7392\n",
      "Epoch 143/1000, Loss: 16.3148, Val Loss: 16.7373\n",
      "Epoch 144/1000, Loss: 16.3120, Val Loss: 16.7344\n",
      "Epoch 145/1000, Loss: 16.3201, Val Loss: 16.7324\n",
      "Epoch 146/1000, Loss: 16.3248, Val Loss: 16.7310\n",
      "Epoch 147/1000, Loss: 16.2997, Val Loss: 16.7303\n",
      "Epoch 148/1000, Loss: 16.2958, Val Loss: 16.7300\n",
      "Epoch 149/1000, Loss: 16.2987, Val Loss: 16.7319\n",
      "Epoch 150/1000, Loss: 16.3023, Val Loss: 16.7326\n",
      "Epoch 151/1000, Loss: 16.3097, Val Loss: 16.7315\n",
      "Epoch 152/1000, Loss: 16.2897, Val Loss: 16.7313\n",
      "Epoch 153/1000, Loss: 16.3103, Val Loss: 16.7311\n",
      "Epoch 154/1000, Loss: 16.3020, Val Loss: 16.7298\n",
      "Epoch 155/1000, Loss: 16.3140, Val Loss: 16.7291\n",
      "Epoch 156/1000, Loss: 16.3013, Val Loss: 16.7284\n",
      "Epoch 157/1000, Loss: 16.3191, Val Loss: 16.7255\n",
      "Epoch 158/1000, Loss: 16.2745, Val Loss: 16.7247\n",
      "Epoch 159/1000, Loss: 16.3373, Val Loss: 16.7231\n",
      "Epoch 160/1000, Loss: 16.2485, Val Loss: 16.7221\n",
      "Epoch 161/1000, Loss: 16.3113, Val Loss: 16.7197\n",
      "Epoch 162/1000, Loss: 16.2941, Val Loss: 16.7185\n",
      "Epoch 163/1000, Loss: 16.2748, Val Loss: 16.7174\n",
      "Epoch 164/1000, Loss: 16.3095, Val Loss: 16.7164\n",
      "Epoch 165/1000, Loss: 16.2888, Val Loss: 16.7153\n",
      "Epoch 166/1000, Loss: 16.2875, Val Loss: 16.7150\n",
      "Epoch 167/1000, Loss: 16.2796, Val Loss: 16.7148\n",
      "Epoch 168/1000, Loss: 16.2694, Val Loss: 16.7141\n",
      "Epoch 169/1000, Loss: 16.2545, Val Loss: 16.7159\n",
      "Epoch 170/1000, Loss: 16.2784, Val Loss: 16.7161\n",
      "Epoch 171/1000, Loss: 16.2832, Val Loss: 16.7160\n",
      "Epoch 172/1000, Loss: 16.2831, Val Loss: 16.7156\n",
      "Epoch 173/1000, Loss: 16.2931, Val Loss: 16.7164\n",
      "Epoch 174/1000, Loss: 16.2718, Val Loss: 16.7172\n",
      "Epoch 175/1000, Loss: 16.2369, Val Loss: 16.7183\n",
      "Epoch 176/1000, Loss: 16.2681, Val Loss: 16.7185\n",
      "Epoch 177/1000, Loss: 16.2847, Val Loss: 16.7183\n",
      "Epoch 178/1000, Loss: 16.2646, Val Loss: 16.7160\n",
      "Epoch 179/1000, Loss: 16.2641, Val Loss: 16.7130\n",
      "Epoch 180/1000, Loss: 16.2867, Val Loss: 16.7093\n",
      "Epoch 181/1000, Loss: 16.2690, Val Loss: 16.7061\n",
      "Epoch 182/1000, Loss: 16.2718, Val Loss: 16.7047\n",
      "Epoch 183/1000, Loss: 16.2668, Val Loss: 16.7044\n",
      "Epoch 184/1000, Loss: 16.3007, Val Loss: 16.7020\n",
      "Epoch 185/1000, Loss: 16.3107, Val Loss: 16.7011\n",
      "Epoch 186/1000, Loss: 16.2784, Val Loss: 16.7007\n",
      "Epoch 187/1000, Loss: 16.2510, Val Loss: 16.7000\n",
      "Epoch 188/1000, Loss: 16.2588, Val Loss: 16.6982\n",
      "Epoch 189/1000, Loss: 16.2402, Val Loss: 16.6971\n",
      "Epoch 190/1000, Loss: 16.2550, Val Loss: 16.6978\n",
      "Epoch 191/1000, Loss: 16.2613, Val Loss: 16.6991\n",
      "Epoch 192/1000, Loss: 16.2405, Val Loss: 16.6985\n",
      "Epoch 193/1000, Loss: 16.2729, Val Loss: 16.6985\n",
      "Epoch 194/1000, Loss: 16.2927, Val Loss: 16.6968\n",
      "Epoch 195/1000, Loss: 16.2362, Val Loss: 16.6968\n",
      "Epoch 196/1000, Loss: 16.2799, Val Loss: 16.6967\n",
      "Epoch 197/1000, Loss: 16.3041, Val Loss: 16.6981\n",
      "Epoch 198/1000, Loss: 16.2880, Val Loss: 16.6994\n",
      "Epoch 199/1000, Loss: 16.2489, Val Loss: 16.7005\n",
      "Epoch 200/1000, Loss: 16.2532, Val Loss: 16.6982\n",
      "Epoch 201/1000, Loss: 16.2515, Val Loss: 16.6981\n",
      "Epoch 202/1000, Loss: 16.2420, Val Loss: 16.6966\n",
      "Epoch 203/1000, Loss: 16.2462, Val Loss: 16.6948\n",
      "Epoch 204/1000, Loss: 16.2543, Val Loss: 16.6931\n",
      "Epoch 205/1000, Loss: 16.2399, Val Loss: 16.6897\n",
      "Epoch 206/1000, Loss: 16.2745, Val Loss: 16.6863\n",
      "Epoch 207/1000, Loss: 16.2525, Val Loss: 16.6826\n",
      "Epoch 208/1000, Loss: 16.2398, Val Loss: 16.6799\n",
      "Epoch 209/1000, Loss: 16.2617, Val Loss: 16.6784\n",
      "Epoch 210/1000, Loss: 16.2540, Val Loss: 16.6763\n",
      "Epoch 211/1000, Loss: 16.2486, Val Loss: 16.6771\n",
      "Epoch 212/1000, Loss: 16.2498, Val Loss: 16.6768\n",
      "Epoch 213/1000, Loss: 16.2699, Val Loss: 16.6778\n",
      "Epoch 214/1000, Loss: 16.2650, Val Loss: 16.6771\n",
      "Epoch 215/1000, Loss: 16.2734, Val Loss: 16.6756\n",
      "Epoch 216/1000, Loss: 16.2359, Val Loss: 16.6747\n",
      "Epoch 217/1000, Loss: 16.2368, Val Loss: 16.6730\n",
      "Epoch 218/1000, Loss: 16.2471, Val Loss: 16.6716\n",
      "Epoch 219/1000, Loss: 16.2313, Val Loss: 16.6696\n",
      "Epoch 220/1000, Loss: 16.2667, Val Loss: 16.6672\n",
      "Epoch 221/1000, Loss: 16.2486, Val Loss: 16.6652\n",
      "Epoch 222/1000, Loss: 16.2138, Val Loss: 16.6658\n",
      "Epoch 223/1000, Loss: 16.2043, Val Loss: 16.6650\n",
      "Epoch 224/1000, Loss: 16.2178, Val Loss: 16.6653\n",
      "Epoch 225/1000, Loss: 16.2473, Val Loss: 16.6673\n",
      "Epoch 226/1000, Loss: 16.2351, Val Loss: 16.6677\n",
      "Epoch 227/1000, Loss: 16.2650, Val Loss: 16.6686\n",
      "Epoch 228/1000, Loss: 16.2337, Val Loss: 16.6693\n",
      "Epoch 229/1000, Loss: 16.2597, Val Loss: 16.6685\n",
      "Epoch 230/1000, Loss: 16.2645, Val Loss: 16.6682\n",
      "Epoch 231/1000, Loss: 16.2427, Val Loss: 16.6663\n",
      "Epoch 232/1000, Loss: 16.2325, Val Loss: 16.6643\n",
      "Epoch 233/1000, Loss: 16.2443, Val Loss: 16.6610\n",
      "Epoch 234/1000, Loss: 16.2285, Val Loss: 16.6599\n",
      "Epoch 235/1000, Loss: 16.2522, Val Loss: 16.6587\n",
      "Epoch 236/1000, Loss: 16.2453, Val Loss: 16.6570\n",
      "Epoch 237/1000, Loss: 16.2418, Val Loss: 16.6560\n",
      "Epoch 238/1000, Loss: 16.2059, Val Loss: 16.6555\n",
      "Epoch 239/1000, Loss: 16.2138, Val Loss: 16.6558\n",
      "Epoch 240/1000, Loss: 16.2191, Val Loss: 16.6565\n",
      "Epoch 241/1000, Loss: 16.2376, Val Loss: 16.6571\n",
      "Epoch 242/1000, Loss: 16.2112, Val Loss: 16.6556\n",
      "Epoch 243/1000, Loss: 16.2341, Val Loss: 16.6539\n",
      "Epoch 244/1000, Loss: 16.2041, Val Loss: 16.6551\n",
      "Epoch 245/1000, Loss: 16.2149, Val Loss: 16.6564\n",
      "Epoch 246/1000, Loss: 16.2037, Val Loss: 16.6575\n",
      "Epoch 247/1000, Loss: 16.2227, Val Loss: 16.6613\n",
      "Epoch 248/1000, Loss: 16.2529, Val Loss: 16.6607\n",
      "Epoch 249/1000, Loss: 16.2324, Val Loss: 16.6615\n",
      "Epoch 250/1000, Loss: 16.2162, Val Loss: 16.6611\n",
      "Epoch 251/1000, Loss: 16.1954, Val Loss: 16.6608\n",
      "Epoch 252/1000, Loss: 16.2390, Val Loss: 16.6588\n",
      "Epoch 253/1000, Loss: 16.2288, Val Loss: 16.6566\n",
      "Epoch 254/1000, Loss: 16.2033, Val Loss: 16.6547\n",
      "Epoch 255/1000, Loss: 16.2219, Val Loss: 16.6503\n",
      "Epoch 256/1000, Loss: 16.2162, Val Loss: 16.6478\n",
      "Epoch 257/1000, Loss: 16.2403, Val Loss: 16.6444\n",
      "Epoch 258/1000, Loss: 16.2016, Val Loss: 16.6416\n",
      "Epoch 259/1000, Loss: 16.1916, Val Loss: 16.6400\n",
      "Epoch 260/1000, Loss: 16.1884, Val Loss: 16.6383\n",
      "Epoch 261/1000, Loss: 16.2144, Val Loss: 16.6373\n",
      "Epoch 262/1000, Loss: 16.2125, Val Loss: 16.6371\n",
      "Epoch 263/1000, Loss: 16.2138, Val Loss: 16.6368\n",
      "Epoch 264/1000, Loss: 16.2571, Val Loss: 16.6367\n",
      "Epoch 265/1000, Loss: 16.2291, Val Loss: 16.6357\n",
      "Epoch 266/1000, Loss: 16.2181, Val Loss: 16.6342\n",
      "Epoch 267/1000, Loss: 16.2008, Val Loss: 16.6328\n",
      "Epoch 268/1000, Loss: 16.1800, Val Loss: 16.6353\n",
      "Epoch 269/1000, Loss: 16.2055, Val Loss: 16.6361\n",
      "Epoch 270/1000, Loss: 16.1754, Val Loss: 16.6375\n",
      "Epoch 271/1000, Loss: 16.1945, Val Loss: 16.6357\n",
      "Epoch 272/1000, Loss: 16.2074, Val Loss: 16.6356\n",
      "Epoch 273/1000, Loss: 16.2085, Val Loss: 16.6338\n",
      "Epoch 274/1000, Loss: 16.2197, Val Loss: 16.6317\n",
      "Epoch 275/1000, Loss: 16.2286, Val Loss: 16.6304\n",
      "Epoch 276/1000, Loss: 16.1732, Val Loss: 16.6303\n",
      "Epoch 277/1000, Loss: 16.1960, Val Loss: 16.6302\n",
      "Epoch 278/1000, Loss: 16.2158, Val Loss: 16.6304\n",
      "Epoch 279/1000, Loss: 16.1833, Val Loss: 16.6293\n",
      "Epoch 280/1000, Loss: 16.1954, Val Loss: 16.6282\n",
      "Epoch 281/1000, Loss: 16.1725, Val Loss: 16.6260\n",
      "Epoch 282/1000, Loss: 16.1547, Val Loss: 16.6250\n",
      "Epoch 283/1000, Loss: 16.1706, Val Loss: 16.6271\n",
      "Epoch 284/1000, Loss: 16.1666, Val Loss: 16.6254\n",
      "Epoch 285/1000, Loss: 16.2237, Val Loss: 16.6244\n",
      "Epoch 286/1000, Loss: 16.2190, Val Loss: 16.6244\n",
      "Epoch 287/1000, Loss: 16.2096, Val Loss: 16.6212\n",
      "Epoch 288/1000, Loss: 16.2249, Val Loss: 16.6170\n",
      "Epoch 289/1000, Loss: 16.1759, Val Loss: 16.6120\n",
      "Epoch 290/1000, Loss: 16.2161, Val Loss: 16.6090\n",
      "Epoch 291/1000, Loss: 16.1713, Val Loss: 16.6067\n",
      "Epoch 292/1000, Loss: 16.1992, Val Loss: 16.6054\n",
      "Epoch 293/1000, Loss: 16.1386, Val Loss: 16.6063\n",
      "Epoch 294/1000, Loss: 16.1927, Val Loss: 16.6077\n",
      "Epoch 295/1000, Loss: 16.2203, Val Loss: 16.6104\n",
      "Epoch 296/1000, Loss: 16.2019, Val Loss: 16.6133\n",
      "Epoch 297/1000, Loss: 16.2355, Val Loss: 16.6156\n",
      "Epoch 298/1000, Loss: 16.1672, Val Loss: 16.6180\n",
      "Epoch 299/1000, Loss: 16.2163, Val Loss: 16.6187\n",
      "Epoch 300/1000, Loss: 16.1843, Val Loss: 16.6187\n",
      "Epoch 301/1000, Loss: 16.2042, Val Loss: 16.6175\n",
      "Epoch 302/1000, Loss: 16.1634, Val Loss: 16.6192\n",
      "Epoch 303/1000, Loss: 16.1847, Val Loss: 16.6205\n",
      "Epoch 304/1000, Loss: 16.1724, Val Loss: 16.6201\n",
      "Epoch 305/1000, Loss: 16.1384, Val Loss: 16.6211\n",
      "Epoch 306/1000, Loss: 16.1815, Val Loss: 16.6209\n",
      "Epoch 307/1000, Loss: 16.1461, Val Loss: 16.6198\n",
      "Epoch 308/1000, Loss: 16.1652, Val Loss: 16.6177\n",
      "Epoch 309/1000, Loss: 16.2258, Val Loss: 16.6151\n",
      "Epoch 310/1000, Loss: 16.1689, Val Loss: 16.6138\n",
      "Epoch 311/1000, Loss: 16.1235, Val Loss: 16.6137\n",
      "Epoch 312/1000, Loss: 16.1749, Val Loss: 16.6150\n",
      "Epoch 313/1000, Loss: 16.1829, Val Loss: 16.6137\n",
      "Epoch 314/1000, Loss: 16.1605, Val Loss: 16.6116\n",
      "Epoch 315/1000, Loss: 16.2041, Val Loss: 16.6079\n",
      "Epoch 316/1000, Loss: 16.2283, Val Loss: 16.6028\n",
      "Epoch 317/1000, Loss: 16.1584, Val Loss: 16.5972\n",
      "Epoch 318/1000, Loss: 16.2090, Val Loss: 16.5943\n",
      "Epoch 319/1000, Loss: 16.1963, Val Loss: 16.5947\n",
      "Epoch 320/1000, Loss: 16.2076, Val Loss: 16.5942\n",
      "Epoch 321/1000, Loss: 16.1890, Val Loss: 16.5941\n",
      "Epoch 322/1000, Loss: 16.1534, Val Loss: 16.5940\n",
      "Epoch 323/1000, Loss: 16.1902, Val Loss: 16.5969\n",
      "Epoch 324/1000, Loss: 16.1710, Val Loss: 16.6023\n",
      "Epoch 325/1000, Loss: 16.1927, Val Loss: 16.6067\n",
      "Epoch 326/1000, Loss: 16.1597, Val Loss: 16.6111\n",
      "Epoch 327/1000, Loss: 16.1504, Val Loss: 16.6127\n",
      "Epoch 328/1000, Loss: 16.1712, Val Loss: 16.6124\n",
      "Epoch 329/1000, Loss: 16.1552, Val Loss: 16.6100\n",
      "Epoch 330/1000, Loss: 16.1742, Val Loss: 16.6077\n",
      "Epoch 331/1000, Loss: 16.1864, Val Loss: 16.6031\n",
      "Epoch 332/1000, Loss: 16.1830, Val Loss: 16.5986\n",
      "Epoch 333/1000, Loss: 16.1972, Val Loss: 16.5915\n",
      "Epoch 334/1000, Loss: 16.1924, Val Loss: 16.5892\n",
      "Epoch 335/1000, Loss: 16.2020, Val Loss: 16.5875\n",
      "Epoch 336/1000, Loss: 16.1511, Val Loss: 16.5864\n",
      "Epoch 337/1000, Loss: 16.1555, Val Loss: 16.5878\n",
      "Epoch 338/1000, Loss: 16.1621, Val Loss: 16.5908\n",
      "Epoch 339/1000, Loss: 16.1537, Val Loss: 16.5958\n",
      "Epoch 340/1000, Loss: 16.2167, Val Loss: 16.5998\n",
      "Epoch 341/1000, Loss: 16.1326, Val Loss: 16.6038\n",
      "Epoch 342/1000, Loss: 16.2100, Val Loss: 16.6054\n",
      "Epoch 343/1000, Loss: 16.1284, Val Loss: 16.6046\n",
      "Epoch 344/1000, Loss: 16.1490, Val Loss: 16.6029\n",
      "Epoch 345/1000, Loss: 16.1596, Val Loss: 16.5980\n",
      "Epoch 346/1000, Loss: 16.1163, Val Loss: 16.5968\n",
      "Epoch 347/1000, Loss: 16.2319, Val Loss: 16.5932\n",
      "Epoch 348/1000, Loss: 16.2056, Val Loss: 16.5865\n",
      "Epoch 349/1000, Loss: 16.1507, Val Loss: 16.5807\n",
      "Epoch 350/1000, Loss: 16.1596, Val Loss: 16.5761\n",
      "Epoch 351/1000, Loss: 16.1621, Val Loss: 16.5727\n",
      "Epoch 352/1000, Loss: 16.1449, Val Loss: 16.5708\n",
      "Epoch 353/1000, Loss: 16.1719, Val Loss: 16.5699\n",
      "Epoch 354/1000, Loss: 16.1515, Val Loss: 16.5703\n",
      "Epoch 355/1000, Loss: 16.1719, Val Loss: 16.5740\n",
      "Epoch 356/1000, Loss: 16.1738, Val Loss: 16.5802\n",
      "Epoch 357/1000, Loss: 16.1957, Val Loss: 16.5837\n",
      "Epoch 358/1000, Loss: 16.1705, Val Loss: 16.5859\n",
      "Epoch 359/1000, Loss: 16.1585, Val Loss: 16.5840\n",
      "Epoch 360/1000, Loss: 16.1692, Val Loss: 16.5819\n",
      "Epoch 361/1000, Loss: 16.1965, Val Loss: 16.5768\n",
      "Epoch 362/1000, Loss: 16.1421, Val Loss: 16.5735\n",
      "Epoch 363/1000, Loss: 16.1604, Val Loss: 16.5716\n",
      "Epoch 364/1000, Loss: 16.1508, Val Loss: 16.5692\n",
      "Epoch 365/1000, Loss: 16.1571, Val Loss: 16.5639\n",
      "Epoch 366/1000, Loss: 16.1330, Val Loss: 16.5614\n",
      "Epoch 367/1000, Loss: 16.1895, Val Loss: 16.5600\n",
      "Epoch 368/1000, Loss: 16.1149, Val Loss: 16.5593\n",
      "Epoch 369/1000, Loss: 16.1114, Val Loss: 16.5611\n",
      "Epoch 370/1000, Loss: 16.1199, Val Loss: 16.5626\n",
      "Epoch 371/1000, Loss: 16.0980, Val Loss: 16.5636\n",
      "Epoch 372/1000, Loss: 16.1393, Val Loss: 16.5646\n",
      "Epoch 373/1000, Loss: 16.1702, Val Loss: 16.5653\n",
      "Epoch 374/1000, Loss: 16.1606, Val Loss: 16.5676\n",
      "Epoch 375/1000, Loss: 16.1898, Val Loss: 16.5701\n",
      "Epoch 376/1000, Loss: 16.1870, Val Loss: 16.5706\n",
      "Epoch 377/1000, Loss: 16.1759, Val Loss: 16.5684\n",
      "Epoch 378/1000, Loss: 16.1177, Val Loss: 16.5655\n",
      "Epoch 379/1000, Loss: 16.1507, Val Loss: 16.5629\n",
      "Epoch 380/1000, Loss: 16.1342, Val Loss: 16.5626\n",
      "Epoch 381/1000, Loss: 16.1686, Val Loss: 16.5629\n",
      "Epoch 382/1000, Loss: 16.1693, Val Loss: 16.5642\n",
      "Epoch 383/1000, Loss: 16.1379, Val Loss: 16.5634\n",
      "Epoch 384/1000, Loss: 16.1276, Val Loss: 16.5605\n",
      "Epoch 385/1000, Loss: 16.1495, Val Loss: 16.5579\n",
      "Epoch 386/1000, Loss: 16.1081, Val Loss: 16.5567\n",
      "Epoch 387/1000, Loss: 16.1446, Val Loss: 16.5573\n",
      "Epoch 388/1000, Loss: 16.0967, Val Loss: 16.5601\n",
      "Epoch 389/1000, Loss: 16.1430, Val Loss: 16.5619\n",
      "Epoch 390/1000, Loss: 16.1454, Val Loss: 16.5630\n",
      "Epoch 391/1000, Loss: 16.1245, Val Loss: 16.5633\n",
      "Epoch 392/1000, Loss: 16.1148, Val Loss: 16.5610\n",
      "Epoch 393/1000, Loss: 16.1568, Val Loss: 16.5575\n",
      "Epoch 394/1000, Loss: 16.1496, Val Loss: 16.5535\n",
      "Epoch 395/1000, Loss: 16.1289, Val Loss: 16.5522\n",
      "Epoch 396/1000, Loss: 16.1366, Val Loss: 16.5511\n",
      "Epoch 397/1000, Loss: 16.1173, Val Loss: 16.5506\n",
      "Epoch 398/1000, Loss: 16.1441, Val Loss: 16.5509\n",
      "Epoch 399/1000, Loss: 16.1176, Val Loss: 16.5532\n",
      "Epoch 400/1000, Loss: 16.1215, Val Loss: 16.5561\n",
      "Epoch 401/1000, Loss: 16.1480, Val Loss: 16.5601\n",
      "Epoch 402/1000, Loss: 16.1090, Val Loss: 16.5642\n",
      "Epoch 403/1000, Loss: 16.1330, Val Loss: 16.5650\n",
      "Epoch 404/1000, Loss: 16.1004, Val Loss: 16.5655\n",
      "Epoch 405/1000, Loss: 16.1238, Val Loss: 16.5632\n",
      "Epoch 406/1000, Loss: 16.0946, Val Loss: 16.5648\n",
      "Epoch 407/1000, Loss: 16.1288, Val Loss: 16.5639\n",
      "Epoch 408/1000, Loss: 16.0884, Val Loss: 16.5624\n",
      "Epoch 409/1000, Loss: 16.1277, Val Loss: 16.5589\n",
      "Epoch 410/1000, Loss: 16.1690, Val Loss: 16.5543\n",
      "Epoch 411/1000, Loss: 16.1542, Val Loss: 16.5486\n",
      "Epoch 412/1000, Loss: 16.1528, Val Loss: 16.5428\n",
      "Epoch 413/1000, Loss: 16.1356, Val Loss: 16.5391\n",
      "Epoch 414/1000, Loss: 16.1368, Val Loss: 16.5374\n",
      "Epoch 415/1000, Loss: 16.0964, Val Loss: 16.5372\n",
      "Epoch 416/1000, Loss: 16.1277, Val Loss: 16.5371\n",
      "Epoch 417/1000, Loss: 16.1230, Val Loss: 16.5401\n",
      "Epoch 418/1000, Loss: 16.0921, Val Loss: 16.5438\n",
      "Epoch 419/1000, Loss: 16.1491, Val Loss: 16.5487\n",
      "Epoch 420/1000, Loss: 16.1524, Val Loss: 16.5533\n",
      "Epoch 421/1000, Loss: 16.1125, Val Loss: 16.5538\n",
      "Epoch 422/1000, Loss: 16.1117, Val Loss: 16.5531\n",
      "Epoch 423/1000, Loss: 16.1410, Val Loss: 16.5506\n",
      "Epoch 424/1000, Loss: 16.1454, Val Loss: 16.5469\n",
      "Epoch 425/1000, Loss: 16.1287, Val Loss: 16.5404\n",
      "Epoch 426/1000, Loss: 16.0810, Val Loss: 16.5367\n",
      "Epoch 427/1000, Loss: 16.1472, Val Loss: 16.5332\n",
      "Epoch 428/1000, Loss: 16.1698, Val Loss: 16.5313\n",
      "Epoch 429/1000, Loss: 16.1391, Val Loss: 16.5315\n",
      "Epoch 430/1000, Loss: 16.1288, Val Loss: 16.5336\n",
      "Epoch 431/1000, Loss: 16.0810, Val Loss: 16.5377\n",
      "Epoch 432/1000, Loss: 16.1276, Val Loss: 16.5382\n",
      "Epoch 433/1000, Loss: 16.1230, Val Loss: 16.5440\n",
      "Epoch 434/1000, Loss: 16.0882, Val Loss: 16.5510\n",
      "Epoch 435/1000, Loss: 16.1331, Val Loss: 16.5533\n",
      "Epoch 436/1000, Loss: 16.1329, Val Loss: 16.5544\n",
      "Epoch 437/1000, Loss: 16.1444, Val Loss: 16.5504\n",
      "Epoch 438/1000, Loss: 16.0653, Val Loss: 16.5450\n",
      "Epoch 439/1000, Loss: 16.1179, Val Loss: 16.5411\n",
      "Epoch 440/1000, Loss: 16.1248, Val Loss: 16.5363\n",
      "Epoch 441/1000, Loss: 16.1000, Val Loss: 16.5310\n",
      "Epoch 442/1000, Loss: 16.0769, Val Loss: 16.5277\n",
      "Epoch 443/1000, Loss: 16.1242, Val Loss: 16.5238\n",
      "Epoch 444/1000, Loss: 16.1564, Val Loss: 16.5282\n",
      "Epoch 445/1000, Loss: 16.1204, Val Loss: 16.5301\n",
      "Epoch 446/1000, Loss: 16.1398, Val Loss: 16.5313\n",
      "Epoch 447/1000, Loss: 16.1055, Val Loss: 16.5300\n",
      "Epoch 448/1000, Loss: 16.1201, Val Loss: 16.5288\n",
      "Epoch 449/1000, Loss: 16.0719, Val Loss: 16.5269\n",
      "Epoch 450/1000, Loss: 16.1381, Val Loss: 16.5279\n",
      "Epoch 451/1000, Loss: 16.1406, Val Loss: 16.5281\n",
      "Epoch 452/1000, Loss: 16.1052, Val Loss: 16.5267\n",
      "Epoch 453/1000, Loss: 16.0825, Val Loss: 16.5266\n",
      "Epoch 454/1000, Loss: 16.1532, Val Loss: 16.5229\n",
      "Epoch 455/1000, Loss: 16.0802, Val Loss: 16.5207\n",
      "Epoch 456/1000, Loss: 16.1209, Val Loss: 16.5191\n",
      "Epoch 457/1000, Loss: 16.0990, Val Loss: 16.5184\n",
      "Epoch 458/1000, Loss: 16.1377, Val Loss: 16.5185\n",
      "Epoch 459/1000, Loss: 16.1512, Val Loss: 16.5202\n",
      "Epoch 460/1000, Loss: 16.1151, Val Loss: 16.5243\n",
      "Epoch 461/1000, Loss: 16.0570, Val Loss: 16.5276\n",
      "Epoch 462/1000, Loss: 16.0876, Val Loss: 16.5301\n",
      "Epoch 463/1000, Loss: 16.1006, Val Loss: 16.5301\n",
      "Epoch 464/1000, Loss: 16.1275, Val Loss: 16.5325\n",
      "Epoch 465/1000, Loss: 16.1455, Val Loss: 16.5323\n",
      "Epoch 466/1000, Loss: 16.1306, Val Loss: 16.5302\n",
      "Epoch 467/1000, Loss: 16.1165, Val Loss: 16.5280\n",
      "Epoch 468/1000, Loss: 16.1192, Val Loss: 16.5256\n",
      "Epoch 469/1000, Loss: 16.0920, Val Loss: 16.5216\n",
      "Epoch 470/1000, Loss: 16.0574, Val Loss: 16.5158\n",
      "Epoch 471/1000, Loss: 16.0999, Val Loss: 16.5127\n",
      "Epoch 472/1000, Loss: 16.1317, Val Loss: 16.5117\n",
      "Epoch 473/1000, Loss: 16.0926, Val Loss: 16.5123\n",
      "Epoch 474/1000, Loss: 16.0653, Val Loss: 16.5167\n",
      "Epoch 475/1000, Loss: 16.1273, Val Loss: 16.5221\n",
      "Epoch 476/1000, Loss: 16.1293, Val Loss: 16.5260\n",
      "Epoch 477/1000, Loss: 16.1112, Val Loss: 16.5291\n",
      "Epoch 478/1000, Loss: 16.1007, Val Loss: 16.5307\n",
      "Epoch 479/1000, Loss: 16.0978, Val Loss: 16.5307\n",
      "Epoch 480/1000, Loss: 16.1209, Val Loss: 16.5281\n",
      "Epoch 481/1000, Loss: 16.0817, Val Loss: 16.5252\n",
      "Epoch 482/1000, Loss: 16.1092, Val Loss: 16.5193\n",
      "Epoch 483/1000, Loss: 16.0906, Val Loss: 16.5129\n",
      "Epoch 484/1000, Loss: 16.1158, Val Loss: 16.5087\n",
      "Epoch 485/1000, Loss: 16.1232, Val Loss: 16.5089\n",
      "Epoch 486/1000, Loss: 16.1205, Val Loss: 16.5098\n",
      "Epoch 487/1000, Loss: 16.0842, Val Loss: 16.5114\n",
      "Epoch 488/1000, Loss: 16.0712, Val Loss: 16.5123\n",
      "Epoch 489/1000, Loss: 16.1168, Val Loss: 16.5106\n",
      "Epoch 490/1000, Loss: 16.1172, Val Loss: 16.5112\n",
      "Epoch 491/1000, Loss: 16.1104, Val Loss: 16.5120\n",
      "Epoch 492/1000, Loss: 16.1156, Val Loss: 16.5107\n",
      "Epoch 493/1000, Loss: 16.0617, Val Loss: 16.5094\n",
      "Epoch 494/1000, Loss: 16.1020, Val Loss: 16.5080\n",
      "Epoch 495/1000, Loss: 16.1323, Val Loss: 16.5085\n",
      "Epoch 496/1000, Loss: 16.1243, Val Loss: 16.5110\n",
      "Epoch 497/1000, Loss: 16.0386, Val Loss: 16.5136\n",
      "Epoch 498/1000, Loss: 16.1446, Val Loss: 16.5133\n",
      "Epoch 499/1000, Loss: 16.0874, Val Loss: 16.5117\n",
      "Epoch 500/1000, Loss: 16.0922, Val Loss: 16.5110\n",
      "Epoch 501/1000, Loss: 16.1029, Val Loss: 16.5101\n",
      "Epoch 502/1000, Loss: 16.0931, Val Loss: 16.5086\n",
      "Epoch 503/1000, Loss: 16.0708, Val Loss: 16.5061\n",
      "Epoch 504/1000, Loss: 16.1090, Val Loss: 16.5095\n",
      "Epoch 505/1000, Loss: 16.1116, Val Loss: 16.5120\n",
      "Epoch 506/1000, Loss: 16.1222, Val Loss: 16.5128\n",
      "Epoch 507/1000, Loss: 16.0801, Val Loss: 16.5113\n",
      "Epoch 508/1000, Loss: 16.0948, Val Loss: 16.5107\n",
      "Epoch 509/1000, Loss: 16.1219, Val Loss: 16.5079\n",
      "Epoch 510/1000, Loss: 16.0709, Val Loss: 16.5083\n",
      "Epoch 511/1000, Loss: 16.0277, Val Loss: 16.5082\n",
      "Epoch 512/1000, Loss: 16.0716, Val Loss: 16.5074\n",
      "Epoch 513/1000, Loss: 16.0958, Val Loss: 16.5067\n",
      "Epoch 514/1000, Loss: 16.0551, Val Loss: 16.5075\n",
      "Epoch 515/1000, Loss: 16.0464, Val Loss: 16.5079\n",
      "Epoch 516/1000, Loss: 16.0461, Val Loss: 16.5096\n",
      "Epoch 517/1000, Loss: 16.0831, Val Loss: 16.5112\n",
      "Epoch 518/1000, Loss: 16.1027, Val Loss: 16.5144\n",
      "Epoch 519/1000, Loss: 16.0549, Val Loss: 16.5160\n",
      "Epoch 520/1000, Loss: 16.0943, Val Loss: 16.5142\n",
      "Epoch 521/1000, Loss: 16.0644, Val Loss: 16.5093\n",
      "Epoch 522/1000, Loss: 16.0905, Val Loss: 16.5028\n",
      "Epoch 523/1000, Loss: 16.1421, Val Loss: 16.4988\n",
      "Epoch 524/1000, Loss: 16.0813, Val Loss: 16.4964\n",
      "Epoch 525/1000, Loss: 16.1235, Val Loss: 16.4971\n",
      "Epoch 526/1000, Loss: 16.0475, Val Loss: 16.4961\n",
      "Epoch 527/1000, Loss: 16.1033, Val Loss: 16.4941\n",
      "Epoch 528/1000, Loss: 16.0834, Val Loss: 16.4920\n",
      "Epoch 529/1000, Loss: 16.1239, Val Loss: 16.4932\n",
      "Epoch 530/1000, Loss: 16.0730, Val Loss: 16.4950\n",
      "Epoch 531/1000, Loss: 16.0543, Val Loss: 16.4956\n",
      "Epoch 532/1000, Loss: 16.0469, Val Loss: 16.4962\n",
      "Epoch 533/1000, Loss: 16.0897, Val Loss: 16.5020\n",
      "Epoch 534/1000, Loss: 16.1089, Val Loss: 16.5059\n",
      "Epoch 535/1000, Loss: 16.0833, Val Loss: 16.5062\n",
      "Epoch 536/1000, Loss: 16.0880, Val Loss: 16.5097\n",
      "Epoch 537/1000, Loss: 16.0030, Val Loss: 16.5111\n",
      "Epoch 538/1000, Loss: 16.0697, Val Loss: 16.5108\n",
      "Epoch 539/1000, Loss: 16.0543, Val Loss: 16.5107\n",
      "Epoch 540/1000, Loss: 16.0670, Val Loss: 16.5105\n",
      "Epoch 541/1000, Loss: 16.0864, Val Loss: 16.5046\n",
      "Epoch 542/1000, Loss: 16.0612, Val Loss: 16.4980\n",
      "Epoch 543/1000, Loss: 16.1490, Val Loss: 16.4914\n",
      "Epoch 544/1000, Loss: 16.0869, Val Loss: 16.4861\n",
      "Epoch 545/1000, Loss: 16.0742, Val Loss: 16.4861\n",
      "Epoch 546/1000, Loss: 16.0820, Val Loss: 16.4888\n",
      "Epoch 547/1000, Loss: 16.0919, Val Loss: 16.4905\n",
      "Epoch 548/1000, Loss: 16.0186, Val Loss: 16.4887\n",
      "Epoch 549/1000, Loss: 16.1049, Val Loss: 16.4912\n",
      "Epoch 550/1000, Loss: 16.0611, Val Loss: 16.4911\n",
      "Epoch 551/1000, Loss: 16.0872, Val Loss: 16.4901\n",
      "Epoch 552/1000, Loss: 16.0744, Val Loss: 16.4891\n",
      "Epoch 553/1000, Loss: 16.0837, Val Loss: 16.4888\n",
      "Epoch 554/1000, Loss: 16.0861, Val Loss: 16.4862\n",
      "Epoch 555/1000, Loss: 16.0628, Val Loss: 16.4840\n",
      "Epoch 556/1000, Loss: 16.1319, Val Loss: 16.4825\n",
      "Epoch 557/1000, Loss: 16.0508, Val Loss: 16.4836\n",
      "Epoch 558/1000, Loss: 16.0889, Val Loss: 16.4862\n",
      "Epoch 559/1000, Loss: 16.0978, Val Loss: 16.4881\n",
      "Epoch 560/1000, Loss: 16.1003, Val Loss: 16.4914\n",
      "Epoch 561/1000, Loss: 16.0940, Val Loss: 16.4952\n",
      "Epoch 562/1000, Loss: 16.0390, Val Loss: 16.4985\n",
      "Epoch 563/1000, Loss: 16.0691, Val Loss: 16.4953\n",
      "Epoch 564/1000, Loss: 16.0463, Val Loss: 16.4992\n",
      "Epoch 565/1000, Loss: 16.0655, Val Loss: 16.5040\n",
      "Epoch 566/1000, Loss: 16.0881, Val Loss: 16.5062\n",
      "Epoch 567/1000, Loss: 16.0761, Val Loss: 16.5070\n",
      "Epoch 568/1000, Loss: 16.0649, Val Loss: 16.5115\n",
      "Epoch 569/1000, Loss: 16.0702, Val Loss: 16.5158\n",
      "Epoch 570/1000, Loss: 16.1004, Val Loss: 16.5138\n",
      "Epoch 571/1000, Loss: 16.0268, Val Loss: 16.5072\n",
      "Epoch 572/1000, Loss: 16.0618, Val Loss: 16.5047\n",
      "Epoch 573/1000, Loss: 16.0605, Val Loss: 16.5021\n",
      "Epoch 574/1000, Loss: 16.1035, Val Loss: 16.4964\n",
      "Epoch 575/1000, Loss: 16.0973, Val Loss: 16.4937\n",
      "Epoch 576/1000, Loss: 16.0422, Val Loss: 16.4923\n",
      "Epoch 577/1000, Loss: 16.0506, Val Loss: 16.4952\n",
      "Epoch 578/1000, Loss: 16.0968, Val Loss: 16.4969\n",
      "Epoch 579/1000, Loss: 16.0506, Val Loss: 16.5016\n",
      "Epoch 580/1000, Loss: 16.0984, Val Loss: 16.5030\n",
      "Epoch 581/1000, Loss: 16.1072, Val Loss: 16.5040\n",
      "Epoch 582/1000, Loss: 16.0546, Val Loss: 16.5015\n",
      "Epoch 583/1000, Loss: 16.0033, Val Loss: 16.4955\n",
      "Epoch 584/1000, Loss: 16.0727, Val Loss: 16.4932\n",
      "Epoch 585/1000, Loss: 16.1080, Val Loss: 16.4896\n",
      "Epoch 586/1000, Loss: 16.0962, Val Loss: 16.4830\n",
      "Epoch 587/1000, Loss: 16.0876, Val Loss: 16.4779\n",
      "Epoch 588/1000, Loss: 16.0904, Val Loss: 16.4767\n",
      "Epoch 589/1000, Loss: 16.0763, Val Loss: 16.4796\n",
      "Epoch 590/1000, Loss: 16.0359, Val Loss: 16.4819\n",
      "Epoch 591/1000, Loss: 16.0994, Val Loss: 16.4850\n",
      "Epoch 592/1000, Loss: 16.0410, Val Loss: 16.4919\n",
      "Epoch 593/1000, Loss: 16.0402, Val Loss: 16.4965\n",
      "Epoch 594/1000, Loss: 16.0909, Val Loss: 16.5006\n",
      "Epoch 595/1000, Loss: 16.0681, Val Loss: 16.5080\n",
      "Epoch 596/1000, Loss: 16.1238, Val Loss: 16.5178\n",
      "Epoch 597/1000, Loss: 16.0653, Val Loss: 16.5231\n",
      "Epoch 598/1000, Loss: 16.0329, Val Loss: 16.5245\n",
      "Epoch 599/1000, Loss: 16.0518, Val Loss: 16.5252\n",
      "Epoch 600/1000, Loss: 16.0391, Val Loss: 16.5244\n",
      "Epoch 601/1000, Loss: 16.0372, Val Loss: 16.5215\n",
      "Epoch 602/1000, Loss: 16.0295, Val Loss: 16.5148\n",
      "Epoch 603/1000, Loss: 16.0661, Val Loss: 16.5088\n",
      "Epoch 604/1000, Loss: 16.0665, Val Loss: 16.5049\n",
      "Epoch 605/1000, Loss: 16.1023, Val Loss: 16.5026\n",
      "Epoch 606/1000, Loss: 16.0547, Val Loss: 16.4993\n",
      "Epoch 607/1000, Loss: 16.0846, Val Loss: 16.4968\n",
      "Epoch 608/1000, Loss: 16.0704, Val Loss: 16.4990\n",
      "Epoch 609/1000, Loss: 16.0327, Val Loss: 16.5017\n",
      "Epoch 610/1000, Loss: 16.1047, Val Loss: 16.5035\n",
      "Epoch 611/1000, Loss: 16.0823, Val Loss: 16.5069\n",
      "Epoch 612/1000, Loss: 16.1125, Val Loss: 16.5053\n",
      "Epoch 613/1000, Loss: 16.0909, Val Loss: 16.5045\n",
      "Epoch 614/1000, Loss: 16.0408, Val Loss: 16.4996\n",
      "Epoch 615/1000, Loss: 16.0662, Val Loss: 16.4941\n",
      "Epoch 616/1000, Loss: 16.0759, Val Loss: 16.4909\n",
      "Epoch 617/1000, Loss: 16.0633, Val Loss: 16.4887\n",
      "Epoch 618/1000, Loss: 16.0723, Val Loss: 16.4835\n",
      "Epoch 619/1000, Loss: 16.0924, Val Loss: 16.4864\n",
      "Epoch 620/1000, Loss: 16.0500, Val Loss: 16.4903\n",
      "Epoch 621/1000, Loss: 16.0885, Val Loss: 16.4935\n",
      "Epoch 622/1000, Loss: 16.0677, Val Loss: 16.4951\n",
      "Epoch 623/1000, Loss: 16.0877, Val Loss: 16.4980\n",
      "Epoch 624/1000, Loss: 16.0454, Val Loss: 16.4900\n",
      "Epoch 625/1000, Loss: 16.0357, Val Loss: 16.4848\n",
      "Epoch 626/1000, Loss: 16.0857, Val Loss: 16.4822\n",
      "Epoch 627/1000, Loss: 16.0907, Val Loss: 16.4806\n",
      "Epoch 628/1000, Loss: 16.1001, Val Loss: 16.4823\n",
      "Early stopping triggered at epoch 628. Best Val Loss: 16.4767\n",
      "Epoch 1/1000, Loss: 16.8144, Val Loss: 17.0254\n",
      "Epoch 2/1000, Loss: 16.7913, Val Loss: 17.0231\n",
      "Epoch 3/1000, Loss: 16.7763, Val Loss: 17.0206\n",
      "Epoch 4/1000, Loss: 16.7665, Val Loss: 17.0178\n",
      "Epoch 5/1000, Loss: 16.7623, Val Loss: 17.0145\n",
      "Epoch 6/1000, Loss: 16.7613, Val Loss: 17.0115\n",
      "Epoch 7/1000, Loss: 16.7562, Val Loss: 17.0081\n",
      "Epoch 8/1000, Loss: 16.7439, Val Loss: 17.0038\n",
      "Epoch 9/1000, Loss: 16.7378, Val Loss: 16.9993\n",
      "Epoch 10/1000, Loss: 16.7323, Val Loss: 16.9943\n",
      "Epoch 11/1000, Loss: 16.7177, Val Loss: 16.9885\n",
      "Epoch 12/1000, Loss: 16.7102, Val Loss: 16.9825\n",
      "Epoch 13/1000, Loss: 16.7047, Val Loss: 16.9766\n",
      "Epoch 14/1000, Loss: 16.6945, Val Loss: 16.9697\n",
      "Epoch 15/1000, Loss: 16.6781, Val Loss: 16.9625\n",
      "Epoch 16/1000, Loss: 16.6691, Val Loss: 16.9555\n",
      "Epoch 17/1000, Loss: 16.6551, Val Loss: 16.9472\n",
      "Epoch 18/1000, Loss: 16.6442, Val Loss: 16.9385\n",
      "Epoch 19/1000, Loss: 16.6323, Val Loss: 16.9295\n",
      "Epoch 20/1000, Loss: 16.6298, Val Loss: 16.9197\n",
      "Epoch 21/1000, Loss: 16.6176, Val Loss: 16.9084\n",
      "Epoch 22/1000, Loss: 16.5952, Val Loss: 16.8962\n",
      "Epoch 23/1000, Loss: 16.5883, Val Loss: 16.8843\n",
      "Epoch 24/1000, Loss: 16.5739, Val Loss: 16.8721\n",
      "Epoch 25/1000, Loss: 16.5608, Val Loss: 16.8601\n",
      "Epoch 26/1000, Loss: 16.5483, Val Loss: 16.8473\n",
      "Epoch 27/1000, Loss: 16.5379, Val Loss: 16.8340\n",
      "Epoch 28/1000, Loss: 16.5140, Val Loss: 16.8214\n",
      "Epoch 29/1000, Loss: 16.5092, Val Loss: 16.8119\n",
      "Epoch 30/1000, Loss: 16.4948, Val Loss: 16.8036\n",
      "Epoch 31/1000, Loss: 16.4702, Val Loss: 16.7938\n",
      "Epoch 32/1000, Loss: 16.4633, Val Loss: 16.7833\n",
      "Epoch 33/1000, Loss: 16.4543, Val Loss: 16.7719\n",
      "Epoch 34/1000, Loss: 16.4241, Val Loss: 16.7603\n",
      "Epoch 35/1000, Loss: 16.4094, Val Loss: 16.7475\n",
      "Epoch 36/1000, Loss: 16.3890, Val Loss: 16.7315\n",
      "Epoch 37/1000, Loss: 16.3762, Val Loss: 16.7140\n",
      "Epoch 38/1000, Loss: 16.3709, Val Loss: 16.6942\n",
      "Epoch 39/1000, Loss: 16.3288, Val Loss: 16.6734\n",
      "Epoch 40/1000, Loss: 16.3210, Val Loss: 16.6500\n",
      "Epoch 41/1000, Loss: 16.3072, Val Loss: 16.6277\n",
      "Epoch 42/1000, Loss: 16.2727, Val Loss: 16.6063\n",
      "Epoch 43/1000, Loss: 16.2502, Val Loss: 16.5828\n",
      "Epoch 44/1000, Loss: 16.2515, Val Loss: 16.5605\n",
      "Epoch 45/1000, Loss: 16.2394, Val Loss: 16.5339\n",
      "Epoch 46/1000, Loss: 16.2089, Val Loss: 16.5111\n",
      "Epoch 47/1000, Loss: 16.1837, Val Loss: 16.4846\n",
      "Epoch 48/1000, Loss: 16.1784, Val Loss: 16.4592\n",
      "Epoch 49/1000, Loss: 16.1348, Val Loss: 16.4327\n",
      "Epoch 50/1000, Loss: 16.1692, Val Loss: 16.4149\n",
      "Epoch 51/1000, Loss: 16.1265, Val Loss: 16.3959\n",
      "Epoch 52/1000, Loss: 16.1240, Val Loss: 16.3765\n",
      "Epoch 53/1000, Loss: 16.0872, Val Loss: 16.3602\n",
      "Epoch 54/1000, Loss: 16.0568, Val Loss: 16.3461\n",
      "Epoch 55/1000, Loss: 16.0785, Val Loss: 16.3343\n",
      "Epoch 56/1000, Loss: 16.0402, Val Loss: 16.3252\n",
      "Epoch 57/1000, Loss: 15.9998, Val Loss: 16.3199\n",
      "Epoch 58/1000, Loss: 16.0226, Val Loss: 16.3210\n",
      "Epoch 59/1000, Loss: 16.0291, Val Loss: 16.3227\n",
      "Epoch 60/1000, Loss: 15.9598, Val Loss: 16.3281\n",
      "Epoch 61/1000, Loss: 16.0160, Val Loss: 16.3284\n",
      "Epoch 62/1000, Loss: 15.9334, Val Loss: 16.3276\n",
      "Epoch 63/1000, Loss: 15.9648, Val Loss: 16.3192\n",
      "Epoch 64/1000, Loss: 15.8784, Val Loss: 16.3189\n",
      "Epoch 65/1000, Loss: 15.9156, Val Loss: 16.3219\n",
      "Epoch 66/1000, Loss: 15.8937, Val Loss: 16.3191\n",
      "Epoch 67/1000, Loss: 15.9857, Val Loss: 16.3220\n",
      "Epoch 68/1000, Loss: 15.9277, Val Loss: 16.3276\n",
      "Epoch 69/1000, Loss: 15.8884, Val Loss: 16.3302\n",
      "Epoch 70/1000, Loss: 15.9297, Val Loss: 16.3330\n",
      "Epoch 71/1000, Loss: 15.9551, Val Loss: 16.3380\n",
      "Epoch 72/1000, Loss: 15.9232, Val Loss: 16.3388\n",
      "Epoch 73/1000, Loss: 15.9150, Val Loss: 16.3344\n",
      "Epoch 74/1000, Loss: 15.9242, Val Loss: 16.3255\n",
      "Epoch 75/1000, Loss: 15.9093, Val Loss: 16.3198\n",
      "Epoch 76/1000, Loss: 15.9332, Val Loss: 16.3116\n",
      "Epoch 77/1000, Loss: 15.8873, Val Loss: 16.3047\n",
      "Epoch 78/1000, Loss: 15.8860, Val Loss: 16.3048\n",
      "Epoch 79/1000, Loss: 15.8673, Val Loss: 16.3095\n",
      "Epoch 80/1000, Loss: 15.9403, Val Loss: 16.3073\n",
      "Epoch 81/1000, Loss: 15.8949, Val Loss: 16.3031\n",
      "Epoch 82/1000, Loss: 15.8498, Val Loss: 16.2937\n",
      "Epoch 83/1000, Loss: 15.9190, Val Loss: 16.2847\n",
      "Epoch 84/1000, Loss: 15.8524, Val Loss: 16.2779\n",
      "Epoch 85/1000, Loss: 15.8922, Val Loss: 16.2708\n",
      "Epoch 86/1000, Loss: 15.9355, Val Loss: 16.2688\n",
      "Epoch 87/1000, Loss: 15.8301, Val Loss: 16.2655\n",
      "Epoch 88/1000, Loss: 15.8546, Val Loss: 16.2632\n",
      "Epoch 89/1000, Loss: 15.8268, Val Loss: 16.2566\n",
      "Epoch 90/1000, Loss: 15.9350, Val Loss: 16.2487\n",
      "Epoch 91/1000, Loss: 15.8996, Val Loss: 16.2405\n",
      "Epoch 92/1000, Loss: 15.8375, Val Loss: 16.2348\n",
      "Epoch 93/1000, Loss: 15.8796, Val Loss: 16.2331\n",
      "Epoch 94/1000, Loss: 15.8588, Val Loss: 16.2297\n",
      "Epoch 95/1000, Loss: 15.8367, Val Loss: 16.2261\n",
      "Epoch 96/1000, Loss: 15.8042, Val Loss: 16.2212\n",
      "Epoch 97/1000, Loss: 15.8989, Val Loss: 16.2155\n",
      "Epoch 98/1000, Loss: 15.9266, Val Loss: 16.2111\n",
      "Epoch 99/1000, Loss: 15.8978, Val Loss: 16.2079\n",
      "Epoch 100/1000, Loss: 15.8463, Val Loss: 16.2083\n",
      "Epoch 101/1000, Loss: 15.8220, Val Loss: 16.2106\n",
      "Epoch 102/1000, Loss: 15.8491, Val Loss: 16.2091\n",
      "Epoch 103/1000, Loss: 15.7995, Val Loss: 16.2138\n",
      "Epoch 104/1000, Loss: 15.9151, Val Loss: 16.2162\n",
      "Epoch 105/1000, Loss: 15.8938, Val Loss: 16.2202\n",
      "Epoch 106/1000, Loss: 15.8954, Val Loss: 16.2214\n",
      "Epoch 107/1000, Loss: 15.7889, Val Loss: 16.2189\n",
      "Epoch 108/1000, Loss: 15.9069, Val Loss: 16.2153\n",
      "Epoch 109/1000, Loss: 15.7884, Val Loss: 16.2135\n",
      "Epoch 110/1000, Loss: 15.8184, Val Loss: 16.2086\n",
      "Epoch 111/1000, Loss: 15.9093, Val Loss: 16.2076\n",
      "Epoch 112/1000, Loss: 15.8208, Val Loss: 16.2081\n",
      "Epoch 113/1000, Loss: 15.8475, Val Loss: 16.2056\n",
      "Epoch 114/1000, Loss: 15.8749, Val Loss: 16.2066\n",
      "Epoch 115/1000, Loss: 15.8040, Val Loss: 16.2041\n",
      "Epoch 116/1000, Loss: 15.7713, Val Loss: 16.2031\n",
      "Epoch 117/1000, Loss: 15.8245, Val Loss: 16.2035\n",
      "Epoch 118/1000, Loss: 15.8105, Val Loss: 16.2011\n",
      "Epoch 119/1000, Loss: 15.8085, Val Loss: 16.1954\n",
      "Epoch 120/1000, Loss: 15.7693, Val Loss: 16.1903\n",
      "Epoch 121/1000, Loss: 15.7881, Val Loss: 16.1838\n",
      "Epoch 122/1000, Loss: 15.7829, Val Loss: 16.1764\n",
      "Epoch 123/1000, Loss: 15.8461, Val Loss: 16.1749\n",
      "Epoch 124/1000, Loss: 15.8178, Val Loss: 16.1761\n",
      "Epoch 125/1000, Loss: 15.8874, Val Loss: 16.1788\n",
      "Epoch 126/1000, Loss: 15.8559, Val Loss: 16.1847\n",
      "Epoch 127/1000, Loss: 15.8567, Val Loss: 16.1922\n",
      "Epoch 128/1000, Loss: 15.8454, Val Loss: 16.2019\n",
      "Epoch 129/1000, Loss: 15.8481, Val Loss: 16.2040\n",
      "Epoch 130/1000, Loss: 15.8453, Val Loss: 16.2044\n",
      "Epoch 131/1000, Loss: 15.8184, Val Loss: 16.2044\n",
      "Epoch 132/1000, Loss: 15.8153, Val Loss: 16.2013\n",
      "Epoch 133/1000, Loss: 15.8473, Val Loss: 16.1909\n",
      "Epoch 134/1000, Loss: 15.8097, Val Loss: 16.1888\n",
      "Epoch 135/1000, Loss: 15.7954, Val Loss: 16.1848\n",
      "Epoch 136/1000, Loss: 15.7795, Val Loss: 16.1848\n",
      "Epoch 137/1000, Loss: 15.7642, Val Loss: 16.1932\n",
      "Epoch 138/1000, Loss: 15.7445, Val Loss: 16.2036\n",
      "Epoch 139/1000, Loss: 15.7593, Val Loss: 16.2099\n",
      "Epoch 140/1000, Loss: 15.8027, Val Loss: 16.2114\n",
      "Epoch 141/1000, Loss: 15.7510, Val Loss: 16.2093\n",
      "Epoch 142/1000, Loss: 15.8108, Val Loss: 16.1984\n",
      "Epoch 143/1000, Loss: 15.8018, Val Loss: 16.1925\n",
      "Epoch 144/1000, Loss: 15.8362, Val Loss: 16.1906\n",
      "Epoch 145/1000, Loss: 15.8679, Val Loss: 16.1857\n",
      "Epoch 146/1000, Loss: 15.7812, Val Loss: 16.1875\n",
      "Epoch 147/1000, Loss: 15.7876, Val Loss: 16.1926\n",
      "Epoch 148/1000, Loss: 15.7940, Val Loss: 16.1920\n",
      "Epoch 149/1000, Loss: 15.7613, Val Loss: 16.1943\n",
      "Epoch 150/1000, Loss: 15.8034, Val Loss: 16.1982\n",
      "Epoch 151/1000, Loss: 15.7232, Val Loss: 16.2091\n",
      "Epoch 152/1000, Loss: 15.7854, Val Loss: 16.2200\n",
      "Epoch 153/1000, Loss: 15.7878, Val Loss: 16.2325\n",
      "Epoch 154/1000, Loss: 15.8235, Val Loss: 16.2396\n",
      "Epoch 155/1000, Loss: 15.7835, Val Loss: 16.2366\n",
      "Epoch 156/1000, Loss: 15.7890, Val Loss: 16.2295\n",
      "Epoch 157/1000, Loss: 15.7669, Val Loss: 16.2260\n",
      "Epoch 158/1000, Loss: 15.8062, Val Loss: 16.2236\n",
      "Epoch 159/1000, Loss: 15.7685, Val Loss: 16.2150\n",
      "Epoch 160/1000, Loss: 15.8153, Val Loss: 16.2185\n",
      "Epoch 161/1000, Loss: 15.7864, Val Loss: 16.2206\n",
      "Epoch 162/1000, Loss: 15.7485, Val Loss: 16.2216\n",
      "Epoch 163/1000, Loss: 15.8017, Val Loss: 16.2188\n",
      "Early stopping triggered at epoch 163. Best Val Loss: 16.1749\n",
      "Epoch 1/1000, Loss: 16.7515, Val Loss: 16.7844\n",
      "Epoch 2/1000, Loss: 16.7116, Val Loss: 16.7852\n",
      "Epoch 3/1000, Loss: 16.6972, Val Loss: 16.7858\n",
      "Epoch 4/1000, Loss: 16.6915, Val Loss: 16.7863\n",
      "Epoch 5/1000, Loss: 16.6548, Val Loss: 16.7863\n",
      "Epoch 6/1000, Loss: 16.6594, Val Loss: 16.7863\n",
      "Epoch 7/1000, Loss: 16.6471, Val Loss: 16.7865\n",
      "Epoch 8/1000, Loss: 16.6207, Val Loss: 16.7866\n",
      "Epoch 9/1000, Loss: 16.6186, Val Loss: 16.7873\n",
      "Epoch 10/1000, Loss: 16.6207, Val Loss: 16.7882\n",
      "Epoch 11/1000, Loss: 16.5905, Val Loss: 16.7880\n",
      "Epoch 12/1000, Loss: 16.5616, Val Loss: 16.7870\n",
      "Epoch 13/1000, Loss: 16.5556, Val Loss: 16.7856\n",
      "Epoch 14/1000, Loss: 16.5405, Val Loss: 16.7839\n",
      "Epoch 15/1000, Loss: 16.5340, Val Loss: 16.7811\n",
      "Epoch 16/1000, Loss: 16.5175, Val Loss: 16.7775\n",
      "Epoch 17/1000, Loss: 16.5060, Val Loss: 16.7736\n",
      "Epoch 18/1000, Loss: 16.4703, Val Loss: 16.7688\n",
      "Epoch 19/1000, Loss: 16.4746, Val Loss: 16.7630\n",
      "Epoch 20/1000, Loss: 16.4418, Val Loss: 16.7567\n",
      "Epoch 21/1000, Loss: 16.4491, Val Loss: 16.7506\n",
      "Epoch 22/1000, Loss: 16.4274, Val Loss: 16.7428\n",
      "Epoch 23/1000, Loss: 16.3806, Val Loss: 16.7350\n",
      "Epoch 24/1000, Loss: 16.3961, Val Loss: 16.7273\n",
      "Epoch 25/1000, Loss: 16.3782, Val Loss: 16.7186\n",
      "Epoch 26/1000, Loss: 16.3565, Val Loss: 16.7090\n",
      "Epoch 27/1000, Loss: 16.3379, Val Loss: 16.6987\n",
      "Epoch 28/1000, Loss: 16.3569, Val Loss: 16.6876\n",
      "Epoch 29/1000, Loss: 16.3402, Val Loss: 16.6761\n",
      "Epoch 30/1000, Loss: 16.3037, Val Loss: 16.6658\n",
      "Epoch 31/1000, Loss: 16.2755, Val Loss: 16.6549\n",
      "Epoch 32/1000, Loss: 16.2842, Val Loss: 16.6449\n",
      "Epoch 33/1000, Loss: 16.2661, Val Loss: 16.6339\n",
      "Epoch 34/1000, Loss: 16.2542, Val Loss: 16.6229\n",
      "Epoch 35/1000, Loss: 16.2807, Val Loss: 16.6177\n",
      "Epoch 36/1000, Loss: 16.2226, Val Loss: 16.6090\n",
      "Epoch 37/1000, Loss: 16.2218, Val Loss: 16.6028\n",
      "Epoch 38/1000, Loss: 16.2719, Val Loss: 16.5985\n",
      "Epoch 39/1000, Loss: 16.2244, Val Loss: 16.5929\n",
      "Epoch 40/1000, Loss: 16.2246, Val Loss: 16.5840\n",
      "Epoch 41/1000, Loss: 16.1902, Val Loss: 16.5719\n",
      "Epoch 42/1000, Loss: 16.1555, Val Loss: 16.5596\n",
      "Epoch 43/1000, Loss: 16.2218, Val Loss: 16.5506\n",
      "Epoch 44/1000, Loss: 16.1766, Val Loss: 16.5449\n",
      "Epoch 45/1000, Loss: 16.1749, Val Loss: 16.5411\n",
      "Epoch 46/1000, Loss: 16.2644, Val Loss: 16.5423\n",
      "Epoch 47/1000, Loss: 16.2025, Val Loss: 16.5448\n",
      "Epoch 48/1000, Loss: 16.1929, Val Loss: 16.5453\n",
      "Epoch 49/1000, Loss: 16.1685, Val Loss: 16.5441\n",
      "Epoch 50/1000, Loss: 16.1363, Val Loss: 16.5422\n",
      "Epoch 51/1000, Loss: 16.1278, Val Loss: 16.5398\n",
      "Epoch 52/1000, Loss: 16.1266, Val Loss: 16.5380\n",
      "Epoch 53/1000, Loss: 16.1576, Val Loss: 16.5371\n",
      "Epoch 54/1000, Loss: 16.1401, Val Loss: 16.5349\n",
      "Epoch 55/1000, Loss: 16.1459, Val Loss: 16.5301\n",
      "Epoch 56/1000, Loss: 16.1618, Val Loss: 16.5264\n",
      "Epoch 57/1000, Loss: 16.1543, Val Loss: 16.5251\n",
      "Epoch 58/1000, Loss: 16.1264, Val Loss: 16.5205\n",
      "Epoch 59/1000, Loss: 16.1109, Val Loss: 16.5173\n",
      "Epoch 60/1000, Loss: 16.1336, Val Loss: 16.5145\n",
      "Epoch 61/1000, Loss: 16.0788, Val Loss: 16.5132\n",
      "Epoch 62/1000, Loss: 16.0928, Val Loss: 16.5122\n",
      "Epoch 63/1000, Loss: 16.0934, Val Loss: 16.5131\n",
      "Epoch 64/1000, Loss: 16.0991, Val Loss: 16.5128\n",
      "Epoch 65/1000, Loss: 16.1177, Val Loss: 16.5118\n",
      "Epoch 66/1000, Loss: 16.0247, Val Loss: 16.5105\n",
      "Epoch 67/1000, Loss: 16.0769, Val Loss: 16.5120\n",
      "Epoch 68/1000, Loss: 16.0676, Val Loss: 16.5132\n",
      "Epoch 69/1000, Loss: 16.0164, Val Loss: 16.5099\n",
      "Epoch 70/1000, Loss: 16.0532, Val Loss: 16.5121\n",
      "Epoch 71/1000, Loss: 16.0231, Val Loss: 16.5110\n",
      "Epoch 72/1000, Loss: 16.0551, Val Loss: 16.5120\n",
      "Epoch 73/1000, Loss: 15.9996, Val Loss: 16.5072\n",
      "Epoch 74/1000, Loss: 16.0456, Val Loss: 16.5006\n",
      "Epoch 75/1000, Loss: 16.0096, Val Loss: 16.4890\n",
      "Epoch 76/1000, Loss: 15.9850, Val Loss: 16.4758\n",
      "Epoch 77/1000, Loss: 15.9982, Val Loss: 16.4633\n",
      "Epoch 78/1000, Loss: 16.0110, Val Loss: 16.4465\n",
      "Epoch 79/1000, Loss: 15.9280, Val Loss: 16.4315\n",
      "Epoch 80/1000, Loss: 15.9683, Val Loss: 16.4164\n",
      "Epoch 81/1000, Loss: 15.9660, Val Loss: 16.4036\n",
      "Epoch 82/1000, Loss: 15.9041, Val Loss: 16.3888\n",
      "Epoch 83/1000, Loss: 15.9962, Val Loss: 16.3753\n",
      "Epoch 84/1000, Loss: 15.9842, Val Loss: 16.3642\n",
      "Epoch 85/1000, Loss: 15.8727, Val Loss: 16.3528\n",
      "Epoch 86/1000, Loss: 15.9760, Val Loss: 16.3446\n",
      "Epoch 87/1000, Loss: 16.0156, Val Loss: 16.3390\n",
      "Epoch 88/1000, Loss: 15.8920, Val Loss: 16.3373\n",
      "Epoch 89/1000, Loss: 15.9465, Val Loss: 16.3338\n",
      "Epoch 90/1000, Loss: 15.9763, Val Loss: 16.3270\n",
      "Epoch 91/1000, Loss: 15.9212, Val Loss: 16.3188\n",
      "Epoch 92/1000, Loss: 15.9314, Val Loss: 16.3109\n",
      "Epoch 93/1000, Loss: 15.9280, Val Loss: 16.3037\n",
      "Epoch 94/1000, Loss: 15.9263, Val Loss: 16.2990\n",
      "Epoch 95/1000, Loss: 15.9342, Val Loss: 16.2909\n",
      "Epoch 96/1000, Loss: 15.9389, Val Loss: 16.2819\n",
      "Epoch 97/1000, Loss: 15.8689, Val Loss: 16.2729\n",
      "Epoch 98/1000, Loss: 15.9580, Val Loss: 16.2616\n",
      "Epoch 99/1000, Loss: 15.9535, Val Loss: 16.2504\n",
      "Epoch 100/1000, Loss: 15.9271, Val Loss: 16.2393\n",
      "Epoch 101/1000, Loss: 15.8013, Val Loss: 16.2297\n",
      "Epoch 102/1000, Loss: 15.9093, Val Loss: 16.2231\n",
      "Epoch 103/1000, Loss: 15.9396, Val Loss: 16.2176\n",
      "Epoch 104/1000, Loss: 15.9008, Val Loss: 16.2136\n",
      "Epoch 105/1000, Loss: 15.8683, Val Loss: 16.2087\n",
      "Epoch 106/1000, Loss: 15.9477, Val Loss: 16.2053\n",
      "Epoch 107/1000, Loss: 15.9196, Val Loss: 16.2053\n",
      "Epoch 108/1000, Loss: 15.9059, Val Loss: 16.2031\n",
      "Epoch 109/1000, Loss: 15.9134, Val Loss: 16.2023\n",
      "Epoch 110/1000, Loss: 15.8995, Val Loss: 16.2025\n",
      "Epoch 111/1000, Loss: 15.8422, Val Loss: 16.2025\n",
      "Epoch 112/1000, Loss: 15.9528, Val Loss: 16.2008\n",
      "Epoch 113/1000, Loss: 15.9630, Val Loss: 16.1987\n",
      "Epoch 114/1000, Loss: 15.9514, Val Loss: 16.1973\n",
      "Epoch 115/1000, Loss: 15.8916, Val Loss: 16.1974\n",
      "Epoch 116/1000, Loss: 15.8529, Val Loss: 16.1975\n",
      "Epoch 117/1000, Loss: 15.8697, Val Loss: 16.1977\n",
      "Epoch 118/1000, Loss: 15.8552, Val Loss: 16.2007\n",
      "Epoch 119/1000, Loss: 15.8636, Val Loss: 16.2003\n",
      "Epoch 120/1000, Loss: 15.8706, Val Loss: 16.1987\n",
      "Epoch 121/1000, Loss: 15.9321, Val Loss: 16.1959\n",
      "Epoch 122/1000, Loss: 15.9169, Val Loss: 16.1958\n",
      "Epoch 123/1000, Loss: 15.8901, Val Loss: 16.1892\n",
      "Epoch 124/1000, Loss: 15.8653, Val Loss: 16.1847\n",
      "Epoch 125/1000, Loss: 15.8524, Val Loss: 16.1808\n",
      "Epoch 126/1000, Loss: 15.8575, Val Loss: 16.1764\n",
      "Epoch 127/1000, Loss: 15.8758, Val Loss: 16.1768\n",
      "Epoch 128/1000, Loss: 15.8934, Val Loss: 16.1745\n",
      "Epoch 129/1000, Loss: 15.8680, Val Loss: 16.1776\n",
      "Epoch 130/1000, Loss: 15.8978, Val Loss: 16.1815\n",
      "Epoch 131/1000, Loss: 15.8859, Val Loss: 16.1841\n",
      "Epoch 132/1000, Loss: 15.8181, Val Loss: 16.1890\n",
      "Epoch 133/1000, Loss: 15.8836, Val Loss: 16.1989\n",
      "Epoch 134/1000, Loss: 15.7845, Val Loss: 16.2087\n",
      "Epoch 135/1000, Loss: 15.8708, Val Loss: 16.2149\n",
      "Epoch 136/1000, Loss: 15.8066, Val Loss: 16.2123\n",
      "Epoch 137/1000, Loss: 15.7998, Val Loss: 16.2066\n",
      "Epoch 138/1000, Loss: 15.8164, Val Loss: 16.2027\n",
      "Epoch 139/1000, Loss: 15.8810, Val Loss: 16.1930\n",
      "Epoch 140/1000, Loss: 15.8291, Val Loss: 16.1827\n",
      "Epoch 141/1000, Loss: 15.8659, Val Loss: 16.1774\n",
      "Epoch 142/1000, Loss: 15.8735, Val Loss: 16.1680\n",
      "Epoch 143/1000, Loss: 15.7564, Val Loss: 16.1679\n",
      "Epoch 144/1000, Loss: 15.8028, Val Loss: 16.1682\n",
      "Epoch 145/1000, Loss: 15.8689, Val Loss: 16.1675\n",
      "Epoch 146/1000, Loss: 15.7843, Val Loss: 16.1731\n",
      "Epoch 147/1000, Loss: 15.8354, Val Loss: 16.1795\n",
      "Epoch 148/1000, Loss: 15.8291, Val Loss: 16.1865\n",
      "Epoch 149/1000, Loss: 15.8288, Val Loss: 16.1994\n",
      "Epoch 150/1000, Loss: 15.8074, Val Loss: 16.2048\n",
      "Epoch 151/1000, Loss: 15.8054, Val Loss: 16.2028\n",
      "Epoch 152/1000, Loss: 15.8357, Val Loss: 16.2006\n",
      "Epoch 153/1000, Loss: 15.8437, Val Loss: 16.1981\n",
      "Epoch 154/1000, Loss: 15.8422, Val Loss: 16.1970\n",
      "Epoch 155/1000, Loss: 15.8637, Val Loss: 16.2004\n",
      "Epoch 156/1000, Loss: 15.8363, Val Loss: 16.2075\n",
      "Epoch 157/1000, Loss: 15.8327, Val Loss: 16.2214\n",
      "Epoch 158/1000, Loss: 15.8228, Val Loss: 16.2337\n",
      "Epoch 159/1000, Loss: 15.8260, Val Loss: 16.2407\n",
      "Epoch 160/1000, Loss: 15.8268, Val Loss: 16.2464\n",
      "Epoch 161/1000, Loss: 15.8297, Val Loss: 16.2466\n",
      "Epoch 162/1000, Loss: 15.7775, Val Loss: 16.2465\n",
      "Epoch 163/1000, Loss: 15.7553, Val Loss: 16.2430\n",
      "Epoch 164/1000, Loss: 15.8729, Val Loss: 16.2476\n",
      "Epoch 165/1000, Loss: 15.7872, Val Loss: 16.2412\n",
      "Epoch 166/1000, Loss: 15.7895, Val Loss: 16.2260\n",
      "Epoch 167/1000, Loss: 15.8588, Val Loss: 16.2292\n",
      "Epoch 168/1000, Loss: 15.8300, Val Loss: 16.2207\n",
      "Epoch 169/1000, Loss: 15.7867, Val Loss: 16.2160\n",
      "Epoch 170/1000, Loss: 15.7507, Val Loss: 16.2105\n",
      "Epoch 171/1000, Loss: 15.8270, Val Loss: 16.2176\n",
      "Epoch 172/1000, Loss: 15.8652, Val Loss: 16.2259\n",
      "Epoch 173/1000, Loss: 15.8848, Val Loss: 16.2273\n",
      "Epoch 174/1000, Loss: 15.8517, Val Loss: 16.2231\n",
      "Epoch 175/1000, Loss: 15.7560, Val Loss: 16.2257\n",
      "Epoch 176/1000, Loss: 15.8375, Val Loss: 16.2305\n",
      "Epoch 177/1000, Loss: 15.8497, Val Loss: 16.2316\n",
      "Epoch 178/1000, Loss: 15.8235, Val Loss: 16.2417\n",
      "Epoch 179/1000, Loss: 15.7885, Val Loss: 16.2428\n",
      "Epoch 180/1000, Loss: 15.7978, Val Loss: 16.2463\n",
      "Epoch 181/1000, Loss: 15.8490, Val Loss: 16.2510\n",
      "Epoch 182/1000, Loss: 15.7612, Val Loss: 16.2295\n",
      "Epoch 183/1000, Loss: 15.7599, Val Loss: 16.2118\n",
      "Epoch 184/1000, Loss: 15.8674, Val Loss: 16.1910\n",
      "Epoch 185/1000, Loss: 15.7590, Val Loss: 16.1814\n",
      "Early stopping triggered at epoch 185. Best Val Loss: 16.1675\n",
      "Epoch 1/1000, Loss: 16.7432, Val Loss: 16.9455\n",
      "Epoch 2/1000, Loss: 16.7316, Val Loss: 16.9440\n",
      "Epoch 3/1000, Loss: 16.7278, Val Loss: 16.9424\n",
      "Epoch 4/1000, Loss: 16.7292, Val Loss: 16.9407\n",
      "Epoch 5/1000, Loss: 16.7208, Val Loss: 16.9391\n",
      "Epoch 6/1000, Loss: 16.7164, Val Loss: 16.9374\n",
      "Epoch 7/1000, Loss: 16.7104, Val Loss: 16.9357\n",
      "Epoch 8/1000, Loss: 16.7079, Val Loss: 16.9338\n",
      "Epoch 9/1000, Loss: 16.7004, Val Loss: 16.9319\n",
      "Epoch 10/1000, Loss: 16.6924, Val Loss: 16.9298\n",
      "Epoch 11/1000, Loss: 16.6899, Val Loss: 16.9279\n",
      "Epoch 12/1000, Loss: 16.6801, Val Loss: 16.9261\n",
      "Epoch 13/1000, Loss: 16.6703, Val Loss: 16.9240\n",
      "Epoch 14/1000, Loss: 16.6577, Val Loss: 16.9216\n",
      "Epoch 15/1000, Loss: 16.6551, Val Loss: 16.9191\n",
      "Epoch 16/1000, Loss: 16.6540, Val Loss: 16.9165\n",
      "Epoch 17/1000, Loss: 16.6451, Val Loss: 16.9135\n",
      "Epoch 18/1000, Loss: 16.6372, Val Loss: 16.9103\n",
      "Epoch 19/1000, Loss: 16.6279, Val Loss: 16.9067\n",
      "Epoch 20/1000, Loss: 16.6158, Val Loss: 16.9026\n",
      "Epoch 21/1000, Loss: 16.6007, Val Loss: 16.8979\n",
      "Epoch 22/1000, Loss: 16.5955, Val Loss: 16.8930\n",
      "Epoch 23/1000, Loss: 16.5859, Val Loss: 16.8877\n",
      "Epoch 24/1000, Loss: 16.5767, Val Loss: 16.8825\n",
      "Epoch 25/1000, Loss: 16.5692, Val Loss: 16.8769\n",
      "Epoch 26/1000, Loss: 16.5605, Val Loss: 16.8711\n",
      "Epoch 27/1000, Loss: 16.5484, Val Loss: 16.8646\n",
      "Epoch 28/1000, Loss: 16.5342, Val Loss: 16.8575\n",
      "Epoch 29/1000, Loss: 16.5336, Val Loss: 16.8498\n",
      "Epoch 30/1000, Loss: 16.5083, Val Loss: 16.8405\n",
      "Epoch 31/1000, Loss: 16.5122, Val Loss: 16.8307\n",
      "Epoch 32/1000, Loss: 16.4771, Val Loss: 16.8205\n",
      "Epoch 33/1000, Loss: 16.4896, Val Loss: 16.8088\n",
      "Epoch 34/1000, Loss: 16.4606, Val Loss: 16.7970\n",
      "Epoch 35/1000, Loss: 16.4541, Val Loss: 16.7892\n",
      "Epoch 36/1000, Loss: 16.4519, Val Loss: 16.7822\n",
      "Epoch 37/1000, Loss: 16.4098, Val Loss: 16.7752\n",
      "Epoch 38/1000, Loss: 16.4179, Val Loss: 16.7658\n",
      "Epoch 39/1000, Loss: 16.4089, Val Loss: 16.7557\n",
      "Epoch 40/1000, Loss: 16.4035, Val Loss: 16.7481\n",
      "Epoch 41/1000, Loss: 16.3890, Val Loss: 16.7465\n",
      "Epoch 42/1000, Loss: 16.3657, Val Loss: 16.7426\n",
      "Epoch 43/1000, Loss: 16.3763, Val Loss: 16.7381\n",
      "Epoch 44/1000, Loss: 16.3427, Val Loss: 16.7405\n",
      "Epoch 45/1000, Loss: 16.3636, Val Loss: 16.7394\n",
      "Epoch 46/1000, Loss: 16.3309, Val Loss: 16.7367\n",
      "Epoch 47/1000, Loss: 16.3095, Val Loss: 16.7316\n",
      "Epoch 48/1000, Loss: 16.3180, Val Loss: 16.7257\n",
      "Epoch 49/1000, Loss: 16.3124, Val Loss: 16.7178\n",
      "Epoch 50/1000, Loss: 16.2990, Val Loss: 16.7078\n",
      "Epoch 51/1000, Loss: 16.2995, Val Loss: 16.7052\n",
      "Epoch 52/1000, Loss: 16.2842, Val Loss: 16.7036\n",
      "Epoch 53/1000, Loss: 16.2538, Val Loss: 16.7039\n",
      "Epoch 54/1000, Loss: 16.2555, Val Loss: 16.7020\n",
      "Epoch 55/1000, Loss: 16.2765, Val Loss: 16.7001\n",
      "Epoch 56/1000, Loss: 16.2505, Val Loss: 16.6968\n",
      "Epoch 57/1000, Loss: 16.2205, Val Loss: 16.6923\n",
      "Epoch 58/1000, Loss: 16.2279, Val Loss: 16.6888\n",
      "Epoch 59/1000, Loss: 16.2565, Val Loss: 16.6827\n",
      "Epoch 60/1000, Loss: 16.2366, Val Loss: 16.6775\n",
      "Epoch 61/1000, Loss: 16.2294, Val Loss: 16.6734\n",
      "Epoch 62/1000, Loss: 16.2485, Val Loss: 16.6750\n",
      "Epoch 63/1000, Loss: 16.2015, Val Loss: 16.6733\n",
      "Epoch 64/1000, Loss: 16.1851, Val Loss: 16.6714\n",
      "Epoch 65/1000, Loss: 16.1929, Val Loss: 16.6684\n",
      "Epoch 66/1000, Loss: 16.1876, Val Loss: 16.6645\n",
      "Epoch 67/1000, Loss: 16.1616, Val Loss: 16.6580\n",
      "Epoch 68/1000, Loss: 16.1745, Val Loss: 16.6472\n",
      "Epoch 69/1000, Loss: 16.1537, Val Loss: 16.6376\n",
      "Epoch 70/1000, Loss: 16.1370, Val Loss: 16.6246\n",
      "Epoch 71/1000, Loss: 16.0928, Val Loss: 16.6115\n",
      "Epoch 72/1000, Loss: 16.0982, Val Loss: 16.5982\n",
      "Epoch 73/1000, Loss: 16.1010, Val Loss: 16.5821\n",
      "Epoch 74/1000, Loss: 16.0981, Val Loss: 16.5668\n",
      "Epoch 75/1000, Loss: 16.0683, Val Loss: 16.5496\n",
      "Epoch 76/1000, Loss: 16.0438, Val Loss: 16.5337\n",
      "Epoch 77/1000, Loss: 16.0403, Val Loss: 16.5157\n",
      "Epoch 78/1000, Loss: 16.0338, Val Loss: 16.4967\n",
      "Epoch 79/1000, Loss: 16.0110, Val Loss: 16.4743\n",
      "Epoch 80/1000, Loss: 15.9728, Val Loss: 16.4569\n",
      "Epoch 81/1000, Loss: 16.0210, Val Loss: 16.4378\n",
      "Epoch 82/1000, Loss: 16.0325, Val Loss: 16.4139\n",
      "Epoch 83/1000, Loss: 15.9599, Val Loss: 16.3917\n",
      "Epoch 84/1000, Loss: 15.9531, Val Loss: 16.3736\n",
      "Epoch 85/1000, Loss: 15.9822, Val Loss: 16.3603\n",
      "Epoch 86/1000, Loss: 15.9791, Val Loss: 16.3501\n",
      "Epoch 87/1000, Loss: 15.9343, Val Loss: 16.3382\n",
      "Epoch 88/1000, Loss: 15.8550, Val Loss: 16.3243\n",
      "Epoch 89/1000, Loss: 15.9626, Val Loss: 16.3137\n",
      "Epoch 90/1000, Loss: 15.9536, Val Loss: 16.3059\n",
      "Epoch 91/1000, Loss: 15.9277, Val Loss: 16.2982\n",
      "Epoch 92/1000, Loss: 15.9297, Val Loss: 16.2900\n",
      "Epoch 93/1000, Loss: 15.8850, Val Loss: 16.2801\n",
      "Epoch 94/1000, Loss: 15.8883, Val Loss: 16.2713\n",
      "Epoch 95/1000, Loss: 15.9118, Val Loss: 16.2625\n",
      "Epoch 96/1000, Loss: 15.9693, Val Loss: 16.2521\n",
      "Epoch 97/1000, Loss: 15.9586, Val Loss: 16.2469\n",
      "Epoch 98/1000, Loss: 15.8561, Val Loss: 16.2406\n",
      "Epoch 99/1000, Loss: 15.9775, Val Loss: 16.2342\n",
      "Epoch 100/1000, Loss: 15.9326, Val Loss: 16.2253\n",
      "Epoch 101/1000, Loss: 15.8923, Val Loss: 16.2163\n",
      "Epoch 102/1000, Loss: 15.8856, Val Loss: 16.2115\n",
      "Epoch 103/1000, Loss: 15.9073, Val Loss: 16.2072\n",
      "Epoch 104/1000, Loss: 15.9187, Val Loss: 16.2034\n",
      "Epoch 105/1000, Loss: 15.9129, Val Loss: 16.1991\n",
      "Epoch 106/1000, Loss: 15.9002, Val Loss: 16.1956\n",
      "Epoch 107/1000, Loss: 15.9072, Val Loss: 16.1950\n",
      "Epoch 108/1000, Loss: 15.9108, Val Loss: 16.1958\n",
      "Epoch 109/1000, Loss: 15.8593, Val Loss: 16.1951\n",
      "Epoch 110/1000, Loss: 15.9033, Val Loss: 16.1968\n",
      "Epoch 111/1000, Loss: 15.9233, Val Loss: 16.1963\n",
      "Epoch 112/1000, Loss: 15.9205, Val Loss: 16.1938\n",
      "Epoch 113/1000, Loss: 15.8604, Val Loss: 16.1922\n",
      "Epoch 114/1000, Loss: 15.8346, Val Loss: 16.1921\n",
      "Epoch 115/1000, Loss: 15.8863, Val Loss: 16.1899\n",
      "Epoch 116/1000, Loss: 15.8600, Val Loss: 16.1885\n",
      "Epoch 117/1000, Loss: 15.8403, Val Loss: 16.1855\n",
      "Epoch 118/1000, Loss: 15.8799, Val Loss: 16.1832\n",
      "Epoch 119/1000, Loss: 15.9562, Val Loss: 16.1833\n",
      "Epoch 120/1000, Loss: 15.9060, Val Loss: 16.1802\n",
      "Epoch 121/1000, Loss: 15.8928, Val Loss: 16.1791\n",
      "Epoch 122/1000, Loss: 15.8797, Val Loss: 16.1754\n",
      "Epoch 123/1000, Loss: 15.9408, Val Loss: 16.1738\n",
      "Epoch 124/1000, Loss: 15.8192, Val Loss: 16.1741\n",
      "Epoch 125/1000, Loss: 15.8478, Val Loss: 16.1779\n",
      "Epoch 126/1000, Loss: 15.8990, Val Loss: 16.1817\n",
      "Epoch 127/1000, Loss: 15.8782, Val Loss: 16.1839\n",
      "Epoch 128/1000, Loss: 15.8349, Val Loss: 16.1870\n",
      "Epoch 129/1000, Loss: 15.7700, Val Loss: 16.1889\n",
      "Epoch 130/1000, Loss: 15.8556, Val Loss: 16.1911\n",
      "Epoch 131/1000, Loss: 15.8665, Val Loss: 16.1877\n",
      "Epoch 132/1000, Loss: 15.8344, Val Loss: 16.1869\n",
      "Epoch 133/1000, Loss: 15.8240, Val Loss: 16.1855\n",
      "Epoch 134/1000, Loss: 15.8567, Val Loss: 16.1871\n",
      "Epoch 135/1000, Loss: 15.8887, Val Loss: 16.1897\n",
      "Epoch 136/1000, Loss: 15.8492, Val Loss: 16.1970\n",
      "Epoch 137/1000, Loss: 15.8070, Val Loss: 16.2035\n",
      "Epoch 138/1000, Loss: 15.8831, Val Loss: 16.2120\n",
      "Epoch 139/1000, Loss: 15.8312, Val Loss: 16.2158\n",
      "Epoch 140/1000, Loss: 15.8739, Val Loss: 16.2134\n",
      "Epoch 141/1000, Loss: 15.8600, Val Loss: 16.2015\n",
      "Epoch 142/1000, Loss: 15.9026, Val Loss: 16.1867\n",
      "Epoch 143/1000, Loss: 15.8844, Val Loss: 16.1741\n",
      "Epoch 144/1000, Loss: 15.8848, Val Loss: 16.1678\n",
      "Epoch 145/1000, Loss: 15.8849, Val Loss: 16.1623\n",
      "Epoch 146/1000, Loss: 15.8515, Val Loss: 16.1569\n",
      "Epoch 147/1000, Loss: 15.8250, Val Loss: 16.1544\n",
      "Epoch 148/1000, Loss: 15.8692, Val Loss: 16.1590\n",
      "Epoch 149/1000, Loss: 15.8892, Val Loss: 16.1645\n",
      "Epoch 150/1000, Loss: 15.7962, Val Loss: 16.1731\n",
      "Epoch 151/1000, Loss: 15.8499, Val Loss: 16.1808\n",
      "Epoch 152/1000, Loss: 15.8960, Val Loss: 16.1919\n",
      "Epoch 153/1000, Loss: 15.8046, Val Loss: 16.1998\n",
      "Epoch 154/1000, Loss: 15.8880, Val Loss: 16.2007\n",
      "Epoch 155/1000, Loss: 15.8912, Val Loss: 16.1949\n",
      "Epoch 156/1000, Loss: 15.9499, Val Loss: 16.1848\n",
      "Epoch 157/1000, Loss: 15.8262, Val Loss: 16.1755\n",
      "Epoch 158/1000, Loss: 15.8196, Val Loss: 16.1684\n",
      "Epoch 159/1000, Loss: 15.7378, Val Loss: 16.1656\n",
      "Epoch 160/1000, Loss: 15.8880, Val Loss: 16.1711\n",
      "Epoch 161/1000, Loss: 15.7809, Val Loss: 16.1802\n",
      "Epoch 162/1000, Loss: 15.8595, Val Loss: 16.1991\n",
      "Epoch 163/1000, Loss: 15.8111, Val Loss: 16.2085\n",
      "Epoch 164/1000, Loss: 15.8336, Val Loss: 16.2242\n",
      "Epoch 165/1000, Loss: 15.8006, Val Loss: 16.2367\n",
      "Epoch 166/1000, Loss: 15.8181, Val Loss: 16.2393\n",
      "Epoch 167/1000, Loss: 15.8101, Val Loss: 16.2286\n",
      "Epoch 168/1000, Loss: 15.8100, Val Loss: 16.2163\n",
      "Epoch 169/1000, Loss: 15.7656, Val Loss: 16.2010\n",
      "Epoch 170/1000, Loss: 15.7888, Val Loss: 16.1847\n",
      "Epoch 171/1000, Loss: 15.7825, Val Loss: 16.1708\n",
      "Epoch 172/1000, Loss: 15.8421, Val Loss: 16.1672\n",
      "Epoch 173/1000, Loss: 15.8212, Val Loss: 16.1641\n",
      "Epoch 174/1000, Loss: 15.8717, Val Loss: 16.1610\n",
      "Epoch 175/1000, Loss: 15.8566, Val Loss: 16.1646\n",
      "Epoch 176/1000, Loss: 15.7888, Val Loss: 16.1689\n",
      "Epoch 177/1000, Loss: 15.8365, Val Loss: 16.1709\n",
      "Epoch 178/1000, Loss: 15.8568, Val Loss: 16.1755\n",
      "Epoch 179/1000, Loss: 15.8403, Val Loss: 16.1767\n",
      "Epoch 180/1000, Loss: 15.7788, Val Loss: 16.1771\n",
      "Epoch 181/1000, Loss: 15.7639, Val Loss: 16.1772\n",
      "Epoch 182/1000, Loss: 15.7858, Val Loss: 16.1823\n",
      "Epoch 183/1000, Loss: 15.7340, Val Loss: 16.1815\n",
      "Epoch 184/1000, Loss: 15.8417, Val Loss: 16.1855\n",
      "Epoch 185/1000, Loss: 15.8361, Val Loss: 16.1830\n",
      "Epoch 186/1000, Loss: 15.8618, Val Loss: 16.1820\n",
      "Epoch 187/1000, Loss: 15.7883, Val Loss: 16.1873\n",
      "Early stopping triggered at epoch 187. Best Val Loss: 16.1544\n",
      "Epoch 1/1000, Loss: 16.7017, Val Loss: 16.9414\n",
      "Epoch 2/1000, Loss: 16.6916, Val Loss: 16.9380\n",
      "Epoch 3/1000, Loss: 16.6909, Val Loss: 16.9346\n",
      "Epoch 4/1000, Loss: 16.6567, Val Loss: 16.9311\n",
      "Epoch 5/1000, Loss: 16.6738, Val Loss: 16.9277\n",
      "Epoch 6/1000, Loss: 16.6184, Val Loss: 16.9235\n",
      "Epoch 7/1000, Loss: 16.6273, Val Loss: 16.9190\n",
      "Epoch 8/1000, Loss: 16.6002, Val Loss: 16.9145\n",
      "Epoch 9/1000, Loss: 16.5598, Val Loss: 16.9113\n",
      "Epoch 10/1000, Loss: 16.5586, Val Loss: 16.9080\n",
      "Epoch 11/1000, Loss: 16.5403, Val Loss: 16.9041\n",
      "Epoch 12/1000, Loss: 16.4930, Val Loss: 16.8999\n",
      "Epoch 13/1000, Loss: 16.4941, Val Loss: 16.8951\n",
      "Epoch 14/1000, Loss: 16.5111, Val Loss: 16.8894\n",
      "Epoch 15/1000, Loss: 16.4649, Val Loss: 16.8825\n",
      "Epoch 16/1000, Loss: 16.4619, Val Loss: 16.8747\n",
      "Epoch 17/1000, Loss: 16.3952, Val Loss: 16.8654\n",
      "Epoch 18/1000, Loss: 16.4192, Val Loss: 16.8551\n",
      "Epoch 19/1000, Loss: 16.4027, Val Loss: 16.8441\n",
      "Epoch 20/1000, Loss: 16.3850, Val Loss: 16.8329\n",
      "Epoch 21/1000, Loss: 16.3912, Val Loss: 16.8214\n",
      "Epoch 22/1000, Loss: 16.3657, Val Loss: 16.8096\n",
      "Epoch 23/1000, Loss: 16.3771, Val Loss: 16.7994\n",
      "Epoch 24/1000, Loss: 16.3614, Val Loss: 16.7887\n",
      "Epoch 25/1000, Loss: 16.3381, Val Loss: 16.7783\n",
      "Epoch 26/1000, Loss: 16.3526, Val Loss: 16.7682\n",
      "Epoch 27/1000, Loss: 16.3166, Val Loss: 16.7579\n",
      "Epoch 28/1000, Loss: 16.2928, Val Loss: 16.7466\n",
      "Epoch 29/1000, Loss: 16.3196, Val Loss: 16.7336\n",
      "Epoch 30/1000, Loss: 16.2710, Val Loss: 16.7206\n",
      "Epoch 31/1000, Loss: 16.3445, Val Loss: 16.7106\n",
      "Epoch 32/1000, Loss: 16.2558, Val Loss: 16.7048\n",
      "Epoch 33/1000, Loss: 16.3325, Val Loss: 16.7001\n",
      "Epoch 34/1000, Loss: 16.3584, Val Loss: 16.6965\n",
      "Epoch 35/1000, Loss: 16.2586, Val Loss: 16.6922\n",
      "Epoch 36/1000, Loss: 16.2668, Val Loss: 16.6878\n",
      "Epoch 37/1000, Loss: 16.3177, Val Loss: 16.6861\n",
      "Epoch 38/1000, Loss: 16.3021, Val Loss: 16.6864\n",
      "Epoch 39/1000, Loss: 16.3204, Val Loss: 16.6852\n",
      "Epoch 40/1000, Loss: 16.3050, Val Loss: 16.6858\n",
      "Epoch 41/1000, Loss: 16.2865, Val Loss: 16.6853\n",
      "Epoch 42/1000, Loss: 16.2680, Val Loss: 16.6839\n",
      "Epoch 43/1000, Loss: 16.2404, Val Loss: 16.6814\n",
      "Epoch 44/1000, Loss: 16.2652, Val Loss: 16.6789\n",
      "Epoch 45/1000, Loss: 16.2674, Val Loss: 16.6752\n",
      "Epoch 46/1000, Loss: 16.2552, Val Loss: 16.6695\n",
      "Epoch 47/1000, Loss: 16.2843, Val Loss: 16.6679\n",
      "Epoch 48/1000, Loss: 16.2290, Val Loss: 16.6655\n",
      "Epoch 49/1000, Loss: 16.2968, Val Loss: 16.6620\n",
      "Epoch 50/1000, Loss: 16.2805, Val Loss: 16.6582\n",
      "Epoch 51/1000, Loss: 16.2669, Val Loss: 16.6550\n",
      "Epoch 52/1000, Loss: 16.2240, Val Loss: 16.6510\n",
      "Epoch 53/1000, Loss: 16.2115, Val Loss: 16.6482\n",
      "Epoch 54/1000, Loss: 16.2633, Val Loss: 16.6471\n",
      "Epoch 55/1000, Loss: 16.2788, Val Loss: 16.6432\n",
      "Epoch 56/1000, Loss: 16.2651, Val Loss: 16.6396\n",
      "Epoch 57/1000, Loss: 16.2481, Val Loss: 16.6372\n",
      "Epoch 58/1000, Loss: 16.2040, Val Loss: 16.6351\n",
      "Epoch 59/1000, Loss: 16.2345, Val Loss: 16.6319\n",
      "Epoch 60/1000, Loss: 16.2200, Val Loss: 16.6272\n",
      "Epoch 61/1000, Loss: 16.2345, Val Loss: 16.6247\n",
      "Epoch 62/1000, Loss: 16.2332, Val Loss: 16.6226\n",
      "Epoch 63/1000, Loss: 16.2244, Val Loss: 16.6186\n",
      "Epoch 64/1000, Loss: 16.2464, Val Loss: 16.6161\n",
      "Epoch 65/1000, Loss: 16.2417, Val Loss: 16.6138\n",
      "Epoch 66/1000, Loss: 16.2311, Val Loss: 16.6113\n",
      "Epoch 67/1000, Loss: 16.2226, Val Loss: 16.6071\n",
      "Epoch 68/1000, Loss: 16.2289, Val Loss: 16.6032\n",
      "Epoch 69/1000, Loss: 16.2053, Val Loss: 16.6005\n",
      "Epoch 70/1000, Loss: 16.2346, Val Loss: 16.5989\n",
      "Epoch 71/1000, Loss: 16.2025, Val Loss: 16.5962\n",
      "Epoch 72/1000, Loss: 16.2005, Val Loss: 16.5943\n",
      "Epoch 73/1000, Loss: 16.2192, Val Loss: 16.5919\n",
      "Epoch 74/1000, Loss: 16.2124, Val Loss: 16.5898\n",
      "Epoch 75/1000, Loss: 16.1916, Val Loss: 16.5873\n",
      "Epoch 76/1000, Loss: 16.1691, Val Loss: 16.5850\n",
      "Epoch 77/1000, Loss: 16.1799, Val Loss: 16.5829\n",
      "Epoch 78/1000, Loss: 16.1672, Val Loss: 16.5798\n",
      "Epoch 79/1000, Loss: 16.2337, Val Loss: 16.5780\n",
      "Epoch 80/1000, Loss: 16.1952, Val Loss: 16.5749\n",
      "Epoch 81/1000, Loss: 16.1770, Val Loss: 16.5720\n",
      "Epoch 82/1000, Loss: 16.2077, Val Loss: 16.5689\n",
      "Epoch 83/1000, Loss: 16.2007, Val Loss: 16.5651\n",
      "Epoch 84/1000, Loss: 16.2139, Val Loss: 16.5611\n",
      "Epoch 85/1000, Loss: 16.1903, Val Loss: 16.5583\n",
      "Epoch 86/1000, Loss: 16.1951, Val Loss: 16.5555\n",
      "Epoch 87/1000, Loss: 16.1984, Val Loss: 16.5519\n",
      "Epoch 88/1000, Loss: 16.1801, Val Loss: 16.5492\n",
      "Epoch 89/1000, Loss: 16.2039, Val Loss: 16.5460\n",
      "Epoch 90/1000, Loss: 16.2062, Val Loss: 16.5448\n",
      "Epoch 91/1000, Loss: 16.2038, Val Loss: 16.5445\n",
      "Epoch 92/1000, Loss: 16.1757, Val Loss: 16.5441\n",
      "Epoch 93/1000, Loss: 16.1413, Val Loss: 16.5414\n",
      "Epoch 94/1000, Loss: 16.1606, Val Loss: 16.5377\n",
      "Epoch 95/1000, Loss: 16.1590, Val Loss: 16.5345\n",
      "Epoch 96/1000, Loss: 16.1518, Val Loss: 16.5333\n",
      "Epoch 97/1000, Loss: 16.1679, Val Loss: 16.5309\n",
      "Epoch 98/1000, Loss: 16.1559, Val Loss: 16.5277\n",
      "Epoch 99/1000, Loss: 16.1498, Val Loss: 16.5262\n",
      "Epoch 100/1000, Loss: 16.1183, Val Loss: 16.5243\n",
      "Epoch 101/1000, Loss: 16.0953, Val Loss: 16.5241\n",
      "Epoch 102/1000, Loss: 16.1645, Val Loss: 16.5208\n",
      "Epoch 103/1000, Loss: 16.1212, Val Loss: 16.5192\n",
      "Epoch 104/1000, Loss: 16.1347, Val Loss: 16.5198\n",
      "Epoch 105/1000, Loss: 16.1618, Val Loss: 16.5190\n",
      "Epoch 106/1000, Loss: 16.1241, Val Loss: 16.5149\n",
      "Epoch 107/1000, Loss: 16.1494, Val Loss: 16.5081\n",
      "Epoch 108/1000, Loss: 16.0946, Val Loss: 16.5028\n",
      "Epoch 109/1000, Loss: 16.1575, Val Loss: 16.4987\n",
      "Epoch 110/1000, Loss: 16.0557, Val Loss: 16.4954\n",
      "Epoch 111/1000, Loss: 16.0972, Val Loss: 16.4908\n",
      "Epoch 112/1000, Loss: 16.1177, Val Loss: 16.4875\n",
      "Epoch 113/1000, Loss: 16.1341, Val Loss: 16.4843\n",
      "Epoch 114/1000, Loss: 16.1142, Val Loss: 16.4829\n",
      "Epoch 115/1000, Loss: 16.0862, Val Loss: 16.4796\n",
      "Epoch 116/1000, Loss: 16.1010, Val Loss: 16.4753\n",
      "Epoch 117/1000, Loss: 16.0194, Val Loss: 16.4731\n",
      "Epoch 118/1000, Loss: 16.0845, Val Loss: 16.4680\n",
      "Epoch 119/1000, Loss: 16.0327, Val Loss: 16.4627\n",
      "Epoch 120/1000, Loss: 16.0378, Val Loss: 16.4542\n",
      "Epoch 121/1000, Loss: 16.0112, Val Loss: 16.4441\n",
      "Epoch 122/1000, Loss: 16.0458, Val Loss: 16.4340\n",
      "Epoch 123/1000, Loss: 16.0747, Val Loss: 16.4259\n",
      "Epoch 124/1000, Loss: 16.0456, Val Loss: 16.4228\n",
      "Epoch 125/1000, Loss: 15.9686, Val Loss: 16.4208\n",
      "Epoch 126/1000, Loss: 16.0218, Val Loss: 16.4189\n",
      "Epoch 127/1000, Loss: 16.0344, Val Loss: 16.4140\n",
      "Epoch 128/1000, Loss: 15.9767, Val Loss: 16.4050\n",
      "Epoch 129/1000, Loss: 15.9678, Val Loss: 16.3963\n",
      "Epoch 130/1000, Loss: 15.9572, Val Loss: 16.3878\n",
      "Epoch 131/1000, Loss: 15.9608, Val Loss: 16.3761\n",
      "Epoch 132/1000, Loss: 15.9593, Val Loss: 16.3618\n",
      "Epoch 133/1000, Loss: 15.9431, Val Loss: 16.3487\n",
      "Epoch 134/1000, Loss: 15.9601, Val Loss: 16.3375\n",
      "Epoch 135/1000, Loss: 15.9604, Val Loss: 16.3271\n",
      "Epoch 136/1000, Loss: 15.9709, Val Loss: 16.3174\n",
      "Epoch 137/1000, Loss: 15.9505, Val Loss: 16.3084\n",
      "Epoch 138/1000, Loss: 15.8997, Val Loss: 16.3010\n",
      "Epoch 139/1000, Loss: 15.8872, Val Loss: 16.2919\n",
      "Epoch 140/1000, Loss: 16.0188, Val Loss: 16.2844\n",
      "Epoch 141/1000, Loss: 15.9354, Val Loss: 16.2769\n",
      "Epoch 142/1000, Loss: 15.8923, Val Loss: 16.2686\n",
      "Epoch 143/1000, Loss: 15.9431, Val Loss: 16.2570\n",
      "Epoch 144/1000, Loss: 15.8620, Val Loss: 16.2453\n",
      "Epoch 145/1000, Loss: 15.9692, Val Loss: 16.2346\n",
      "Epoch 146/1000, Loss: 15.8743, Val Loss: 16.2275\n",
      "Epoch 147/1000, Loss: 15.8887, Val Loss: 16.2184\n",
      "Epoch 148/1000, Loss: 15.8878, Val Loss: 16.2097\n",
      "Epoch 149/1000, Loss: 15.9081, Val Loss: 16.1997\n",
      "Epoch 150/1000, Loss: 15.8810, Val Loss: 16.1905\n",
      "Epoch 151/1000, Loss: 15.9197, Val Loss: 16.1816\n",
      "Epoch 152/1000, Loss: 15.8936, Val Loss: 16.1744\n",
      "Epoch 153/1000, Loss: 15.8723, Val Loss: 16.1696\n",
      "Epoch 154/1000, Loss: 15.9393, Val Loss: 16.1685\n",
      "Epoch 155/1000, Loss: 15.9207, Val Loss: 16.1664\n",
      "Epoch 156/1000, Loss: 15.9364, Val Loss: 16.1664\n",
      "Epoch 157/1000, Loss: 15.8700, Val Loss: 16.1648\n",
      "Epoch 158/1000, Loss: 15.8667, Val Loss: 16.1668\n",
      "Epoch 159/1000, Loss: 15.8607, Val Loss: 16.1694\n",
      "Epoch 160/1000, Loss: 15.8760, Val Loss: 16.1707\n",
      "Epoch 161/1000, Loss: 15.8804, Val Loss: 16.1760\n",
      "Epoch 162/1000, Loss: 15.8868, Val Loss: 16.1830\n",
      "Epoch 163/1000, Loss: 15.8574, Val Loss: 16.1858\n",
      "Epoch 164/1000, Loss: 15.9087, Val Loss: 16.1828\n",
      "Epoch 165/1000, Loss: 15.8888, Val Loss: 16.1803\n",
      "Epoch 166/1000, Loss: 15.8975, Val Loss: 16.1809\n",
      "Epoch 167/1000, Loss: 15.8369, Val Loss: 16.1795\n",
      "Epoch 168/1000, Loss: 15.9032, Val Loss: 16.1775\n",
      "Epoch 169/1000, Loss: 15.8554, Val Loss: 16.1717\n",
      "Epoch 170/1000, Loss: 15.8803, Val Loss: 16.1675\n",
      "Epoch 171/1000, Loss: 15.9174, Val Loss: 16.1648\n",
      "Epoch 172/1000, Loss: 15.8544, Val Loss: 16.1647\n",
      "Epoch 173/1000, Loss: 15.8224, Val Loss: 16.1638\n",
      "Epoch 174/1000, Loss: 15.8837, Val Loss: 16.1643\n",
      "Epoch 175/1000, Loss: 15.8282, Val Loss: 16.1699\n",
      "Epoch 176/1000, Loss: 15.8441, Val Loss: 16.1772\n",
      "Epoch 177/1000, Loss: 15.8064, Val Loss: 16.1789\n",
      "Epoch 178/1000, Loss: 15.8731, Val Loss: 16.1873\n",
      "Epoch 179/1000, Loss: 15.8419, Val Loss: 16.1891\n",
      "Epoch 180/1000, Loss: 15.8689, Val Loss: 16.1893\n",
      "Epoch 181/1000, Loss: 15.8326, Val Loss: 16.1826\n",
      "Epoch 182/1000, Loss: 15.8865, Val Loss: 16.1720\n",
      "Epoch 183/1000, Loss: 15.8321, Val Loss: 16.1627\n",
      "Epoch 184/1000, Loss: 15.8425, Val Loss: 16.1572\n",
      "Epoch 185/1000, Loss: 15.9058, Val Loss: 16.1502\n",
      "Epoch 186/1000, Loss: 15.8624, Val Loss: 16.1534\n",
      "Epoch 187/1000, Loss: 15.7729, Val Loss: 16.1563\n",
      "Epoch 188/1000, Loss: 15.8404, Val Loss: 16.1618\n",
      "Epoch 189/1000, Loss: 15.8505, Val Loss: 16.1677\n",
      "Epoch 190/1000, Loss: 15.8491, Val Loss: 16.1686\n",
      "Epoch 191/1000, Loss: 15.8112, Val Loss: 16.1615\n",
      "Epoch 192/1000, Loss: 15.8054, Val Loss: 16.1534\n",
      "Epoch 193/1000, Loss: 15.8343, Val Loss: 16.1414\n",
      "Epoch 194/1000, Loss: 15.8536, Val Loss: 16.1357\n",
      "Epoch 195/1000, Loss: 15.8468, Val Loss: 16.1321\n",
      "Epoch 196/1000, Loss: 15.9212, Val Loss: 16.1311\n",
      "Epoch 197/1000, Loss: 15.8247, Val Loss: 16.1342\n",
      "Epoch 198/1000, Loss: 15.7771, Val Loss: 16.1417\n",
      "Epoch 199/1000, Loss: 15.7918, Val Loss: 16.1493\n",
      "Epoch 200/1000, Loss: 15.7837, Val Loss: 16.1630\n",
      "Epoch 201/1000, Loss: 15.8245, Val Loss: 16.1757\n",
      "Epoch 202/1000, Loss: 15.8390, Val Loss: 16.1871\n",
      "Epoch 203/1000, Loss: 15.8309, Val Loss: 16.1786\n",
      "Epoch 204/1000, Loss: 15.8132, Val Loss: 16.1727\n",
      "Epoch 205/1000, Loss: 15.8081, Val Loss: 16.1656\n",
      "Epoch 206/1000, Loss: 15.7587, Val Loss: 16.1557\n",
      "Epoch 207/1000, Loss: 15.8552, Val Loss: 16.1492\n",
      "Epoch 208/1000, Loss: 15.8102, Val Loss: 16.1487\n",
      "Epoch 209/1000, Loss: 15.8483, Val Loss: 16.1467\n",
      "Epoch 210/1000, Loss: 15.8667, Val Loss: 16.1509\n",
      "Epoch 211/1000, Loss: 15.8092, Val Loss: 16.1581\n",
      "Epoch 212/1000, Loss: 15.7478, Val Loss: 16.1765\n",
      "Epoch 213/1000, Loss: 15.8273, Val Loss: 16.1956\n",
      "Epoch 214/1000, Loss: 15.7892, Val Loss: 16.2051\n",
      "Epoch 215/1000, Loss: 15.7648, Val Loss: 16.1990\n",
      "Epoch 216/1000, Loss: 15.7758, Val Loss: 16.1789\n",
      "Epoch 217/1000, Loss: 15.7748, Val Loss: 16.1610\n",
      "Epoch 218/1000, Loss: 15.7727, Val Loss: 16.1506\n",
      "Epoch 219/1000, Loss: 15.8033, Val Loss: 16.1461\n",
      "Epoch 220/1000, Loss: 15.7205, Val Loss: 16.1561\n",
      "Epoch 221/1000, Loss: 15.7980, Val Loss: 16.1711\n",
      "Epoch 222/1000, Loss: 15.7734, Val Loss: 16.1773\n",
      "Epoch 223/1000, Loss: 15.8168, Val Loss: 16.1874\n",
      "Epoch 224/1000, Loss: 15.7458, Val Loss: 16.1996\n",
      "Epoch 225/1000, Loss: 15.7719, Val Loss: 16.1996\n",
      "Epoch 226/1000, Loss: 15.8306, Val Loss: 16.1969\n",
      "Epoch 227/1000, Loss: 15.8332, Val Loss: 16.2025\n",
      "Epoch 228/1000, Loss: 15.7023, Val Loss: 16.2055\n",
      "Epoch 229/1000, Loss: 15.7809, Val Loss: 16.2010\n",
      "Epoch 230/1000, Loss: 15.7785, Val Loss: 16.1988\n",
      "Epoch 231/1000, Loss: 15.7528, Val Loss: 16.2034\n",
      "Epoch 232/1000, Loss: 15.8032, Val Loss: 16.1995\n",
      "Epoch 233/1000, Loss: 15.8879, Val Loss: 16.1950\n",
      "Epoch 234/1000, Loss: 15.7744, Val Loss: 16.1990\n",
      "Epoch 235/1000, Loss: 15.8065, Val Loss: 16.1968\n",
      "Epoch 236/1000, Loss: 15.8493, Val Loss: 16.2006\n",
      "Early stopping triggered at epoch 236. Best Val Loss: 16.1311\n",
      "Epoch 1/1000, Loss: 17.0085, Val Loss: 17.3372\n",
      "Epoch 2/1000, Loss: 17.0036, Val Loss: 17.3337\n",
      "Epoch 3/1000, Loss: 16.9917, Val Loss: 17.3302\n",
      "Epoch 4/1000, Loss: 16.9895, Val Loss: 17.3266\n",
      "Epoch 5/1000, Loss: 16.9802, Val Loss: 17.3231\n",
      "Epoch 6/1000, Loss: 16.9774, Val Loss: 17.3195\n",
      "Epoch 7/1000, Loss: 16.9613, Val Loss: 17.3158\n",
      "Epoch 8/1000, Loss: 16.9656, Val Loss: 17.3120\n",
      "Epoch 9/1000, Loss: 16.9538, Val Loss: 17.3085\n",
      "Epoch 10/1000, Loss: 16.9505, Val Loss: 17.3052\n",
      "Epoch 11/1000, Loss: 16.9389, Val Loss: 17.3021\n",
      "Epoch 12/1000, Loss: 16.9329, Val Loss: 17.2987\n",
      "Epoch 13/1000, Loss: 16.9170, Val Loss: 17.2952\n",
      "Epoch 14/1000, Loss: 16.9112, Val Loss: 17.2921\n",
      "Epoch 15/1000, Loss: 16.9194, Val Loss: 17.2888\n",
      "Epoch 16/1000, Loss: 16.8928, Val Loss: 17.2853\n",
      "Epoch 17/1000, Loss: 16.9007, Val Loss: 17.2817\n",
      "Epoch 18/1000, Loss: 16.8769, Val Loss: 17.2778\n",
      "Epoch 19/1000, Loss: 16.8558, Val Loss: 17.2733\n",
      "Epoch 20/1000, Loss: 16.8581, Val Loss: 17.2690\n",
      "Epoch 21/1000, Loss: 16.8611, Val Loss: 17.2634\n",
      "Epoch 22/1000, Loss: 16.8489, Val Loss: 17.2582\n",
      "Epoch 23/1000, Loss: 16.8246, Val Loss: 17.2526\n",
      "Epoch 24/1000, Loss: 16.8124, Val Loss: 17.2462\n",
      "Epoch 25/1000, Loss: 16.7898, Val Loss: 17.2394\n",
      "Epoch 26/1000, Loss: 16.7996, Val Loss: 17.2331\n",
      "Epoch 27/1000, Loss: 16.8004, Val Loss: 17.2277\n",
      "Epoch 28/1000, Loss: 16.7751, Val Loss: 17.2217\n",
      "Epoch 29/1000, Loss: 16.7815, Val Loss: 17.2157\n",
      "Epoch 30/1000, Loss: 16.7439, Val Loss: 17.2101\n",
      "Epoch 31/1000, Loss: 16.7621, Val Loss: 17.2042\n",
      "Epoch 32/1000, Loss: 16.7428, Val Loss: 17.1985\n",
      "Epoch 33/1000, Loss: 16.7475, Val Loss: 17.1929\n",
      "Epoch 34/1000, Loss: 16.7463, Val Loss: 17.1865\n",
      "Epoch 35/1000, Loss: 16.7357, Val Loss: 17.1804\n",
      "Epoch 36/1000, Loss: 16.7057, Val Loss: 17.1730\n",
      "Epoch 37/1000, Loss: 16.7351, Val Loss: 17.1654\n",
      "Epoch 38/1000, Loss: 16.7357, Val Loss: 17.1606\n",
      "Epoch 39/1000, Loss: 16.6908, Val Loss: 17.1531\n",
      "Epoch 40/1000, Loss: 16.6936, Val Loss: 17.1455\n",
      "Epoch 41/1000, Loss: 16.6458, Val Loss: 17.1349\n",
      "Epoch 42/1000, Loss: 16.6554, Val Loss: 17.1240\n",
      "Epoch 43/1000, Loss: 16.6752, Val Loss: 17.1123\n",
      "Epoch 44/1000, Loss: 16.6409, Val Loss: 17.1019\n",
      "Epoch 45/1000, Loss: 16.6499, Val Loss: 17.0935\n",
      "Epoch 46/1000, Loss: 16.6437, Val Loss: 17.0855\n",
      "Epoch 47/1000, Loss: 16.6456, Val Loss: 17.0765\n",
      "Epoch 48/1000, Loss: 16.6359, Val Loss: 17.0681\n",
      "Epoch 49/1000, Loss: 16.5544, Val Loss: 17.0595\n",
      "Epoch 50/1000, Loss: 16.5771, Val Loss: 17.0491\n",
      "Epoch 51/1000, Loss: 16.6139, Val Loss: 17.0399\n",
      "Epoch 52/1000, Loss: 16.5623, Val Loss: 17.0290\n",
      "Epoch 53/1000, Loss: 16.5324, Val Loss: 17.0152\n",
      "Epoch 54/1000, Loss: 16.5471, Val Loss: 17.0002\n",
      "Epoch 55/1000, Loss: 16.5323, Val Loss: 16.9855\n",
      "Epoch 56/1000, Loss: 16.5236, Val Loss: 16.9712\n",
      "Epoch 57/1000, Loss: 16.4645, Val Loss: 16.9564\n",
      "Epoch 58/1000, Loss: 16.4772, Val Loss: 16.9426\n",
      "Epoch 59/1000, Loss: 16.4893, Val Loss: 16.9258\n",
      "Epoch 60/1000, Loss: 16.4720, Val Loss: 16.9104\n",
      "Epoch 61/1000, Loss: 16.4859, Val Loss: 16.8955\n",
      "Epoch 62/1000, Loss: 16.4401, Val Loss: 16.8789\n",
      "Epoch 63/1000, Loss: 16.4443, Val Loss: 16.8607\n",
      "Epoch 64/1000, Loss: 16.4421, Val Loss: 16.8431\n",
      "Epoch 65/1000, Loss: 16.3884, Val Loss: 16.8258\n",
      "Epoch 66/1000, Loss: 16.4019, Val Loss: 16.8055\n",
      "Epoch 67/1000, Loss: 16.3757, Val Loss: 16.7868\n",
      "Epoch 68/1000, Loss: 16.3614, Val Loss: 16.7680\n",
      "Epoch 69/1000, Loss: 16.3611, Val Loss: 16.7471\n",
      "Epoch 70/1000, Loss: 16.2998, Val Loss: 16.7290\n",
      "Epoch 71/1000, Loss: 16.3088, Val Loss: 16.7128\n",
      "Epoch 72/1000, Loss: 16.2789, Val Loss: 16.6967\n",
      "Epoch 73/1000, Loss: 16.2679, Val Loss: 16.6821\n",
      "Epoch 74/1000, Loss: 16.2422, Val Loss: 16.6675\n",
      "Epoch 75/1000, Loss: 16.1864, Val Loss: 16.6511\n",
      "Epoch 76/1000, Loss: 16.1796, Val Loss: 16.6388\n",
      "Epoch 77/1000, Loss: 16.1805, Val Loss: 16.6212\n",
      "Epoch 78/1000, Loss: 16.1544, Val Loss: 16.6038\n",
      "Epoch 79/1000, Loss: 16.1208, Val Loss: 16.5864\n",
      "Epoch 80/1000, Loss: 16.0899, Val Loss: 16.5650\n",
      "Epoch 81/1000, Loss: 16.0945, Val Loss: 16.5416\n",
      "Epoch 82/1000, Loss: 16.0413, Val Loss: 16.5109\n",
      "Epoch 83/1000, Loss: 16.0659, Val Loss: 16.4851\n",
      "Epoch 84/1000, Loss: 16.0224, Val Loss: 16.4610\n",
      "Epoch 85/1000, Loss: 15.9739, Val Loss: 16.4430\n",
      "Epoch 86/1000, Loss: 16.0067, Val Loss: 16.4265\n",
      "Epoch 87/1000, Loss: 16.0468, Val Loss: 16.4084\n",
      "Epoch 88/1000, Loss: 15.9655, Val Loss: 16.3941\n",
      "Epoch 89/1000, Loss: 15.9237, Val Loss: 16.3725\n",
      "Epoch 90/1000, Loss: 15.9400, Val Loss: 16.3461\n",
      "Epoch 91/1000, Loss: 15.9804, Val Loss: 16.3242\n",
      "Epoch 92/1000, Loss: 15.9881, Val Loss: 16.3099\n",
      "Epoch 93/1000, Loss: 15.8957, Val Loss: 16.3000\n",
      "Epoch 94/1000, Loss: 15.8397, Val Loss: 16.2857\n",
      "Epoch 95/1000, Loss: 15.8963, Val Loss: 16.2770\n",
      "Epoch 96/1000, Loss: 15.9313, Val Loss: 16.2740\n",
      "Epoch 97/1000, Loss: 15.8739, Val Loss: 16.2705\n",
      "Epoch 98/1000, Loss: 15.9013, Val Loss: 16.2708\n",
      "Epoch 99/1000, Loss: 15.8673, Val Loss: 16.2663\n",
      "Epoch 100/1000, Loss: 15.8127, Val Loss: 16.2499\n",
      "Epoch 101/1000, Loss: 15.9351, Val Loss: 16.2289\n",
      "Epoch 102/1000, Loss: 15.9189, Val Loss: 16.2092\n",
      "Epoch 103/1000, Loss: 15.8859, Val Loss: 16.1997\n",
      "Epoch 104/1000, Loss: 15.8865, Val Loss: 16.1944\n",
      "Epoch 105/1000, Loss: 15.9356, Val Loss: 16.1865\n",
      "Epoch 106/1000, Loss: 15.8857, Val Loss: 16.1801\n",
      "Epoch 107/1000, Loss: 15.8873, Val Loss: 16.1733\n",
      "Epoch 108/1000, Loss: 15.9020, Val Loss: 16.1760\n",
      "Epoch 109/1000, Loss: 15.9316, Val Loss: 16.1789\n",
      "Epoch 110/1000, Loss: 15.9499, Val Loss: 16.1833\n",
      "Epoch 111/1000, Loss: 15.8822, Val Loss: 16.1834\n",
      "Epoch 112/1000, Loss: 15.8821, Val Loss: 16.1831\n",
      "Epoch 113/1000, Loss: 15.9186, Val Loss: 16.1795\n",
      "Epoch 114/1000, Loss: 15.8950, Val Loss: 16.1754\n",
      "Epoch 115/1000, Loss: 15.8608, Val Loss: 16.1688\n",
      "Epoch 116/1000, Loss: 15.8430, Val Loss: 16.1679\n",
      "Epoch 117/1000, Loss: 15.8900, Val Loss: 16.1708\n",
      "Epoch 118/1000, Loss: 15.7812, Val Loss: 16.1741\n",
      "Epoch 119/1000, Loss: 15.8219, Val Loss: 16.1825\n",
      "Epoch 120/1000, Loss: 15.8873, Val Loss: 16.1908\n",
      "Epoch 121/1000, Loss: 15.9078, Val Loss: 16.1955\n",
      "Epoch 122/1000, Loss: 15.8903, Val Loss: 16.1985\n",
      "Epoch 123/1000, Loss: 15.8395, Val Loss: 16.2038\n",
      "Epoch 124/1000, Loss: 15.8908, Val Loss: 16.2029\n",
      "Epoch 125/1000, Loss: 15.7888, Val Loss: 16.1994\n",
      "Epoch 126/1000, Loss: 15.8354, Val Loss: 16.1982\n",
      "Epoch 127/1000, Loss: 15.8125, Val Loss: 16.1974\n",
      "Epoch 128/1000, Loss: 15.8670, Val Loss: 16.1996\n",
      "Epoch 129/1000, Loss: 15.8084, Val Loss: 16.2050\n",
      "Epoch 130/1000, Loss: 15.8601, Val Loss: 16.2105\n",
      "Epoch 131/1000, Loss: 15.8255, Val Loss: 16.2152\n",
      "Epoch 132/1000, Loss: 15.8881, Val Loss: 16.2121\n",
      "Epoch 133/1000, Loss: 15.8473, Val Loss: 16.2013\n",
      "Epoch 134/1000, Loss: 15.8512, Val Loss: 16.1906\n",
      "Epoch 135/1000, Loss: 15.8797, Val Loss: 16.1824\n",
      "Epoch 136/1000, Loss: 15.9028, Val Loss: 16.1762\n",
      "Epoch 137/1000, Loss: 15.9033, Val Loss: 16.1724\n",
      "Epoch 138/1000, Loss: 15.9311, Val Loss: 16.1709\n",
      "Epoch 139/1000, Loss: 15.8890, Val Loss: 16.1715\n",
      "Epoch 140/1000, Loss: 15.8572, Val Loss: 16.1711\n",
      "Epoch 141/1000, Loss: 15.8501, Val Loss: 16.1776\n",
      "Epoch 142/1000, Loss: 15.7566, Val Loss: 16.1857\n",
      "Epoch 143/1000, Loss: 15.7895, Val Loss: 16.1930\n",
      "Epoch 144/1000, Loss: 15.8850, Val Loss: 16.1944\n",
      "Epoch 145/1000, Loss: 15.8787, Val Loss: 16.1897\n",
      "Epoch 146/1000, Loss: 15.8664, Val Loss: 16.1811\n",
      "Epoch 147/1000, Loss: 15.8822, Val Loss: 16.1746\n",
      "Epoch 148/1000, Loss: 15.7887, Val Loss: 16.1685\n",
      "Epoch 149/1000, Loss: 15.9082, Val Loss: 16.1665\n",
      "Epoch 150/1000, Loss: 15.8120, Val Loss: 16.1648\n",
      "Epoch 151/1000, Loss: 15.8391, Val Loss: 16.1656\n",
      "Epoch 152/1000, Loss: 15.8315, Val Loss: 16.1725\n",
      "Epoch 153/1000, Loss: 15.8019, Val Loss: 16.1784\n",
      "Epoch 154/1000, Loss: 15.7147, Val Loss: 16.1925\n",
      "Epoch 155/1000, Loss: 15.8373, Val Loss: 16.2018\n",
      "Epoch 156/1000, Loss: 15.8102, Val Loss: 16.2037\n",
      "Epoch 157/1000, Loss: 15.8159, Val Loss: 16.2047\n",
      "Epoch 158/1000, Loss: 15.8548, Val Loss: 16.2060\n",
      "Epoch 159/1000, Loss: 15.8159, Val Loss: 16.2042\n",
      "Epoch 160/1000, Loss: 15.8680, Val Loss: 16.2035\n",
      "Epoch 161/1000, Loss: 15.8168, Val Loss: 16.2003\n",
      "Epoch 162/1000, Loss: 15.8120, Val Loss: 16.2009\n",
      "Epoch 163/1000, Loss: 15.8762, Val Loss: 16.1996\n",
      "Epoch 164/1000, Loss: 15.8128, Val Loss: 16.1971\n",
      "Epoch 165/1000, Loss: 15.8348, Val Loss: 16.1940\n",
      "Epoch 166/1000, Loss: 15.7977, Val Loss: 16.1881\n",
      "Epoch 167/1000, Loss: 15.8406, Val Loss: 16.1868\n",
      "Epoch 168/1000, Loss: 15.8475, Val Loss: 16.1838\n",
      "Epoch 169/1000, Loss: 15.8075, Val Loss: 16.1778\n",
      "Epoch 170/1000, Loss: 15.7547, Val Loss: 16.1740\n",
      "Epoch 171/1000, Loss: 15.8119, Val Loss: 16.1710\n",
      "Epoch 172/1000, Loss: 15.8595, Val Loss: 16.1749\n",
      "Epoch 173/1000, Loss: 15.8300, Val Loss: 16.1771\n",
      "Epoch 174/1000, Loss: 15.8238, Val Loss: 16.1814\n",
      "Epoch 175/1000, Loss: 15.8305, Val Loss: 16.1838\n",
      "Epoch 176/1000, Loss: 15.7792, Val Loss: 16.1881\n",
      "Epoch 177/1000, Loss: 15.7999, Val Loss: 16.2000\n",
      "Epoch 178/1000, Loss: 15.7439, Val Loss: 16.2052\n",
      "Epoch 179/1000, Loss: 15.8002, Val Loss: 16.2105\n",
      "Epoch 180/1000, Loss: 15.7690, Val Loss: 16.2146\n",
      "Epoch 181/1000, Loss: 15.7294, Val Loss: 16.2134\n",
      "Epoch 182/1000, Loss: 15.8447, Val Loss: 16.2129\n",
      "Epoch 183/1000, Loss: 15.7762, Val Loss: 16.2097\n",
      "Epoch 184/1000, Loss: 15.7802, Val Loss: 16.2107\n",
      "Epoch 185/1000, Loss: 15.8193, Val Loss: 16.2102\n",
      "Epoch 186/1000, Loss: 15.7508, Val Loss: 16.2115\n",
      "Epoch 187/1000, Loss: 15.8217, Val Loss: 16.2103\n",
      "Epoch 188/1000, Loss: 15.7751, Val Loss: 16.2171\n",
      "Epoch 189/1000, Loss: 15.8048, Val Loss: 16.2249\n",
      "Epoch 190/1000, Loss: 15.7167, Val Loss: 16.2367\n",
      "Early stopping triggered at epoch 190. Best Val Loss: 16.1648\n",
      "Epoch 1/1000, Loss: 16.9313, Val Loss: 17.1949\n",
      "Epoch 2/1000, Loss: 16.9099, Val Loss: 17.1915\n",
      "Epoch 3/1000, Loss: 16.9037, Val Loss: 17.1875\n",
      "Epoch 4/1000, Loss: 16.8973, Val Loss: 17.1831\n",
      "Epoch 5/1000, Loss: 16.8917, Val Loss: 17.1781\n",
      "Epoch 6/1000, Loss: 16.8819, Val Loss: 17.1733\n",
      "Epoch 7/1000, Loss: 16.8678, Val Loss: 17.1677\n",
      "Epoch 8/1000, Loss: 16.8508, Val Loss: 17.1625\n",
      "Epoch 9/1000, Loss: 16.8559, Val Loss: 17.1578\n",
      "Epoch 10/1000, Loss: 16.8450, Val Loss: 17.1532\n",
      "Epoch 11/1000, Loss: 16.8329, Val Loss: 17.1487\n",
      "Epoch 12/1000, Loss: 16.8302, Val Loss: 17.1442\n",
      "Epoch 13/1000, Loss: 16.8179, Val Loss: 17.1399\n",
      "Epoch 14/1000, Loss: 16.8168, Val Loss: 17.1349\n",
      "Epoch 15/1000, Loss: 16.8010, Val Loss: 17.1292\n",
      "Epoch 16/1000, Loss: 16.7955, Val Loss: 17.1223\n",
      "Epoch 17/1000, Loss: 16.7787, Val Loss: 17.1142\n",
      "Epoch 18/1000, Loss: 16.7620, Val Loss: 17.1060\n",
      "Epoch 19/1000, Loss: 16.7511, Val Loss: 17.0973\n",
      "Epoch 20/1000, Loss: 16.7383, Val Loss: 17.0877\n",
      "Epoch 21/1000, Loss: 16.7245, Val Loss: 17.0784\n",
      "Epoch 22/1000, Loss: 16.7118, Val Loss: 17.0687\n",
      "Epoch 23/1000, Loss: 16.7039, Val Loss: 17.0585\n",
      "Epoch 24/1000, Loss: 16.6924, Val Loss: 17.0495\n",
      "Epoch 25/1000, Loss: 16.6791, Val Loss: 17.0397\n",
      "Epoch 26/1000, Loss: 16.6612, Val Loss: 17.0277\n",
      "Epoch 27/1000, Loss: 16.6557, Val Loss: 17.0170\n",
      "Epoch 28/1000, Loss: 16.6430, Val Loss: 17.0052\n",
      "Epoch 29/1000, Loss: 16.6219, Val Loss: 16.9951\n",
      "Epoch 30/1000, Loss: 16.6162, Val Loss: 16.9861\n",
      "Epoch 31/1000, Loss: 16.5948, Val Loss: 16.9768\n",
      "Epoch 32/1000, Loss: 16.5825, Val Loss: 16.9683\n",
      "Epoch 33/1000, Loss: 16.5666, Val Loss: 16.9593\n",
      "Epoch 34/1000, Loss: 16.5272, Val Loss: 16.9508\n",
      "Epoch 35/1000, Loss: 16.5142, Val Loss: 16.9426\n",
      "Epoch 36/1000, Loss: 16.5076, Val Loss: 16.9346\n",
      "Epoch 37/1000, Loss: 16.5019, Val Loss: 16.9258\n",
      "Epoch 38/1000, Loss: 16.4655, Val Loss: 16.9158\n",
      "Epoch 39/1000, Loss: 16.4465, Val Loss: 16.9060\n",
      "Epoch 40/1000, Loss: 16.4210, Val Loss: 16.8954\n",
      "Epoch 41/1000, Loss: 16.3798, Val Loss: 16.8811\n",
      "Epoch 42/1000, Loss: 16.3814, Val Loss: 16.8670\n",
      "Epoch 43/1000, Loss: 16.3311, Val Loss: 16.8527\n",
      "Epoch 44/1000, Loss: 16.3493, Val Loss: 16.8389\n",
      "Epoch 45/1000, Loss: 16.3171, Val Loss: 16.8233\n",
      "Epoch 46/1000, Loss: 16.2969, Val Loss: 16.8071\n",
      "Epoch 47/1000, Loss: 16.2905, Val Loss: 16.7897\n",
      "Epoch 48/1000, Loss: 16.2542, Val Loss: 16.7745\n",
      "Epoch 49/1000, Loss: 16.2912, Val Loss: 16.7590\n",
      "Epoch 50/1000, Loss: 16.2197, Val Loss: 16.7395\n",
      "Epoch 51/1000, Loss: 16.2447, Val Loss: 16.7179\n",
      "Epoch 52/1000, Loss: 16.2418, Val Loss: 16.6978\n",
      "Epoch 53/1000, Loss: 16.1499, Val Loss: 16.6750\n",
      "Epoch 54/1000, Loss: 16.1587, Val Loss: 16.6534\n",
      "Epoch 55/1000, Loss: 16.1800, Val Loss: 16.6364\n",
      "Epoch 56/1000, Loss: 16.1132, Val Loss: 16.6207\n",
      "Epoch 57/1000, Loss: 16.1436, Val Loss: 16.6105\n",
      "Epoch 58/1000, Loss: 16.0906, Val Loss: 16.6014\n",
      "Epoch 59/1000, Loss: 16.0501, Val Loss: 16.5914\n",
      "Epoch 60/1000, Loss: 16.0381, Val Loss: 16.5793\n",
      "Epoch 61/1000, Loss: 16.0794, Val Loss: 16.5619\n",
      "Epoch 62/1000, Loss: 16.0730, Val Loss: 16.5419\n",
      "Epoch 63/1000, Loss: 16.0445, Val Loss: 16.5223\n",
      "Epoch 64/1000, Loss: 16.0293, Val Loss: 16.5045\n",
      "Epoch 65/1000, Loss: 16.0214, Val Loss: 16.4904\n",
      "Epoch 66/1000, Loss: 15.9842, Val Loss: 16.4811\n",
      "Epoch 67/1000, Loss: 15.9792, Val Loss: 16.4756\n",
      "Epoch 68/1000, Loss: 16.0335, Val Loss: 16.4706\n",
      "Epoch 69/1000, Loss: 15.9852, Val Loss: 16.4598\n",
      "Epoch 70/1000, Loss: 15.9741, Val Loss: 16.4481\n",
      "Epoch 71/1000, Loss: 15.9722, Val Loss: 16.4298\n",
      "Epoch 72/1000, Loss: 16.0293, Val Loss: 16.4137\n",
      "Epoch 73/1000, Loss: 15.9470, Val Loss: 16.4023\n",
      "Epoch 74/1000, Loss: 15.9496, Val Loss: 16.3916\n",
      "Epoch 75/1000, Loss: 15.8911, Val Loss: 16.3806\n",
      "Epoch 76/1000, Loss: 15.8973, Val Loss: 16.3754\n",
      "Epoch 77/1000, Loss: 15.9550, Val Loss: 16.3709\n",
      "Epoch 78/1000, Loss: 15.9285, Val Loss: 16.3697\n",
      "Epoch 79/1000, Loss: 15.9615, Val Loss: 16.3651\n",
      "Epoch 80/1000, Loss: 15.9154, Val Loss: 16.3576\n",
      "Epoch 81/1000, Loss: 15.9626, Val Loss: 16.3524\n",
      "Epoch 82/1000, Loss: 15.9322, Val Loss: 16.3474\n",
      "Epoch 83/1000, Loss: 15.9122, Val Loss: 16.3421\n",
      "Epoch 84/1000, Loss: 15.8751, Val Loss: 16.3357\n",
      "Epoch 85/1000, Loss: 15.8827, Val Loss: 16.3266\n",
      "Epoch 86/1000, Loss: 15.8830, Val Loss: 16.3161\n",
      "Epoch 87/1000, Loss: 15.8709, Val Loss: 16.3092\n",
      "Epoch 88/1000, Loss: 15.8824, Val Loss: 16.3010\n",
      "Epoch 89/1000, Loss: 15.8775, Val Loss: 16.2926\n",
      "Epoch 90/1000, Loss: 15.8530, Val Loss: 16.2862\n",
      "Epoch 91/1000, Loss: 15.8956, Val Loss: 16.2800\n",
      "Epoch 92/1000, Loss: 15.9016, Val Loss: 16.2754\n",
      "Epoch 93/1000, Loss: 15.9465, Val Loss: 16.2668\n",
      "Epoch 94/1000, Loss: 15.8737, Val Loss: 16.2591\n",
      "Epoch 95/1000, Loss: 15.8523, Val Loss: 16.2537\n",
      "Epoch 96/1000, Loss: 15.8601, Val Loss: 16.2546\n",
      "Epoch 97/1000, Loss: 15.9022, Val Loss: 16.2517\n",
      "Epoch 98/1000, Loss: 15.8509, Val Loss: 16.2474\n",
      "Epoch 99/1000, Loss: 15.8329, Val Loss: 16.2364\n",
      "Epoch 100/1000, Loss: 15.8378, Val Loss: 16.2273\n",
      "Epoch 101/1000, Loss: 15.8598, Val Loss: 16.2198\n",
      "Epoch 102/1000, Loss: 15.8615, Val Loss: 16.2141\n",
      "Epoch 103/1000, Loss: 15.8588, Val Loss: 16.2081\n",
      "Epoch 104/1000, Loss: 15.8704, Val Loss: 16.2037\n",
      "Epoch 105/1000, Loss: 15.8124, Val Loss: 16.2005\n",
      "Epoch 106/1000, Loss: 15.7976, Val Loss: 16.1966\n",
      "Epoch 107/1000, Loss: 15.7938, Val Loss: 16.1948\n",
      "Epoch 108/1000, Loss: 15.8525, Val Loss: 16.1901\n",
      "Epoch 109/1000, Loss: 15.8344, Val Loss: 16.1863\n",
      "Epoch 110/1000, Loss: 15.8272, Val Loss: 16.1802\n",
      "Epoch 111/1000, Loss: 15.8946, Val Loss: 16.1768\n",
      "Epoch 112/1000, Loss: 15.8750, Val Loss: 16.1769\n",
      "Epoch 113/1000, Loss: 15.8533, Val Loss: 16.1784\n",
      "Epoch 114/1000, Loss: 15.8162, Val Loss: 16.1861\n",
      "Epoch 115/1000, Loss: 15.8904, Val Loss: 16.1915\n",
      "Epoch 116/1000, Loss: 15.8707, Val Loss: 16.1958\n",
      "Epoch 117/1000, Loss: 15.8642, Val Loss: 16.2004\n",
      "Epoch 118/1000, Loss: 15.8546, Val Loss: 16.2017\n",
      "Epoch 119/1000, Loss: 15.8757, Val Loss: 16.1990\n",
      "Epoch 120/1000, Loss: 15.8836, Val Loss: 16.1939\n",
      "Epoch 121/1000, Loss: 15.9032, Val Loss: 16.1844\n",
      "Epoch 122/1000, Loss: 15.8551, Val Loss: 16.1707\n",
      "Epoch 123/1000, Loss: 15.8707, Val Loss: 16.1621\n",
      "Epoch 124/1000, Loss: 15.9040, Val Loss: 16.1555\n",
      "Epoch 125/1000, Loss: 15.8356, Val Loss: 16.1557\n",
      "Epoch 126/1000, Loss: 15.7800, Val Loss: 16.1591\n",
      "Epoch 127/1000, Loss: 15.8432, Val Loss: 16.1637\n",
      "Epoch 128/1000, Loss: 15.8097, Val Loss: 16.1665\n",
      "Epoch 129/1000, Loss: 15.8274, Val Loss: 16.1698\n",
      "Epoch 130/1000, Loss: 15.7921, Val Loss: 16.1736\n",
      "Epoch 131/1000, Loss: 15.8396, Val Loss: 16.1775\n",
      "Epoch 132/1000, Loss: 15.7998, Val Loss: 16.1781\n",
      "Epoch 133/1000, Loss: 15.7984, Val Loss: 16.1788\n",
      "Epoch 134/1000, Loss: 15.8006, Val Loss: 16.1792\n",
      "Epoch 135/1000, Loss: 15.8479, Val Loss: 16.1795\n",
      "Epoch 136/1000, Loss: 15.8277, Val Loss: 16.1758\n",
      "Epoch 137/1000, Loss: 15.8107, Val Loss: 16.1732\n",
      "Epoch 138/1000, Loss: 15.7603, Val Loss: 16.1685\n",
      "Epoch 139/1000, Loss: 15.8810, Val Loss: 16.1622\n",
      "Epoch 140/1000, Loss: 15.7657, Val Loss: 16.1591\n",
      "Epoch 141/1000, Loss: 15.8129, Val Loss: 16.1612\n",
      "Epoch 142/1000, Loss: 15.8495, Val Loss: 16.1633\n",
      "Epoch 143/1000, Loss: 15.7430, Val Loss: 16.1667\n",
      "Epoch 144/1000, Loss: 15.8643, Val Loss: 16.1712\n",
      "Epoch 145/1000, Loss: 15.8832, Val Loss: 16.1749\n",
      "Epoch 146/1000, Loss: 15.8490, Val Loss: 16.1790\n",
      "Epoch 147/1000, Loss: 15.7354, Val Loss: 16.1831\n",
      "Epoch 148/1000, Loss: 15.8004, Val Loss: 16.1895\n",
      "Epoch 149/1000, Loss: 15.7413, Val Loss: 16.1997\n",
      "Epoch 150/1000, Loss: 15.7217, Val Loss: 16.2075\n",
      "Epoch 151/1000, Loss: 15.8354, Val Loss: 16.2103\n",
      "Epoch 152/1000, Loss: 15.8269, Val Loss: 16.2135\n",
      "Epoch 153/1000, Loss: 15.8109, Val Loss: 16.2131\n",
      "Epoch 154/1000, Loss: 15.7867, Val Loss: 16.2099\n",
      "Epoch 155/1000, Loss: 15.7758, Val Loss: 16.1982\n",
      "Epoch 156/1000, Loss: 15.7833, Val Loss: 16.1958\n",
      "Epoch 157/1000, Loss: 15.7723, Val Loss: 16.1940\n",
      "Epoch 158/1000, Loss: 15.7880, Val Loss: 16.1939\n",
      "Epoch 159/1000, Loss: 15.7449, Val Loss: 16.1930\n",
      "Epoch 160/1000, Loss: 15.7556, Val Loss: 16.1988\n",
      "Epoch 161/1000, Loss: 15.7811, Val Loss: 16.2047\n",
      "Epoch 162/1000, Loss: 15.7699, Val Loss: 16.2031\n",
      "Epoch 163/1000, Loss: 15.7992, Val Loss: 16.2106\n",
      "Epoch 164/1000, Loss: 15.7551, Val Loss: 16.2188\n",
      "Early stopping triggered at epoch 164. Best Val Loss: 16.1555\n",
      "Epoch 1/1000, Loss: 16.8629, Val Loss: 17.1537\n",
      "Epoch 2/1000, Loss: 16.8465, Val Loss: 17.1482\n",
      "Epoch 3/1000, Loss: 16.8330, Val Loss: 17.1423\n",
      "Epoch 4/1000, Loss: 16.8371, Val Loss: 17.1363\n",
      "Epoch 5/1000, Loss: 16.8169, Val Loss: 17.1304\n",
      "Epoch 6/1000, Loss: 16.8005, Val Loss: 17.1242\n",
      "Epoch 7/1000, Loss: 16.7947, Val Loss: 17.1179\n",
      "Epoch 8/1000, Loss: 16.7887, Val Loss: 17.1118\n",
      "Epoch 9/1000, Loss: 16.7680, Val Loss: 17.1043\n",
      "Epoch 10/1000, Loss: 16.7477, Val Loss: 17.0972\n",
      "Epoch 11/1000, Loss: 16.7379, Val Loss: 17.0895\n",
      "Epoch 12/1000, Loss: 16.7286, Val Loss: 17.0815\n",
      "Epoch 13/1000, Loss: 16.7184, Val Loss: 17.0734\n",
      "Epoch 14/1000, Loss: 16.7097, Val Loss: 17.0649\n",
      "Epoch 15/1000, Loss: 16.6588, Val Loss: 17.0545\n",
      "Epoch 16/1000, Loss: 16.6576, Val Loss: 17.0439\n",
      "Epoch 17/1000, Loss: 16.6403, Val Loss: 17.0307\n",
      "Epoch 18/1000, Loss: 16.6455, Val Loss: 17.0167\n",
      "Epoch 19/1000, Loss: 16.6130, Val Loss: 17.0015\n",
      "Epoch 20/1000, Loss: 16.5952, Val Loss: 16.9859\n",
      "Epoch 21/1000, Loss: 16.5868, Val Loss: 16.9716\n",
      "Epoch 22/1000, Loss: 16.5809, Val Loss: 16.9576\n",
      "Epoch 23/1000, Loss: 16.5494, Val Loss: 16.9442\n",
      "Epoch 24/1000, Loss: 16.5598, Val Loss: 16.9302\n",
      "Epoch 25/1000, Loss: 16.5202, Val Loss: 16.9149\n",
      "Epoch 26/1000, Loss: 16.5223, Val Loss: 16.8995\n",
      "Epoch 27/1000, Loss: 16.5087, Val Loss: 16.8865\n",
      "Epoch 28/1000, Loss: 16.4925, Val Loss: 16.8719\n",
      "Epoch 29/1000, Loss: 16.4862, Val Loss: 16.8560\n",
      "Epoch 30/1000, Loss: 16.4510, Val Loss: 16.8384\n",
      "Epoch 31/1000, Loss: 16.4524, Val Loss: 16.8228\n",
      "Epoch 32/1000, Loss: 16.4492, Val Loss: 16.8097\n",
      "Epoch 33/1000, Loss: 16.4263, Val Loss: 16.7969\n",
      "Epoch 34/1000, Loss: 16.4129, Val Loss: 16.7854\n",
      "Epoch 35/1000, Loss: 16.3964, Val Loss: 16.7734\n",
      "Epoch 36/1000, Loss: 16.3848, Val Loss: 16.7617\n",
      "Epoch 37/1000, Loss: 16.3881, Val Loss: 16.7491\n",
      "Epoch 38/1000, Loss: 16.3656, Val Loss: 16.7364\n",
      "Epoch 39/1000, Loss: 16.3186, Val Loss: 16.7245\n",
      "Epoch 40/1000, Loss: 16.3367, Val Loss: 16.7125\n",
      "Epoch 41/1000, Loss: 16.3204, Val Loss: 16.7017\n",
      "Epoch 42/1000, Loss: 16.3011, Val Loss: 16.6914\n",
      "Epoch 43/1000, Loss: 16.2900, Val Loss: 16.6823\n",
      "Epoch 44/1000, Loss: 16.2717, Val Loss: 16.6759\n",
      "Epoch 45/1000, Loss: 16.2877, Val Loss: 16.6657\n",
      "Epoch 46/1000, Loss: 16.2473, Val Loss: 16.6555\n",
      "Epoch 47/1000, Loss: 16.2676, Val Loss: 16.6464\n",
      "Epoch 48/1000, Loss: 16.2182, Val Loss: 16.6383\n",
      "Epoch 49/1000, Loss: 16.2062, Val Loss: 16.6321\n",
      "Epoch 50/1000, Loss: 16.2149, Val Loss: 16.6231\n",
      "Epoch 51/1000, Loss: 16.1921, Val Loss: 16.6181\n",
      "Epoch 52/1000, Loss: 16.1413, Val Loss: 16.6113\n",
      "Epoch 53/1000, Loss: 16.1252, Val Loss: 16.6043\n",
      "Epoch 54/1000, Loss: 16.1607, Val Loss: 16.5980\n",
      "Epoch 55/1000, Loss: 16.1660, Val Loss: 16.5933\n",
      "Epoch 56/1000, Loss: 16.0690, Val Loss: 16.5866\n",
      "Epoch 57/1000, Loss: 16.0954, Val Loss: 16.5781\n",
      "Epoch 58/1000, Loss: 16.0853, Val Loss: 16.5676\n",
      "Epoch 59/1000, Loss: 16.0573, Val Loss: 16.5566\n",
      "Epoch 60/1000, Loss: 16.0383, Val Loss: 16.5473\n",
      "Epoch 61/1000, Loss: 16.0265, Val Loss: 16.5397\n",
      "Epoch 62/1000, Loss: 16.0478, Val Loss: 16.5295\n",
      "Epoch 63/1000, Loss: 15.9848, Val Loss: 16.5108\n",
      "Epoch 64/1000, Loss: 16.0720, Val Loss: 16.4967\n",
      "Epoch 65/1000, Loss: 16.0171, Val Loss: 16.4857\n",
      "Epoch 66/1000, Loss: 16.0094, Val Loss: 16.4721\n",
      "Epoch 67/1000, Loss: 15.9574, Val Loss: 16.4600\n",
      "Epoch 68/1000, Loss: 15.9351, Val Loss: 16.4460\n",
      "Epoch 69/1000, Loss: 15.9697, Val Loss: 16.4384\n",
      "Epoch 70/1000, Loss: 15.9403, Val Loss: 16.4330\n",
      "Epoch 71/1000, Loss: 15.9257, Val Loss: 16.4255\n",
      "Epoch 72/1000, Loss: 15.9323, Val Loss: 16.4156\n",
      "Epoch 73/1000, Loss: 15.9484, Val Loss: 16.4047\n",
      "Epoch 74/1000, Loss: 15.9212, Val Loss: 16.3897\n",
      "Epoch 75/1000, Loss: 15.8720, Val Loss: 16.3806\n",
      "Epoch 76/1000, Loss: 15.9354, Val Loss: 16.3689\n",
      "Epoch 77/1000, Loss: 15.8464, Val Loss: 16.3571\n",
      "Epoch 78/1000, Loss: 15.9131, Val Loss: 16.3475\n",
      "Epoch 79/1000, Loss: 15.8550, Val Loss: 16.3309\n",
      "Epoch 80/1000, Loss: 15.9215, Val Loss: 16.3106\n",
      "Epoch 81/1000, Loss: 15.8428, Val Loss: 16.2939\n",
      "Epoch 82/1000, Loss: 15.8784, Val Loss: 16.2823\n",
      "Epoch 83/1000, Loss: 15.9571, Val Loss: 16.2793\n",
      "Epoch 84/1000, Loss: 15.8416, Val Loss: 16.2775\n",
      "Epoch 85/1000, Loss: 15.8949, Val Loss: 16.2809\n",
      "Epoch 86/1000, Loss: 15.8650, Val Loss: 16.2897\n",
      "Epoch 87/1000, Loss: 15.9457, Val Loss: 16.2924\n",
      "Epoch 88/1000, Loss: 15.9325, Val Loss: 16.2922\n",
      "Epoch 89/1000, Loss: 15.8894, Val Loss: 16.2965\n",
      "Epoch 90/1000, Loss: 15.9299, Val Loss: 16.2954\n",
      "Epoch 91/1000, Loss: 15.9510, Val Loss: 16.2942\n",
      "Epoch 92/1000, Loss: 15.9419, Val Loss: 16.2846\n",
      "Epoch 93/1000, Loss: 15.9113, Val Loss: 16.2742\n",
      "Epoch 94/1000, Loss: 15.8962, Val Loss: 16.2640\n",
      "Epoch 95/1000, Loss: 15.9116, Val Loss: 16.2581\n",
      "Epoch 96/1000, Loss: 15.9144, Val Loss: 16.2466\n",
      "Epoch 97/1000, Loss: 15.8438, Val Loss: 16.2353\n",
      "Epoch 98/1000, Loss: 15.9134, Val Loss: 16.2251\n",
      "Epoch 99/1000, Loss: 15.8136, Val Loss: 16.2170\n",
      "Epoch 100/1000, Loss: 15.9077, Val Loss: 16.2113\n",
      "Epoch 101/1000, Loss: 15.9137, Val Loss: 16.2118\n",
      "Epoch 102/1000, Loss: 15.8744, Val Loss: 16.2125\n",
      "Epoch 103/1000, Loss: 15.8875, Val Loss: 16.2150\n",
      "Epoch 104/1000, Loss: 15.9108, Val Loss: 16.2150\n",
      "Epoch 105/1000, Loss: 15.8330, Val Loss: 16.2138\n",
      "Epoch 106/1000, Loss: 15.9009, Val Loss: 16.2110\n",
      "Epoch 107/1000, Loss: 15.8481, Val Loss: 16.2074\n",
      "Epoch 108/1000, Loss: 15.8304, Val Loss: 16.2095\n",
      "Epoch 109/1000, Loss: 15.9128, Val Loss: 16.2115\n",
      "Epoch 110/1000, Loss: 15.8917, Val Loss: 16.2025\n",
      "Epoch 111/1000, Loss: 15.9226, Val Loss: 16.1915\n",
      "Epoch 112/1000, Loss: 15.8812, Val Loss: 16.1868\n",
      "Epoch 113/1000, Loss: 15.7916, Val Loss: 16.1832\n",
      "Epoch 114/1000, Loss: 15.8374, Val Loss: 16.1850\n",
      "Epoch 115/1000, Loss: 15.8657, Val Loss: 16.1868\n",
      "Epoch 116/1000, Loss: 15.7982, Val Loss: 16.1916\n",
      "Epoch 117/1000, Loss: 15.8433, Val Loss: 16.2031\n",
      "Epoch 118/1000, Loss: 15.8896, Val Loss: 16.2084\n",
      "Epoch 119/1000, Loss: 15.8473, Val Loss: 16.2091\n",
      "Epoch 120/1000, Loss: 15.8374, Val Loss: 16.2048\n",
      "Epoch 121/1000, Loss: 15.9488, Val Loss: 16.1942\n",
      "Epoch 122/1000, Loss: 15.8195, Val Loss: 16.1835\n",
      "Epoch 123/1000, Loss: 15.8810, Val Loss: 16.1861\n",
      "Epoch 124/1000, Loss: 15.8873, Val Loss: 16.1915\n",
      "Epoch 125/1000, Loss: 15.8271, Val Loss: 16.1946\n",
      "Epoch 126/1000, Loss: 15.8223, Val Loss: 16.1970\n",
      "Epoch 127/1000, Loss: 15.8204, Val Loss: 16.2087\n",
      "Epoch 128/1000, Loss: 15.8975, Val Loss: 16.2181\n",
      "Epoch 129/1000, Loss: 15.8445, Val Loss: 16.2255\n",
      "Epoch 130/1000, Loss: 15.8604, Val Loss: 16.2246\n",
      "Epoch 131/1000, Loss: 15.8448, Val Loss: 16.2217\n",
      "Epoch 132/1000, Loss: 15.8104, Val Loss: 16.2233\n",
      "Epoch 133/1000, Loss: 15.8255, Val Loss: 16.2280\n",
      "Epoch 134/1000, Loss: 15.7936, Val Loss: 16.2318\n",
      "Epoch 135/1000, Loss: 15.9183, Val Loss: 16.2357\n",
      "Epoch 136/1000, Loss: 15.8125, Val Loss: 16.2309\n",
      "Epoch 137/1000, Loss: 15.8719, Val Loss: 16.2290\n",
      "Epoch 138/1000, Loss: 15.8203, Val Loss: 16.2297\n",
      "Epoch 139/1000, Loss: 15.8007, Val Loss: 16.2290\n",
      "Epoch 140/1000, Loss: 15.8983, Val Loss: 16.2328\n",
      "Epoch 141/1000, Loss: 15.8708, Val Loss: 16.2345\n",
      "Epoch 142/1000, Loss: 15.8312, Val Loss: 16.2284\n",
      "Epoch 143/1000, Loss: 15.8064, Val Loss: 16.2176\n",
      "Epoch 144/1000, Loss: 15.8254, Val Loss: 16.2022\n",
      "Epoch 145/1000, Loss: 15.8530, Val Loss: 16.1988\n",
      "Epoch 146/1000, Loss: 15.8354, Val Loss: 16.2021\n",
      "Epoch 147/1000, Loss: 15.8181, Val Loss: 16.2046\n",
      "Epoch 148/1000, Loss: 15.8352, Val Loss: 16.2059\n",
      "Epoch 149/1000, Loss: 15.8383, Val Loss: 16.2059\n",
      "Epoch 150/1000, Loss: 15.8301, Val Loss: 16.2067\n",
      "Epoch 151/1000, Loss: 15.7933, Val Loss: 16.2074\n",
      "Epoch 152/1000, Loss: 15.8490, Val Loss: 16.2079\n",
      "Epoch 153/1000, Loss: 15.8641, Val Loss: 16.2106\n",
      "Early stopping triggered at epoch 153. Best Val Loss: 16.1832\n",
      "Epoch 1/1000, Loss: 16.8611, Val Loss: 17.1470\n",
      "Epoch 2/1000, Loss: 16.8525, Val Loss: 17.1437\n",
      "Epoch 3/1000, Loss: 16.8514, Val Loss: 17.1401\n",
      "Epoch 4/1000, Loss: 16.8492, Val Loss: 17.1363\n",
      "Epoch 5/1000, Loss: 16.8381, Val Loss: 17.1329\n",
      "Epoch 6/1000, Loss: 16.8296, Val Loss: 17.1291\n",
      "Epoch 7/1000, Loss: 16.8347, Val Loss: 17.1253\n",
      "Epoch 8/1000, Loss: 16.8291, Val Loss: 17.1215\n",
      "Epoch 9/1000, Loss: 16.8274, Val Loss: 17.1179\n",
      "Epoch 10/1000, Loss: 16.8090, Val Loss: 17.1139\n",
      "Epoch 11/1000, Loss: 16.8056, Val Loss: 17.1097\n",
      "Epoch 12/1000, Loss: 16.7979, Val Loss: 17.1051\n",
      "Epoch 13/1000, Loss: 16.7931, Val Loss: 17.1007\n",
      "Epoch 14/1000, Loss: 16.7711, Val Loss: 17.0960\n",
      "Epoch 15/1000, Loss: 16.7642, Val Loss: 17.0917\n",
      "Epoch 16/1000, Loss: 16.7580, Val Loss: 17.0868\n",
      "Epoch 17/1000, Loss: 16.7452, Val Loss: 17.0819\n",
      "Epoch 18/1000, Loss: 16.7250, Val Loss: 17.0771\n",
      "Epoch 19/1000, Loss: 16.7120, Val Loss: 17.0715\n",
      "Epoch 20/1000, Loss: 16.7214, Val Loss: 17.0658\n",
      "Epoch 21/1000, Loss: 16.6699, Val Loss: 17.0589\n",
      "Epoch 22/1000, Loss: 16.6781, Val Loss: 17.0516\n",
      "Epoch 23/1000, Loss: 16.6508, Val Loss: 17.0450\n",
      "Epoch 24/1000, Loss: 16.6857, Val Loss: 17.0378\n",
      "Epoch 25/1000, Loss: 16.6645, Val Loss: 17.0298\n",
      "Epoch 26/1000, Loss: 16.6424, Val Loss: 17.0232\n",
      "Epoch 27/1000, Loss: 16.6131, Val Loss: 17.0174\n",
      "Epoch 28/1000, Loss: 16.6128, Val Loss: 17.0108\n",
      "Epoch 29/1000, Loss: 16.5740, Val Loss: 17.0038\n",
      "Epoch 30/1000, Loss: 16.5861, Val Loss: 16.9972\n",
      "Epoch 31/1000, Loss: 16.5686, Val Loss: 16.9900\n",
      "Epoch 32/1000, Loss: 16.5777, Val Loss: 16.9835\n",
      "Epoch 33/1000, Loss: 16.5830, Val Loss: 16.9776\n",
      "Epoch 34/1000, Loss: 16.5482, Val Loss: 16.9700\n",
      "Epoch 35/1000, Loss: 16.5095, Val Loss: 16.9603\n",
      "Epoch 36/1000, Loss: 16.5395, Val Loss: 16.9487\n",
      "Epoch 37/1000, Loss: 16.4965, Val Loss: 16.9384\n",
      "Epoch 38/1000, Loss: 16.5483, Val Loss: 16.9324\n",
      "Epoch 39/1000, Loss: 16.5131, Val Loss: 16.9283\n",
      "Epoch 40/1000, Loss: 16.5385, Val Loss: 16.9271\n",
      "Epoch 41/1000, Loss: 16.4883, Val Loss: 16.9239\n",
      "Epoch 42/1000, Loss: 16.4960, Val Loss: 16.9183\n",
      "Epoch 43/1000, Loss: 16.5049, Val Loss: 16.9132\n",
      "Epoch 44/1000, Loss: 16.4583, Val Loss: 16.9071\n",
      "Epoch 45/1000, Loss: 16.4981, Val Loss: 16.9004\n",
      "Epoch 46/1000, Loss: 16.4611, Val Loss: 16.8955\n",
      "Epoch 47/1000, Loss: 16.4680, Val Loss: 16.8921\n",
      "Epoch 48/1000, Loss: 16.4747, Val Loss: 16.8871\n",
      "Epoch 49/1000, Loss: 16.4564, Val Loss: 16.8796\n",
      "Epoch 50/1000, Loss: 16.4572, Val Loss: 16.8723\n",
      "Epoch 51/1000, Loss: 16.4230, Val Loss: 16.8635\n",
      "Epoch 52/1000, Loss: 16.4268, Val Loss: 16.8537\n",
      "Epoch 53/1000, Loss: 16.4513, Val Loss: 16.8464\n",
      "Epoch 54/1000, Loss: 16.3969, Val Loss: 16.8386\n",
      "Epoch 55/1000, Loss: 16.3888, Val Loss: 16.8318\n",
      "Epoch 56/1000, Loss: 16.4318, Val Loss: 16.8256\n",
      "Epoch 57/1000, Loss: 16.4134, Val Loss: 16.8184\n",
      "Epoch 58/1000, Loss: 16.3791, Val Loss: 16.8114\n",
      "Epoch 59/1000, Loss: 16.3855, Val Loss: 16.8044\n",
      "Epoch 60/1000, Loss: 16.3644, Val Loss: 16.7982\n",
      "Epoch 61/1000, Loss: 16.3704, Val Loss: 16.7924\n",
      "Epoch 62/1000, Loss: 16.3461, Val Loss: 16.7861\n",
      "Epoch 63/1000, Loss: 16.3570, Val Loss: 16.7794\n",
      "Epoch 64/1000, Loss: 16.3431, Val Loss: 16.7714\n",
      "Epoch 65/1000, Loss: 16.3375, Val Loss: 16.7640\n",
      "Epoch 66/1000, Loss: 16.2924, Val Loss: 16.7553\n",
      "Epoch 67/1000, Loss: 16.3348, Val Loss: 16.7454\n",
      "Epoch 68/1000, Loss: 16.3033, Val Loss: 16.7360\n",
      "Epoch 69/1000, Loss: 16.2893, Val Loss: 16.7264\n",
      "Epoch 70/1000, Loss: 16.2546, Val Loss: 16.7166\n",
      "Epoch 71/1000, Loss: 16.2626, Val Loss: 16.7059\n",
      "Epoch 72/1000, Loss: 16.2803, Val Loss: 16.6960\n",
      "Epoch 73/1000, Loss: 16.2378, Val Loss: 16.6867\n",
      "Epoch 74/1000, Loss: 16.2230, Val Loss: 16.6757\n",
      "Epoch 75/1000, Loss: 16.2597, Val Loss: 16.6659\n",
      "Epoch 76/1000, Loss: 16.2067, Val Loss: 16.6540\n",
      "Epoch 77/1000, Loss: 16.2072, Val Loss: 16.6421\n",
      "Epoch 78/1000, Loss: 16.1738, Val Loss: 16.6299\n",
      "Epoch 79/1000, Loss: 16.1819, Val Loss: 16.6160\n",
      "Epoch 80/1000, Loss: 16.1418, Val Loss: 16.6032\n",
      "Epoch 81/1000, Loss: 16.1226, Val Loss: 16.5895\n",
      "Epoch 82/1000, Loss: 16.1161, Val Loss: 16.5775\n",
      "Epoch 83/1000, Loss: 16.1552, Val Loss: 16.5657\n",
      "Epoch 84/1000, Loss: 16.1043, Val Loss: 16.5521\n",
      "Epoch 85/1000, Loss: 16.0676, Val Loss: 16.5387\n",
      "Epoch 86/1000, Loss: 16.0771, Val Loss: 16.5206\n",
      "Epoch 87/1000, Loss: 16.0761, Val Loss: 16.5019\n",
      "Epoch 88/1000, Loss: 16.0410, Val Loss: 16.4816\n",
      "Epoch 89/1000, Loss: 16.0253, Val Loss: 16.4656\n",
      "Epoch 90/1000, Loss: 16.0056, Val Loss: 16.4528\n",
      "Epoch 91/1000, Loss: 15.9988, Val Loss: 16.4386\n",
      "Epoch 92/1000, Loss: 15.9248, Val Loss: 16.4277\n",
      "Epoch 93/1000, Loss: 16.0277, Val Loss: 16.4111\n",
      "Epoch 94/1000, Loss: 16.0062, Val Loss: 16.3882\n",
      "Epoch 95/1000, Loss: 15.9235, Val Loss: 16.3664\n",
      "Epoch 96/1000, Loss: 15.9251, Val Loss: 16.3445\n",
      "Epoch 97/1000, Loss: 15.8959, Val Loss: 16.3257\n",
      "Epoch 98/1000, Loss: 15.9678, Val Loss: 16.3057\n",
      "Epoch 99/1000, Loss: 15.9192, Val Loss: 16.2903\n",
      "Epoch 100/1000, Loss: 15.9702, Val Loss: 16.2738\n",
      "Epoch 101/1000, Loss: 15.9160, Val Loss: 16.2583\n",
      "Epoch 102/1000, Loss: 15.8712, Val Loss: 16.2471\n",
      "Epoch 103/1000, Loss: 15.8692, Val Loss: 16.2367\n",
      "Epoch 104/1000, Loss: 15.9408, Val Loss: 16.2308\n",
      "Epoch 105/1000, Loss: 15.9250, Val Loss: 16.2218\n",
      "Epoch 106/1000, Loss: 15.9109, Val Loss: 16.2161\n",
      "Epoch 107/1000, Loss: 15.8622, Val Loss: 16.2089\n",
      "Epoch 108/1000, Loss: 15.8903, Val Loss: 16.2038\n",
      "Epoch 109/1000, Loss: 15.8961, Val Loss: 16.1991\n",
      "Epoch 110/1000, Loss: 15.9375, Val Loss: 16.1955\n",
      "Epoch 111/1000, Loss: 15.8874, Val Loss: 16.1931\n",
      "Epoch 112/1000, Loss: 15.9492, Val Loss: 16.1932\n",
      "Epoch 113/1000, Loss: 15.8977, Val Loss: 16.1920\n",
      "Epoch 114/1000, Loss: 15.8443, Val Loss: 16.1890\n",
      "Epoch 115/1000, Loss: 15.9635, Val Loss: 16.1873\n",
      "Epoch 116/1000, Loss: 15.7929, Val Loss: 16.1872\n",
      "Epoch 117/1000, Loss: 15.8824, Val Loss: 16.1848\n",
      "Epoch 118/1000, Loss: 15.8515, Val Loss: 16.1840\n",
      "Epoch 119/1000, Loss: 15.8436, Val Loss: 16.1818\n",
      "Epoch 120/1000, Loss: 15.8639, Val Loss: 16.1808\n",
      "Epoch 121/1000, Loss: 15.9178, Val Loss: 16.1816\n",
      "Epoch 122/1000, Loss: 15.8920, Val Loss: 16.1821\n",
      "Epoch 123/1000, Loss: 15.8587, Val Loss: 16.1824\n",
      "Epoch 124/1000, Loss: 15.9228, Val Loss: 16.1860\n",
      "Epoch 125/1000, Loss: 15.9582, Val Loss: 16.1875\n",
      "Epoch 126/1000, Loss: 15.8618, Val Loss: 16.1892\n",
      "Epoch 127/1000, Loss: 15.8316, Val Loss: 16.1913\n",
      "Epoch 128/1000, Loss: 15.8472, Val Loss: 16.1916\n",
      "Epoch 129/1000, Loss: 15.8830, Val Loss: 16.1887\n",
      "Epoch 130/1000, Loss: 15.8830, Val Loss: 16.1892\n",
      "Epoch 131/1000, Loss: 15.9079, Val Loss: 16.1876\n",
      "Epoch 132/1000, Loss: 15.9183, Val Loss: 16.1846\n",
      "Epoch 133/1000, Loss: 15.9007, Val Loss: 16.1816\n",
      "Epoch 134/1000, Loss: 15.8888, Val Loss: 16.1785\n",
      "Epoch 135/1000, Loss: 15.9417, Val Loss: 16.1757\n",
      "Epoch 136/1000, Loss: 15.9541, Val Loss: 16.1748\n",
      "Epoch 137/1000, Loss: 15.8164, Val Loss: 16.1725\n",
      "Epoch 138/1000, Loss: 15.8984, Val Loss: 16.1722\n",
      "Epoch 139/1000, Loss: 15.8448, Val Loss: 16.1714\n",
      "Epoch 140/1000, Loss: 15.8675, Val Loss: 16.1770\n",
      "Epoch 141/1000, Loss: 15.8432, Val Loss: 16.1801\n",
      "Epoch 142/1000, Loss: 15.8833, Val Loss: 16.1867\n",
      "Epoch 143/1000, Loss: 15.8608, Val Loss: 16.1918\n",
      "Epoch 144/1000, Loss: 15.8329, Val Loss: 16.1946\n",
      "Epoch 145/1000, Loss: 15.7821, Val Loss: 16.1942\n",
      "Epoch 146/1000, Loss: 15.8338, Val Loss: 16.1915\n",
      "Epoch 147/1000, Loss: 15.8554, Val Loss: 16.1971\n",
      "Epoch 148/1000, Loss: 15.8802, Val Loss: 16.2009\n",
      "Epoch 149/1000, Loss: 15.8749, Val Loss: 16.2047\n",
      "Epoch 150/1000, Loss: 15.7565, Val Loss: 16.2122\n",
      "Epoch 151/1000, Loss: 15.8021, Val Loss: 16.2165\n",
      "Epoch 152/1000, Loss: 15.8578, Val Loss: 16.2205\n",
      "Epoch 153/1000, Loss: 15.9111, Val Loss: 16.2164\n",
      "Epoch 154/1000, Loss: 15.8720, Val Loss: 16.2108\n",
      "Epoch 155/1000, Loss: 15.8722, Val Loss: 16.1958\n",
      "Epoch 156/1000, Loss: 15.8485, Val Loss: 16.1919\n",
      "Epoch 157/1000, Loss: 15.7950, Val Loss: 16.1939\n",
      "Epoch 158/1000, Loss: 15.8633, Val Loss: 16.1991\n",
      "Epoch 159/1000, Loss: 15.8203, Val Loss: 16.2005\n",
      "Epoch 160/1000, Loss: 15.8110, Val Loss: 16.2000\n",
      "Epoch 161/1000, Loss: 15.8682, Val Loss: 16.2000\n",
      "Epoch 162/1000, Loss: 15.7830, Val Loss: 16.2071\n",
      "Epoch 163/1000, Loss: 15.8292, Val Loss: 16.2074\n",
      "Epoch 164/1000, Loss: 15.8411, Val Loss: 16.2109\n",
      "Epoch 165/1000, Loss: 15.7937, Val Loss: 16.2123\n",
      "Epoch 166/1000, Loss: 15.7960, Val Loss: 16.2146\n",
      "Epoch 167/1000, Loss: 15.7656, Val Loss: 16.2149\n",
      "Epoch 168/1000, Loss: 15.8284, Val Loss: 16.2159\n",
      "Epoch 169/1000, Loss: 15.8321, Val Loss: 16.2194\n",
      "Epoch 170/1000, Loss: 15.7482, Val Loss: 16.2334\n",
      "Epoch 171/1000, Loss: 15.7298, Val Loss: 16.2462\n",
      "Epoch 172/1000, Loss: 15.8383, Val Loss: 16.2569\n",
      "Epoch 173/1000, Loss: 15.7855, Val Loss: 16.2498\n",
      "Epoch 174/1000, Loss: 15.8074, Val Loss: 16.2430\n",
      "Epoch 175/1000, Loss: 15.8860, Val Loss: 16.2255\n",
      "Epoch 176/1000, Loss: 15.8223, Val Loss: 16.2133\n",
      "Epoch 177/1000, Loss: 15.8176, Val Loss: 16.2123\n",
      "Epoch 178/1000, Loss: 15.8877, Val Loss: 16.2135\n",
      "Epoch 179/1000, Loss: 15.8633, Val Loss: 16.2191\n",
      "Early stopping triggered at epoch 179. Best Val Loss: 16.1714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "error_one_input = []\n",
    "std_one_input = []\n",
    "max_error_one_input = []\n",
    "\n",
    "for i in range(10):\n",
    "    bias_predictor = train_bias_predictor(torch.tensor(X_train['reconstruction_error_percentage'].values.reshape(-1,1), dtype=torch.float32).to(device), \n",
    "                                          torch.tensor(y_train, dtype=torch.float32).to(device),\n",
    "                                          torch.tensor(X_val['reconstruction_error_percentage'].values.reshape(-1,1), dtype=torch.float32).to(device),\n",
    "                                          torch.tensor(y_val, dtype=torch.float32).to(device),\n",
    "                                          epochs=1000, learning_rate=0.001, patience=40)\n",
    "    features = torch.tensor(X_test['reconstruction_error_percentage'].values.reshape(-1,1), dtype=torch.float32).to(device)\n",
    "    predictions = bias_predictor(features).squeeze().cpu().detach().numpy()\n",
    "    \n",
    "    X_test['predicted_bias_with_one_input'] = np.nan\n",
    "    X_test['predicted_bias_with_one_input'] = predictions\n",
    "\n",
    "    X_test['new_predictions_with_one_input'] = X_test['predictions'] + X_test['predicted_bias_with_one_input']\n",
    "    X_test['new_error_percentage_with_one_input'] = (abs(X_test['target'] - X_test['new_predictions_with_one_input']) / X_test['target']) * 100\n",
    "    avg_error = X_test['new_error_percentage_with_one_input'].mean()\n",
    "    std_error = X_test['new_error_percentage_with_one_input'].std()\n",
    "    max_error = X_test['new_error_percentage_with_one_input'].max()\n",
    "    error_one_input.append(avg_error)\n",
    "    std_one_input.append(std_error)\n",
    "    max_error_one_input.append(max_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'reconstruction_error_percentage_array' not in X.columns:\n",
    "#     X['reconstruction_error_percentage_array'] = [[] for _ in range(len(X))]\n",
    "\n",
    "#     arr = []\n",
    "\n",
    "#     for i in range(len(X)):\n",
    "#         temp_array = []\n",
    "#         for j in range(9):\n",
    "#             temp_array.append(reconstruction_error_percentage[i+j+1])\n",
    "#         # X.at[i, 'reconstruction_error_percentage_array'] = temp_array\n",
    "#         arr.append(temp_array)\n",
    "\n",
    "#     X['reconstruction_error_percentage_array'] = arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train val test split\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BiasPredictor(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BiasPredictor, self).__init__()\n",
    "#         self.fc1 = nn.Linear(9, 256)\n",
    "#         self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "#         self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "#         self.fc3 = nn.Linear(128, 64)\n",
    "#         self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "#         self.fc4 = nn.Linear(64, 32)\n",
    "#         self.dropout4 = nn.Dropout(0.3)\n",
    "        \n",
    "#         self.fc5 = nn.Linear(32, 16)\n",
    "#         self.fc6 = nn.Linear(16, 8)\n",
    "#         self.fc7 = nn.Linear(8, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout1(x)\n",
    "        \n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.dropout2(x)\n",
    "        \n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         x = self.dropout3(x)\n",
    "        \n",
    "#         x = F.relu(self.fc4(x))\n",
    "#         x = self.dropout4(x)\n",
    "        \n",
    "#         x = F.relu(self.fc5(x))\n",
    "#         x = F.relu(self.fc6(x))\n",
    "#         return self.fc7(x)\n",
    "\n",
    "class BiasPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiasPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        # self.dropout4 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        # self.dropout5 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc6 = nn.Linear(32, 16)\n",
    "        self.fc7 = nn.Linear(16, 8)\n",
    "        self.fc8 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        # x = self.dropout4(x)\n",
    "        \n",
    "        x = F.relu(self.bn5(self.fc5(x)))\n",
    "        # x = self.dropout5(x)\n",
    "        \n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        return self.fc8(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_predictor_path = './models/bias_predictor_new.pth'\n",
    "\n",
    "# # X_train X_val and X_test are the dataframes that contains other columns. In the actual training process we just \n",
    "# # want to use the reconstruction error percentage column, hence we will extract that column and convert it to a tensor\n",
    "\n",
    "# if not os.path.exists(bias_predictor_path):\n",
    "#     bias_predictor = train_bias_predictor(torch.tensor(X_train['reconstruction_error_percentage'].values.reshape(-1,1), dtype=torch.float32).to(device), \n",
    "#                                           torch.tensor(y_train, dtype=torch.float32).to(device),\n",
    "#                                           torch.tensor(X_val['reconstruction_error_percentage'].values.reshape(-1,1), dtype=torch.float32).to(device),\n",
    "#                                           torch.tensor(y_val, dtype=torch.float32).to(device),\n",
    "#                                           epochs=1000, learning_rate=0.0001, patience=30)\n",
    "# else:\n",
    "#     bias_predictor = BiasPredictor().to(device)\n",
    "#     bias_predictor.load_state_dict(torch.load(bias_predictor_path))\n",
    "    \n",
    "\n",
    "# if not os.path.exists(bias_predictor_path):\n",
    "#     bias_predictor = train_bias_predictor(\n",
    "#         torch.tensor(\n",
    "#             np.vstack(X_train['reconstruction_error_percentage_array'].values),  # Stack arrays into a 2D array\n",
    "#             dtype=torch.float32\n",
    "#         ).to(device),\n",
    "#         torch.tensor(y_train, dtype=torch.float32).to(device),\n",
    "#         torch.tensor(\n",
    "#             np.vstack(X_val['reconstruction_error_percentage_array'].values),  # Stack arrays into a 2D array\n",
    "#             dtype=torch.float32\n",
    "#         ).to(device),\n",
    "#         torch.tensor(y_val, dtype=torch.float32).to(device),\n",
    "#         epochs=1000,\n",
    "#         learning_rate=0.001,\n",
    "#         patience=50\n",
    "#     )\n",
    "# else:\n",
    "#     bias_predictor = BiasPredictor().to(device)\n",
    "#     bias_predictor.load_state_dict(torch.load(bias_predictor_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = torch.tensor(np.vstack(X_test['reconstruction_error_percentage_array'].values), dtype=torch.float32).to(device)\n",
    "# predictions = bias_predictor(features).squeeze().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_df = pd.DataFrame({'predictions': predictions, 'target': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where predictions and target have the same sign\n",
    "# np.sum(np.sign(predictions) == np.sign(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test['predicted_bias_with_array_input'] = np.nan\n",
    "# X_test['predicted_bias_with_array_input'] = predictions\n",
    "\n",
    "# X_test['new_predictions_with_array_input'] = X_test['predictions'] + X_test['predicted_bias_with_array_input']\n",
    "# X_test['new_error_percentage_with_array_input'] = (abs(X_test['target'] - X_test['new_predictions_with_array_input']) / X_test['target']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test[['original_error_percentage', 'new_error_percentage_with_one_input', 'new_error_percentage_with_array_input']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 16.7120, Val Loss: 16.9217\n",
      "Epoch 2/1000, Loss: 16.6897, Val Loss: 16.9177\n",
      "Epoch 3/1000, Loss: 16.6696, Val Loss: 16.9134\n",
      "Epoch 4/1000, Loss: 16.6572, Val Loss: 16.9092\n",
      "Epoch 5/1000, Loss: 16.6330, Val Loss: 16.9045\n",
      "Epoch 6/1000, Loss: 16.6154, Val Loss: 16.8989\n",
      "Epoch 7/1000, Loss: 16.5991, Val Loss: 16.8931\n",
      "Epoch 8/1000, Loss: 16.5826, Val Loss: 16.8862\n",
      "Epoch 9/1000, Loss: 16.5630, Val Loss: 16.8785\n",
      "Epoch 10/1000, Loss: 16.5519, Val Loss: 16.8712\n",
      "Epoch 11/1000, Loss: 16.5079, Val Loss: 16.8640\n",
      "Epoch 12/1000, Loss: 16.4856, Val Loss: 16.8543\n",
      "Epoch 13/1000, Loss: 16.4653, Val Loss: 16.8419\n",
      "Epoch 14/1000, Loss: 16.4044, Val Loss: 16.8278\n",
      "Epoch 15/1000, Loss: 16.3803, Val Loss: 16.8139\n",
      "Epoch 16/1000, Loss: 16.3462, Val Loss: 16.7973\n",
      "Epoch 17/1000, Loss: 16.2913, Val Loss: 16.7765\n",
      "Epoch 18/1000, Loss: 16.2731, Val Loss: 16.7543\n",
      "Epoch 19/1000, Loss: 16.2203, Val Loss: 16.7282\n",
      "Epoch 20/1000, Loss: 16.1714, Val Loss: 16.7001\n",
      "Epoch 21/1000, Loss: 16.1321, Val Loss: 16.6700\n",
      "Epoch 22/1000, Loss: 16.0960, Val Loss: 16.6372\n",
      "Epoch 23/1000, Loss: 16.0481, Val Loss: 16.6031\n",
      "Epoch 24/1000, Loss: 15.9941, Val Loss: 16.5652\n",
      "Epoch 25/1000, Loss: 15.9521, Val Loss: 16.5222\n",
      "Epoch 26/1000, Loss: 15.9532, Val Loss: 16.4727\n",
      "Epoch 27/1000, Loss: 15.8805, Val Loss: 16.4187\n",
      "Epoch 28/1000, Loss: 15.8718, Val Loss: 16.3654\n",
      "Epoch 29/1000, Loss: 15.8227, Val Loss: 16.3190\n",
      "Epoch 30/1000, Loss: 15.7376, Val Loss: 16.2714\n",
      "Epoch 31/1000, Loss: 15.7506, Val Loss: 16.2299\n",
      "Epoch 32/1000, Loss: 15.7161, Val Loss: 16.1942\n",
      "Epoch 33/1000, Loss: 15.6448, Val Loss: 16.1498\n",
      "Epoch 34/1000, Loss: 15.6165, Val Loss: 16.1046\n",
      "Epoch 35/1000, Loss: 15.5491, Val Loss: 16.0539\n",
      "Epoch 36/1000, Loss: 15.5316, Val Loss: 16.0003\n",
      "Epoch 37/1000, Loss: 15.4981, Val Loss: 15.9726\n",
      "Epoch 38/1000, Loss: 15.4812, Val Loss: 15.9451\n",
      "Epoch 39/1000, Loss: 15.4081, Val Loss: 15.9116\n",
      "Epoch 40/1000, Loss: 15.4041, Val Loss: 15.8813\n",
      "Epoch 41/1000, Loss: 15.3529, Val Loss: 15.8411\n",
      "Epoch 42/1000, Loss: 15.3422, Val Loss: 15.8006\n",
      "Epoch 43/1000, Loss: 15.2651, Val Loss: 15.7578\n",
      "Epoch 44/1000, Loss: 15.3210, Val Loss: 15.7325\n",
      "Epoch 45/1000, Loss: 15.1997, Val Loss: 15.7006\n",
      "Epoch 46/1000, Loss: 15.2147, Val Loss: 15.6715\n",
      "Epoch 47/1000, Loss: 15.0996, Val Loss: 15.6510\n",
      "Epoch 48/1000, Loss: 15.0427, Val Loss: 15.6264\n",
      "Epoch 49/1000, Loss: 15.0538, Val Loss: 15.5955\n",
      "Epoch 50/1000, Loss: 15.0461, Val Loss: 15.5777\n",
      "Epoch 51/1000, Loss: 15.0500, Val Loss: 15.5551\n",
      "Epoch 52/1000, Loss: 14.9886, Val Loss: 15.5338\n",
      "Epoch 53/1000, Loss: 14.9380, Val Loss: 15.5198\n",
      "Epoch 54/1000, Loss: 15.0093, Val Loss: 15.5094\n",
      "Epoch 55/1000, Loss: 14.8562, Val Loss: 15.4997\n",
      "Epoch 56/1000, Loss: 14.8862, Val Loss: 15.4876\n",
      "Epoch 57/1000, Loss: 14.8462, Val Loss: 15.4615\n",
      "Epoch 58/1000, Loss: 14.7856, Val Loss: 15.4394\n",
      "Epoch 59/1000, Loss: 14.8707, Val Loss: 15.4260\n",
      "Epoch 60/1000, Loss: 14.7551, Val Loss: 15.4187\n",
      "Epoch 61/1000, Loss: 14.7574, Val Loss: 15.4044\n",
      "Epoch 62/1000, Loss: 14.7124, Val Loss: 15.3861\n",
      "Epoch 63/1000, Loss: 14.6802, Val Loss: 15.3728\n",
      "Epoch 64/1000, Loss: 14.6365, Val Loss: 15.3564\n",
      "Epoch 65/1000, Loss: 14.6438, Val Loss: 15.3463\n",
      "Epoch 66/1000, Loss: 14.6264, Val Loss: 15.3325\n",
      "Epoch 67/1000, Loss: 14.5562, Val Loss: 15.3279\n",
      "Epoch 68/1000, Loss: 14.4967, Val Loss: 15.3164\n",
      "Epoch 69/1000, Loss: 14.5376, Val Loss: 15.3029\n",
      "Epoch 70/1000, Loss: 14.4144, Val Loss: 15.2889\n",
      "Epoch 71/1000, Loss: 14.5258, Val Loss: 15.2756\n",
      "Epoch 72/1000, Loss: 14.4052, Val Loss: 15.2657\n",
      "Epoch 73/1000, Loss: 14.4807, Val Loss: 15.2521\n",
      "Epoch 74/1000, Loss: 14.4628, Val Loss: 15.2393\n",
      "Epoch 75/1000, Loss: 14.3271, Val Loss: 15.2289\n",
      "Epoch 76/1000, Loss: 14.1990, Val Loss: 15.2222\n",
      "Epoch 77/1000, Loss: 14.2919, Val Loss: 15.2128\n",
      "Epoch 78/1000, Loss: 14.3800, Val Loss: 15.1951\n",
      "Epoch 79/1000, Loss: 14.2480, Val Loss: 15.1776\n",
      "Epoch 80/1000, Loss: 14.1169, Val Loss: 15.1476\n",
      "Epoch 81/1000, Loss: 14.1720, Val Loss: 15.1118\n",
      "Epoch 82/1000, Loss: 14.2203, Val Loss: 15.0868\n",
      "Epoch 83/1000, Loss: 14.1241, Val Loss: 15.0675\n",
      "Epoch 84/1000, Loss: 14.1524, Val Loss: 15.0462\n",
      "Epoch 85/1000, Loss: 14.0646, Val Loss: 14.9980\n",
      "Epoch 86/1000, Loss: 13.9343, Val Loss: 14.9605\n",
      "Epoch 87/1000, Loss: 13.8790, Val Loss: 14.9374\n",
      "Epoch 88/1000, Loss: 14.0340, Val Loss: 14.9136\n",
      "Epoch 89/1000, Loss: 13.9540, Val Loss: 14.9028\n",
      "Epoch 90/1000, Loss: 13.9276, Val Loss: 14.9054\n",
      "Epoch 91/1000, Loss: 13.7139, Val Loss: 14.9022\n",
      "Epoch 92/1000, Loss: 13.8223, Val Loss: 14.8739\n",
      "Epoch 93/1000, Loss: 13.8453, Val Loss: 14.8145\n",
      "Epoch 94/1000, Loss: 13.7270, Val Loss: 14.7624\n",
      "Epoch 95/1000, Loss: 13.8477, Val Loss: 14.7284\n",
      "Epoch 96/1000, Loss: 13.5073, Val Loss: 14.6850\n",
      "Epoch 97/1000, Loss: 13.6340, Val Loss: 14.6396\n",
      "Epoch 98/1000, Loss: 13.6995, Val Loss: 14.6023\n",
      "Epoch 99/1000, Loss: 13.6805, Val Loss: 14.5710\n",
      "Epoch 100/1000, Loss: 13.5495, Val Loss: 14.5190\n",
      "Epoch 101/1000, Loss: 13.6358, Val Loss: 14.4751\n",
      "Epoch 102/1000, Loss: 13.4826, Val Loss: 14.4394\n",
      "Epoch 103/1000, Loss: 13.3798, Val Loss: 14.4040\n",
      "Epoch 104/1000, Loss: 13.5318, Val Loss: 14.3769\n",
      "Epoch 105/1000, Loss: 13.4490, Val Loss: 14.3790\n",
      "Epoch 106/1000, Loss: 13.2336, Val Loss: 14.3944\n",
      "Epoch 107/1000, Loss: 13.4619, Val Loss: 14.3743\n",
      "Epoch 108/1000, Loss: 13.1466, Val Loss: 14.3483\n",
      "Epoch 109/1000, Loss: 13.1466, Val Loss: 14.3238\n",
      "Epoch 110/1000, Loss: 13.3492, Val Loss: 14.3142\n",
      "Epoch 111/1000, Loss: 13.0196, Val Loss: 14.3359\n",
      "Epoch 112/1000, Loss: 13.0096, Val Loss: 14.3742\n",
      "Epoch 113/1000, Loss: 12.9918, Val Loss: 14.3920\n",
      "Epoch 114/1000, Loss: 13.2003, Val Loss: 14.3769\n",
      "Epoch 115/1000, Loss: 13.0284, Val Loss: 14.3314\n",
      "Epoch 116/1000, Loss: 12.8863, Val Loss: 14.3271\n",
      "Epoch 117/1000, Loss: 13.0325, Val Loss: 14.3186\n",
      "Epoch 118/1000, Loss: 12.8210, Val Loss: 14.3008\n",
      "Epoch 119/1000, Loss: 13.0609, Val Loss: 14.2652\n",
      "Epoch 120/1000, Loss: 12.8071, Val Loss: 14.1861\n",
      "Epoch 121/1000, Loss: 12.9564, Val Loss: 14.1170\n",
      "Epoch 122/1000, Loss: 12.6965, Val Loss: 14.1106\n",
      "Epoch 123/1000, Loss: 12.8747, Val Loss: 14.0860\n",
      "Epoch 124/1000, Loss: 12.8914, Val Loss: 14.0630\n",
      "Epoch 125/1000, Loss: 12.6416, Val Loss: 14.0568\n",
      "Epoch 126/1000, Loss: 12.7508, Val Loss: 14.0429\n",
      "Epoch 127/1000, Loss: 12.6893, Val Loss: 14.0173\n",
      "Epoch 128/1000, Loss: 12.6727, Val Loss: 14.0139\n",
      "Epoch 129/1000, Loss: 12.6172, Val Loss: 14.0312\n",
      "Epoch 130/1000, Loss: 12.6916, Val Loss: 14.0303\n",
      "Epoch 131/1000, Loss: 12.5849, Val Loss: 14.0238\n",
      "Epoch 132/1000, Loss: 12.6366, Val Loss: 14.0338\n",
      "Epoch 133/1000, Loss: 12.6835, Val Loss: 14.0389\n",
      "Epoch 134/1000, Loss: 12.7191, Val Loss: 14.0476\n",
      "Epoch 135/1000, Loss: 12.5851, Val Loss: 14.0357\n",
      "Epoch 136/1000, Loss: 12.6295, Val Loss: 14.0284\n",
      "Epoch 137/1000, Loss: 12.6776, Val Loss: 14.0188\n",
      "Epoch 138/1000, Loss: 12.6513, Val Loss: 14.0077\n",
      "Epoch 139/1000, Loss: 12.5562, Val Loss: 14.0044\n",
      "Epoch 140/1000, Loss: 12.5038, Val Loss: 14.0003\n",
      "Epoch 141/1000, Loss: 12.3381, Val Loss: 14.0019\n",
      "Epoch 142/1000, Loss: 12.4570, Val Loss: 14.0207\n",
      "Epoch 143/1000, Loss: 12.7358, Val Loss: 14.0412\n",
      "Epoch 144/1000, Loss: 12.5020, Val Loss: 14.0636\n",
      "Epoch 145/1000, Loss: 12.5395, Val Loss: 14.0854\n",
      "Epoch 146/1000, Loss: 12.3212, Val Loss: 14.1096\n",
      "Epoch 147/1000, Loss: 12.0955, Val Loss: 14.1159\n",
      "Epoch 148/1000, Loss: 12.2494, Val Loss: 14.1147\n",
      "Epoch 149/1000, Loss: 12.3323, Val Loss: 14.1172\n",
      "Epoch 150/1000, Loss: 12.4575, Val Loss: 14.0897\n",
      "Epoch 151/1000, Loss: 12.1429, Val Loss: 14.0169\n",
      "Epoch 152/1000, Loss: 12.1589, Val Loss: 13.9602\n",
      "Epoch 153/1000, Loss: 12.2668, Val Loss: 13.9324\n",
      "Epoch 154/1000, Loss: 12.2808, Val Loss: 13.9114\n",
      "Epoch 155/1000, Loss: 11.9544, Val Loss: 13.8986\n",
      "Epoch 156/1000, Loss: 12.1204, Val Loss: 13.9227\n",
      "Epoch 157/1000, Loss: 12.3832, Val Loss: 14.0068\n",
      "Epoch 158/1000, Loss: 12.2435, Val Loss: 14.0580\n",
      "Epoch 159/1000, Loss: 12.4260, Val Loss: 14.0703\n",
      "Epoch 160/1000, Loss: 12.0898, Val Loss: 14.0634\n",
      "Epoch 161/1000, Loss: 12.2527, Val Loss: 14.0436\n",
      "Epoch 162/1000, Loss: 12.1960, Val Loss: 14.0514\n",
      "Epoch 163/1000, Loss: 12.1387, Val Loss: 14.0491\n",
      "Epoch 164/1000, Loss: 11.8938, Val Loss: 14.0424\n",
      "Epoch 165/1000, Loss: 12.2577, Val Loss: 14.0305\n",
      "Epoch 166/1000, Loss: 11.9084, Val Loss: 13.9945\n",
      "Epoch 167/1000, Loss: 12.0403, Val Loss: 13.9249\n",
      "Epoch 168/1000, Loss: 11.9841, Val Loss: 13.8726\n",
      "Epoch 169/1000, Loss: 12.0627, Val Loss: 13.8306\n",
      "Epoch 170/1000, Loss: 11.8937, Val Loss: 13.7837\n",
      "Epoch 171/1000, Loss: 11.9077, Val Loss: 13.8057\n",
      "Epoch 172/1000, Loss: 12.0834, Val Loss: 13.8418\n",
      "Epoch 173/1000, Loss: 11.9189, Val Loss: 13.8877\n",
      "Epoch 174/1000, Loss: 12.0663, Val Loss: 13.9066\n",
      "Epoch 175/1000, Loss: 11.9206, Val Loss: 13.9107\n",
      "Epoch 176/1000, Loss: 11.5934, Val Loss: 13.9155\n",
      "Epoch 177/1000, Loss: 11.8836, Val Loss: 13.9033\n",
      "Epoch 178/1000, Loss: 11.8376, Val Loss: 13.9062\n",
      "Epoch 179/1000, Loss: 11.9149, Val Loss: 13.9244\n",
      "Epoch 180/1000, Loss: 11.7097, Val Loss: 13.9695\n",
      "Epoch 181/1000, Loss: 12.1218, Val Loss: 14.0028\n",
      "Epoch 182/1000, Loss: 11.8714, Val Loss: 14.0508\n",
      "Epoch 183/1000, Loss: 11.5826, Val Loss: 14.1118\n",
      "Epoch 184/1000, Loss: 11.6239, Val Loss: 14.1771\n",
      "Epoch 185/1000, Loss: 11.6713, Val Loss: 14.2048\n",
      "Epoch 186/1000, Loss: 11.9403, Val Loss: 14.2234\n",
      "Epoch 187/1000, Loss: 11.5498, Val Loss: 14.2283\n",
      "Epoch 188/1000, Loss: 11.6722, Val Loss: 14.2192\n",
      "Epoch 189/1000, Loss: 11.7018, Val Loss: 14.1856\n",
      "Epoch 190/1000, Loss: 11.7672, Val Loss: 14.1472\n",
      "Epoch 191/1000, Loss: 11.4812, Val Loss: 14.1163\n",
      "Epoch 192/1000, Loss: 11.5673, Val Loss: 14.1390\n",
      "Epoch 193/1000, Loss: 11.8557, Val Loss: 14.1090\n",
      "Epoch 194/1000, Loss: 11.5965, Val Loss: 14.1098\n",
      "Epoch 195/1000, Loss: 11.5597, Val Loss: 14.1207\n",
      "Epoch 196/1000, Loss: 11.3830, Val Loss: 14.1438\n",
      "Epoch 197/1000, Loss: 11.7228, Val Loss: 14.1517\n",
      "Epoch 198/1000, Loss: 11.3389, Val Loss: 14.1691\n",
      "Epoch 199/1000, Loss: 11.5878, Val Loss: 14.1635\n",
      "Epoch 200/1000, Loss: 11.5164, Val Loss: 14.1474\n",
      "Epoch 201/1000, Loss: 11.4065, Val Loss: 14.1362\n",
      "Epoch 202/1000, Loss: 11.4791, Val Loss: 14.1339\n",
      "Epoch 203/1000, Loss: 11.6071, Val Loss: 14.1548\n",
      "Epoch 204/1000, Loss: 11.4073, Val Loss: 14.1870\n",
      "Epoch 205/1000, Loss: 11.2358, Val Loss: 14.1553\n",
      "Epoch 206/1000, Loss: 11.5011, Val Loss: 14.0852\n",
      "Epoch 207/1000, Loss: 11.5582, Val Loss: 14.0680\n",
      "Epoch 208/1000, Loss: 11.2327, Val Loss: 14.0996\n",
      "Epoch 209/1000, Loss: 11.5077, Val Loss: 14.1463\n",
      "Epoch 210/1000, Loss: 11.2975, Val Loss: 14.2307\n",
      "Early stopping triggered at epoch 210. Best Val Loss: 13.7837\n",
      "Epoch 1/1000, Loss: 16.6719, Val Loss: 16.7922\n",
      "Epoch 2/1000, Loss: 16.6296, Val Loss: 16.7934\n",
      "Epoch 3/1000, Loss: 16.6068, Val Loss: 16.7936\n",
      "Epoch 4/1000, Loss: 16.5922, Val Loss: 16.7933\n",
      "Epoch 5/1000, Loss: 16.5882, Val Loss: 16.7922\n",
      "Epoch 6/1000, Loss: 16.5646, Val Loss: 16.7899\n",
      "Epoch 7/1000, Loss: 16.5504, Val Loss: 16.7873\n",
      "Epoch 8/1000, Loss: 16.5408, Val Loss: 16.7835\n",
      "Epoch 9/1000, Loss: 16.5197, Val Loss: 16.7788\n",
      "Epoch 10/1000, Loss: 16.4949, Val Loss: 16.7727\n",
      "Epoch 11/1000, Loss: 16.4479, Val Loss: 16.7658\n",
      "Epoch 12/1000, Loss: 16.4431, Val Loss: 16.7576\n",
      "Epoch 13/1000, Loss: 16.4332, Val Loss: 16.7503\n",
      "Epoch 14/1000, Loss: 16.3602, Val Loss: 16.7407\n",
      "Epoch 15/1000, Loss: 16.3386, Val Loss: 16.7304\n",
      "Epoch 16/1000, Loss: 16.2999, Val Loss: 16.7181\n",
      "Epoch 17/1000, Loss: 16.2444, Val Loss: 16.7027\n",
      "Epoch 18/1000, Loss: 16.2149, Val Loss: 16.6870\n",
      "Epoch 19/1000, Loss: 16.1248, Val Loss: 16.6659\n",
      "Epoch 20/1000, Loss: 16.0663, Val Loss: 16.6371\n",
      "Epoch 21/1000, Loss: 16.0358, Val Loss: 16.6019\n",
      "Epoch 22/1000, Loss: 15.9786, Val Loss: 16.5601\n",
      "Epoch 23/1000, Loss: 15.9455, Val Loss: 16.5119\n",
      "Epoch 24/1000, Loss: 15.8768, Val Loss: 16.4629\n",
      "Epoch 25/1000, Loss: 15.8032, Val Loss: 16.4082\n",
      "Epoch 26/1000, Loss: 15.7845, Val Loss: 16.3660\n",
      "Epoch 27/1000, Loss: 15.7258, Val Loss: 16.3224\n",
      "Epoch 28/1000, Loss: 15.6689, Val Loss: 16.2797\n",
      "Epoch 29/1000, Loss: 15.6103, Val Loss: 16.2223\n",
      "Epoch 30/1000, Loss: 15.5630, Val Loss: 16.1649\n",
      "Epoch 31/1000, Loss: 15.5101, Val Loss: 16.1065\n",
      "Epoch 32/1000, Loss: 15.4542, Val Loss: 16.0397\n",
      "Epoch 33/1000, Loss: 15.3710, Val Loss: 15.9720\n",
      "Epoch 34/1000, Loss: 15.3616, Val Loss: 15.9023\n",
      "Epoch 35/1000, Loss: 15.2894, Val Loss: 15.8406\n",
      "Epoch 36/1000, Loss: 15.2087, Val Loss: 15.7760\n",
      "Epoch 37/1000, Loss: 15.1230, Val Loss: 15.7168\n",
      "Epoch 38/1000, Loss: 15.1939, Val Loss: 15.6799\n",
      "Epoch 39/1000, Loss: 15.0759, Val Loss: 15.6411\n",
      "Epoch 40/1000, Loss: 15.0591, Val Loss: 15.5902\n",
      "Epoch 41/1000, Loss: 15.0771, Val Loss: 15.5357\n",
      "Epoch 42/1000, Loss: 14.9605, Val Loss: 15.4887\n",
      "Epoch 43/1000, Loss: 14.9099, Val Loss: 15.4563\n",
      "Epoch 44/1000, Loss: 14.8957, Val Loss: 15.4323\n",
      "Epoch 45/1000, Loss: 14.8995, Val Loss: 15.4197\n",
      "Epoch 46/1000, Loss: 14.8174, Val Loss: 15.4053\n",
      "Epoch 47/1000, Loss: 14.7063, Val Loss: 15.3914\n",
      "Epoch 48/1000, Loss: 14.6807, Val Loss: 15.3881\n",
      "Epoch 49/1000, Loss: 14.6888, Val Loss: 15.3833\n",
      "Epoch 50/1000, Loss: 14.7668, Val Loss: 15.3641\n",
      "Epoch 51/1000, Loss: 14.7226, Val Loss: 15.3442\n",
      "Epoch 52/1000, Loss: 14.6477, Val Loss: 15.3188\n",
      "Epoch 53/1000, Loss: 14.6414, Val Loss: 15.3000\n",
      "Epoch 54/1000, Loss: 14.5652, Val Loss: 15.2871\n",
      "Epoch 55/1000, Loss: 14.6118, Val Loss: 15.2582\n",
      "Epoch 56/1000, Loss: 14.5832, Val Loss: 15.2365\n",
      "Epoch 57/1000, Loss: 14.6557, Val Loss: 15.2309\n",
      "Epoch 58/1000, Loss: 14.5991, Val Loss: 15.2113\n",
      "Epoch 59/1000, Loss: 14.4534, Val Loss: 15.1898\n",
      "Epoch 60/1000, Loss: 14.4614, Val Loss: 15.1661\n",
      "Epoch 61/1000, Loss: 14.4981, Val Loss: 15.1541\n",
      "Epoch 62/1000, Loss: 14.5184, Val Loss: 15.1347\n",
      "Epoch 63/1000, Loss: 14.4439, Val Loss: 15.1086\n",
      "Epoch 64/1000, Loss: 14.4323, Val Loss: 15.0758\n",
      "Epoch 65/1000, Loss: 14.2721, Val Loss: 15.0429\n",
      "Epoch 66/1000, Loss: 14.3070, Val Loss: 15.0153\n",
      "Epoch 67/1000, Loss: 14.3001, Val Loss: 14.9774\n",
      "Epoch 68/1000, Loss: 14.3314, Val Loss: 14.9583\n",
      "Epoch 69/1000, Loss: 14.2441, Val Loss: 14.9472\n",
      "Epoch 70/1000, Loss: 14.1608, Val Loss: 14.9779\n",
      "Epoch 71/1000, Loss: 14.2296, Val Loss: 14.9813\n",
      "Epoch 72/1000, Loss: 14.1263, Val Loss: 14.9502\n",
      "Epoch 73/1000, Loss: 14.2110, Val Loss: 14.8953\n",
      "Epoch 74/1000, Loss: 14.0444, Val Loss: 14.8659\n",
      "Epoch 75/1000, Loss: 14.0557, Val Loss: 14.8565\n",
      "Epoch 76/1000, Loss: 14.0623, Val Loss: 14.8611\n",
      "Epoch 77/1000, Loss: 14.1780, Val Loss: 14.8768\n",
      "Epoch 78/1000, Loss: 14.0859, Val Loss: 14.8475\n",
      "Epoch 79/1000, Loss: 14.0400, Val Loss: 14.7281\n",
      "Epoch 80/1000, Loss: 14.0311, Val Loss: 14.6126\n",
      "Epoch 81/1000, Loss: 13.8967, Val Loss: 14.5672\n",
      "Epoch 82/1000, Loss: 13.9394, Val Loss: 14.5647\n",
      "Epoch 83/1000, Loss: 13.7926, Val Loss: 14.5602\n",
      "Epoch 84/1000, Loss: 13.8224, Val Loss: 14.5289\n",
      "Epoch 85/1000, Loss: 13.8781, Val Loss: 14.4699\n",
      "Epoch 86/1000, Loss: 13.7352, Val Loss: 14.4281\n",
      "Epoch 87/1000, Loss: 13.6947, Val Loss: 14.4053\n",
      "Epoch 88/1000, Loss: 13.8801, Val Loss: 14.3856\n",
      "Epoch 89/1000, Loss: 13.8432, Val Loss: 14.3909\n",
      "Epoch 90/1000, Loss: 13.7858, Val Loss: 14.4004\n",
      "Epoch 91/1000, Loss: 13.5670, Val Loss: 14.3979\n",
      "Epoch 92/1000, Loss: 13.7006, Val Loss: 14.3735\n",
      "Epoch 93/1000, Loss: 13.5973, Val Loss: 14.3396\n",
      "Epoch 94/1000, Loss: 13.6534, Val Loss: 14.3292\n",
      "Epoch 95/1000, Loss: 13.6358, Val Loss: 14.3119\n",
      "Epoch 96/1000, Loss: 13.5899, Val Loss: 14.3114\n",
      "Epoch 97/1000, Loss: 13.6725, Val Loss: 14.2376\n",
      "Epoch 98/1000, Loss: 13.3138, Val Loss: 14.2175\n",
      "Epoch 99/1000, Loss: 13.4475, Val Loss: 14.2115\n",
      "Epoch 100/1000, Loss: 13.4044, Val Loss: 14.1996\n",
      "Epoch 101/1000, Loss: 13.6590, Val Loss: 14.1923\n",
      "Epoch 102/1000, Loss: 13.2609, Val Loss: 14.1777\n",
      "Epoch 103/1000, Loss: 13.2927, Val Loss: 14.1485\n",
      "Epoch 104/1000, Loss: 13.2578, Val Loss: 14.1116\n",
      "Epoch 105/1000, Loss: 13.3241, Val Loss: 14.0904\n",
      "Epoch 106/1000, Loss: 13.4352, Val Loss: 14.0828\n",
      "Epoch 107/1000, Loss: 13.4103, Val Loss: 14.0961\n",
      "Epoch 108/1000, Loss: 13.3810, Val Loss: 14.0891\n",
      "Epoch 109/1000, Loss: 13.2509, Val Loss: 14.0743\n",
      "Epoch 110/1000, Loss: 13.1907, Val Loss: 14.0669\n",
      "Epoch 111/1000, Loss: 13.1124, Val Loss: 14.0838\n",
      "Epoch 112/1000, Loss: 13.2900, Val Loss: 14.0997\n",
      "Epoch 113/1000, Loss: 13.0565, Val Loss: 14.0890\n",
      "Epoch 114/1000, Loss: 13.1888, Val Loss: 14.0684\n",
      "Epoch 115/1000, Loss: 13.1213, Val Loss: 14.0505\n",
      "Epoch 116/1000, Loss: 12.9061, Val Loss: 14.0240\n",
      "Epoch 117/1000, Loss: 13.0237, Val Loss: 14.0103\n",
      "Epoch 118/1000, Loss: 13.2110, Val Loss: 14.0101\n",
      "Epoch 119/1000, Loss: 13.0993, Val Loss: 13.9956\n",
      "Epoch 120/1000, Loss: 13.0793, Val Loss: 13.9876\n",
      "Epoch 121/1000, Loss: 13.1951, Val Loss: 14.0267\n",
      "Epoch 122/1000, Loss: 12.9254, Val Loss: 14.0596\n",
      "Epoch 123/1000, Loss: 13.1023, Val Loss: 14.0610\n",
      "Epoch 124/1000, Loss: 12.9949, Val Loss: 14.0621\n",
      "Epoch 125/1000, Loss: 12.8813, Val Loss: 14.0569\n",
      "Epoch 126/1000, Loss: 12.7801, Val Loss: 14.0527\n",
      "Epoch 127/1000, Loss: 12.8721, Val Loss: 14.0425\n",
      "Epoch 128/1000, Loss: 12.8184, Val Loss: 14.0237\n",
      "Epoch 129/1000, Loss: 13.1956, Val Loss: 14.0076\n",
      "Epoch 130/1000, Loss: 13.0534, Val Loss: 13.9930\n",
      "Epoch 131/1000, Loss: 12.8357, Val Loss: 13.9919\n",
      "Epoch 132/1000, Loss: 12.8175, Val Loss: 13.9885\n",
      "Epoch 133/1000, Loss: 12.7712, Val Loss: 13.9798\n",
      "Epoch 134/1000, Loss: 12.8272, Val Loss: 13.9699\n",
      "Epoch 135/1000, Loss: 12.6839, Val Loss: 13.9762\n",
      "Epoch 136/1000, Loss: 12.6608, Val Loss: 13.9732\n",
      "Epoch 137/1000, Loss: 12.4564, Val Loss: 13.9722\n",
      "Epoch 138/1000, Loss: 12.6111, Val Loss: 13.9625\n",
      "Epoch 139/1000, Loss: 12.7175, Val Loss: 13.9444\n",
      "Epoch 140/1000, Loss: 12.6772, Val Loss: 13.9398\n",
      "Epoch 141/1000, Loss: 12.7088, Val Loss: 13.9185\n",
      "Epoch 142/1000, Loss: 13.4956, Val Loss: 13.9651\n",
      "Epoch 143/1000, Loss: 12.5601, Val Loss: 14.0398\n",
      "Epoch 144/1000, Loss: 12.7240, Val Loss: 14.0424\n",
      "Epoch 145/1000, Loss: 12.6285, Val Loss: 14.0184\n",
      "Epoch 146/1000, Loss: 12.7888, Val Loss: 14.0478\n",
      "Epoch 147/1000, Loss: 12.5338, Val Loss: 14.0945\n",
      "Epoch 148/1000, Loss: 12.7230, Val Loss: 14.1180\n",
      "Epoch 149/1000, Loss: 12.5211, Val Loss: 14.1060\n",
      "Epoch 150/1000, Loss: 12.8442, Val Loss: 14.0661\n",
      "Epoch 151/1000, Loss: 12.8514, Val Loss: 14.0302\n",
      "Epoch 152/1000, Loss: 12.6127, Val Loss: 13.9981\n",
      "Epoch 153/1000, Loss: 12.5712, Val Loss: 14.0039\n",
      "Epoch 154/1000, Loss: 12.4828, Val Loss: 14.0124\n",
      "Epoch 155/1000, Loss: 12.4106, Val Loss: 14.0233\n",
      "Epoch 156/1000, Loss: 12.5485, Val Loss: 14.0191\n",
      "Epoch 157/1000, Loss: 12.6469, Val Loss: 14.0164\n",
      "Epoch 158/1000, Loss: 12.4209, Val Loss: 14.0099\n",
      "Epoch 159/1000, Loss: 12.4153, Val Loss: 13.9957\n",
      "Epoch 160/1000, Loss: 12.5371, Val Loss: 14.0273\n",
      "Epoch 161/1000, Loss: 12.4560, Val Loss: 14.0595\n",
      "Epoch 162/1000, Loss: 12.5537, Val Loss: 14.0602\n",
      "Epoch 163/1000, Loss: 12.4242, Val Loss: 14.0589\n",
      "Epoch 164/1000, Loss: 12.2864, Val Loss: 14.0591\n",
      "Epoch 165/1000, Loss: 12.2918, Val Loss: 14.0521\n",
      "Epoch 166/1000, Loss: 12.1970, Val Loss: 14.0721\n",
      "Epoch 167/1000, Loss: 12.2899, Val Loss: 14.1063\n",
      "Epoch 168/1000, Loss: 12.1897, Val Loss: 14.1217\n",
      "Epoch 169/1000, Loss: 12.6321, Val Loss: 14.1362\n",
      "Epoch 170/1000, Loss: 12.1369, Val Loss: 14.1353\n",
      "Epoch 171/1000, Loss: 12.3453, Val Loss: 14.0893\n",
      "Epoch 172/1000, Loss: 12.4171, Val Loss: 14.0350\n",
      "Epoch 173/1000, Loss: 12.2054, Val Loss: 13.9821\n",
      "Epoch 174/1000, Loss: 12.3197, Val Loss: 13.9932\n",
      "Epoch 175/1000, Loss: 12.1874, Val Loss: 14.0296\n",
      "Epoch 176/1000, Loss: 12.1350, Val Loss: 14.0117\n",
      "Epoch 177/1000, Loss: 12.2547, Val Loss: 13.9686\n",
      "Epoch 178/1000, Loss: 12.2231, Val Loss: 13.9132\n",
      "Epoch 179/1000, Loss: 12.2892, Val Loss: 13.8918\n",
      "Epoch 180/1000, Loss: 12.3766, Val Loss: 13.9226\n",
      "Epoch 181/1000, Loss: 12.3573, Val Loss: 14.0038\n",
      "Epoch 182/1000, Loss: 11.9940, Val Loss: 14.1128\n",
      "Epoch 183/1000, Loss: 12.0567, Val Loss: 14.1782\n",
      "Epoch 184/1000, Loss: 12.2921, Val Loss: 14.1760\n",
      "Epoch 185/1000, Loss: 12.1250, Val Loss: 14.0974\n",
      "Epoch 186/1000, Loss: 12.3445, Val Loss: 14.0154\n",
      "Epoch 187/1000, Loss: 12.0881, Val Loss: 13.9765\n",
      "Epoch 188/1000, Loss: 11.8096, Val Loss: 13.9797\n",
      "Epoch 189/1000, Loss: 12.1096, Val Loss: 13.9994\n",
      "Epoch 190/1000, Loss: 12.1062, Val Loss: 14.0043\n",
      "Epoch 191/1000, Loss: 12.0183, Val Loss: 14.0054\n",
      "Epoch 192/1000, Loss: 11.9976, Val Loss: 13.9898\n",
      "Epoch 193/1000, Loss: 11.9080, Val Loss: 13.9693\n",
      "Epoch 194/1000, Loss: 11.9461, Val Loss: 13.9225\n",
      "Epoch 195/1000, Loss: 11.7949, Val Loss: 13.8872\n",
      "Epoch 196/1000, Loss: 11.9458, Val Loss: 13.8863\n",
      "Epoch 197/1000, Loss: 11.9411, Val Loss: 13.8842\n",
      "Epoch 198/1000, Loss: 11.7945, Val Loss: 13.8601\n",
      "Epoch 199/1000, Loss: 11.6816, Val Loss: 13.8295\n",
      "Epoch 200/1000, Loss: 12.1208, Val Loss: 13.8280\n",
      "Epoch 201/1000, Loss: 11.8421, Val Loss: 13.8550\n",
      "Epoch 202/1000, Loss: 11.8413, Val Loss: 13.8715\n",
      "Epoch 203/1000, Loss: 11.6679, Val Loss: 13.9119\n",
      "Epoch 204/1000, Loss: 11.6482, Val Loss: 13.9259\n",
      "Epoch 205/1000, Loss: 11.8131, Val Loss: 13.9033\n",
      "Epoch 206/1000, Loss: 12.0271, Val Loss: 13.8526\n",
      "Epoch 207/1000, Loss: 11.5866, Val Loss: 13.8371\n",
      "Epoch 208/1000, Loss: 11.6642, Val Loss: 13.8337\n",
      "Epoch 209/1000, Loss: 11.5680, Val Loss: 13.8163\n",
      "Epoch 210/1000, Loss: 11.3407, Val Loss: 13.8071\n",
      "Epoch 211/1000, Loss: 11.6484, Val Loss: 13.8171\n",
      "Epoch 212/1000, Loss: 11.4619, Val Loss: 13.8217\n",
      "Epoch 213/1000, Loss: 11.0744, Val Loss: 13.8477\n",
      "Epoch 214/1000, Loss: 11.5511, Val Loss: 13.8973\n",
      "Epoch 215/1000, Loss: 11.5832, Val Loss: 13.9441\n",
      "Epoch 216/1000, Loss: 11.4888, Val Loss: 13.9476\n",
      "Epoch 217/1000, Loss: 11.3451, Val Loss: 13.9654\n",
      "Epoch 218/1000, Loss: 11.3639, Val Loss: 13.9867\n",
      "Epoch 219/1000, Loss: 11.4768, Val Loss: 14.0071\n",
      "Epoch 220/1000, Loss: 11.6633, Val Loss: 14.0291\n",
      "Epoch 221/1000, Loss: 11.3138, Val Loss: 14.0656\n",
      "Epoch 222/1000, Loss: 11.5261, Val Loss: 14.1013\n",
      "Epoch 223/1000, Loss: 11.4236, Val Loss: 14.1225\n",
      "Epoch 224/1000, Loss: 11.4872, Val Loss: 14.0828\n",
      "Epoch 225/1000, Loss: 11.4992, Val Loss: 14.0754\n",
      "Epoch 226/1000, Loss: 11.6577, Val Loss: 14.1215\n",
      "Epoch 227/1000, Loss: 11.4494, Val Loss: 14.1681\n",
      "Epoch 228/1000, Loss: 11.3733, Val Loss: 14.1819\n",
      "Epoch 229/1000, Loss: 11.5871, Val Loss: 14.1768\n",
      "Epoch 230/1000, Loss: 11.4942, Val Loss: 14.1297\n",
      "Epoch 231/1000, Loss: 11.4586, Val Loss: 14.0848\n",
      "Epoch 232/1000, Loss: 11.3050, Val Loss: 14.0547\n",
      "Epoch 233/1000, Loss: 11.2253, Val Loss: 14.0376\n",
      "Epoch 234/1000, Loss: 11.2172, Val Loss: 14.0393\n",
      "Epoch 235/1000, Loss: 11.0954, Val Loss: 14.0310\n",
      "Epoch 236/1000, Loss: 11.3580, Val Loss: 14.0392\n",
      "Epoch 237/1000, Loss: 11.2894, Val Loss: 14.0712\n",
      "Epoch 238/1000, Loss: 11.1687, Val Loss: 14.0820\n",
      "Epoch 239/1000, Loss: 11.2489, Val Loss: 14.0704\n",
      "Epoch 240/1000, Loss: 11.5368, Val Loss: 14.0796\n",
      "Epoch 241/1000, Loss: 11.1451, Val Loss: 14.0684\n",
      "Epoch 242/1000, Loss: 11.1412, Val Loss: 14.0376\n",
      "Epoch 243/1000, Loss: 11.2610, Val Loss: 13.9941\n",
      "Epoch 244/1000, Loss: 11.0891, Val Loss: 13.9881\n",
      "Epoch 245/1000, Loss: 10.9800, Val Loss: 14.0289\n",
      "Epoch 246/1000, Loss: 11.1695, Val Loss: 14.0534\n",
      "Epoch 247/1000, Loss: 10.9728, Val Loss: 14.0837\n",
      "Epoch 248/1000, Loss: 11.2158, Val Loss: 14.1256\n",
      "Epoch 249/1000, Loss: 11.2321, Val Loss: 14.1093\n",
      "Epoch 250/1000, Loss: 11.2356, Val Loss: 14.0717\n",
      "Early stopping triggered at epoch 250. Best Val Loss: 13.8071\n",
      "Epoch 1/1000, Loss: 16.6632, Val Loss: 16.8131\n",
      "Epoch 2/1000, Loss: 16.6359, Val Loss: 16.8113\n",
      "Epoch 3/1000, Loss: 16.6224, Val Loss: 16.8094\n",
      "Epoch 4/1000, Loss: 16.6167, Val Loss: 16.8074\n",
      "Epoch 5/1000, Loss: 16.5973, Val Loss: 16.8054\n",
      "Epoch 6/1000, Loss: 16.5765, Val Loss: 16.8028\n",
      "Epoch 7/1000, Loss: 16.5625, Val Loss: 16.7995\n",
      "Epoch 8/1000, Loss: 16.5356, Val Loss: 16.7953\n",
      "Epoch 9/1000, Loss: 16.5404, Val Loss: 16.7904\n",
      "Epoch 10/1000, Loss: 16.5105, Val Loss: 16.7856\n",
      "Epoch 11/1000, Loss: 16.4973, Val Loss: 16.7802\n",
      "Epoch 12/1000, Loss: 16.4817, Val Loss: 16.7733\n",
      "Epoch 13/1000, Loss: 16.4425, Val Loss: 16.7646\n",
      "Epoch 14/1000, Loss: 16.4132, Val Loss: 16.7544\n",
      "Epoch 15/1000, Loss: 16.4019, Val Loss: 16.7439\n",
      "Epoch 16/1000, Loss: 16.3684, Val Loss: 16.7330\n",
      "Epoch 17/1000, Loss: 16.3173, Val Loss: 16.7210\n",
      "Epoch 18/1000, Loss: 16.2736, Val Loss: 16.7068\n",
      "Epoch 19/1000, Loss: 16.2499, Val Loss: 16.6916\n",
      "Epoch 20/1000, Loss: 16.1817, Val Loss: 16.6770\n",
      "Epoch 21/1000, Loss: 16.1471, Val Loss: 16.6618\n",
      "Epoch 22/1000, Loss: 16.1144, Val Loss: 16.6441\n",
      "Epoch 23/1000, Loss: 16.0212, Val Loss: 16.6193\n",
      "Epoch 24/1000, Loss: 15.9836, Val Loss: 16.5891\n",
      "Epoch 25/1000, Loss: 15.9681, Val Loss: 16.5556\n",
      "Epoch 26/1000, Loss: 15.8862, Val Loss: 16.5165\n",
      "Epoch 27/1000, Loss: 15.8486, Val Loss: 16.4722\n",
      "Epoch 28/1000, Loss: 15.7804, Val Loss: 16.4282\n",
      "Epoch 29/1000, Loss: 15.7601, Val Loss: 16.3835\n",
      "Epoch 30/1000, Loss: 15.7234, Val Loss: 16.3347\n",
      "Epoch 31/1000, Loss: 15.7386, Val Loss: 16.2932\n",
      "Epoch 32/1000, Loss: 15.6250, Val Loss: 16.2561\n",
      "Epoch 33/1000, Loss: 15.5575, Val Loss: 16.2142\n",
      "Epoch 34/1000, Loss: 15.5261, Val Loss: 16.1695\n",
      "Epoch 35/1000, Loss: 15.5062, Val Loss: 16.1220\n",
      "Epoch 36/1000, Loss: 15.4633, Val Loss: 16.0638\n",
      "Epoch 37/1000, Loss: 15.4040, Val Loss: 16.0017\n",
      "Epoch 38/1000, Loss: 15.3817, Val Loss: 15.9393\n",
      "Epoch 39/1000, Loss: 15.3370, Val Loss: 15.8954\n",
      "Epoch 40/1000, Loss: 15.3105, Val Loss: 15.8631\n",
      "Epoch 41/1000, Loss: 15.3369, Val Loss: 15.8387\n",
      "Epoch 42/1000, Loss: 15.2793, Val Loss: 15.8135\n",
      "Epoch 43/1000, Loss: 15.2291, Val Loss: 15.7935\n",
      "Epoch 44/1000, Loss: 15.2105, Val Loss: 15.7824\n",
      "Epoch 45/1000, Loss: 15.1505, Val Loss: 15.7763\n",
      "Epoch 46/1000, Loss: 15.0883, Val Loss: 15.7695\n",
      "Epoch 47/1000, Loss: 15.1361, Val Loss: 15.7624\n",
      "Epoch 48/1000, Loss: 15.0488, Val Loss: 15.7574\n",
      "Epoch 49/1000, Loss: 15.0985, Val Loss: 15.7559\n",
      "Epoch 50/1000, Loss: 15.0720, Val Loss: 15.7529\n",
      "Epoch 51/1000, Loss: 15.1586, Val Loss: 15.7404\n",
      "Epoch 52/1000, Loss: 15.1431, Val Loss: 15.7302\n",
      "Epoch 53/1000, Loss: 15.1006, Val Loss: 15.7186\n",
      "Epoch 54/1000, Loss: 15.1718, Val Loss: 15.7022\n",
      "Epoch 55/1000, Loss: 14.9736, Val Loss: 15.6910\n",
      "Epoch 56/1000, Loss: 14.9979, Val Loss: 15.6744\n",
      "Epoch 57/1000, Loss: 15.0799, Val Loss: 15.6649\n",
      "Epoch 58/1000, Loss: 14.9962, Val Loss: 15.6678\n",
      "Epoch 59/1000, Loss: 15.0508, Val Loss: 15.6743\n",
      "Epoch 60/1000, Loss: 14.9695, Val Loss: 15.6877\n",
      "Epoch 61/1000, Loss: 14.8203, Val Loss: 15.6912\n",
      "Epoch 62/1000, Loss: 14.8959, Val Loss: 15.6880\n",
      "Epoch 63/1000, Loss: 14.9029, Val Loss: 15.6985\n",
      "Epoch 64/1000, Loss: 14.9096, Val Loss: 15.6951\n",
      "Epoch 65/1000, Loss: 14.8183, Val Loss: 15.6977\n",
      "Epoch 66/1000, Loss: 14.8977, Val Loss: 15.6838\n",
      "Epoch 67/1000, Loss: 14.7995, Val Loss: 15.6699\n",
      "Epoch 68/1000, Loss: 14.8029, Val Loss: 15.6473\n",
      "Epoch 69/1000, Loss: 14.8640, Val Loss: 15.6136\n",
      "Epoch 70/1000, Loss: 14.7207, Val Loss: 15.5813\n",
      "Epoch 71/1000, Loss: 14.6514, Val Loss: 15.5526\n",
      "Epoch 72/1000, Loss: 14.8075, Val Loss: 15.5318\n",
      "Epoch 73/1000, Loss: 14.7768, Val Loss: 15.5082\n",
      "Epoch 74/1000, Loss: 14.6867, Val Loss: 15.4875\n",
      "Epoch 75/1000, Loss: 14.7948, Val Loss: 15.4653\n",
      "Epoch 76/1000, Loss: 14.8110, Val Loss: 15.4534\n",
      "Epoch 77/1000, Loss: 14.6734, Val Loss: 15.4478\n",
      "Epoch 78/1000, Loss: 14.6810, Val Loss: 15.4451\n",
      "Epoch 79/1000, Loss: 14.7341, Val Loss: 15.4515\n",
      "Epoch 80/1000, Loss: 14.6465, Val Loss: 15.4562\n",
      "Epoch 81/1000, Loss: 14.6431, Val Loss: 15.4554\n",
      "Epoch 82/1000, Loss: 14.5249, Val Loss: 15.4482\n",
      "Epoch 83/1000, Loss: 14.5714, Val Loss: 15.4419\n",
      "Epoch 84/1000, Loss: 14.5162, Val Loss: 15.4384\n",
      "Epoch 85/1000, Loss: 14.5180, Val Loss: 15.4228\n",
      "Epoch 86/1000, Loss: 14.4538, Val Loss: 15.4130\n",
      "Epoch 87/1000, Loss: 14.4218, Val Loss: 15.3912\n",
      "Epoch 88/1000, Loss: 14.4442, Val Loss: 15.3756\n",
      "Epoch 89/1000, Loss: 14.3805, Val Loss: 15.3624\n",
      "Epoch 90/1000, Loss: 14.3517, Val Loss: 15.3410\n",
      "Epoch 91/1000, Loss: 14.4015, Val Loss: 15.3238\n",
      "Epoch 92/1000, Loss: 14.4021, Val Loss: 15.2996\n",
      "Epoch 93/1000, Loss: 14.3144, Val Loss: 15.2769\n",
      "Epoch 94/1000, Loss: 14.1766, Val Loss: 15.2513\n",
      "Epoch 95/1000, Loss: 14.3720, Val Loss: 15.2273\n",
      "Epoch 96/1000, Loss: 14.1484, Val Loss: 15.2058\n",
      "Epoch 97/1000, Loss: 14.2248, Val Loss: 15.1992\n",
      "Epoch 98/1000, Loss: 13.9579, Val Loss: 15.1912\n",
      "Epoch 99/1000, Loss: 14.3517, Val Loss: 15.1864\n",
      "Epoch 100/1000, Loss: 14.1456, Val Loss: 15.1904\n",
      "Epoch 101/1000, Loss: 14.0352, Val Loss: 15.1961\n",
      "Epoch 102/1000, Loss: 14.3299, Val Loss: 15.2136\n",
      "Epoch 103/1000, Loss: 14.0626, Val Loss: 15.2312\n",
      "Epoch 104/1000, Loss: 14.0472, Val Loss: 15.2337\n",
      "Epoch 105/1000, Loss: 13.9823, Val Loss: 15.2310\n",
      "Epoch 106/1000, Loss: 13.9735, Val Loss: 15.2169\n",
      "Epoch 107/1000, Loss: 14.1043, Val Loss: 15.1894\n",
      "Epoch 108/1000, Loss: 13.8995, Val Loss: 15.1499\n",
      "Epoch 109/1000, Loss: 13.9308, Val Loss: 15.1186\n",
      "Epoch 110/1000, Loss: 13.9408, Val Loss: 15.0835\n",
      "Epoch 111/1000, Loss: 13.9541, Val Loss: 15.0648\n",
      "Epoch 112/1000, Loss: 13.9457, Val Loss: 15.0462\n",
      "Epoch 113/1000, Loss: 13.8529, Val Loss: 15.0407\n",
      "Epoch 114/1000, Loss: 13.9352, Val Loss: 15.0424\n",
      "Epoch 115/1000, Loss: 13.8490, Val Loss: 15.0494\n",
      "Epoch 116/1000, Loss: 13.7320, Val Loss: 15.0663\n",
      "Epoch 117/1000, Loss: 13.6888, Val Loss: 15.0761\n",
      "Epoch 118/1000, Loss: 13.5301, Val Loss: 15.0906\n",
      "Epoch 119/1000, Loss: 13.6056, Val Loss: 15.0945\n",
      "Epoch 120/1000, Loss: 13.5426, Val Loss: 15.0918\n",
      "Epoch 121/1000, Loss: 13.5587, Val Loss: 15.0579\n",
      "Epoch 122/1000, Loss: 13.4069, Val Loss: 15.0221\n",
      "Epoch 123/1000, Loss: 13.4705, Val Loss: 14.9895\n",
      "Epoch 124/1000, Loss: 13.3344, Val Loss: 14.9514\n",
      "Epoch 125/1000, Loss: 13.3372, Val Loss: 14.9033\n",
      "Epoch 126/1000, Loss: 13.2496, Val Loss: 14.8386\n",
      "Epoch 127/1000, Loss: 13.3062, Val Loss: 14.8149\n",
      "Epoch 128/1000, Loss: 13.2241, Val Loss: 14.7923\n",
      "Epoch 129/1000, Loss: 13.1368, Val Loss: 14.7575\n",
      "Epoch 130/1000, Loss: 13.1429, Val Loss: 14.7353\n",
      "Epoch 131/1000, Loss: 13.1643, Val Loss: 14.7110\n",
      "Epoch 132/1000, Loss: 13.0130, Val Loss: 14.6956\n",
      "Epoch 133/1000, Loss: 13.1400, Val Loss: 14.7036\n",
      "Epoch 134/1000, Loss: 13.3175, Val Loss: 14.7062\n",
      "Epoch 135/1000, Loss: 13.0465, Val Loss: 14.6505\n",
      "Epoch 136/1000, Loss: 13.3104, Val Loss: 14.5794\n",
      "Epoch 137/1000, Loss: 13.0913, Val Loss: 14.5177\n",
      "Epoch 138/1000, Loss: 13.1295, Val Loss: 14.4308\n",
      "Epoch 139/1000, Loss: 12.8894, Val Loss: 14.3875\n",
      "Epoch 140/1000, Loss: 13.0952, Val Loss: 14.3909\n",
      "Epoch 141/1000, Loss: 12.9739, Val Loss: 14.3696\n",
      "Epoch 142/1000, Loss: 13.2241, Val Loss: 14.3612\n",
      "Epoch 143/1000, Loss: 12.7526, Val Loss: 14.4063\n",
      "Epoch 144/1000, Loss: 12.9965, Val Loss: 14.4069\n",
      "Epoch 145/1000, Loss: 13.0614, Val Loss: 14.3406\n",
      "Epoch 146/1000, Loss: 12.9675, Val Loss: 14.3321\n",
      "Epoch 147/1000, Loss: 12.6573, Val Loss: 14.3982\n",
      "Epoch 148/1000, Loss: 12.9843, Val Loss: 14.3977\n",
      "Epoch 149/1000, Loss: 12.8463, Val Loss: 14.3078\n",
      "Epoch 150/1000, Loss: 12.6764, Val Loss: 14.2811\n",
      "Epoch 151/1000, Loss: 12.4337, Val Loss: 14.2771\n",
      "Epoch 152/1000, Loss: 12.6895, Val Loss: 14.2453\n",
      "Epoch 153/1000, Loss: 12.6877, Val Loss: 14.2523\n",
      "Epoch 154/1000, Loss: 12.4554, Val Loss: 14.2935\n",
      "Epoch 155/1000, Loss: 12.5640, Val Loss: 14.3655\n",
      "Epoch 156/1000, Loss: 12.5534, Val Loss: 14.3784\n",
      "Epoch 157/1000, Loss: 12.5466, Val Loss: 14.3641\n",
      "Epoch 158/1000, Loss: 12.5947, Val Loss: 14.3430\n",
      "Epoch 159/1000, Loss: 12.2974, Val Loss: 14.3339\n",
      "Epoch 160/1000, Loss: 12.2943, Val Loss: 14.3281\n",
      "Epoch 161/1000, Loss: 12.3240, Val Loss: 14.3199\n",
      "Epoch 162/1000, Loss: 12.4393, Val Loss: 14.3216\n",
      "Epoch 163/1000, Loss: 12.3272, Val Loss: 14.3497\n",
      "Epoch 164/1000, Loss: 12.4578, Val Loss: 14.3746\n",
      "Epoch 165/1000, Loss: 12.2269, Val Loss: 14.3717\n",
      "Epoch 166/1000, Loss: 12.0499, Val Loss: 14.3831\n",
      "Epoch 167/1000, Loss: 12.2117, Val Loss: 14.3837\n",
      "Epoch 168/1000, Loss: 12.1726, Val Loss: 14.3877\n",
      "Epoch 169/1000, Loss: 12.2492, Val Loss: 14.3879\n",
      "Epoch 170/1000, Loss: 12.2304, Val Loss: 14.3501\n",
      "Epoch 171/1000, Loss: 12.1556, Val Loss: 14.2981\n",
      "Epoch 172/1000, Loss: 12.1901, Val Loss: 14.2738\n",
      "Epoch 173/1000, Loss: 11.9475, Val Loss: 14.2509\n",
      "Epoch 174/1000, Loss: 12.1986, Val Loss: 14.2381\n",
      "Epoch 175/1000, Loss: 11.8232, Val Loss: 14.2319\n",
      "Epoch 176/1000, Loss: 11.9357, Val Loss: 14.2293\n",
      "Epoch 177/1000, Loss: 11.8451, Val Loss: 14.2749\n",
      "Epoch 178/1000, Loss: 11.9710, Val Loss: 14.3302\n",
      "Epoch 179/1000, Loss: 12.0553, Val Loss: 14.3324\n",
      "Epoch 180/1000, Loss: 11.8379, Val Loss: 14.3408\n",
      "Epoch 181/1000, Loss: 11.9649, Val Loss: 14.3840\n",
      "Epoch 182/1000, Loss: 11.8087, Val Loss: 14.4566\n",
      "Epoch 183/1000, Loss: 12.2774, Val Loss: 14.3966\n",
      "Epoch 184/1000, Loss: 11.9734, Val Loss: 14.3483\n",
      "Epoch 185/1000, Loss: 11.8448, Val Loss: 14.3310\n",
      "Epoch 186/1000, Loss: 11.9368, Val Loss: 14.3193\n",
      "Epoch 187/1000, Loss: 11.7679, Val Loss: 14.3667\n",
      "Epoch 188/1000, Loss: 11.8610, Val Loss: 14.4467\n",
      "Epoch 189/1000, Loss: 11.7499, Val Loss: 14.4576\n",
      "Epoch 190/1000, Loss: 11.8689, Val Loss: 14.4023\n",
      "Epoch 191/1000, Loss: 11.5117, Val Loss: 14.3643\n",
      "Epoch 192/1000, Loss: 11.8059, Val Loss: 14.3839\n",
      "Epoch 193/1000, Loss: 11.6831, Val Loss: 14.4333\n",
      "Epoch 194/1000, Loss: 11.6225, Val Loss: 14.4124\n",
      "Epoch 195/1000, Loss: 11.8301, Val Loss: 14.2903\n",
      "Epoch 196/1000, Loss: 11.5892, Val Loss: 14.2113\n",
      "Epoch 197/1000, Loss: 11.6612, Val Loss: 14.1886\n",
      "Epoch 198/1000, Loss: 11.7075, Val Loss: 14.1667\n",
      "Epoch 199/1000, Loss: 11.7071, Val Loss: 14.2181\n",
      "Epoch 200/1000, Loss: 11.7112, Val Loss: 14.2640\n",
      "Epoch 201/1000, Loss: 11.7081, Val Loss: 14.2763\n",
      "Epoch 202/1000, Loss: 11.5832, Val Loss: 14.2723\n",
      "Epoch 203/1000, Loss: 11.4499, Val Loss: 14.2713\n",
      "Epoch 204/1000, Loss: 11.6086, Val Loss: 14.2771\n",
      "Epoch 205/1000, Loss: 11.4044, Val Loss: 14.2866\n",
      "Epoch 206/1000, Loss: 11.6968, Val Loss: 14.2572\n",
      "Epoch 207/1000, Loss: 11.7022, Val Loss: 14.1901\n",
      "Epoch 208/1000, Loss: 11.6429, Val Loss: 14.1849\n",
      "Epoch 209/1000, Loss: 11.5654, Val Loss: 14.2455\n",
      "Epoch 210/1000, Loss: 11.5114, Val Loss: 14.3362\n",
      "Epoch 211/1000, Loss: 11.4094, Val Loss: 14.4413\n",
      "Epoch 212/1000, Loss: 11.3412, Val Loss: 14.5488\n",
      "Epoch 213/1000, Loss: 11.6810, Val Loss: 14.6458\n",
      "Epoch 214/1000, Loss: 11.2826, Val Loss: 14.7363\n",
      "Epoch 215/1000, Loss: 11.3925, Val Loss: 14.7364\n",
      "Epoch 216/1000, Loss: 11.2393, Val Loss: 14.6248\n",
      "Epoch 217/1000, Loss: 11.5164, Val Loss: 14.4493\n",
      "Epoch 218/1000, Loss: 11.4400, Val Loss: 14.3504\n",
      "Epoch 219/1000, Loss: 11.3880, Val Loss: 14.3010\n",
      "Epoch 220/1000, Loss: 11.2965, Val Loss: 14.2762\n",
      "Epoch 221/1000, Loss: 11.3142, Val Loss: 14.2773\n",
      "Epoch 222/1000, Loss: 11.4586, Val Loss: 14.2643\n",
      "Epoch 223/1000, Loss: 11.3266, Val Loss: 14.2670\n",
      "Epoch 224/1000, Loss: 11.3564, Val Loss: 14.2625\n",
      "Epoch 225/1000, Loss: 11.1656, Val Loss: 14.2340\n",
      "Epoch 226/1000, Loss: 11.4502, Val Loss: 14.2152\n",
      "Epoch 227/1000, Loss: 11.1905, Val Loss: 14.2113\n",
      "Epoch 228/1000, Loss: 11.1378, Val Loss: 14.2422\n",
      "Epoch 229/1000, Loss: 11.3695, Val Loss: 14.3744\n",
      "Epoch 230/1000, Loss: 11.3446, Val Loss: 14.5079\n",
      "Epoch 231/1000, Loss: 11.3254, Val Loss: 14.4278\n",
      "Epoch 232/1000, Loss: 11.1021, Val Loss: 14.3740\n",
      "Epoch 233/1000, Loss: 11.0755, Val Loss: 14.3691\n",
      "Epoch 234/1000, Loss: 11.2925, Val Loss: 14.4155\n",
      "Epoch 235/1000, Loss: 11.2195, Val Loss: 14.5217\n",
      "Epoch 236/1000, Loss: 11.1760, Val Loss: 14.6308\n",
      "Epoch 237/1000, Loss: 10.9597, Val Loss: 14.6606\n",
      "Epoch 238/1000, Loss: 11.1632, Val Loss: 14.5551\n",
      "Early stopping triggered at epoch 238. Best Val Loss: 14.1667\n",
      "Epoch 1/1000, Loss: 16.7105, Val Loss: 16.8086\n",
      "Epoch 2/1000, Loss: 16.6961, Val Loss: 16.8068\n",
      "Epoch 3/1000, Loss: 16.6800, Val Loss: 16.8051\n",
      "Epoch 4/1000, Loss: 16.6696, Val Loss: 16.8034\n",
      "Epoch 5/1000, Loss: 16.6696, Val Loss: 16.8015\n",
      "Epoch 6/1000, Loss: 16.6577, Val Loss: 16.7995\n",
      "Epoch 7/1000, Loss: 16.6557, Val Loss: 16.7971\n",
      "Epoch 8/1000, Loss: 16.6516, Val Loss: 16.7944\n",
      "Epoch 9/1000, Loss: 16.6457, Val Loss: 16.7915\n",
      "Epoch 10/1000, Loss: 16.6374, Val Loss: 16.7878\n",
      "Epoch 11/1000, Loss: 16.6278, Val Loss: 16.7834\n",
      "Epoch 12/1000, Loss: 16.6212, Val Loss: 16.7786\n",
      "Epoch 13/1000, Loss: 16.6122, Val Loss: 16.7735\n",
      "Epoch 14/1000, Loss: 16.5973, Val Loss: 16.7678\n",
      "Epoch 15/1000, Loss: 16.5828, Val Loss: 16.7614\n",
      "Epoch 16/1000, Loss: 16.5727, Val Loss: 16.7541\n",
      "Epoch 17/1000, Loss: 16.5547, Val Loss: 16.7455\n",
      "Epoch 18/1000, Loss: 16.5344, Val Loss: 16.7355\n",
      "Epoch 19/1000, Loss: 16.5218, Val Loss: 16.7241\n",
      "Epoch 20/1000, Loss: 16.4938, Val Loss: 16.7108\n",
      "Epoch 21/1000, Loss: 16.4766, Val Loss: 16.6969\n",
      "Epoch 22/1000, Loss: 16.4552, Val Loss: 16.6819\n",
      "Epoch 23/1000, Loss: 16.4245, Val Loss: 16.6639\n",
      "Epoch 24/1000, Loss: 16.4051, Val Loss: 16.6439\n",
      "Epoch 25/1000, Loss: 16.3841, Val Loss: 16.6221\n",
      "Epoch 26/1000, Loss: 16.3646, Val Loss: 16.6019\n",
      "Epoch 27/1000, Loss: 16.3316, Val Loss: 16.5805\n",
      "Epoch 28/1000, Loss: 16.3149, Val Loss: 16.5571\n",
      "Epoch 29/1000, Loss: 16.2707, Val Loss: 16.5330\n",
      "Epoch 30/1000, Loss: 16.2424, Val Loss: 16.5077\n",
      "Epoch 31/1000, Loss: 16.2113, Val Loss: 16.4801\n",
      "Epoch 32/1000, Loss: 16.1819, Val Loss: 16.4455\n",
      "Epoch 33/1000, Loss: 16.1586, Val Loss: 16.4121\n",
      "Epoch 34/1000, Loss: 16.1066, Val Loss: 16.3800\n",
      "Epoch 35/1000, Loss: 16.0851, Val Loss: 16.3485\n",
      "Epoch 36/1000, Loss: 16.0299, Val Loss: 16.3162\n",
      "Epoch 37/1000, Loss: 15.9900, Val Loss: 16.2789\n",
      "Epoch 38/1000, Loss: 15.9840, Val Loss: 16.2419\n",
      "Epoch 39/1000, Loss: 15.9428, Val Loss: 16.2069\n",
      "Epoch 40/1000, Loss: 15.8701, Val Loss: 16.1740\n",
      "Epoch 41/1000, Loss: 15.8548, Val Loss: 16.1353\n",
      "Epoch 42/1000, Loss: 15.7783, Val Loss: 16.0987\n",
      "Epoch 43/1000, Loss: 15.7121, Val Loss: 16.0539\n",
      "Epoch 44/1000, Loss: 15.6694, Val Loss: 16.0078\n",
      "Epoch 45/1000, Loss: 15.6615, Val Loss: 15.9693\n",
      "Epoch 46/1000, Loss: 15.6010, Val Loss: 15.9302\n",
      "Epoch 47/1000, Loss: 15.5577, Val Loss: 15.8762\n",
      "Epoch 48/1000, Loss: 15.4680, Val Loss: 15.8167\n",
      "Epoch 49/1000, Loss: 15.4236, Val Loss: 15.7689\n",
      "Epoch 50/1000, Loss: 15.3729, Val Loss: 15.7394\n",
      "Epoch 51/1000, Loss: 15.3354, Val Loss: 15.7002\n",
      "Epoch 52/1000, Loss: 15.2443, Val Loss: 15.6376\n",
      "Epoch 53/1000, Loss: 15.2169, Val Loss: 15.5717\n",
      "Epoch 54/1000, Loss: 15.1171, Val Loss: 15.5110\n",
      "Epoch 55/1000, Loss: 15.0749, Val Loss: 15.4531\n",
      "Epoch 56/1000, Loss: 15.0327, Val Loss: 15.4127\n",
      "Epoch 57/1000, Loss: 14.9452, Val Loss: 15.3733\n",
      "Epoch 58/1000, Loss: 14.9050, Val Loss: 15.3212\n",
      "Epoch 59/1000, Loss: 14.8703, Val Loss: 15.2754\n",
      "Epoch 60/1000, Loss: 14.7432, Val Loss: 15.2139\n",
      "Epoch 61/1000, Loss: 14.7096, Val Loss: 15.1917\n",
      "Epoch 62/1000, Loss: 14.6388, Val Loss: 15.1650\n",
      "Epoch 63/1000, Loss: 14.6122, Val Loss: 15.1001\n",
      "Epoch 64/1000, Loss: 14.5120, Val Loss: 15.0409\n",
      "Epoch 65/1000, Loss: 14.4637, Val Loss: 14.9730\n",
      "Epoch 66/1000, Loss: 14.4579, Val Loss: 14.9449\n",
      "Epoch 67/1000, Loss: 14.3665, Val Loss: 14.9382\n",
      "Epoch 68/1000, Loss: 14.3600, Val Loss: 14.9145\n",
      "Epoch 69/1000, Loss: 14.2237, Val Loss: 14.8453\n",
      "Epoch 70/1000, Loss: 14.1742, Val Loss: 14.7835\n",
      "Epoch 71/1000, Loss: 14.1462, Val Loss: 14.7720\n",
      "Epoch 72/1000, Loss: 14.0520, Val Loss: 14.7741\n",
      "Epoch 73/1000, Loss: 13.9812, Val Loss: 14.7515\n",
      "Epoch 74/1000, Loss: 14.0690, Val Loss: 14.7072\n",
      "Epoch 75/1000, Loss: 14.0976, Val Loss: 14.6756\n",
      "Epoch 76/1000, Loss: 14.0456, Val Loss: 14.6261\n",
      "Epoch 77/1000, Loss: 13.8239, Val Loss: 14.5995\n",
      "Epoch 78/1000, Loss: 13.8113, Val Loss: 14.5826\n",
      "Epoch 79/1000, Loss: 13.8673, Val Loss: 14.5523\n",
      "Epoch 80/1000, Loss: 13.7411, Val Loss: 14.5164\n",
      "Epoch 81/1000, Loss: 13.7984, Val Loss: 14.4757\n",
      "Epoch 82/1000, Loss: 13.6865, Val Loss: 14.4484\n",
      "Epoch 83/1000, Loss: 13.8338, Val Loss: 14.4317\n",
      "Epoch 84/1000, Loss: 13.7771, Val Loss: 14.3613\n",
      "Epoch 85/1000, Loss: 13.7984, Val Loss: 14.2913\n",
      "Epoch 86/1000, Loss: 13.7619, Val Loss: 14.2555\n",
      "Epoch 87/1000, Loss: 13.7167, Val Loss: 14.2523\n",
      "Epoch 88/1000, Loss: 13.5682, Val Loss: 14.2397\n",
      "Epoch 89/1000, Loss: 13.7766, Val Loss: 14.1667\n",
      "Epoch 90/1000, Loss: 13.7261, Val Loss: 14.1799\n",
      "Epoch 91/1000, Loss: 13.6092, Val Loss: 14.2072\n",
      "Epoch 92/1000, Loss: 13.6538, Val Loss: 14.2382\n",
      "Epoch 93/1000, Loss: 13.6268, Val Loss: 14.2392\n",
      "Epoch 94/1000, Loss: 13.6356, Val Loss: 14.2078\n",
      "Epoch 95/1000, Loss: 13.5270, Val Loss: 14.1795\n",
      "Epoch 96/1000, Loss: 13.5788, Val Loss: 14.1453\n",
      "Epoch 97/1000, Loss: 13.5330, Val Loss: 14.1057\n",
      "Epoch 98/1000, Loss: 13.5377, Val Loss: 14.0729\n",
      "Epoch 99/1000, Loss: 13.3870, Val Loss: 14.0507\n",
      "Epoch 100/1000, Loss: 13.4615, Val Loss: 14.0274\n",
      "Epoch 101/1000, Loss: 13.5023, Val Loss: 14.0103\n",
      "Epoch 102/1000, Loss: 13.2968, Val Loss: 13.9879\n",
      "Epoch 103/1000, Loss: 13.3624, Val Loss: 13.9708\n",
      "Epoch 104/1000, Loss: 13.2780, Val Loss: 13.9640\n",
      "Epoch 105/1000, Loss: 13.3554, Val Loss: 13.9710\n",
      "Epoch 106/1000, Loss: 13.2593, Val Loss: 13.9898\n",
      "Epoch 107/1000, Loss: 13.3055, Val Loss: 14.0220\n",
      "Epoch 108/1000, Loss: 13.1631, Val Loss: 14.0384\n",
      "Epoch 109/1000, Loss: 13.1219, Val Loss: 14.0809\n",
      "Epoch 110/1000, Loss: 13.1738, Val Loss: 14.0666\n",
      "Epoch 111/1000, Loss: 13.1223, Val Loss: 14.0399\n",
      "Epoch 112/1000, Loss: 13.1800, Val Loss: 13.9963\n",
      "Epoch 113/1000, Loss: 13.1463, Val Loss: 13.9566\n",
      "Epoch 114/1000, Loss: 13.1696, Val Loss: 13.9326\n",
      "Epoch 115/1000, Loss: 13.0621, Val Loss: 13.9244\n",
      "Epoch 116/1000, Loss: 13.1011, Val Loss: 13.9243\n",
      "Epoch 117/1000, Loss: 13.1538, Val Loss: 13.9163\n",
      "Epoch 118/1000, Loss: 12.9432, Val Loss: 13.9033\n",
      "Epoch 119/1000, Loss: 13.0556, Val Loss: 13.8952\n",
      "Epoch 120/1000, Loss: 13.0262, Val Loss: 13.8810\n",
      "Epoch 121/1000, Loss: 12.9637, Val Loss: 13.8739\n",
      "Epoch 122/1000, Loss: 13.0456, Val Loss: 13.8774\n",
      "Epoch 123/1000, Loss: 12.9272, Val Loss: 13.8931\n",
      "Epoch 124/1000, Loss: 12.8580, Val Loss: 13.9207\n",
      "Epoch 125/1000, Loss: 12.9037, Val Loss: 13.9527\n",
      "Epoch 126/1000, Loss: 12.9894, Val Loss: 13.9858\n",
      "Epoch 127/1000, Loss: 12.8860, Val Loss: 14.0011\n",
      "Epoch 128/1000, Loss: 12.8590, Val Loss: 14.0140\n",
      "Epoch 129/1000, Loss: 12.9987, Val Loss: 14.0108\n",
      "Epoch 130/1000, Loss: 12.8837, Val Loss: 13.9934\n",
      "Epoch 131/1000, Loss: 12.9613, Val Loss: 13.9726\n",
      "Epoch 132/1000, Loss: 12.6794, Val Loss: 13.9690\n",
      "Epoch 133/1000, Loss: 13.0008, Val Loss: 13.9818\n",
      "Epoch 134/1000, Loss: 12.7854, Val Loss: 14.0004\n",
      "Epoch 135/1000, Loss: 12.7773, Val Loss: 14.0093\n",
      "Epoch 136/1000, Loss: 12.8148, Val Loss: 14.0079\n",
      "Epoch 137/1000, Loss: 13.0369, Val Loss: 13.9961\n",
      "Epoch 138/1000, Loss: 12.7093, Val Loss: 13.9682\n",
      "Epoch 139/1000, Loss: 12.6938, Val Loss: 13.9287\n",
      "Epoch 140/1000, Loss: 12.6427, Val Loss: 13.8879\n",
      "Epoch 141/1000, Loss: 12.6168, Val Loss: 13.8604\n",
      "Epoch 142/1000, Loss: 12.5174, Val Loss: 13.8376\n",
      "Epoch 143/1000, Loss: 12.5409, Val Loss: 13.8307\n",
      "Epoch 144/1000, Loss: 12.5063, Val Loss: 13.8278\n",
      "Epoch 145/1000, Loss: 12.5418, Val Loss: 13.8495\n",
      "Epoch 146/1000, Loss: 12.4877, Val Loss: 13.8606\n",
      "Epoch 147/1000, Loss: 12.7877, Val Loss: 13.8487\n",
      "Epoch 148/1000, Loss: 12.7278, Val Loss: 13.8452\n",
      "Epoch 149/1000, Loss: 12.5876, Val Loss: 13.8564\n",
      "Epoch 150/1000, Loss: 12.7612, Val Loss: 13.8707\n",
      "Epoch 151/1000, Loss: 12.4008, Val Loss: 13.8828\n",
      "Epoch 152/1000, Loss: 12.2423, Val Loss: 13.8803\n",
      "Epoch 153/1000, Loss: 12.2975, Val Loss: 13.8548\n",
      "Epoch 154/1000, Loss: 12.3449, Val Loss: 13.8450\n",
      "Epoch 155/1000, Loss: 12.6266, Val Loss: 13.8245\n",
      "Epoch 156/1000, Loss: 12.2126, Val Loss: 13.7978\n",
      "Epoch 157/1000, Loss: 12.4186, Val Loss: 13.8030\n",
      "Epoch 158/1000, Loss: 12.5990, Val Loss: 13.8023\n",
      "Epoch 159/1000, Loss: 12.3938, Val Loss: 13.8178\n",
      "Epoch 160/1000, Loss: 12.4296, Val Loss: 13.8466\n",
      "Epoch 161/1000, Loss: 12.1276, Val Loss: 13.8729\n",
      "Epoch 162/1000, Loss: 12.2418, Val Loss: 13.8947\n",
      "Epoch 163/1000, Loss: 12.3751, Val Loss: 13.9098\n",
      "Epoch 164/1000, Loss: 12.2402, Val Loss: 13.9491\n",
      "Epoch 165/1000, Loss: 12.1458, Val Loss: 13.9825\n",
      "Epoch 166/1000, Loss: 12.2447, Val Loss: 14.0295\n",
      "Epoch 167/1000, Loss: 12.1625, Val Loss: 14.0567\n",
      "Epoch 168/1000, Loss: 12.3141, Val Loss: 14.0745\n",
      "Epoch 169/1000, Loss: 12.2730, Val Loss: 14.0888\n",
      "Epoch 170/1000, Loss: 11.9447, Val Loss: 14.0931\n",
      "Epoch 171/1000, Loss: 12.0378, Val Loss: 14.0711\n",
      "Epoch 172/1000, Loss: 12.0979, Val Loss: 14.0508\n",
      "Epoch 173/1000, Loss: 12.1133, Val Loss: 14.0319\n",
      "Epoch 174/1000, Loss: 11.9105, Val Loss: 14.0504\n",
      "Epoch 175/1000, Loss: 12.3330, Val Loss: 14.0509\n",
      "Epoch 176/1000, Loss: 12.1515, Val Loss: 14.0575\n",
      "Epoch 177/1000, Loss: 11.9737, Val Loss: 14.0721\n",
      "Epoch 178/1000, Loss: 12.2284, Val Loss: 14.0855\n",
      "Epoch 179/1000, Loss: 11.9106, Val Loss: 14.0975\n",
      "Epoch 180/1000, Loss: 11.8939, Val Loss: 14.0833\n",
      "Epoch 181/1000, Loss: 12.1796, Val Loss: 14.0430\n",
      "Epoch 182/1000, Loss: 12.0818, Val Loss: 14.0376\n",
      "Epoch 183/1000, Loss: 11.9072, Val Loss: 14.0142\n",
      "Epoch 184/1000, Loss: 11.8207, Val Loss: 14.0036\n",
      "Epoch 185/1000, Loss: 12.1416, Val Loss: 13.9968\n",
      "Epoch 186/1000, Loss: 11.9480, Val Loss: 13.9956\n",
      "Epoch 187/1000, Loss: 11.9030, Val Loss: 13.9732\n",
      "Epoch 188/1000, Loss: 11.9121, Val Loss: 13.9343\n",
      "Epoch 189/1000, Loss: 11.8893, Val Loss: 13.9138\n",
      "Epoch 190/1000, Loss: 11.9360, Val Loss: 13.8940\n",
      "Epoch 191/1000, Loss: 11.9492, Val Loss: 13.9517\n",
      "Epoch 192/1000, Loss: 11.8535, Val Loss: 14.0056\n",
      "Epoch 193/1000, Loss: 11.7816, Val Loss: 14.0062\n",
      "Epoch 194/1000, Loss: 11.8288, Val Loss: 14.0098\n",
      "Epoch 195/1000, Loss: 11.7568, Val Loss: 14.0395\n",
      "Epoch 196/1000, Loss: 11.8420, Val Loss: 14.0663\n",
      "Early stopping triggered at epoch 196. Best Val Loss: 13.7978\n",
      "Epoch 1/1000, Loss: 17.0771, Val Loss: 17.4134\n",
      "Epoch 2/1000, Loss: 17.0680, Val Loss: 17.4106\n",
      "Epoch 3/1000, Loss: 17.0645, Val Loss: 17.4077\n",
      "Epoch 4/1000, Loss: 17.0647, Val Loss: 17.4047\n",
      "Epoch 5/1000, Loss: 17.0537, Val Loss: 17.4015\n",
      "Epoch 6/1000, Loss: 17.0482, Val Loss: 17.3979\n",
      "Epoch 7/1000, Loss: 17.0455, Val Loss: 17.3940\n",
      "Epoch 8/1000, Loss: 17.0400, Val Loss: 17.3899\n",
      "Epoch 9/1000, Loss: 17.0383, Val Loss: 17.3857\n",
      "Epoch 10/1000, Loss: 17.0301, Val Loss: 17.3813\n",
      "Epoch 11/1000, Loss: 17.0250, Val Loss: 17.3767\n",
      "Epoch 12/1000, Loss: 17.0173, Val Loss: 17.3719\n",
      "Epoch 13/1000, Loss: 17.0150, Val Loss: 17.3667\n",
      "Epoch 14/1000, Loss: 17.0058, Val Loss: 17.3612\n",
      "Epoch 15/1000, Loss: 17.0031, Val Loss: 17.3557\n",
      "Epoch 16/1000, Loss: 16.9953, Val Loss: 17.3502\n",
      "Epoch 17/1000, Loss: 16.9849, Val Loss: 17.3443\n",
      "Epoch 18/1000, Loss: 16.9800, Val Loss: 17.3380\n",
      "Epoch 19/1000, Loss: 16.9691, Val Loss: 17.3307\n",
      "Epoch 20/1000, Loss: 16.9575, Val Loss: 17.3223\n",
      "Epoch 21/1000, Loss: 16.9506, Val Loss: 17.3123\n",
      "Epoch 22/1000, Loss: 16.9404, Val Loss: 17.3016\n",
      "Epoch 23/1000, Loss: 16.9290, Val Loss: 17.2899\n",
      "Epoch 24/1000, Loss: 16.9141, Val Loss: 17.2773\n",
      "Epoch 25/1000, Loss: 16.9022, Val Loss: 17.2638\n",
      "Epoch 26/1000, Loss: 16.8934, Val Loss: 17.2497\n",
      "Epoch 27/1000, Loss: 16.8721, Val Loss: 17.2341\n",
      "Epoch 28/1000, Loss: 16.8621, Val Loss: 17.2186\n",
      "Epoch 29/1000, Loss: 16.8475, Val Loss: 17.2041\n",
      "Epoch 30/1000, Loss: 16.8349, Val Loss: 17.1908\n",
      "Epoch 31/1000, Loss: 16.8179, Val Loss: 17.1783\n",
      "Epoch 32/1000, Loss: 16.8119, Val Loss: 17.1656\n",
      "Epoch 33/1000, Loss: 16.7941, Val Loss: 17.1526\n",
      "Epoch 34/1000, Loss: 16.7862, Val Loss: 17.1393\n",
      "Epoch 35/1000, Loss: 16.7691, Val Loss: 17.1255\n",
      "Epoch 36/1000, Loss: 16.7530, Val Loss: 17.1117\n",
      "Epoch 37/1000, Loss: 16.7367, Val Loss: 17.0978\n",
      "Epoch 38/1000, Loss: 16.7213, Val Loss: 17.0838\n",
      "Epoch 39/1000, Loss: 16.7133, Val Loss: 17.0687\n",
      "Epoch 40/1000, Loss: 16.6854, Val Loss: 17.0535\n",
      "Epoch 41/1000, Loss: 16.6745, Val Loss: 17.0376\n",
      "Epoch 42/1000, Loss: 16.6552, Val Loss: 17.0217\n",
      "Epoch 43/1000, Loss: 16.6351, Val Loss: 17.0055\n",
      "Epoch 44/1000, Loss: 16.6132, Val Loss: 16.9880\n",
      "Epoch 45/1000, Loss: 16.5866, Val Loss: 16.9712\n",
      "Epoch 46/1000, Loss: 16.5674, Val Loss: 16.9536\n",
      "Epoch 47/1000, Loss: 16.5517, Val Loss: 16.9346\n",
      "Epoch 48/1000, Loss: 16.5174, Val Loss: 16.9142\n",
      "Epoch 49/1000, Loss: 16.4985, Val Loss: 16.8911\n",
      "Epoch 50/1000, Loss: 16.4670, Val Loss: 16.8659\n",
      "Epoch 51/1000, Loss: 16.4408, Val Loss: 16.8436\n",
      "Epoch 52/1000, Loss: 16.3933, Val Loss: 16.8163\n",
      "Epoch 53/1000, Loss: 16.3654, Val Loss: 16.7820\n",
      "Epoch 54/1000, Loss: 16.3203, Val Loss: 16.7569\n",
      "Epoch 55/1000, Loss: 16.2930, Val Loss: 16.7273\n",
      "Epoch 56/1000, Loss: 16.2467, Val Loss: 16.6875\n",
      "Epoch 57/1000, Loss: 16.1821, Val Loss: 16.6446\n",
      "Epoch 58/1000, Loss: 16.1332, Val Loss: 16.5979\n",
      "Epoch 59/1000, Loss: 16.0926, Val Loss: 16.5439\n",
      "Epoch 60/1000, Loss: 16.0406, Val Loss: 16.4816\n",
      "Epoch 61/1000, Loss: 15.9605, Val Loss: 16.4161\n",
      "Epoch 62/1000, Loss: 15.9199, Val Loss: 16.3455\n",
      "Epoch 63/1000, Loss: 15.8584, Val Loss: 16.2587\n",
      "Epoch 64/1000, Loss: 15.7663, Val Loss: 16.1735\n",
      "Epoch 65/1000, Loss: 15.7338, Val Loss: 16.0834\n",
      "Epoch 66/1000, Loss: 15.6533, Val Loss: 16.0073\n",
      "Epoch 67/1000, Loss: 15.6177, Val Loss: 15.9359\n",
      "Epoch 68/1000, Loss: 15.5261, Val Loss: 15.8612\n",
      "Epoch 69/1000, Loss: 15.4887, Val Loss: 15.7712\n",
      "Epoch 70/1000, Loss: 15.4369, Val Loss: 15.6640\n",
      "Epoch 71/1000, Loss: 15.3309, Val Loss: 15.5442\n",
      "Epoch 72/1000, Loss: 15.2768, Val Loss: 15.4585\n",
      "Epoch 73/1000, Loss: 15.1925, Val Loss: 15.3690\n",
      "Epoch 74/1000, Loss: 15.1140, Val Loss: 15.3102\n",
      "Epoch 75/1000, Loss: 15.0729, Val Loss: 15.2104\n",
      "Epoch 76/1000, Loss: 15.0103, Val Loss: 15.1052\n",
      "Epoch 77/1000, Loss: 15.0487, Val Loss: 15.1179\n",
      "Epoch 78/1000, Loss: 14.9083, Val Loss: 15.1293\n",
      "Epoch 79/1000, Loss: 14.8954, Val Loss: 15.0792\n",
      "Epoch 80/1000, Loss: 14.8505, Val Loss: 15.0048\n",
      "Epoch 81/1000, Loss: 14.7534, Val Loss: 14.9525\n",
      "Epoch 82/1000, Loss: 14.6485, Val Loss: 14.9311\n",
      "Epoch 83/1000, Loss: 14.6356, Val Loss: 14.9082\n",
      "Epoch 84/1000, Loss: 14.5610, Val Loss: 14.8345\n",
      "Epoch 85/1000, Loss: 14.4333, Val Loss: 14.7712\n",
      "Epoch 86/1000, Loss: 14.4045, Val Loss: 14.6557\n",
      "Epoch 87/1000, Loss: 14.3149, Val Loss: 14.5954\n",
      "Epoch 88/1000, Loss: 14.2046, Val Loss: 14.5588\n",
      "Epoch 89/1000, Loss: 14.1795, Val Loss: 14.5284\n",
      "Epoch 90/1000, Loss: 14.1569, Val Loss: 14.4697\n",
      "Epoch 91/1000, Loss: 14.1030, Val Loss: 14.4308\n",
      "Epoch 92/1000, Loss: 14.0345, Val Loss: 14.3810\n",
      "Epoch 93/1000, Loss: 13.9438, Val Loss: 14.3433\n",
      "Epoch 94/1000, Loss: 13.9246, Val Loss: 14.3135\n",
      "Epoch 95/1000, Loss: 14.0263, Val Loss: 14.2909\n",
      "Epoch 96/1000, Loss: 13.9267, Val Loss: 14.2827\n",
      "Epoch 97/1000, Loss: 13.7646, Val Loss: 14.2854\n",
      "Epoch 98/1000, Loss: 13.8281, Val Loss: 14.2468\n",
      "Epoch 99/1000, Loss: 13.7205, Val Loss: 14.2025\n",
      "Epoch 100/1000, Loss: 13.8182, Val Loss: 14.1624\n",
      "Epoch 101/1000, Loss: 13.7007, Val Loss: 14.1600\n",
      "Epoch 102/1000, Loss: 13.6538, Val Loss: 14.1405\n",
      "Epoch 103/1000, Loss: 13.6784, Val Loss: 14.0996\n",
      "Epoch 104/1000, Loss: 13.6887, Val Loss: 14.0873\n",
      "Epoch 105/1000, Loss: 13.5591, Val Loss: 14.0727\n",
      "Epoch 106/1000, Loss: 13.5664, Val Loss: 14.0582\n",
      "Epoch 107/1000, Loss: 13.5211, Val Loss: 14.0593\n",
      "Epoch 108/1000, Loss: 13.7121, Val Loss: 14.0641\n",
      "Epoch 109/1000, Loss: 13.4109, Val Loss: 14.0490\n",
      "Epoch 110/1000, Loss: 13.4021, Val Loss: 14.0305\n",
      "Epoch 111/1000, Loss: 13.3047, Val Loss: 14.0040\n",
      "Epoch 112/1000, Loss: 13.3795, Val Loss: 13.9981\n",
      "Epoch 113/1000, Loss: 13.3357, Val Loss: 13.9885\n",
      "Epoch 114/1000, Loss: 13.4902, Val Loss: 13.9894\n",
      "Epoch 115/1000, Loss: 13.3017, Val Loss: 13.9919\n",
      "Epoch 116/1000, Loss: 13.2247, Val Loss: 13.9894\n",
      "Epoch 117/1000, Loss: 13.1350, Val Loss: 13.9788\n",
      "Epoch 118/1000, Loss: 13.1695, Val Loss: 13.9817\n",
      "Epoch 119/1000, Loss: 13.2199, Val Loss: 13.9830\n",
      "Epoch 120/1000, Loss: 13.4374, Val Loss: 13.9930\n",
      "Epoch 121/1000, Loss: 13.2159, Val Loss: 14.0004\n",
      "Epoch 122/1000, Loss: 13.1916, Val Loss: 13.9968\n",
      "Epoch 123/1000, Loss: 13.0177, Val Loss: 13.9962\n",
      "Epoch 124/1000, Loss: 13.0542, Val Loss: 13.9918\n",
      "Epoch 125/1000, Loss: 13.1059, Val Loss: 13.9968\n",
      "Epoch 126/1000, Loss: 13.0607, Val Loss: 14.0030\n",
      "Epoch 127/1000, Loss: 13.1186, Val Loss: 14.0049\n",
      "Epoch 128/1000, Loss: 12.9864, Val Loss: 14.0150\n",
      "Epoch 129/1000, Loss: 13.0308, Val Loss: 14.0206\n",
      "Epoch 130/1000, Loss: 12.9053, Val Loss: 14.0245\n",
      "Epoch 131/1000, Loss: 13.0523, Val Loss: 14.0174\n",
      "Epoch 132/1000, Loss: 12.9455, Val Loss: 14.0088\n",
      "Epoch 133/1000, Loss: 13.0641, Val Loss: 14.0132\n",
      "Epoch 134/1000, Loss: 12.8890, Val Loss: 14.0073\n",
      "Epoch 135/1000, Loss: 13.0771, Val Loss: 13.9695\n",
      "Epoch 136/1000, Loss: 12.9729, Val Loss: 13.9417\n",
      "Epoch 137/1000, Loss: 13.0309, Val Loss: 13.9302\n",
      "Epoch 138/1000, Loss: 12.9078, Val Loss: 13.9201\n",
      "Epoch 139/1000, Loss: 12.9659, Val Loss: 13.9308\n",
      "Epoch 140/1000, Loss: 12.8953, Val Loss: 13.9336\n",
      "Epoch 141/1000, Loss: 12.8397, Val Loss: 13.9200\n",
      "Epoch 142/1000, Loss: 12.8897, Val Loss: 13.8783\n",
      "Epoch 143/1000, Loss: 12.7568, Val Loss: 13.8728\n",
      "Epoch 144/1000, Loss: 12.8816, Val Loss: 13.8463\n",
      "Epoch 145/1000, Loss: 12.7746, Val Loss: 13.8387\n",
      "Epoch 146/1000, Loss: 12.8096, Val Loss: 13.8371\n",
      "Epoch 147/1000, Loss: 12.6626, Val Loss: 13.8351\n",
      "Epoch 148/1000, Loss: 12.8978, Val Loss: 13.8257\n",
      "Epoch 149/1000, Loss: 12.6262, Val Loss: 13.8342\n",
      "Epoch 150/1000, Loss: 12.7327, Val Loss: 13.8240\n",
      "Epoch 151/1000, Loss: 12.9183, Val Loss: 13.8118\n",
      "Epoch 152/1000, Loss: 12.5396, Val Loss: 13.8194\n",
      "Epoch 153/1000, Loss: 12.4526, Val Loss: 13.8202\n",
      "Epoch 154/1000, Loss: 12.4442, Val Loss: 13.8194\n",
      "Epoch 155/1000, Loss: 12.3527, Val Loss: 13.8258\n",
      "Epoch 156/1000, Loss: 12.5569, Val Loss: 13.8338\n",
      "Epoch 157/1000, Loss: 12.5951, Val Loss: 13.8514\n",
      "Epoch 158/1000, Loss: 12.5039, Val Loss: 13.8531\n",
      "Epoch 159/1000, Loss: 12.5383, Val Loss: 13.8268\n",
      "Epoch 160/1000, Loss: 12.6278, Val Loss: 13.8127\n",
      "Epoch 161/1000, Loss: 12.8240, Val Loss: 13.7992\n",
      "Epoch 162/1000, Loss: 12.4214, Val Loss: 13.8171\n",
      "Epoch 163/1000, Loss: 12.5137, Val Loss: 13.8427\n",
      "Epoch 164/1000, Loss: 12.4358, Val Loss: 13.8699\n",
      "Epoch 165/1000, Loss: 12.5564, Val Loss: 13.8711\n",
      "Epoch 166/1000, Loss: 12.4314, Val Loss: 13.8670\n",
      "Epoch 167/1000, Loss: 12.5015, Val Loss: 13.8565\n",
      "Epoch 168/1000, Loss: 12.4213, Val Loss: 13.8360\n",
      "Epoch 169/1000, Loss: 12.6159, Val Loss: 13.8320\n",
      "Epoch 170/1000, Loss: 12.2526, Val Loss: 13.8279\n",
      "Epoch 171/1000, Loss: 12.1960, Val Loss: 13.8290\n",
      "Epoch 172/1000, Loss: 12.4971, Val Loss: 13.8300\n",
      "Epoch 173/1000, Loss: 12.3124, Val Loss: 13.8118\n",
      "Epoch 174/1000, Loss: 12.2580, Val Loss: 13.8135\n",
      "Epoch 175/1000, Loss: 12.3177, Val Loss: 13.8107\n",
      "Epoch 176/1000, Loss: 12.1886, Val Loss: 13.8106\n",
      "Epoch 177/1000, Loss: 12.4160, Val Loss: 13.8050\n",
      "Epoch 178/1000, Loss: 12.2609, Val Loss: 13.8239\n",
      "Epoch 179/1000, Loss: 11.9266, Val Loss: 13.8616\n",
      "Epoch 180/1000, Loss: 12.1497, Val Loss: 13.8842\n",
      "Epoch 181/1000, Loss: 12.2413, Val Loss: 13.9141\n",
      "Epoch 182/1000, Loss: 12.0807, Val Loss: 13.9111\n",
      "Epoch 183/1000, Loss: 12.3210, Val Loss: 13.8963\n",
      "Epoch 184/1000, Loss: 12.0103, Val Loss: 13.8838\n",
      "Epoch 185/1000, Loss: 11.8656, Val Loss: 13.8868\n",
      "Epoch 186/1000, Loss: 11.8991, Val Loss: 13.8812\n",
      "Epoch 187/1000, Loss: 11.8977, Val Loss: 13.8821\n",
      "Epoch 188/1000, Loss: 12.0038, Val Loss: 13.8694\n",
      "Epoch 189/1000, Loss: 12.0822, Val Loss: 13.8628\n",
      "Epoch 190/1000, Loss: 12.1404, Val Loss: 13.8696\n",
      "Epoch 191/1000, Loss: 11.8357, Val Loss: 13.8699\n",
      "Epoch 192/1000, Loss: 11.6241, Val Loss: 13.8610\n",
      "Epoch 193/1000, Loss: 11.9736, Val Loss: 13.8547\n",
      "Epoch 194/1000, Loss: 11.8533, Val Loss: 13.8686\n",
      "Epoch 195/1000, Loss: 12.1314, Val Loss: 13.9086\n",
      "Epoch 196/1000, Loss: 11.8987, Val Loss: 13.9161\n",
      "Epoch 197/1000, Loss: 11.6568, Val Loss: 13.9493\n",
      "Epoch 198/1000, Loss: 11.9748, Val Loss: 14.0043\n",
      "Epoch 199/1000, Loss: 12.1048, Val Loss: 14.0467\n",
      "Epoch 200/1000, Loss: 11.7896, Val Loss: 14.0699\n",
      "Epoch 201/1000, Loss: 12.0258, Val Loss: 14.0647\n",
      "Early stopping triggered at epoch 201. Best Val Loss: 13.7992\n",
      "Epoch 1/1000, Loss: 16.8147, Val Loss: 17.0703\n",
      "Epoch 2/1000, Loss: 16.7821, Val Loss: 17.0647\n",
      "Epoch 3/1000, Loss: 16.7448, Val Loss: 17.0595\n",
      "Epoch 4/1000, Loss: 16.7346, Val Loss: 17.0544\n",
      "Epoch 5/1000, Loss: 16.7348, Val Loss: 17.0486\n",
      "Epoch 6/1000, Loss: 16.7074, Val Loss: 17.0422\n",
      "Epoch 7/1000, Loss: 16.6888, Val Loss: 17.0354\n",
      "Epoch 8/1000, Loss: 16.6780, Val Loss: 17.0278\n",
      "Epoch 9/1000, Loss: 16.6625, Val Loss: 17.0190\n",
      "Epoch 10/1000, Loss: 16.6427, Val Loss: 17.0102\n",
      "Epoch 11/1000, Loss: 16.6282, Val Loss: 17.0011\n",
      "Epoch 12/1000, Loss: 16.6024, Val Loss: 16.9910\n",
      "Epoch 13/1000, Loss: 16.5838, Val Loss: 16.9809\n",
      "Epoch 14/1000, Loss: 16.5778, Val Loss: 16.9717\n",
      "Epoch 15/1000, Loss: 16.5158, Val Loss: 16.9630\n",
      "Epoch 16/1000, Loss: 16.5007, Val Loss: 16.9532\n",
      "Epoch 17/1000, Loss: 16.4495, Val Loss: 16.9412\n",
      "Epoch 18/1000, Loss: 16.4443, Val Loss: 16.9270\n",
      "Epoch 19/1000, Loss: 16.3878, Val Loss: 16.9089\n",
      "Epoch 20/1000, Loss: 16.3575, Val Loss: 16.8877\n",
      "Epoch 21/1000, Loss: 16.3407, Val Loss: 16.8624\n",
      "Epoch 22/1000, Loss: 16.2757, Val Loss: 16.8343\n",
      "Epoch 23/1000, Loss: 16.2362, Val Loss: 16.8026\n",
      "Epoch 24/1000, Loss: 16.2034, Val Loss: 16.7703\n",
      "Epoch 25/1000, Loss: 16.1454, Val Loss: 16.7362\n",
      "Epoch 26/1000, Loss: 16.1181, Val Loss: 16.7028\n",
      "Epoch 27/1000, Loss: 16.0572, Val Loss: 16.6645\n",
      "Epoch 28/1000, Loss: 15.9788, Val Loss: 16.6222\n",
      "Epoch 29/1000, Loss: 16.0016, Val Loss: 16.5735\n",
      "Epoch 30/1000, Loss: 15.8929, Val Loss: 16.5186\n",
      "Epoch 31/1000, Loss: 15.8735, Val Loss: 16.4584\n",
      "Epoch 32/1000, Loss: 15.8025, Val Loss: 16.3977\n",
      "Epoch 33/1000, Loss: 15.7611, Val Loss: 16.3402\n",
      "Epoch 34/1000, Loss: 15.6969, Val Loss: 16.2834\n",
      "Epoch 35/1000, Loss: 15.6932, Val Loss: 16.2345\n",
      "Epoch 36/1000, Loss: 15.6309, Val Loss: 16.1883\n",
      "Epoch 37/1000, Loss: 15.5481, Val Loss: 16.1362\n",
      "Epoch 38/1000, Loss: 15.5290, Val Loss: 16.0783\n",
      "Epoch 39/1000, Loss: 15.4268, Val Loss: 16.0125\n",
      "Epoch 40/1000, Loss: 15.4203, Val Loss: 15.9470\n",
      "Epoch 41/1000, Loss: 15.4212, Val Loss: 15.8899\n",
      "Epoch 42/1000, Loss: 15.3757, Val Loss: 15.8487\n",
      "Epoch 43/1000, Loss: 15.2885, Val Loss: 15.8160\n",
      "Epoch 44/1000, Loss: 15.2796, Val Loss: 15.7877\n",
      "Epoch 45/1000, Loss: 15.2244, Val Loss: 15.7647\n",
      "Epoch 46/1000, Loss: 15.2670, Val Loss: 15.7376\n",
      "Epoch 47/1000, Loss: 15.1791, Val Loss: 15.7113\n",
      "Epoch 48/1000, Loss: 15.2443, Val Loss: 15.6948\n",
      "Epoch 49/1000, Loss: 15.0901, Val Loss: 15.6852\n",
      "Epoch 50/1000, Loss: 15.0633, Val Loss: 15.6814\n",
      "Epoch 51/1000, Loss: 15.0653, Val Loss: 15.6795\n",
      "Epoch 52/1000, Loss: 15.0771, Val Loss: 15.6744\n",
      "Epoch 53/1000, Loss: 14.9617, Val Loss: 15.6662\n",
      "Epoch 54/1000, Loss: 14.9605, Val Loss: 15.6369\n",
      "Epoch 55/1000, Loss: 14.9520, Val Loss: 15.6099\n",
      "Epoch 56/1000, Loss: 14.9080, Val Loss: 15.5970\n",
      "Epoch 57/1000, Loss: 14.9238, Val Loss: 15.5752\n",
      "Epoch 58/1000, Loss: 14.7849, Val Loss: 15.5622\n",
      "Epoch 59/1000, Loss: 14.8356, Val Loss: 15.5286\n",
      "Epoch 60/1000, Loss: 14.8529, Val Loss: 15.4975\n",
      "Epoch 61/1000, Loss: 14.6755, Val Loss: 15.4740\n",
      "Epoch 62/1000, Loss: 14.6971, Val Loss: 15.4539\n",
      "Epoch 63/1000, Loss: 14.7704, Val Loss: 15.4382\n",
      "Epoch 64/1000, Loss: 14.5976, Val Loss: 15.4290\n",
      "Epoch 65/1000, Loss: 14.7920, Val Loss: 15.4091\n",
      "Epoch 66/1000, Loss: 14.5305, Val Loss: 15.3919\n",
      "Epoch 67/1000, Loss: 14.5820, Val Loss: 15.3621\n",
      "Epoch 68/1000, Loss: 14.5016, Val Loss: 15.3334\n",
      "Epoch 69/1000, Loss: 14.5433, Val Loss: 15.3132\n",
      "Epoch 70/1000, Loss: 14.4656, Val Loss: 15.3012\n",
      "Epoch 71/1000, Loss: 14.4652, Val Loss: 15.2724\n",
      "Epoch 72/1000, Loss: 14.3874, Val Loss: 15.2546\n",
      "Epoch 73/1000, Loss: 14.3479, Val Loss: 15.2091\n",
      "Epoch 74/1000, Loss: 14.4397, Val Loss: 15.1669\n",
      "Epoch 75/1000, Loss: 14.3448, Val Loss: 15.1244\n",
      "Epoch 76/1000, Loss: 14.1866, Val Loss: 15.0942\n",
      "Epoch 77/1000, Loss: 14.2904, Val Loss: 15.0816\n",
      "Epoch 78/1000, Loss: 14.2011, Val Loss: 15.0667\n",
      "Epoch 79/1000, Loss: 14.2367, Val Loss: 15.0513\n",
      "Epoch 80/1000, Loss: 14.1100, Val Loss: 15.0062\n",
      "Epoch 81/1000, Loss: 14.1131, Val Loss: 14.9683\n",
      "Epoch 82/1000, Loss: 14.0482, Val Loss: 14.9281\n",
      "Epoch 83/1000, Loss: 13.9702, Val Loss: 14.9008\n",
      "Epoch 84/1000, Loss: 13.9803, Val Loss: 14.8737\n",
      "Epoch 85/1000, Loss: 14.0164, Val Loss: 14.8460\n",
      "Epoch 86/1000, Loss: 13.8820, Val Loss: 14.8186\n",
      "Epoch 87/1000, Loss: 13.7591, Val Loss: 14.7894\n",
      "Epoch 88/1000, Loss: 13.8497, Val Loss: 14.7429\n",
      "Epoch 89/1000, Loss: 13.9234, Val Loss: 14.6970\n",
      "Epoch 90/1000, Loss: 13.6664, Val Loss: 14.6466\n",
      "Epoch 91/1000, Loss: 13.7307, Val Loss: 14.6096\n",
      "Epoch 92/1000, Loss: 13.5358, Val Loss: 14.5535\n",
      "Epoch 93/1000, Loss: 13.6294, Val Loss: 14.4938\n",
      "Epoch 94/1000, Loss: 13.4901, Val Loss: 14.4528\n",
      "Epoch 95/1000, Loss: 13.5545, Val Loss: 14.4210\n",
      "Epoch 96/1000, Loss: 13.5358, Val Loss: 14.3912\n",
      "Epoch 97/1000, Loss: 13.5901, Val Loss: 14.3771\n",
      "Epoch 98/1000, Loss: 13.4339, Val Loss: 14.3684\n",
      "Epoch 99/1000, Loss: 13.2851, Val Loss: 14.3818\n",
      "Epoch 100/1000, Loss: 13.2810, Val Loss: 14.3764\n",
      "Epoch 101/1000, Loss: 13.4282, Val Loss: 14.3562\n",
      "Epoch 102/1000, Loss: 13.4263, Val Loss: 14.3193\n",
      "Epoch 103/1000, Loss: 13.2806, Val Loss: 14.3071\n",
      "Epoch 104/1000, Loss: 13.2070, Val Loss: 14.2846\n",
      "Epoch 105/1000, Loss: 13.4041, Val Loss: 14.2536\n",
      "Epoch 106/1000, Loss: 13.2280, Val Loss: 14.2250\n",
      "Epoch 107/1000, Loss: 13.3488, Val Loss: 14.1996\n",
      "Epoch 108/1000, Loss: 13.1723, Val Loss: 14.1969\n",
      "Epoch 109/1000, Loss: 13.2638, Val Loss: 14.2128\n",
      "Epoch 110/1000, Loss: 13.0089, Val Loss: 14.2412\n",
      "Epoch 111/1000, Loss: 13.1490, Val Loss: 14.2785\n",
      "Epoch 112/1000, Loss: 13.0791, Val Loss: 14.2598\n",
      "Epoch 113/1000, Loss: 13.0296, Val Loss: 14.2533\n",
      "Epoch 114/1000, Loss: 13.0801, Val Loss: 14.2332\n",
      "Epoch 115/1000, Loss: 13.0043, Val Loss: 14.2001\n",
      "Epoch 116/1000, Loss: 13.0734, Val Loss: 14.1539\n",
      "Epoch 117/1000, Loss: 12.9455, Val Loss: 14.0922\n",
      "Epoch 118/1000, Loss: 12.8310, Val Loss: 14.0384\n",
      "Epoch 119/1000, Loss: 12.8794, Val Loss: 14.0147\n",
      "Epoch 120/1000, Loss: 12.7037, Val Loss: 14.0092\n",
      "Epoch 121/1000, Loss: 12.8541, Val Loss: 13.9977\n",
      "Epoch 122/1000, Loss: 12.5719, Val Loss: 14.0070\n",
      "Epoch 123/1000, Loss: 12.9617, Val Loss: 14.0311\n",
      "Epoch 124/1000, Loss: 12.8353, Val Loss: 14.0609\n",
      "Epoch 125/1000, Loss: 12.8176, Val Loss: 14.0717\n",
      "Epoch 126/1000, Loss: 12.7562, Val Loss: 14.0898\n",
      "Epoch 127/1000, Loss: 12.6298, Val Loss: 14.1047\n",
      "Epoch 128/1000, Loss: 12.6569, Val Loss: 14.0989\n",
      "Epoch 129/1000, Loss: 12.7460, Val Loss: 14.0665\n",
      "Epoch 130/1000, Loss: 12.6081, Val Loss: 14.0324\n",
      "Epoch 131/1000, Loss: 13.0165, Val Loss: 14.0046\n",
      "Epoch 132/1000, Loss: 12.6542, Val Loss: 13.9819\n",
      "Epoch 133/1000, Loss: 12.7443, Val Loss: 13.9527\n",
      "Epoch 134/1000, Loss: 12.7654, Val Loss: 13.9428\n",
      "Epoch 135/1000, Loss: 12.7413, Val Loss: 13.9386\n",
      "Epoch 136/1000, Loss: 12.5853, Val Loss: 13.9559\n",
      "Epoch 137/1000, Loss: 12.5502, Val Loss: 14.0123\n",
      "Epoch 138/1000, Loss: 12.5902, Val Loss: 14.0962\n",
      "Epoch 139/1000, Loss: 12.6603, Val Loss: 14.1321\n",
      "Epoch 140/1000, Loss: 12.5145, Val Loss: 14.1155\n",
      "Epoch 141/1000, Loss: 12.5015, Val Loss: 14.0847\n",
      "Epoch 142/1000, Loss: 12.3494, Val Loss: 14.0874\n",
      "Epoch 143/1000, Loss: 12.4547, Val Loss: 14.0556\n",
      "Epoch 144/1000, Loss: 12.4437, Val Loss: 13.9928\n",
      "Epoch 145/1000, Loss: 12.3432, Val Loss: 13.9408\n",
      "Epoch 146/1000, Loss: 12.6662, Val Loss: 13.9223\n",
      "Epoch 147/1000, Loss: 12.5400, Val Loss: 13.8949\n",
      "Epoch 148/1000, Loss: 12.4402, Val Loss: 13.9025\n",
      "Epoch 149/1000, Loss: 12.4504, Val Loss: 13.9095\n",
      "Epoch 150/1000, Loss: 12.5474, Val Loss: 13.9115\n",
      "Epoch 151/1000, Loss: 12.2914, Val Loss: 13.9347\n",
      "Epoch 152/1000, Loss: 12.1793, Val Loss: 13.9578\n",
      "Epoch 153/1000, Loss: 12.2418, Val Loss: 13.9913\n",
      "Epoch 154/1000, Loss: 12.4349, Val Loss: 14.0164\n",
      "Epoch 155/1000, Loss: 12.3035, Val Loss: 14.0275\n",
      "Epoch 156/1000, Loss: 12.3317, Val Loss: 13.9954\n",
      "Epoch 157/1000, Loss: 12.2554, Val Loss: 13.9546\n",
      "Epoch 158/1000, Loss: 12.0431, Val Loss: 13.9313\n",
      "Epoch 159/1000, Loss: 12.1336, Val Loss: 13.9085\n",
      "Epoch 160/1000, Loss: 12.2765, Val Loss: 13.8933\n",
      "Epoch 161/1000, Loss: 11.9077, Val Loss: 13.9033\n",
      "Epoch 162/1000, Loss: 12.1564, Val Loss: 13.9134\n",
      "Epoch 163/1000, Loss: 12.0855, Val Loss: 13.9382\n",
      "Epoch 164/1000, Loss: 11.9598, Val Loss: 13.9673\n",
      "Epoch 165/1000, Loss: 11.8672, Val Loss: 13.9791\n",
      "Epoch 166/1000, Loss: 11.8905, Val Loss: 13.9880\n",
      "Epoch 167/1000, Loss: 12.0061, Val Loss: 13.9809\n",
      "Epoch 168/1000, Loss: 11.8353, Val Loss: 14.0099\n",
      "Epoch 169/1000, Loss: 11.9395, Val Loss: 14.0170\n",
      "Epoch 170/1000, Loss: 11.9005, Val Loss: 13.9998\n",
      "Epoch 171/1000, Loss: 11.9347, Val Loss: 13.9969\n",
      "Epoch 172/1000, Loss: 11.7291, Val Loss: 14.0075\n",
      "Epoch 173/1000, Loss: 11.9257, Val Loss: 14.0143\n",
      "Epoch 174/1000, Loss: 11.8743, Val Loss: 14.0340\n",
      "Epoch 175/1000, Loss: 11.7715, Val Loss: 14.0246\n",
      "Epoch 176/1000, Loss: 12.0661, Val Loss: 13.9842\n",
      "Epoch 177/1000, Loss: 11.7779, Val Loss: 13.9445\n",
      "Epoch 178/1000, Loss: 11.9906, Val Loss: 13.9261\n",
      "Epoch 179/1000, Loss: 11.7136, Val Loss: 13.9184\n",
      "Epoch 180/1000, Loss: 11.6862, Val Loss: 13.9427\n",
      "Epoch 181/1000, Loss: 11.6978, Val Loss: 13.9742\n",
      "Epoch 182/1000, Loss: 11.6134, Val Loss: 14.0346\n",
      "Epoch 183/1000, Loss: 11.6741, Val Loss: 14.1008\n",
      "Epoch 184/1000, Loss: 11.4664, Val Loss: 14.1144\n",
      "Epoch 185/1000, Loss: 11.5729, Val Loss: 14.1152\n",
      "Epoch 186/1000, Loss: 11.5107, Val Loss: 14.1770\n",
      "Epoch 187/1000, Loss: 11.9651, Val Loss: 14.1383\n",
      "Epoch 188/1000, Loss: 11.7812, Val Loss: 14.0710\n",
      "Epoch 189/1000, Loss: 11.5552, Val Loss: 14.0096\n",
      "Epoch 190/1000, Loss: 11.5488, Val Loss: 13.9500\n",
      "Epoch 191/1000, Loss: 11.7313, Val Loss: 13.9375\n",
      "Epoch 192/1000, Loss: 11.5315, Val Loss: 13.8812\n",
      "Epoch 193/1000, Loss: 11.4980, Val Loss: 13.8316\n",
      "Epoch 194/1000, Loss: 11.7818, Val Loss: 13.8775\n",
      "Epoch 195/1000, Loss: 11.7038, Val Loss: 13.9408\n",
      "Epoch 196/1000, Loss: 11.6320, Val Loss: 14.0098\n",
      "Epoch 197/1000, Loss: 11.7587, Val Loss: 14.0359\n",
      "Epoch 198/1000, Loss: 11.3880, Val Loss: 14.0373\n",
      "Epoch 199/1000, Loss: 11.7994, Val Loss: 14.0196\n",
      "Epoch 200/1000, Loss: 11.4940, Val Loss: 13.9937\n",
      "Epoch 201/1000, Loss: 11.4400, Val Loss: 13.9928\n",
      "Epoch 202/1000, Loss: 11.4313, Val Loss: 14.0241\n",
      "Epoch 203/1000, Loss: 11.3620, Val Loss: 14.0878\n",
      "Epoch 204/1000, Loss: 11.3507, Val Loss: 14.1165\n",
      "Epoch 205/1000, Loss: 11.3949, Val Loss: 14.1227\n",
      "Epoch 206/1000, Loss: 11.3501, Val Loss: 14.1318\n",
      "Epoch 207/1000, Loss: 11.1691, Val Loss: 14.1331\n",
      "Epoch 208/1000, Loss: 11.3970, Val Loss: 14.1342\n",
      "Epoch 209/1000, Loss: 11.4009, Val Loss: 14.1146\n",
      "Epoch 210/1000, Loss: 11.3433, Val Loss: 14.0661\n",
      "Epoch 211/1000, Loss: 11.2387, Val Loss: 14.0637\n",
      "Epoch 212/1000, Loss: 11.3945, Val Loss: 14.0787\n",
      "Epoch 213/1000, Loss: 11.1644, Val Loss: 14.0851\n",
      "Epoch 214/1000, Loss: 11.3446, Val Loss: 14.0696\n",
      "Epoch 215/1000, Loss: 10.9114, Val Loss: 14.0531\n",
      "Epoch 216/1000, Loss: 11.2719, Val Loss: 14.0478\n",
      "Epoch 217/1000, Loss: 11.3245, Val Loss: 14.0760\n",
      "Epoch 218/1000, Loss: 11.3447, Val Loss: 14.1103\n",
      "Epoch 219/1000, Loss: 11.1764, Val Loss: 14.1574\n",
      "Epoch 220/1000, Loss: 11.3374, Val Loss: 14.1849\n",
      "Epoch 221/1000, Loss: 11.2802, Val Loss: 14.2568\n",
      "Epoch 222/1000, Loss: 11.1689, Val Loss: 14.2796\n",
      "Epoch 223/1000, Loss: 11.2082, Val Loss: 14.2782\n",
      "Epoch 224/1000, Loss: 11.3211, Val Loss: 14.2937\n",
      "Epoch 225/1000, Loss: 11.0297, Val Loss: 14.3263\n",
      "Epoch 226/1000, Loss: 11.0372, Val Loss: 14.2914\n",
      "Epoch 227/1000, Loss: 11.1947, Val Loss: 14.2017\n",
      "Epoch 228/1000, Loss: 10.7806, Val Loss: 14.1779\n",
      "Epoch 229/1000, Loss: 11.2404, Val Loss: 14.1147\n",
      "Epoch 230/1000, Loss: 11.2475, Val Loss: 14.0436\n",
      "Epoch 231/1000, Loss: 10.9667, Val Loss: 14.0375\n",
      "Epoch 232/1000, Loss: 11.0306, Val Loss: 14.0486\n",
      "Epoch 233/1000, Loss: 11.1116, Val Loss: 14.0615\n",
      "Early stopping triggered at epoch 233. Best Val Loss: 13.8316\n",
      "Epoch 1/1000, Loss: 16.7814, Val Loss: 16.9687\n",
      "Epoch 2/1000, Loss: 16.7516, Val Loss: 16.9644\n",
      "Epoch 3/1000, Loss: 16.7377, Val Loss: 16.9604\n",
      "Epoch 4/1000, Loss: 16.7254, Val Loss: 16.9562\n",
      "Epoch 5/1000, Loss: 16.7161, Val Loss: 16.9508\n",
      "Epoch 6/1000, Loss: 16.7050, Val Loss: 16.9454\n",
      "Epoch 7/1000, Loss: 16.6941, Val Loss: 16.9396\n",
      "Epoch 8/1000, Loss: 16.6787, Val Loss: 16.9332\n",
      "Epoch 9/1000, Loss: 16.6660, Val Loss: 16.9262\n",
      "Epoch 10/1000, Loss: 16.6519, Val Loss: 16.9183\n",
      "Epoch 11/1000, Loss: 16.6252, Val Loss: 16.9096\n",
      "Epoch 12/1000, Loss: 16.6055, Val Loss: 16.9004\n",
      "Epoch 13/1000, Loss: 16.5906, Val Loss: 16.8896\n",
      "Epoch 14/1000, Loss: 16.5645, Val Loss: 16.8769\n",
      "Epoch 15/1000, Loss: 16.5463, Val Loss: 16.8625\n",
      "Epoch 16/1000, Loss: 16.5150, Val Loss: 16.8461\n",
      "Epoch 17/1000, Loss: 16.4988, Val Loss: 16.8274\n",
      "Epoch 18/1000, Loss: 16.4617, Val Loss: 16.8060\n",
      "Epoch 19/1000, Loss: 16.4446, Val Loss: 16.7814\n",
      "Epoch 20/1000, Loss: 16.4099, Val Loss: 16.7562\n",
      "Epoch 21/1000, Loss: 16.3854, Val Loss: 16.7291\n",
      "Epoch 22/1000, Loss: 16.3634, Val Loss: 16.7026\n",
      "Epoch 23/1000, Loss: 16.3196, Val Loss: 16.6721\n",
      "Epoch 24/1000, Loss: 16.2992, Val Loss: 16.6405\n",
      "Epoch 25/1000, Loss: 16.2657, Val Loss: 16.6098\n",
      "Epoch 26/1000, Loss: 16.2311, Val Loss: 16.5782\n",
      "Epoch 27/1000, Loss: 16.1955, Val Loss: 16.5459\n",
      "Epoch 28/1000, Loss: 16.1892, Val Loss: 16.5135\n",
      "Epoch 29/1000, Loss: 16.1374, Val Loss: 16.4803\n",
      "Epoch 30/1000, Loss: 16.1330, Val Loss: 16.4473\n",
      "Epoch 31/1000, Loss: 16.0929, Val Loss: 16.4095\n",
      "Epoch 32/1000, Loss: 16.0430, Val Loss: 16.3706\n",
      "Epoch 33/1000, Loss: 16.0108, Val Loss: 16.3323\n",
      "Epoch 34/1000, Loss: 15.9768, Val Loss: 16.2916\n",
      "Epoch 35/1000, Loss: 15.9631, Val Loss: 16.2567\n",
      "Epoch 36/1000, Loss: 15.9114, Val Loss: 16.2263\n",
      "Epoch 37/1000, Loss: 15.8815, Val Loss: 16.1997\n",
      "Epoch 38/1000, Loss: 15.8329, Val Loss: 16.1642\n",
      "Epoch 39/1000, Loss: 15.8201, Val Loss: 16.1352\n",
      "Epoch 40/1000, Loss: 15.7850, Val Loss: 16.1005\n",
      "Epoch 41/1000, Loss: 15.7357, Val Loss: 16.0680\n",
      "Epoch 42/1000, Loss: 15.6998, Val Loss: 16.0501\n",
      "Epoch 43/1000, Loss: 15.6790, Val Loss: 16.0243\n",
      "Epoch 44/1000, Loss: 15.6279, Val Loss: 15.9867\n",
      "Epoch 45/1000, Loss: 15.5923, Val Loss: 15.9289\n",
      "Epoch 46/1000, Loss: 15.5197, Val Loss: 15.8800\n",
      "Epoch 47/1000, Loss: 15.4885, Val Loss: 15.8535\n",
      "Epoch 48/1000, Loss: 15.4399, Val Loss: 15.8308\n",
      "Epoch 49/1000, Loss: 15.3985, Val Loss: 15.7783\n",
      "Epoch 50/1000, Loss: 15.3718, Val Loss: 15.7266\n",
      "Epoch 51/1000, Loss: 15.3297, Val Loss: 15.6893\n",
      "Epoch 52/1000, Loss: 15.2457, Val Loss: 15.6369\n",
      "Epoch 53/1000, Loss: 15.1979, Val Loss: 15.5824\n",
      "Epoch 54/1000, Loss: 15.1206, Val Loss: 15.5520\n",
      "Epoch 55/1000, Loss: 15.1180, Val Loss: 15.5112\n",
      "Epoch 56/1000, Loss: 15.0434, Val Loss: 15.4300\n",
      "Epoch 57/1000, Loss: 14.9958, Val Loss: 15.3550\n",
      "Epoch 58/1000, Loss: 14.8985, Val Loss: 15.2954\n",
      "Epoch 59/1000, Loss: 14.8959, Val Loss: 15.2360\n",
      "Epoch 60/1000, Loss: 14.7491, Val Loss: 15.1590\n",
      "Epoch 61/1000, Loss: 14.7253, Val Loss: 15.0952\n",
      "Epoch 62/1000, Loss: 14.7655, Val Loss: 14.9994\n",
      "Epoch 63/1000, Loss: 14.6951, Val Loss: 14.9558\n",
      "Epoch 64/1000, Loss: 14.6223, Val Loss: 14.9116\n",
      "Epoch 65/1000, Loss: 14.5180, Val Loss: 14.8677\n",
      "Epoch 66/1000, Loss: 14.4652, Val Loss: 14.8285\n",
      "Epoch 67/1000, Loss: 14.4816, Val Loss: 14.7839\n",
      "Epoch 68/1000, Loss: 14.3849, Val Loss: 14.7021\n",
      "Epoch 69/1000, Loss: 14.3085, Val Loss: 14.6157\n",
      "Epoch 70/1000, Loss: 14.3155, Val Loss: 14.5548\n",
      "Epoch 71/1000, Loss: 14.3928, Val Loss: 14.5449\n",
      "Epoch 72/1000, Loss: 14.2267, Val Loss: 14.5832\n",
      "Epoch 73/1000, Loss: 14.2617, Val Loss: 14.5236\n",
      "Epoch 74/1000, Loss: 14.2270, Val Loss: 14.4223\n",
      "Epoch 75/1000, Loss: 14.0671, Val Loss: 14.3657\n",
      "Epoch 76/1000, Loss: 14.1016, Val Loss: 14.3484\n",
      "Epoch 77/1000, Loss: 14.0493, Val Loss: 14.3604\n",
      "Epoch 78/1000, Loss: 13.9671, Val Loss: 14.3511\n",
      "Epoch 79/1000, Loss: 13.9783, Val Loss: 14.3163\n",
      "Epoch 80/1000, Loss: 13.9252, Val Loss: 14.2984\n",
      "Epoch 81/1000, Loss: 13.8096, Val Loss: 14.2780\n",
      "Epoch 82/1000, Loss: 13.9954, Val Loss: 14.2288\n",
      "Epoch 83/1000, Loss: 13.8127, Val Loss: 14.2094\n",
      "Epoch 84/1000, Loss: 13.7946, Val Loss: 14.2022\n",
      "Epoch 85/1000, Loss: 13.8486, Val Loss: 14.1857\n",
      "Epoch 86/1000, Loss: 13.7591, Val Loss: 14.1711\n",
      "Epoch 87/1000, Loss: 13.7690, Val Loss: 14.1430\n",
      "Epoch 88/1000, Loss: 13.6413, Val Loss: 14.1235\n",
      "Epoch 89/1000, Loss: 13.7020, Val Loss: 14.0982\n",
      "Epoch 90/1000, Loss: 13.7020, Val Loss: 14.0946\n",
      "Epoch 91/1000, Loss: 13.6767, Val Loss: 14.0899\n",
      "Epoch 92/1000, Loss: 13.7266, Val Loss: 14.0961\n",
      "Epoch 93/1000, Loss: 13.5578, Val Loss: 14.0903\n",
      "Epoch 94/1000, Loss: 13.6629, Val Loss: 14.0759\n",
      "Epoch 95/1000, Loss: 13.6599, Val Loss: 14.0972\n",
      "Epoch 96/1000, Loss: 13.7254, Val Loss: 14.1119\n",
      "Epoch 97/1000, Loss: 13.4905, Val Loss: 14.1255\n",
      "Epoch 98/1000, Loss: 13.6040, Val Loss: 14.1482\n",
      "Epoch 99/1000, Loss: 13.4597, Val Loss: 14.1462\n",
      "Epoch 100/1000, Loss: 13.4747, Val Loss: 14.1083\n",
      "Epoch 101/1000, Loss: 13.3989, Val Loss: 14.0902\n",
      "Epoch 102/1000, Loss: 13.4671, Val Loss: 14.0972\n",
      "Epoch 103/1000, Loss: 13.5454, Val Loss: 14.0881\n",
      "Epoch 104/1000, Loss: 13.4875, Val Loss: 14.0477\n",
      "Epoch 105/1000, Loss: 13.4194, Val Loss: 14.0084\n",
      "Epoch 106/1000, Loss: 13.3099, Val Loss: 13.9913\n",
      "Epoch 107/1000, Loss: 13.2784, Val Loss: 13.9901\n",
      "Epoch 108/1000, Loss: 13.3991, Val Loss: 13.9798\n",
      "Epoch 109/1000, Loss: 13.5576, Val Loss: 13.9736\n",
      "Epoch 110/1000, Loss: 13.2474, Val Loss: 13.9863\n",
      "Epoch 111/1000, Loss: 13.1115, Val Loss: 13.9860\n",
      "Epoch 112/1000, Loss: 13.3692, Val Loss: 13.9504\n",
      "Epoch 113/1000, Loss: 13.1205, Val Loss: 13.9441\n",
      "Epoch 114/1000, Loss: 13.1901, Val Loss: 13.9399\n",
      "Epoch 115/1000, Loss: 13.3233, Val Loss: 13.9449\n",
      "Epoch 116/1000, Loss: 13.2671, Val Loss: 13.9611\n",
      "Epoch 117/1000, Loss: 13.2334, Val Loss: 13.9766\n",
      "Epoch 118/1000, Loss: 13.2687, Val Loss: 13.9768\n",
      "Epoch 119/1000, Loss: 13.2763, Val Loss: 13.9641\n",
      "Epoch 120/1000, Loss: 13.2266, Val Loss: 13.9188\n",
      "Epoch 121/1000, Loss: 12.9125, Val Loss: 13.8906\n",
      "Epoch 122/1000, Loss: 13.1070, Val Loss: 13.8739\n",
      "Epoch 123/1000, Loss: 12.9977, Val Loss: 13.8717\n",
      "Epoch 124/1000, Loss: 12.9850, Val Loss: 13.8672\n",
      "Epoch 125/1000, Loss: 13.0221, Val Loss: 13.8663\n",
      "Epoch 126/1000, Loss: 12.9681, Val Loss: 13.8641\n",
      "Epoch 127/1000, Loss: 13.0869, Val Loss: 13.8731\n",
      "Epoch 128/1000, Loss: 12.8821, Val Loss: 13.8866\n",
      "Epoch 129/1000, Loss: 12.9632, Val Loss: 13.8889\n",
      "Epoch 130/1000, Loss: 12.8676, Val Loss: 13.9002\n",
      "Epoch 131/1000, Loss: 12.9642, Val Loss: 13.8967\n",
      "Epoch 132/1000, Loss: 12.8957, Val Loss: 13.8936\n",
      "Epoch 133/1000, Loss: 13.0076, Val Loss: 13.8715\n",
      "Epoch 134/1000, Loss: 12.9086, Val Loss: 13.8613\n",
      "Epoch 135/1000, Loss: 12.8900, Val Loss: 13.8542\n",
      "Epoch 136/1000, Loss: 12.8698, Val Loss: 13.8515\n",
      "Epoch 137/1000, Loss: 12.7433, Val Loss: 13.8376\n",
      "Epoch 138/1000, Loss: 12.6979, Val Loss: 13.8318\n",
      "Epoch 139/1000, Loss: 12.8021, Val Loss: 13.8439\n",
      "Epoch 140/1000, Loss: 12.9317, Val Loss: 13.8663\n",
      "Epoch 141/1000, Loss: 12.7835, Val Loss: 13.8887\n",
      "Epoch 142/1000, Loss: 12.6692, Val Loss: 13.9198\n",
      "Epoch 143/1000, Loss: 12.7836, Val Loss: 13.9676\n",
      "Epoch 144/1000, Loss: 12.7558, Val Loss: 14.0104\n",
      "Epoch 145/1000, Loss: 12.4802, Val Loss: 14.0325\n",
      "Epoch 146/1000, Loss: 12.7903, Val Loss: 14.0019\n",
      "Epoch 147/1000, Loss: 12.6408, Val Loss: 13.9702\n",
      "Epoch 148/1000, Loss: 12.5672, Val Loss: 13.9518\n",
      "Epoch 149/1000, Loss: 12.8130, Val Loss: 13.8997\n",
      "Epoch 150/1000, Loss: 12.5722, Val Loss: 13.8437\n",
      "Epoch 151/1000, Loss: 12.4461, Val Loss: 13.8372\n",
      "Epoch 152/1000, Loss: 12.5800, Val Loss: 13.8701\n",
      "Epoch 153/1000, Loss: 12.7395, Val Loss: 13.8980\n",
      "Epoch 154/1000, Loss: 12.6969, Val Loss: 13.8941\n",
      "Epoch 155/1000, Loss: 12.4212, Val Loss: 13.8905\n",
      "Epoch 156/1000, Loss: 12.3875, Val Loss: 13.8739\n",
      "Epoch 157/1000, Loss: 12.6383, Val Loss: 13.8822\n",
      "Epoch 158/1000, Loss: 12.2839, Val Loss: 13.8965\n",
      "Epoch 159/1000, Loss: 12.3513, Val Loss: 13.9129\n",
      "Epoch 160/1000, Loss: 12.2891, Val Loss: 13.9272\n",
      "Epoch 161/1000, Loss: 12.4288, Val Loss: 13.9147\n",
      "Epoch 162/1000, Loss: 12.4610, Val Loss: 13.8880\n",
      "Epoch 163/1000, Loss: 12.3923, Val Loss: 13.8512\n",
      "Epoch 164/1000, Loss: 12.4377, Val Loss: 13.8475\n",
      "Epoch 165/1000, Loss: 12.1487, Val Loss: 13.8706\n",
      "Epoch 166/1000, Loss: 12.3308, Val Loss: 13.8757\n",
      "Epoch 167/1000, Loss: 12.2417, Val Loss: 13.8843\n",
      "Epoch 168/1000, Loss: 12.1770, Val Loss: 13.9194\n",
      "Epoch 169/1000, Loss: 12.1655, Val Loss: 13.9405\n",
      "Epoch 170/1000, Loss: 12.3103, Val Loss: 13.9116\n",
      "Epoch 171/1000, Loss: 12.2495, Val Loss: 13.8769\n",
      "Epoch 172/1000, Loss: 12.1482, Val Loss: 13.8702\n",
      "Epoch 173/1000, Loss: 12.2644, Val Loss: 13.8941\n",
      "Epoch 174/1000, Loss: 12.2821, Val Loss: 13.9182\n",
      "Epoch 175/1000, Loss: 12.1971, Val Loss: 13.9634\n",
      "Epoch 176/1000, Loss: 12.1002, Val Loss: 14.0098\n",
      "Epoch 177/1000, Loss: 12.1367, Val Loss: 14.0804\n",
      "Epoch 178/1000, Loss: 12.0068, Val Loss: 14.1356\n",
      "Early stopping triggered at epoch 178. Best Val Loss: 13.8318\n",
      "Epoch 1/1000, Loss: 16.7728, Val Loss: 17.0214\n",
      "Epoch 2/1000, Loss: 16.7487, Val Loss: 17.0159\n",
      "Epoch 3/1000, Loss: 16.7282, Val Loss: 17.0102\n",
      "Epoch 4/1000, Loss: 16.7020, Val Loss: 17.0040\n",
      "Epoch 5/1000, Loss: 16.6918, Val Loss: 16.9970\n",
      "Epoch 6/1000, Loss: 16.6719, Val Loss: 16.9888\n",
      "Epoch 7/1000, Loss: 16.6466, Val Loss: 16.9794\n",
      "Epoch 8/1000, Loss: 16.6256, Val Loss: 16.9683\n",
      "Epoch 9/1000, Loss: 16.6021, Val Loss: 16.9550\n",
      "Epoch 10/1000, Loss: 16.5831, Val Loss: 16.9394\n",
      "Epoch 11/1000, Loss: 16.5620, Val Loss: 16.9239\n",
      "Epoch 12/1000, Loss: 16.5403, Val Loss: 16.9067\n",
      "Epoch 13/1000, Loss: 16.5186, Val Loss: 16.8880\n",
      "Epoch 14/1000, Loss: 16.5072, Val Loss: 16.8689\n",
      "Epoch 15/1000, Loss: 16.4716, Val Loss: 16.8507\n",
      "Epoch 16/1000, Loss: 16.4396, Val Loss: 16.8326\n",
      "Epoch 17/1000, Loss: 16.4146, Val Loss: 16.8155\n",
      "Epoch 18/1000, Loss: 16.3887, Val Loss: 16.7989\n",
      "Epoch 19/1000, Loss: 16.3574, Val Loss: 16.7834\n",
      "Epoch 20/1000, Loss: 16.3383, Val Loss: 16.7665\n",
      "Epoch 21/1000, Loss: 16.3082, Val Loss: 16.7488\n",
      "Epoch 22/1000, Loss: 16.2734, Val Loss: 16.7277\n",
      "Epoch 23/1000, Loss: 16.2101, Val Loss: 16.7034\n",
      "Epoch 24/1000, Loss: 16.1973, Val Loss: 16.6756\n",
      "Epoch 25/1000, Loss: 16.1405, Val Loss: 16.6478\n",
      "Epoch 26/1000, Loss: 16.1081, Val Loss: 16.6183\n",
      "Epoch 27/1000, Loss: 16.0724, Val Loss: 16.5868\n",
      "Epoch 28/1000, Loss: 16.0460, Val Loss: 16.5507\n",
      "Epoch 29/1000, Loss: 15.9888, Val Loss: 16.5109\n",
      "Epoch 30/1000, Loss: 15.9609, Val Loss: 16.4645\n",
      "Epoch 31/1000, Loss: 15.9078, Val Loss: 16.4194\n",
      "Epoch 32/1000, Loss: 15.8525, Val Loss: 16.3676\n",
      "Epoch 33/1000, Loss: 15.8201, Val Loss: 16.3177\n",
      "Epoch 34/1000, Loss: 15.7784, Val Loss: 16.2681\n",
      "Epoch 35/1000, Loss: 15.7104, Val Loss: 16.2234\n",
      "Epoch 36/1000, Loss: 15.6413, Val Loss: 16.1819\n",
      "Epoch 37/1000, Loss: 15.5844, Val Loss: 16.1444\n",
      "Epoch 38/1000, Loss: 15.5368, Val Loss: 16.0980\n",
      "Epoch 39/1000, Loss: 15.4967, Val Loss: 16.0365\n",
      "Epoch 40/1000, Loss: 15.4540, Val Loss: 15.9712\n",
      "Epoch 41/1000, Loss: 15.4712, Val Loss: 15.9180\n",
      "Epoch 42/1000, Loss: 15.3117, Val Loss: 15.8615\n",
      "Epoch 43/1000, Loss: 15.3284, Val Loss: 15.8166\n",
      "Epoch 44/1000, Loss: 15.2243, Val Loss: 15.7823\n",
      "Epoch 45/1000, Loss: 15.2415, Val Loss: 15.7311\n",
      "Epoch 46/1000, Loss: 15.1626, Val Loss: 15.6762\n",
      "Epoch 47/1000, Loss: 15.0894, Val Loss: 15.6146\n",
      "Epoch 48/1000, Loss: 15.0577, Val Loss: 15.5451\n",
      "Epoch 49/1000, Loss: 15.0492, Val Loss: 15.4779\n",
      "Epoch 50/1000, Loss: 14.9799, Val Loss: 15.4367\n",
      "Epoch 51/1000, Loss: 14.9478, Val Loss: 15.4145\n",
      "Epoch 52/1000, Loss: 14.9056, Val Loss: 15.4018\n",
      "Epoch 53/1000, Loss: 14.7898, Val Loss: 15.3841\n",
      "Epoch 54/1000, Loss: 14.8099, Val Loss: 15.3567\n",
      "Epoch 55/1000, Loss: 14.7240, Val Loss: 15.3109\n",
      "Epoch 56/1000, Loss: 14.7064, Val Loss: 15.2668\n",
      "Epoch 57/1000, Loss: 14.6604, Val Loss: 15.2386\n",
      "Epoch 58/1000, Loss: 14.6074, Val Loss: 15.2227\n",
      "Epoch 59/1000, Loss: 14.5765, Val Loss: 15.2146\n",
      "Epoch 60/1000, Loss: 14.5955, Val Loss: 15.2026\n",
      "Epoch 61/1000, Loss: 14.5221, Val Loss: 15.1755\n",
      "Epoch 62/1000, Loss: 14.5403, Val Loss: 15.1423\n",
      "Epoch 63/1000, Loss: 14.5361, Val Loss: 15.0890\n",
      "Epoch 64/1000, Loss: 14.4946, Val Loss: 15.0365\n",
      "Epoch 65/1000, Loss: 14.3944, Val Loss: 14.9836\n",
      "Epoch 66/1000, Loss: 14.4474, Val Loss: 14.9287\n",
      "Epoch 67/1000, Loss: 14.3291, Val Loss: 14.8837\n",
      "Epoch 68/1000, Loss: 14.2382, Val Loss: 14.8689\n",
      "Epoch 69/1000, Loss: 14.2255, Val Loss: 14.8296\n",
      "Epoch 70/1000, Loss: 14.2299, Val Loss: 14.7483\n",
      "Epoch 71/1000, Loss: 14.1965, Val Loss: 14.7005\n",
      "Epoch 72/1000, Loss: 14.1951, Val Loss: 14.6910\n",
      "Epoch 73/1000, Loss: 14.0338, Val Loss: 14.6912\n",
      "Epoch 74/1000, Loss: 14.0994, Val Loss: 14.7093\n",
      "Epoch 75/1000, Loss: 14.0833, Val Loss: 14.6948\n",
      "Epoch 76/1000, Loss: 14.0648, Val Loss: 14.6429\n",
      "Epoch 77/1000, Loss: 13.9082, Val Loss: 14.6057\n",
      "Epoch 78/1000, Loss: 13.9262, Val Loss: 14.5551\n",
      "Epoch 79/1000, Loss: 13.7889, Val Loss: 14.5234\n",
      "Epoch 80/1000, Loss: 13.9514, Val Loss: 14.4982\n",
      "Epoch 81/1000, Loss: 13.7867, Val Loss: 14.4701\n",
      "Epoch 82/1000, Loss: 13.7597, Val Loss: 14.4282\n",
      "Epoch 83/1000, Loss: 13.7756, Val Loss: 14.3993\n",
      "Epoch 84/1000, Loss: 13.7907, Val Loss: 14.3799\n",
      "Epoch 85/1000, Loss: 13.7539, Val Loss: 14.3739\n",
      "Epoch 86/1000, Loss: 13.7538, Val Loss: 14.3783\n",
      "Epoch 87/1000, Loss: 13.6129, Val Loss: 14.3311\n",
      "Epoch 88/1000, Loss: 13.5267, Val Loss: 14.2808\n",
      "Epoch 89/1000, Loss: 13.6113, Val Loss: 14.2432\n",
      "Epoch 90/1000, Loss: 13.6799, Val Loss: 14.2217\n",
      "Epoch 91/1000, Loss: 13.3554, Val Loss: 14.2085\n",
      "Epoch 92/1000, Loss: 13.3800, Val Loss: 14.2116\n",
      "Epoch 93/1000, Loss: 13.4566, Val Loss: 14.2162\n",
      "Epoch 94/1000, Loss: 13.4893, Val Loss: 14.1973\n",
      "Epoch 95/1000, Loss: 13.3401, Val Loss: 14.1779\n",
      "Epoch 96/1000, Loss: 13.3861, Val Loss: 14.1597\n",
      "Epoch 97/1000, Loss: 13.4564, Val Loss: 14.1715\n",
      "Epoch 98/1000, Loss: 13.3541, Val Loss: 14.1790\n",
      "Epoch 99/1000, Loss: 13.4441, Val Loss: 14.1523\n",
      "Epoch 100/1000, Loss: 13.3844, Val Loss: 14.1396\n",
      "Epoch 101/1000, Loss: 13.4030, Val Loss: 14.1355\n",
      "Epoch 102/1000, Loss: 13.1191, Val Loss: 14.1200\n",
      "Epoch 103/1000, Loss: 13.2499, Val Loss: 14.1220\n",
      "Epoch 104/1000, Loss: 13.5729, Val Loss: 14.1297\n",
      "Epoch 105/1000, Loss: 13.1297, Val Loss: 14.1105\n",
      "Epoch 106/1000, Loss: 13.3413, Val Loss: 14.0692\n",
      "Epoch 107/1000, Loss: 13.2117, Val Loss: 14.0515\n",
      "Epoch 108/1000, Loss: 13.2937, Val Loss: 14.0348\n",
      "Epoch 109/1000, Loss: 13.3573, Val Loss: 14.0358\n",
      "Epoch 110/1000, Loss: 13.1242, Val Loss: 14.0509\n",
      "Epoch 111/1000, Loss: 13.0700, Val Loss: 14.0510\n",
      "Epoch 112/1000, Loss: 13.3318, Val Loss: 14.0164\n",
      "Epoch 113/1000, Loss: 13.0496, Val Loss: 13.9811\n",
      "Epoch 114/1000, Loss: 13.0535, Val Loss: 13.9857\n",
      "Epoch 115/1000, Loss: 13.2401, Val Loss: 13.9914\n",
      "Epoch 116/1000, Loss: 13.0656, Val Loss: 13.9728\n",
      "Epoch 117/1000, Loss: 13.0534, Val Loss: 13.9500\n",
      "Epoch 118/1000, Loss: 13.1240, Val Loss: 13.9374\n",
      "Epoch 119/1000, Loss: 12.9371, Val Loss: 13.9447\n",
      "Epoch 120/1000, Loss: 12.8127, Val Loss: 13.9678\n",
      "Epoch 121/1000, Loss: 12.7946, Val Loss: 13.9754\n",
      "Epoch 122/1000, Loss: 12.9681, Val Loss: 13.9850\n",
      "Epoch 123/1000, Loss: 12.8795, Val Loss: 13.9541\n",
      "Epoch 124/1000, Loss: 12.9087, Val Loss: 13.9362\n",
      "Epoch 125/1000, Loss: 12.6943, Val Loss: 13.9449\n",
      "Epoch 126/1000, Loss: 12.9701, Val Loss: 13.9560\n",
      "Epoch 127/1000, Loss: 13.0326, Val Loss: 13.9652\n",
      "Epoch 128/1000, Loss: 12.9186, Val Loss: 13.9498\n",
      "Epoch 129/1000, Loss: 13.0901, Val Loss: 13.9447\n",
      "Epoch 130/1000, Loss: 12.9814, Val Loss: 13.9402\n",
      "Epoch 131/1000, Loss: 13.0133, Val Loss: 13.9241\n",
      "Epoch 132/1000, Loss: 12.8568, Val Loss: 13.9456\n",
      "Epoch 133/1000, Loss: 13.0183, Val Loss: 13.9689\n",
      "Epoch 134/1000, Loss: 12.9076, Val Loss: 14.0202\n",
      "Epoch 135/1000, Loss: 12.6795, Val Loss: 14.0614\n",
      "Epoch 136/1000, Loss: 12.8482, Val Loss: 14.0860\n",
      "Epoch 137/1000, Loss: 12.9374, Val Loss: 14.0619\n",
      "Epoch 138/1000, Loss: 12.6277, Val Loss: 14.0226\n",
      "Epoch 139/1000, Loss: 12.9441, Val Loss: 13.9967\n",
      "Epoch 140/1000, Loss: 12.8864, Val Loss: 13.9918\n",
      "Epoch 141/1000, Loss: 12.5540, Val Loss: 13.9834\n",
      "Epoch 142/1000, Loss: 12.6832, Val Loss: 13.9878\n",
      "Epoch 143/1000, Loss: 12.6339, Val Loss: 14.0217\n",
      "Epoch 144/1000, Loss: 12.5698, Val Loss: 14.0247\n",
      "Epoch 145/1000, Loss: 12.5521, Val Loss: 14.0124\n",
      "Epoch 146/1000, Loss: 12.5006, Val Loss: 14.0101\n",
      "Epoch 147/1000, Loss: 12.5013, Val Loss: 14.0062\n",
      "Epoch 148/1000, Loss: 12.5987, Val Loss: 13.9947\n",
      "Epoch 149/1000, Loss: 12.5024, Val Loss: 13.9893\n",
      "Epoch 150/1000, Loss: 12.3855, Val Loss: 13.9923\n",
      "Epoch 151/1000, Loss: 12.3811, Val Loss: 13.9869\n",
      "Epoch 152/1000, Loss: 12.6443, Val Loss: 13.9694\n",
      "Epoch 153/1000, Loss: 12.5093, Val Loss: 13.9654\n",
      "Epoch 154/1000, Loss: 12.4986, Val Loss: 13.9604\n",
      "Epoch 155/1000, Loss: 12.2948, Val Loss: 13.9647\n",
      "Epoch 156/1000, Loss: 12.3699, Val Loss: 13.9736\n",
      "Epoch 157/1000, Loss: 12.8410, Val Loss: 14.0203\n",
      "Epoch 158/1000, Loss: 12.3870, Val Loss: 14.0016\n",
      "Epoch 159/1000, Loss: 12.3422, Val Loss: 13.9565\n",
      "Epoch 160/1000, Loss: 12.2268, Val Loss: 13.9221\n",
      "Epoch 161/1000, Loss: 12.4496, Val Loss: 13.9301\n",
      "Epoch 162/1000, Loss: 12.4253, Val Loss: 13.9319\n",
      "Epoch 163/1000, Loss: 12.2390, Val Loss: 13.9323\n",
      "Epoch 164/1000, Loss: 12.3988, Val Loss: 13.9631\n",
      "Epoch 165/1000, Loss: 12.1385, Val Loss: 13.9967\n",
      "Epoch 166/1000, Loss: 12.0989, Val Loss: 14.0157\n",
      "Epoch 167/1000, Loss: 12.1724, Val Loss: 14.0069\n",
      "Epoch 168/1000, Loss: 12.1809, Val Loss: 14.0034\n",
      "Epoch 169/1000, Loss: 12.4015, Val Loss: 13.9897\n",
      "Epoch 170/1000, Loss: 12.1010, Val Loss: 13.9652\n",
      "Epoch 171/1000, Loss: 12.0132, Val Loss: 13.9299\n",
      "Epoch 172/1000, Loss: 12.1438, Val Loss: 13.9062\n",
      "Epoch 173/1000, Loss: 12.0314, Val Loss: 13.9146\n",
      "Epoch 174/1000, Loss: 12.0189, Val Loss: 13.9441\n",
      "Epoch 175/1000, Loss: 12.0276, Val Loss: 13.9873\n",
      "Epoch 176/1000, Loss: 11.7393, Val Loss: 14.0354\n",
      "Epoch 177/1000, Loss: 12.0152, Val Loss: 14.1039\n",
      "Epoch 178/1000, Loss: 11.8181, Val Loss: 14.1389\n",
      "Epoch 179/1000, Loss: 11.9841, Val Loss: 14.1443\n",
      "Epoch 180/1000, Loss: 12.1990, Val Loss: 14.1186\n",
      "Epoch 181/1000, Loss: 12.0294, Val Loss: 14.1278\n",
      "Epoch 182/1000, Loss: 11.8986, Val Loss: 14.1281\n",
      "Epoch 183/1000, Loss: 12.1710, Val Loss: 14.1256\n",
      "Epoch 184/1000, Loss: 11.9314, Val Loss: 14.0971\n",
      "Epoch 185/1000, Loss: 11.9306, Val Loss: 14.0727\n",
      "Epoch 186/1000, Loss: 11.8037, Val Loss: 14.0279\n",
      "Epoch 187/1000, Loss: 11.9783, Val Loss: 14.0323\n",
      "Epoch 188/1000, Loss: 11.9898, Val Loss: 14.0535\n",
      "Epoch 189/1000, Loss: 11.5585, Val Loss: 14.0927\n",
      "Epoch 190/1000, Loss: 11.9420, Val Loss: 14.0835\n",
      "Epoch 191/1000, Loss: 11.8646, Val Loss: 14.0268\n",
      "Epoch 192/1000, Loss: 11.5234, Val Loss: 13.9805\n",
      "Epoch 193/1000, Loss: 11.8576, Val Loss: 13.9691\n",
      "Epoch 194/1000, Loss: 11.8176, Val Loss: 13.9620\n",
      "Epoch 195/1000, Loss: 11.7066, Val Loss: 13.9740\n",
      "Epoch 196/1000, Loss: 11.5290, Val Loss: 14.0318\n",
      "Epoch 197/1000, Loss: 11.8963, Val Loss: 14.0966\n",
      "Epoch 198/1000, Loss: 11.8081, Val Loss: 14.1494\n",
      "Epoch 199/1000, Loss: 11.8417, Val Loss: 14.1444\n",
      "Epoch 200/1000, Loss: 11.6894, Val Loss: 14.1150\n",
      "Epoch 201/1000, Loss: 11.8143, Val Loss: 14.0539\n",
      "Epoch 202/1000, Loss: 11.5593, Val Loss: 14.0260\n",
      "Epoch 203/1000, Loss: 11.6090, Val Loss: 14.0180\n",
      "Epoch 204/1000, Loss: 11.6946, Val Loss: 14.0276\n",
      "Epoch 205/1000, Loss: 11.5555, Val Loss: 14.0522\n",
      "Epoch 206/1000, Loss: 11.7450, Val Loss: 14.0339\n",
      "Epoch 207/1000, Loss: 11.6303, Val Loss: 14.0361\n",
      "Epoch 208/1000, Loss: 11.5926, Val Loss: 14.0390\n",
      "Epoch 209/1000, Loss: 11.7971, Val Loss: 14.0447\n",
      "Epoch 210/1000, Loss: 11.6003, Val Loss: 14.0462\n",
      "Epoch 211/1000, Loss: 11.5432, Val Loss: 13.9990\n",
      "Epoch 212/1000, Loss: 11.4989, Val Loss: 13.9651\n",
      "Early stopping triggered at epoch 212. Best Val Loss: 13.9062\n",
      "Epoch 1/1000, Loss: 16.9963, Val Loss: 17.3210\n",
      "Epoch 2/1000, Loss: 16.9627, Val Loss: 17.3153\n",
      "Epoch 3/1000, Loss: 16.9312, Val Loss: 17.3090\n",
      "Epoch 4/1000, Loss: 16.9048, Val Loss: 17.3024\n",
      "Epoch 5/1000, Loss: 16.8911, Val Loss: 17.2952\n",
      "Epoch 6/1000, Loss: 16.8810, Val Loss: 17.2873\n",
      "Epoch 7/1000, Loss: 16.8423, Val Loss: 17.2786\n",
      "Epoch 8/1000, Loss: 16.8250, Val Loss: 17.2696\n",
      "Epoch 9/1000, Loss: 16.8264, Val Loss: 17.2602\n",
      "Epoch 10/1000, Loss: 16.7952, Val Loss: 17.2507\n",
      "Epoch 11/1000, Loss: 16.7952, Val Loss: 17.2425\n",
      "Epoch 12/1000, Loss: 16.7569, Val Loss: 17.2335\n",
      "Epoch 13/1000, Loss: 16.7160, Val Loss: 17.2224\n",
      "Epoch 14/1000, Loss: 16.7061, Val Loss: 17.2110\n",
      "Epoch 15/1000, Loss: 16.6845, Val Loss: 17.1996\n",
      "Epoch 16/1000, Loss: 16.6349, Val Loss: 17.1867\n",
      "Epoch 17/1000, Loss: 16.5929, Val Loss: 17.1730\n",
      "Epoch 18/1000, Loss: 16.5854, Val Loss: 17.1585\n",
      "Epoch 19/1000, Loss: 16.5526, Val Loss: 17.1415\n",
      "Epoch 20/1000, Loss: 16.5239, Val Loss: 17.1229\n",
      "Epoch 21/1000, Loss: 16.4816, Val Loss: 17.1028\n",
      "Epoch 22/1000, Loss: 16.4436, Val Loss: 17.0805\n",
      "Epoch 23/1000, Loss: 16.4254, Val Loss: 17.0575\n",
      "Epoch 24/1000, Loss: 16.3746, Val Loss: 17.0329\n",
      "Epoch 25/1000, Loss: 16.3214, Val Loss: 17.0026\n",
      "Epoch 26/1000, Loss: 16.2934, Val Loss: 16.9671\n",
      "Epoch 27/1000, Loss: 16.2198, Val Loss: 16.9304\n",
      "Epoch 28/1000, Loss: 16.2139, Val Loss: 16.8952\n",
      "Epoch 29/1000, Loss: 16.1501, Val Loss: 16.8556\n",
      "Epoch 30/1000, Loss: 16.1444, Val Loss: 16.8203\n",
      "Epoch 31/1000, Loss: 16.1105, Val Loss: 16.7841\n",
      "Epoch 32/1000, Loss: 16.1070, Val Loss: 16.7472\n",
      "Epoch 33/1000, Loss: 16.0464, Val Loss: 16.7049\n",
      "Epoch 34/1000, Loss: 15.9866, Val Loss: 16.6603\n",
      "Epoch 35/1000, Loss: 15.9353, Val Loss: 16.6137\n",
      "Epoch 36/1000, Loss: 15.9464, Val Loss: 16.5710\n",
      "Epoch 37/1000, Loss: 15.9011, Val Loss: 16.5392\n",
      "Epoch 38/1000, Loss: 15.8954, Val Loss: 16.5167\n",
      "Epoch 39/1000, Loss: 15.8610, Val Loss: 16.4988\n",
      "Epoch 40/1000, Loss: 15.8926, Val Loss: 16.4863\n",
      "Epoch 41/1000, Loss: 15.7928, Val Loss: 16.4689\n",
      "Epoch 42/1000, Loss: 15.8241, Val Loss: 16.4566\n",
      "Epoch 43/1000, Loss: 15.7646, Val Loss: 16.4424\n",
      "Epoch 44/1000, Loss: 15.7549, Val Loss: 16.4264\n",
      "Epoch 45/1000, Loss: 15.6710, Val Loss: 16.4060\n",
      "Epoch 46/1000, Loss: 15.7073, Val Loss: 16.3908\n",
      "Epoch 47/1000, Loss: 15.6787, Val Loss: 16.3790\n",
      "Epoch 48/1000, Loss: 15.6330, Val Loss: 16.3695\n",
      "Epoch 49/1000, Loss: 15.6570, Val Loss: 16.3589\n",
      "Epoch 50/1000, Loss: 15.6230, Val Loss: 16.3397\n",
      "Epoch 51/1000, Loss: 15.6020, Val Loss: 16.3199\n",
      "Epoch 52/1000, Loss: 15.6189, Val Loss: 16.3021\n",
      "Epoch 53/1000, Loss: 15.5579, Val Loss: 16.2869\n",
      "Epoch 54/1000, Loss: 15.5765, Val Loss: 16.2708\n",
      "Epoch 55/1000, Loss: 15.5586, Val Loss: 16.2571\n",
      "Epoch 56/1000, Loss: 15.4753, Val Loss: 16.2398\n",
      "Epoch 57/1000, Loss: 15.3800, Val Loss: 16.2159\n",
      "Epoch 58/1000, Loss: 15.4198, Val Loss: 16.1979\n",
      "Epoch 59/1000, Loss: 15.3847, Val Loss: 16.1680\n",
      "Epoch 60/1000, Loss: 15.3755, Val Loss: 16.1514\n",
      "Epoch 61/1000, Loss: 15.3964, Val Loss: 16.1356\n",
      "Epoch 62/1000, Loss: 15.3578, Val Loss: 16.1221\n",
      "Epoch 63/1000, Loss: 15.3338, Val Loss: 16.1112\n",
      "Epoch 64/1000, Loss: 15.3686, Val Loss: 16.1037\n",
      "Epoch 65/1000, Loss: 15.2742, Val Loss: 16.0922\n",
      "Epoch 66/1000, Loss: 15.2615, Val Loss: 16.0751\n",
      "Epoch 67/1000, Loss: 15.3173, Val Loss: 16.0603\n",
      "Epoch 68/1000, Loss: 15.2820, Val Loss: 16.0407\n",
      "Epoch 69/1000, Loss: 15.0631, Val Loss: 16.0226\n",
      "Epoch 70/1000, Loss: 15.2075, Val Loss: 16.0001\n",
      "Epoch 71/1000, Loss: 15.1697, Val Loss: 15.9883\n",
      "Epoch 72/1000, Loss: 15.0722, Val Loss: 15.9770\n",
      "Epoch 73/1000, Loss: 15.0868, Val Loss: 15.9613\n",
      "Epoch 74/1000, Loss: 15.0878, Val Loss: 15.9295\n",
      "Epoch 75/1000, Loss: 15.0709, Val Loss: 15.9014\n",
      "Epoch 76/1000, Loss: 14.9785, Val Loss: 15.8870\n",
      "Epoch 77/1000, Loss: 15.0006, Val Loss: 15.8953\n",
      "Epoch 78/1000, Loss: 15.0634, Val Loss: 15.9069\n",
      "Epoch 79/1000, Loss: 14.9374, Val Loss: 15.9144\n",
      "Epoch 80/1000, Loss: 15.0692, Val Loss: 15.8988\n",
      "Epoch 81/1000, Loss: 14.9181, Val Loss: 15.8823\n",
      "Epoch 82/1000, Loss: 14.7981, Val Loss: 15.8861\n",
      "Epoch 83/1000, Loss: 14.8994, Val Loss: 15.8816\n",
      "Epoch 84/1000, Loss: 14.8207, Val Loss: 15.8807\n",
      "Epoch 85/1000, Loss: 14.7924, Val Loss: 15.8801\n",
      "Epoch 86/1000, Loss: 14.8780, Val Loss: 15.8574\n",
      "Epoch 87/1000, Loss: 14.7706, Val Loss: 15.8206\n",
      "Epoch 88/1000, Loss: 14.7677, Val Loss: 15.7956\n",
      "Epoch 89/1000, Loss: 14.8168, Val Loss: 15.7851\n",
      "Epoch 90/1000, Loss: 14.5513, Val Loss: 15.7675\n",
      "Epoch 91/1000, Loss: 14.8320, Val Loss: 15.7490\n",
      "Epoch 92/1000, Loss: 14.6510, Val Loss: 15.7381\n",
      "Epoch 93/1000, Loss: 14.7520, Val Loss: 15.7461\n",
      "Epoch 94/1000, Loss: 14.4962, Val Loss: 15.7543\n",
      "Epoch 95/1000, Loss: 14.6010, Val Loss: 15.7646\n",
      "Epoch 96/1000, Loss: 14.5675, Val Loss: 15.7782\n",
      "Epoch 97/1000, Loss: 14.6146, Val Loss: 15.7782\n",
      "Epoch 98/1000, Loss: 14.5824, Val Loss: 15.7836\n",
      "Epoch 99/1000, Loss: 14.5597, Val Loss: 15.7822\n",
      "Epoch 100/1000, Loss: 14.3829, Val Loss: 15.7811\n",
      "Epoch 101/1000, Loss: 14.3737, Val Loss: 15.7589\n",
      "Epoch 102/1000, Loss: 14.5082, Val Loss: 15.6909\n",
      "Epoch 103/1000, Loss: 14.5382, Val Loss: 15.6370\n",
      "Epoch 104/1000, Loss: 14.4260, Val Loss: 15.5895\n",
      "Epoch 105/1000, Loss: 14.3175, Val Loss: 15.5571\n",
      "Epoch 106/1000, Loss: 14.3163, Val Loss: 15.5304\n",
      "Epoch 107/1000, Loss: 14.3289, Val Loss: 15.5370\n",
      "Epoch 108/1000, Loss: 14.2151, Val Loss: 15.5482\n",
      "Epoch 109/1000, Loss: 14.2954, Val Loss: 15.5668\n",
      "Epoch 110/1000, Loss: 14.1061, Val Loss: 15.5800\n",
      "Epoch 111/1000, Loss: 14.0657, Val Loss: 15.5856\n",
      "Epoch 112/1000, Loss: 14.1935, Val Loss: 15.5675\n",
      "Epoch 113/1000, Loss: 14.2150, Val Loss: 15.5325\n",
      "Epoch 114/1000, Loss: 14.0431, Val Loss: 15.4916\n",
      "Epoch 115/1000, Loss: 14.0415, Val Loss: 15.4307\n",
      "Epoch 116/1000, Loss: 14.1106, Val Loss: 15.3919\n",
      "Epoch 117/1000, Loss: 14.0395, Val Loss: 15.3738\n",
      "Epoch 118/1000, Loss: 13.8888, Val Loss: 15.3744\n",
      "Epoch 119/1000, Loss: 13.9115, Val Loss: 15.3561\n",
      "Epoch 120/1000, Loss: 14.0300, Val Loss: 15.3545\n",
      "Epoch 121/1000, Loss: 14.1197, Val Loss: 15.3616\n",
      "Epoch 122/1000, Loss: 13.8906, Val Loss: 15.3657\n",
      "Epoch 123/1000, Loss: 13.9291, Val Loss: 15.3548\n",
      "Epoch 124/1000, Loss: 13.9994, Val Loss: 15.3569\n",
      "Epoch 125/1000, Loss: 13.8345, Val Loss: 15.3599\n",
      "Epoch 126/1000, Loss: 13.7166, Val Loss: 15.3758\n",
      "Epoch 127/1000, Loss: 13.9154, Val Loss: 15.3783\n",
      "Epoch 128/1000, Loss: 13.6861, Val Loss: 15.3617\n",
      "Epoch 129/1000, Loss: 13.8355, Val Loss: 15.3260\n",
      "Epoch 130/1000, Loss: 13.5775, Val Loss: 15.2821\n",
      "Epoch 131/1000, Loss: 13.7884, Val Loss: 15.2576\n",
      "Epoch 132/1000, Loss: 13.5825, Val Loss: 15.2372\n",
      "Epoch 133/1000, Loss: 13.4905, Val Loss: 15.2496\n",
      "Epoch 134/1000, Loss: 13.7311, Val Loss: 15.2954\n",
      "Epoch 135/1000, Loss: 13.5257, Val Loss: 15.2927\n",
      "Epoch 136/1000, Loss: 13.6325, Val Loss: 15.2553\n",
      "Epoch 137/1000, Loss: 13.4944, Val Loss: 15.2415\n",
      "Epoch 138/1000, Loss: 13.4005, Val Loss: 15.2012\n",
      "Epoch 139/1000, Loss: 13.3613, Val Loss: 15.1440\n",
      "Epoch 140/1000, Loss: 13.2860, Val Loss: 15.1097\n",
      "Epoch 141/1000, Loss: 13.4942, Val Loss: 15.0472\n",
      "Epoch 142/1000, Loss: 13.0904, Val Loss: 15.0201\n",
      "Epoch 143/1000, Loss: 13.4538, Val Loss: 14.9634\n",
      "Epoch 144/1000, Loss: 13.0974, Val Loss: 14.9085\n",
      "Epoch 145/1000, Loss: 13.1596, Val Loss: 14.8654\n",
      "Epoch 146/1000, Loss: 13.1316, Val Loss: 14.8331\n",
      "Epoch 147/1000, Loss: 13.0292, Val Loss: 14.8539\n",
      "Epoch 148/1000, Loss: 12.9707, Val Loss: 14.9085\n",
      "Epoch 149/1000, Loss: 13.2877, Val Loss: 14.8499\n",
      "Epoch 150/1000, Loss: 12.9996, Val Loss: 14.8107\n",
      "Epoch 151/1000, Loss: 12.9332, Val Loss: 14.8240\n",
      "Epoch 152/1000, Loss: 12.9092, Val Loss: 14.8411\n",
      "Epoch 153/1000, Loss: 12.6736, Val Loss: 14.7887\n",
      "Epoch 154/1000, Loss: 12.7065, Val Loss: 14.6567\n",
      "Epoch 155/1000, Loss: 12.6648, Val Loss: 14.5625\n",
      "Epoch 156/1000, Loss: 12.8925, Val Loss: 14.5096\n",
      "Epoch 157/1000, Loss: 12.7478, Val Loss: 14.4784\n",
      "Epoch 158/1000, Loss: 12.7253, Val Loss: 14.4546\n",
      "Epoch 159/1000, Loss: 12.4316, Val Loss: 14.4800\n",
      "Epoch 160/1000, Loss: 12.5392, Val Loss: 14.5220\n",
      "Epoch 161/1000, Loss: 12.5037, Val Loss: 14.5068\n",
      "Epoch 162/1000, Loss: 12.5653, Val Loss: 14.5028\n",
      "Epoch 163/1000, Loss: 12.5026, Val Loss: 14.4807\n",
      "Epoch 164/1000, Loss: 12.5001, Val Loss: 14.4689\n",
      "Epoch 165/1000, Loss: 12.4014, Val Loss: 14.4841\n",
      "Epoch 166/1000, Loss: 12.4634, Val Loss: 14.4270\n",
      "Epoch 167/1000, Loss: 12.1947, Val Loss: 14.3719\n",
      "Epoch 168/1000, Loss: 12.2703, Val Loss: 14.3132\n",
      "Epoch 169/1000, Loss: 12.1692, Val Loss: 14.3494\n",
      "Epoch 170/1000, Loss: 12.4757, Val Loss: 14.4094\n",
      "Epoch 171/1000, Loss: 12.6291, Val Loss: 14.3634\n",
      "Epoch 172/1000, Loss: 11.9595, Val Loss: 14.3247\n",
      "Epoch 173/1000, Loss: 12.0822, Val Loss: 14.2783\n",
      "Epoch 174/1000, Loss: 12.2753, Val Loss: 14.2783\n",
      "Epoch 175/1000, Loss: 12.2969, Val Loss: 14.3319\n",
      "Epoch 176/1000, Loss: 12.2018, Val Loss: 14.4067\n",
      "Epoch 177/1000, Loss: 11.8835, Val Loss: 14.4345\n",
      "Epoch 178/1000, Loss: 11.9661, Val Loss: 14.4175\n",
      "Epoch 179/1000, Loss: 12.1619, Val Loss: 14.3577\n",
      "Epoch 180/1000, Loss: 12.1831, Val Loss: 14.2686\n",
      "Epoch 181/1000, Loss: 12.2041, Val Loss: 14.2457\n",
      "Epoch 182/1000, Loss: 12.1729, Val Loss: 14.2496\n",
      "Epoch 183/1000, Loss: 12.1095, Val Loss: 14.3054\n",
      "Epoch 184/1000, Loss: 11.9579, Val Loss: 14.3322\n",
      "Epoch 185/1000, Loss: 12.0310, Val Loss: 14.3527\n",
      "Epoch 186/1000, Loss: 11.9052, Val Loss: 14.3879\n",
      "Epoch 187/1000, Loss: 11.8603, Val Loss: 14.4513\n",
      "Epoch 188/1000, Loss: 11.9429, Val Loss: 14.4918\n",
      "Epoch 189/1000, Loss: 11.6706, Val Loss: 14.4815\n",
      "Epoch 190/1000, Loss: 11.6985, Val Loss: 14.4000\n",
      "Epoch 191/1000, Loss: 11.7924, Val Loss: 14.2960\n",
      "Epoch 192/1000, Loss: 11.6235, Val Loss: 14.2594\n",
      "Epoch 193/1000, Loss: 11.6909, Val Loss: 14.2696\n",
      "Epoch 194/1000, Loss: 11.9201, Val Loss: 14.2703\n",
      "Epoch 195/1000, Loss: 11.7802, Val Loss: 14.2234\n",
      "Epoch 196/1000, Loss: 11.7984, Val Loss: 14.1982\n",
      "Epoch 197/1000, Loss: 11.8613, Val Loss: 14.2093\n",
      "Epoch 198/1000, Loss: 11.7225, Val Loss: 14.2545\n",
      "Epoch 199/1000, Loss: 11.6838, Val Loss: 14.3523\n",
      "Epoch 200/1000, Loss: 11.7440, Val Loss: 14.4380\n",
      "Epoch 201/1000, Loss: 11.8099, Val Loss: 14.2244\n",
      "Epoch 202/1000, Loss: 11.6717, Val Loss: 14.1376\n",
      "Epoch 203/1000, Loss: 11.7337, Val Loss: 14.1139\n",
      "Epoch 204/1000, Loss: 11.8077, Val Loss: 14.1387\n",
      "Epoch 205/1000, Loss: 11.6283, Val Loss: 14.2661\n",
      "Epoch 206/1000, Loss: 11.8479, Val Loss: 14.3668\n",
      "Epoch 207/1000, Loss: 11.6708, Val Loss: 14.3323\n",
      "Epoch 208/1000, Loss: 11.5629, Val Loss: 14.2807\n",
      "Epoch 209/1000, Loss: 11.8926, Val Loss: 14.2919\n",
      "Epoch 210/1000, Loss: 11.8572, Val Loss: 14.3186\n",
      "Epoch 211/1000, Loss: 11.6192, Val Loss: 14.3605\n",
      "Epoch 212/1000, Loss: 11.7474, Val Loss: 14.4419\n",
      "Epoch 213/1000, Loss: 11.6766, Val Loss: 14.4804\n",
      "Epoch 214/1000, Loss: 11.5816, Val Loss: 14.4679\n",
      "Epoch 215/1000, Loss: 11.5911, Val Loss: 14.4796\n",
      "Epoch 216/1000, Loss: 11.6169, Val Loss: 14.4785\n",
      "Epoch 217/1000, Loss: 11.5189, Val Loss: 14.4866\n",
      "Epoch 218/1000, Loss: 11.5747, Val Loss: 14.4900\n",
      "Epoch 219/1000, Loss: 11.3152, Val Loss: 14.4734\n",
      "Epoch 220/1000, Loss: 11.4757, Val Loss: 14.4797\n",
      "Epoch 221/1000, Loss: 11.2702, Val Loss: 14.4882\n",
      "Epoch 222/1000, Loss: 11.4040, Val Loss: 14.5371\n",
      "Epoch 223/1000, Loss: 11.3692, Val Loss: 14.5760\n",
      "Epoch 224/1000, Loss: 11.6723, Val Loss: 14.5924\n",
      "Epoch 225/1000, Loss: 11.5150, Val Loss: 14.5897\n",
      "Epoch 226/1000, Loss: 11.5074, Val Loss: 14.5597\n",
      "Epoch 227/1000, Loss: 11.2719, Val Loss: 14.5699\n",
      "Epoch 228/1000, Loss: 11.3564, Val Loss: 14.5508\n",
      "Epoch 229/1000, Loss: 11.2048, Val Loss: 14.5148\n",
      "Epoch 230/1000, Loss: 11.3152, Val Loss: 14.4825\n",
      "Epoch 231/1000, Loss: 11.2125, Val Loss: 14.4964\n",
      "Epoch 232/1000, Loss: 11.2885, Val Loss: 14.5163\n",
      "Epoch 233/1000, Loss: 11.1627, Val Loss: 14.4612\n",
      "Epoch 234/1000, Loss: 11.1974, Val Loss: 14.4059\n",
      "Epoch 235/1000, Loss: 11.0159, Val Loss: 14.3802\n",
      "Epoch 236/1000, Loss: 11.3726, Val Loss: 14.4158\n",
      "Epoch 237/1000, Loss: 11.4195, Val Loss: 14.6112\n",
      "Epoch 238/1000, Loss: 11.3029, Val Loss: 14.7457\n",
      "Epoch 239/1000, Loss: 11.1492, Val Loss: 14.6246\n",
      "Epoch 240/1000, Loss: 11.1832, Val Loss: 14.5008\n",
      "Epoch 241/1000, Loss: 11.2813, Val Loss: 14.4881\n",
      "Epoch 242/1000, Loss: 10.9670, Val Loss: 14.5022\n",
      "Epoch 243/1000, Loss: 11.1040, Val Loss: 14.5417\n",
      "Early stopping triggered at epoch 243. Best Val Loss: 14.1139\n",
      "Epoch 1/1000, Loss: 16.8101, Val Loss: 17.0370\n",
      "Epoch 2/1000, Loss: 16.7944, Val Loss: 17.0342\n",
      "Epoch 3/1000, Loss: 16.7833, Val Loss: 17.0316\n",
      "Epoch 4/1000, Loss: 16.7694, Val Loss: 17.0287\n",
      "Epoch 5/1000, Loss: 16.7653, Val Loss: 17.0252\n",
      "Epoch 6/1000, Loss: 16.7610, Val Loss: 17.0214\n",
      "Epoch 7/1000, Loss: 16.7544, Val Loss: 17.0171\n",
      "Epoch 8/1000, Loss: 16.7501, Val Loss: 17.0124\n",
      "Epoch 9/1000, Loss: 16.7407, Val Loss: 17.0076\n",
      "Epoch 10/1000, Loss: 16.7294, Val Loss: 17.0023\n",
      "Epoch 11/1000, Loss: 16.7191, Val Loss: 16.9942\n",
      "Epoch 12/1000, Loss: 16.7050, Val Loss: 16.9860\n",
      "Epoch 13/1000, Loss: 16.6962, Val Loss: 16.9787\n",
      "Epoch 14/1000, Loss: 16.6885, Val Loss: 16.9717\n",
      "Epoch 15/1000, Loss: 16.6760, Val Loss: 16.9645\n",
      "Epoch 16/1000, Loss: 16.6684, Val Loss: 16.9568\n",
      "Epoch 17/1000, Loss: 16.6517, Val Loss: 16.9495\n",
      "Epoch 18/1000, Loss: 16.6424, Val Loss: 16.9422\n",
      "Epoch 19/1000, Loss: 16.6316, Val Loss: 16.9345\n",
      "Epoch 20/1000, Loss: 16.6176, Val Loss: 16.9256\n",
      "Epoch 21/1000, Loss: 16.5972, Val Loss: 16.9160\n",
      "Epoch 22/1000, Loss: 16.5948, Val Loss: 16.9057\n",
      "Epoch 23/1000, Loss: 16.5720, Val Loss: 16.8957\n",
      "Epoch 24/1000, Loss: 16.5584, Val Loss: 16.8847\n",
      "Epoch 25/1000, Loss: 16.5476, Val Loss: 16.8734\n",
      "Epoch 26/1000, Loss: 16.5375, Val Loss: 16.8608\n",
      "Epoch 27/1000, Loss: 16.5080, Val Loss: 16.8474\n",
      "Epoch 28/1000, Loss: 16.5046, Val Loss: 16.8329\n",
      "Epoch 29/1000, Loss: 16.4750, Val Loss: 16.8165\n",
      "Epoch 30/1000, Loss: 16.4696, Val Loss: 16.7994\n",
      "Epoch 31/1000, Loss: 16.4392, Val Loss: 16.7827\n",
      "Epoch 32/1000, Loss: 16.4174, Val Loss: 16.7652\n",
      "Epoch 33/1000, Loss: 16.4038, Val Loss: 16.7520\n",
      "Epoch 34/1000, Loss: 16.3814, Val Loss: 16.7402\n",
      "Epoch 35/1000, Loss: 16.3648, Val Loss: 16.7239\n",
      "Epoch 36/1000, Loss: 16.3314, Val Loss: 16.7017\n",
      "Epoch 37/1000, Loss: 16.3308, Val Loss: 16.6762\n",
      "Epoch 38/1000, Loss: 16.3008, Val Loss: 16.6452\n",
      "Epoch 39/1000, Loss: 16.2809, Val Loss: 16.6131\n",
      "Epoch 40/1000, Loss: 16.2467, Val Loss: 16.5818\n",
      "Epoch 41/1000, Loss: 16.2309, Val Loss: 16.5612\n",
      "Epoch 42/1000, Loss: 16.1985, Val Loss: 16.5403\n",
      "Epoch 43/1000, Loss: 16.1854, Val Loss: 16.5195\n",
      "Epoch 44/1000, Loss: 16.1466, Val Loss: 16.4946\n",
      "Epoch 45/1000, Loss: 16.1259, Val Loss: 16.4656\n",
      "Epoch 46/1000, Loss: 16.0882, Val Loss: 16.4345\n",
      "Epoch 47/1000, Loss: 16.0571, Val Loss: 16.4035\n",
      "Epoch 48/1000, Loss: 16.0296, Val Loss: 16.3751\n",
      "Epoch 49/1000, Loss: 15.9983, Val Loss: 16.3484\n",
      "Epoch 50/1000, Loss: 15.9939, Val Loss: 16.3302\n",
      "Epoch 51/1000, Loss: 15.9322, Val Loss: 16.3102\n",
      "Epoch 52/1000, Loss: 15.9061, Val Loss: 16.2863\n",
      "Epoch 53/1000, Loss: 15.8546, Val Loss: 16.2623\n",
      "Epoch 54/1000, Loss: 15.8407, Val Loss: 16.2292\n",
      "Epoch 55/1000, Loss: 15.8100, Val Loss: 16.1944\n",
      "Epoch 56/1000, Loss: 15.7449, Val Loss: 16.1523\n",
      "Epoch 57/1000, Loss: 15.7264, Val Loss: 16.1112\n",
      "Epoch 58/1000, Loss: 15.7019, Val Loss: 16.0814\n",
      "Epoch 59/1000, Loss: 15.6551, Val Loss: 16.0597\n",
      "Epoch 60/1000, Loss: 15.6092, Val Loss: 16.0368\n",
      "Epoch 61/1000, Loss: 15.5708, Val Loss: 16.0064\n",
      "Epoch 62/1000, Loss: 15.5258, Val Loss: 15.9646\n",
      "Epoch 63/1000, Loss: 15.5104, Val Loss: 15.9163\n",
      "Epoch 64/1000, Loss: 15.4573, Val Loss: 15.8849\n",
      "Epoch 65/1000, Loss: 15.3748, Val Loss: 15.8526\n",
      "Epoch 66/1000, Loss: 15.3583, Val Loss: 15.8325\n",
      "Epoch 67/1000, Loss: 15.3340, Val Loss: 15.8396\n",
      "Epoch 68/1000, Loss: 15.3085, Val Loss: 15.8202\n",
      "Epoch 69/1000, Loss: 15.2273, Val Loss: 15.7746\n",
      "Epoch 70/1000, Loss: 15.1613, Val Loss: 15.7197\n",
      "Epoch 71/1000, Loss: 15.1220, Val Loss: 15.6826\n",
      "Epoch 72/1000, Loss: 15.0330, Val Loss: 15.6624\n",
      "Epoch 73/1000, Loss: 15.0885, Val Loss: 15.6495\n",
      "Epoch 74/1000, Loss: 15.0233, Val Loss: 15.6149\n",
      "Epoch 75/1000, Loss: 14.9835, Val Loss: 15.5711\n",
      "Epoch 76/1000, Loss: 14.9068, Val Loss: 15.5190\n",
      "Epoch 77/1000, Loss: 14.9116, Val Loss: 15.4666\n",
      "Epoch 78/1000, Loss: 14.8687, Val Loss: 15.4242\n",
      "Epoch 79/1000, Loss: 14.8297, Val Loss: 15.4053\n",
      "Epoch 80/1000, Loss: 14.6944, Val Loss: 15.3876\n",
      "Epoch 81/1000, Loss: 14.6875, Val Loss: 15.3626\n",
      "Epoch 82/1000, Loss: 14.6598, Val Loss: 15.3242\n",
      "Epoch 83/1000, Loss: 14.5617, Val Loss: 15.2916\n",
      "Epoch 84/1000, Loss: 14.4950, Val Loss: 15.2599\n",
      "Epoch 85/1000, Loss: 14.4805, Val Loss: 15.2080\n",
      "Epoch 86/1000, Loss: 14.3459, Val Loss: 15.1621\n",
      "Epoch 87/1000, Loss: 14.2772, Val Loss: 15.1329\n",
      "Epoch 88/1000, Loss: 14.2708, Val Loss: 15.1084\n",
      "Epoch 89/1000, Loss: 14.3854, Val Loss: 15.0582\n",
      "Epoch 90/1000, Loss: 14.1188, Val Loss: 14.9926\n",
      "Epoch 91/1000, Loss: 14.2967, Val Loss: 14.9421\n",
      "Epoch 92/1000, Loss: 14.1842, Val Loss: 14.8862\n",
      "Epoch 93/1000, Loss: 14.0012, Val Loss: 14.8382\n",
      "Epoch 94/1000, Loss: 14.0230, Val Loss: 14.8291\n",
      "Epoch 95/1000, Loss: 14.1841, Val Loss: 14.8123\n",
      "Epoch 96/1000, Loss: 13.9120, Val Loss: 14.7273\n",
      "Epoch 97/1000, Loss: 13.7729, Val Loss: 14.6497\n",
      "Epoch 98/1000, Loss: 13.9347, Val Loss: 14.5907\n",
      "Epoch 99/1000, Loss: 13.9260, Val Loss: 14.5744\n",
      "Epoch 100/1000, Loss: 13.7142, Val Loss: 14.5758\n",
      "Epoch 101/1000, Loss: 13.5100, Val Loss: 14.5391\n",
      "Epoch 102/1000, Loss: 13.7572, Val Loss: 14.4758\n",
      "Epoch 103/1000, Loss: 13.6131, Val Loss: 14.4139\n",
      "Epoch 104/1000, Loss: 13.5366, Val Loss: 14.3613\n",
      "Epoch 105/1000, Loss: 13.4133, Val Loss: 14.3207\n",
      "Epoch 106/1000, Loss: 13.4687, Val Loss: 14.2994\n",
      "Epoch 107/1000, Loss: 13.3657, Val Loss: 14.3066\n",
      "Epoch 108/1000, Loss: 13.5337, Val Loss: 14.2888\n",
      "Epoch 109/1000, Loss: 13.4717, Val Loss: 14.2376\n",
      "Epoch 110/1000, Loss: 13.4370, Val Loss: 14.1881\n",
      "Epoch 111/1000, Loss: 13.3221, Val Loss: 14.1508\n",
      "Epoch 112/1000, Loss: 13.1670, Val Loss: 14.1130\n",
      "Epoch 113/1000, Loss: 13.2132, Val Loss: 14.0879\n",
      "Epoch 114/1000, Loss: 13.3457, Val Loss: 14.0702\n",
      "Epoch 115/1000, Loss: 13.3108, Val Loss: 14.0053\n",
      "Epoch 116/1000, Loss: 13.2834, Val Loss: 13.9544\n",
      "Epoch 117/1000, Loss: 13.2874, Val Loss: 13.9807\n",
      "Epoch 118/1000, Loss: 13.2219, Val Loss: 14.0112\n",
      "Epoch 119/1000, Loss: 13.1736, Val Loss: 13.9739\n",
      "Epoch 120/1000, Loss: 13.1854, Val Loss: 13.9872\n",
      "Epoch 121/1000, Loss: 13.0162, Val Loss: 14.0175\n",
      "Epoch 122/1000, Loss: 13.1562, Val Loss: 14.0371\n",
      "Epoch 123/1000, Loss: 13.3262, Val Loss: 14.0473\n",
      "Epoch 124/1000, Loss: 13.0873, Val Loss: 14.0539\n",
      "Epoch 125/1000, Loss: 13.1123, Val Loss: 14.0429\n",
      "Epoch 126/1000, Loss: 13.0919, Val Loss: 14.0547\n",
      "Epoch 127/1000, Loss: 13.0349, Val Loss: 14.0675\n",
      "Epoch 128/1000, Loss: 12.8692, Val Loss: 14.0733\n",
      "Epoch 129/1000, Loss: 12.9920, Val Loss: 14.0623\n",
      "Epoch 130/1000, Loss: 12.9474, Val Loss: 14.0273\n",
      "Epoch 131/1000, Loss: 13.0471, Val Loss: 13.9910\n",
      "Epoch 132/1000, Loss: 12.8740, Val Loss: 13.9599\n",
      "Epoch 133/1000, Loss: 12.7668, Val Loss: 13.9352\n",
      "Epoch 134/1000, Loss: 12.6876, Val Loss: 13.9093\n",
      "Epoch 135/1000, Loss: 12.8744, Val Loss: 13.8882\n",
      "Epoch 136/1000, Loss: 12.8627, Val Loss: 13.8671\n",
      "Epoch 137/1000, Loss: 12.6792, Val Loss: 13.8692\n",
      "Epoch 138/1000, Loss: 12.8823, Val Loss: 13.8865\n",
      "Epoch 139/1000, Loss: 12.6797, Val Loss: 13.9027\n",
      "Epoch 140/1000, Loss: 12.9409, Val Loss: 13.9308\n",
      "Epoch 141/1000, Loss: 13.0167, Val Loss: 13.9704\n",
      "Epoch 142/1000, Loss: 12.8102, Val Loss: 13.9275\n",
      "Epoch 143/1000, Loss: 12.6176, Val Loss: 13.9016\n",
      "Epoch 144/1000, Loss: 12.5233, Val Loss: 13.8976\n",
      "Epoch 145/1000, Loss: 12.6609, Val Loss: 13.8928\n",
      "Epoch 146/1000, Loss: 12.5676, Val Loss: 13.9025\n",
      "Epoch 147/1000, Loss: 12.6519, Val Loss: 13.9390\n",
      "Epoch 148/1000, Loss: 12.5902, Val Loss: 13.9829\n",
      "Epoch 149/1000, Loss: 12.7207, Val Loss: 13.9514\n",
      "Epoch 150/1000, Loss: 12.6259, Val Loss: 13.9024\n",
      "Epoch 151/1000, Loss: 12.5766, Val Loss: 13.8694\n",
      "Epoch 152/1000, Loss: 12.4318, Val Loss: 13.8660\n",
      "Epoch 153/1000, Loss: 12.3880, Val Loss: 13.8867\n",
      "Epoch 154/1000, Loss: 12.4493, Val Loss: 13.8997\n",
      "Epoch 155/1000, Loss: 12.2914, Val Loss: 13.8796\n",
      "Epoch 156/1000, Loss: 12.4216, Val Loss: 13.8604\n",
      "Epoch 157/1000, Loss: 12.4624, Val Loss: 13.8632\n",
      "Epoch 158/1000, Loss: 12.3332, Val Loss: 13.8842\n",
      "Epoch 159/1000, Loss: 12.3830, Val Loss: 13.8402\n",
      "Epoch 160/1000, Loss: 12.5097, Val Loss: 13.7787\n",
      "Epoch 161/1000, Loss: 12.5570, Val Loss: 13.7589\n",
      "Epoch 162/1000, Loss: 12.0897, Val Loss: 13.7682\n",
      "Epoch 163/1000, Loss: 12.5734, Val Loss: 13.7984\n",
      "Epoch 164/1000, Loss: 12.1896, Val Loss: 13.8288\n",
      "Epoch 165/1000, Loss: 12.3042, Val Loss: 13.8776\n",
      "Epoch 166/1000, Loss: 12.2317, Val Loss: 13.9451\n",
      "Epoch 167/1000, Loss: 12.4140, Val Loss: 13.9028\n",
      "Epoch 168/1000, Loss: 12.1708, Val Loss: 13.8670\n",
      "Epoch 169/1000, Loss: 12.0873, Val Loss: 13.8659\n",
      "Epoch 170/1000, Loss: 12.2545, Val Loss: 13.8822\n",
      "Epoch 171/1000, Loss: 12.2261, Val Loss: 13.9088\n",
      "Epoch 172/1000, Loss: 12.0358, Val Loss: 13.9344\n",
      "Epoch 173/1000, Loss: 11.9266, Val Loss: 13.9906\n",
      "Epoch 174/1000, Loss: 12.3854, Val Loss: 14.0325\n",
      "Epoch 175/1000, Loss: 12.0850, Val Loss: 14.0526\n",
      "Epoch 176/1000, Loss: 11.9894, Val Loss: 14.0494\n",
      "Epoch 177/1000, Loss: 12.0106, Val Loss: 14.0398\n",
      "Epoch 178/1000, Loss: 11.9373, Val Loss: 14.0323\n",
      "Epoch 179/1000, Loss: 12.0124, Val Loss: 14.0028\n",
      "Epoch 180/1000, Loss: 12.0839, Val Loss: 13.9909\n",
      "Epoch 181/1000, Loss: 12.1286, Val Loss: 13.9834\n",
      "Epoch 182/1000, Loss: 11.8813, Val Loss: 13.9677\n",
      "Epoch 183/1000, Loss: 12.1746, Val Loss: 13.9944\n",
      "Epoch 184/1000, Loss: 11.9855, Val Loss: 14.0193\n",
      "Epoch 185/1000, Loss: 11.8142, Val Loss: 14.0143\n",
      "Epoch 186/1000, Loss: 11.8682, Val Loss: 14.0082\n",
      "Epoch 187/1000, Loss: 11.9435, Val Loss: 14.0144\n",
      "Epoch 188/1000, Loss: 11.8450, Val Loss: 14.0135\n",
      "Epoch 189/1000, Loss: 11.9551, Val Loss: 13.9748\n",
      "Epoch 190/1000, Loss: 11.9652, Val Loss: 13.9319\n",
      "Epoch 191/1000, Loss: 11.9575, Val Loss: 13.9260\n",
      "Epoch 192/1000, Loss: 11.6386, Val Loss: 13.9625\n",
      "Epoch 193/1000, Loss: 11.8740, Val Loss: 13.9846\n",
      "Epoch 194/1000, Loss: 11.8023, Val Loss: 14.0017\n",
      "Epoch 195/1000, Loss: 11.6012, Val Loss: 14.0300\n",
      "Epoch 196/1000, Loss: 11.4974, Val Loss: 14.0536\n",
      "Epoch 197/1000, Loss: 11.6336, Val Loss: 14.0547\n",
      "Epoch 198/1000, Loss: 11.6796, Val Loss: 14.0628\n",
      "Epoch 199/1000, Loss: 11.6568, Val Loss: 14.0699\n",
      "Epoch 200/1000, Loss: 11.6341, Val Loss: 14.0648\n",
      "Epoch 201/1000, Loss: 11.6001, Val Loss: 13.9880\n",
      "Early stopping triggered at epoch 201. Best Val Loss: 13.7589\n"
     ]
    }
   ],
   "source": [
    "error_arr_input = []\n",
    "std_arr_input = []\n",
    "max_error_arr_input = []\n",
    "\n",
    "for i in range(10):\n",
    "    bias_predictor = train_bias_predictor(\n",
    "        torch.tensor(\n",
    "            np.vstack(X_train['reconstruction_error_percentage_array'].values),  # Stack arrays into a 2D array\n",
    "            dtype=torch.float32\n",
    "        ).to(device),\n",
    "        torch.tensor(y_train, dtype=torch.float32).to(device),\n",
    "        torch.tensor(\n",
    "            np.vstack(X_val['reconstruction_error_percentage_array'].values),  # Stack arrays into a 2D array\n",
    "            dtype=torch.float32\n",
    "        ).to(device),\n",
    "        torch.tensor(y_val, dtype=torch.float32).to(device),\n",
    "        epochs=1000,\n",
    "        learning_rate=0.001,\n",
    "        patience=40\n",
    "    )\n",
    "    features = torch.tensor(np.vstack(X_test['reconstruction_error_percentage_array'].values), dtype=torch.float32).to(device)\n",
    "    predictions = bias_predictor(features).squeeze().cpu().detach().numpy()\n",
    "\n",
    "    X_test['predicted_bias_with_array_input'] = np.nan\n",
    "    X_test['predicted_bias_with_array_input'] = predictions\n",
    "\n",
    "    X_test['new_predictions_with_array_input'] = X_test['predictions'] + X_test['predicted_bias_with_array_input']\n",
    "    X_test['new_error_percentage_with_array_input'] = (abs(X_test['target'] - X_test['new_predictions_with_array_input']) / X_test['target']) * 100\n",
    "    avg_error = X_test['new_error_percentage_with_array_input'].mean()\n",
    "    std_error = X_test['new_error_percentage_with_array_input'].std()\n",
    "    max_error = X_test['new_error_percentage_with_array_input'].max()\n",
    "    error_arr_input.append(avg_error)\n",
    "    std_arr_input.append(std_error)\n",
    "    max_error_arr_input.append(max_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame({'error_one_input': error_one_input, 'std_one_input': std_one_input, 'error_arr_input': error_arr_input, 'std_arr_input': std_arr_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_one_input</th>\n",
       "      <th>std_one_input</th>\n",
       "      <th>error_arr_input</th>\n",
       "      <th>std_arr_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.188081</td>\n",
       "      <td>1.965631</td>\n",
       "      <td>1.775319</td>\n",
       "      <td>1.615696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.217889</td>\n",
       "      <td>2.049757</td>\n",
       "      <td>1.757309</td>\n",
       "      <td>1.625015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.178148</td>\n",
       "      <td>1.967640</td>\n",
       "      <td>1.844594</td>\n",
       "      <td>1.658586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.189042</td>\n",
       "      <td>1.972966</td>\n",
       "      <td>1.794713</td>\n",
       "      <td>1.623295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.160044</td>\n",
       "      <td>1.946068</td>\n",
       "      <td>1.787984</td>\n",
       "      <td>1.615875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.205350</td>\n",
       "      <td>1.988628</td>\n",
       "      <td>1.784632</td>\n",
       "      <td>1.628415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.242609</td>\n",
       "      <td>2.024037</td>\n",
       "      <td>1.749680</td>\n",
       "      <td>1.603995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.247472</td>\n",
       "      <td>2.028750</td>\n",
       "      <td>1.809000</td>\n",
       "      <td>1.640362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.141825</td>\n",
       "      <td>1.931153</td>\n",
       "      <td>1.820880</td>\n",
       "      <td>1.650240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.227493</td>\n",
       "      <td>2.010775</td>\n",
       "      <td>1.766831</td>\n",
       "      <td>1.622973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   error_one_input  std_one_input  error_arr_input  std_arr_input\n",
       "0         2.188081       1.965631         1.775319       1.615696\n",
       "1         2.217889       2.049757         1.757309       1.625015\n",
       "2         2.178148       1.967640         1.844594       1.658586\n",
       "3         2.189042       1.972966         1.794713       1.623295\n",
       "4         2.160044       1.946068         1.787984       1.615875\n",
       "5         2.205350       1.988628         1.784632       1.628415\n",
       "6         2.242609       2.024037         1.749680       1.603995\n",
       "7         2.247472       2.028750         1.809000       1.640362\n",
       "8         2.141825       1.931153         1.820880       1.650240\n",
       "9         2.227493       2.010775         1.766831       1.622973"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_one_input</th>\n",
       "      <th>std_one_input</th>\n",
       "      <th>error_arr_input</th>\n",
       "      <th>std_arr_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.199795</td>\n",
       "      <td>1.988540</td>\n",
       "      <td>1.789094</td>\n",
       "      <td>1.628445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.034791</td>\n",
       "      <td>0.038615</td>\n",
       "      <td>0.029418</td>\n",
       "      <td>0.016707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.141825</td>\n",
       "      <td>1.931153</td>\n",
       "      <td>1.749680</td>\n",
       "      <td>1.603995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.180631</td>\n",
       "      <td>1.966133</td>\n",
       "      <td>1.768953</td>\n",
       "      <td>1.617650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.197196</td>\n",
       "      <td>1.980797</td>\n",
       "      <td>1.786308</td>\n",
       "      <td>1.624155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.225092</td>\n",
       "      <td>2.020721</td>\n",
       "      <td>1.805428</td>\n",
       "      <td>1.637376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.247472</td>\n",
       "      <td>2.049757</td>\n",
       "      <td>1.844594</td>\n",
       "      <td>1.658586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       error_one_input  std_one_input  error_arr_input  std_arr_input\n",
       "count        10.000000      10.000000        10.000000      10.000000\n",
       "mean          2.199795       1.988540         1.789094       1.628445\n",
       "std           0.034791       0.038615         0.029418       0.016707\n",
       "min           2.141825       1.931153         1.749680       1.603995\n",
       "25%           2.180631       1.966133         1.768953       1.617650\n",
       "50%           2.197196       1.980797         1.786308       1.624155\n",
       "75%           2.225092       2.020721         1.805428       1.637376\n",
       "max           2.247472       2.049757         1.844594       1.658586"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1602.000000\n",
       "mean        2.530712\n",
       "std         2.374628\n",
       "min         0.000122\n",
       "25%         0.767609\n",
       "50%         1.662241\n",
       "75%         3.795965\n",
       "max        18.144303\n",
       "Name: original_error_percentage, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['original_error_percentage'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reconstruction_error_percentage</th>\n",
       "      <th>target</th>\n",
       "      <th>predictions</th>\n",
       "      <th>bias</th>\n",
       "      <th>reconstruction_error_percentage_array</th>\n",
       "      <th>predicted_bias_with_one_input</th>\n",
       "      <th>new_predictions_with_one_input</th>\n",
       "      <th>original_error_percentage</th>\n",
       "      <th>new_error_percentage_with_one_input</th>\n",
       "      <th>predicted_bias_with_array_input</th>\n",
       "      <th>new_predictions_with_array_input</th>\n",
       "      <th>new_error_percentage_with_array_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.872467</td>\n",
       "      <td>30.429846</td>\n",
       "      <td>28.182663</td>\n",
       "      <td>2.247183</td>\n",
       "      <td>[0.865808988877958, 0.8668465829545656, 0.8671...</td>\n",
       "      <td>0.334249</td>\n",
       "      <td>28.516912</td>\n",
       "      <td>7.384799</td>\n",
       "      <td>6.286375</td>\n",
       "      <td>1.296565</td>\n",
       "      <td>29.479228</td>\n",
       "      <td>3.123966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789</th>\n",
       "      <td>6.734052</td>\n",
       "      <td>307.836670</td>\n",
       "      <td>308.924255</td>\n",
       "      <td>-1.087585</td>\n",
       "      <td>[10.185029277702128, 2.2025611446663187, 5.732...</td>\n",
       "      <td>-0.591945</td>\n",
       "      <td>308.332310</td>\n",
       "      <td>0.353300</td>\n",
       "      <td>0.161008</td>\n",
       "      <td>-0.630219</td>\n",
       "      <td>308.294036</td>\n",
       "      <td>0.148574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3110</th>\n",
       "      <td>1.485681</td>\n",
       "      <td>82.987839</td>\n",
       "      <td>82.817764</td>\n",
       "      <td>0.170074</td>\n",
       "      <td>[0.7517255375362865, 0.6664800255598702, 0.686...</td>\n",
       "      <td>-0.038676</td>\n",
       "      <td>82.779089</td>\n",
       "      <td>0.204939</td>\n",
       "      <td>0.251543</td>\n",
       "      <td>1.086244</td>\n",
       "      <td>83.904008</td>\n",
       "      <td>1.103980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7057</th>\n",
       "      <td>2.748172</td>\n",
       "      <td>369.807526</td>\n",
       "      <td>371.961212</td>\n",
       "      <td>-2.153687</td>\n",
       "      <td>[1.3816415978691088, 4.0989117322815245, 0.985...</td>\n",
       "      <td>0.574940</td>\n",
       "      <td>372.536152</td>\n",
       "      <td>0.582380</td>\n",
       "      <td>0.737850</td>\n",
       "      <td>-1.152370</td>\n",
       "      <td>370.808843</td>\n",
       "      <td>0.270767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>0.990742</td>\n",
       "      <td>67.861076</td>\n",
       "      <td>65.243706</td>\n",
       "      <td>2.617371</td>\n",
       "      <td>[1.2778963613711947, 0.7747250560136459, 1.508...</td>\n",
       "      <td>0.140140</td>\n",
       "      <td>65.383846</td>\n",
       "      <td>3.856954</td>\n",
       "      <td>3.650444</td>\n",
       "      <td>1.253283</td>\n",
       "      <td>66.496989</td>\n",
       "      <td>2.010118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>1.913761</td>\n",
       "      <td>79.646431</td>\n",
       "      <td>76.761215</td>\n",
       "      <td>2.885216</td>\n",
       "      <td>[0.6348746987879306, 0.5525179639740401, 1.215...</td>\n",
       "      <td>-0.037892</td>\n",
       "      <td>76.723323</td>\n",
       "      <td>3.622530</td>\n",
       "      <td>3.670105</td>\n",
       "      <td>0.222062</td>\n",
       "      <td>76.983277</td>\n",
       "      <td>3.343721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>1.169535</td>\n",
       "      <td>86.870735</td>\n",
       "      <td>88.112328</td>\n",
       "      <td>-1.241592</td>\n",
       "      <td>[1.2430595625005405, 1.1607447280814434, 1.275...</td>\n",
       "      <td>-0.027142</td>\n",
       "      <td>88.085186</td>\n",
       "      <td>1.429241</td>\n",
       "      <td>1.397997</td>\n",
       "      <td>0.883410</td>\n",
       "      <td>88.995737</td>\n",
       "      <td>2.446166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>3.809520</td>\n",
       "      <td>205.387939</td>\n",
       "      <td>204.572617</td>\n",
       "      <td>0.815323</td>\n",
       "      <td>[9.967309715008783, 4.466418914873627, 14.0424...</td>\n",
       "      <td>1.664630</td>\n",
       "      <td>206.237247</td>\n",
       "      <td>0.396967</td>\n",
       "      <td>0.413514</td>\n",
       "      <td>-1.189483</td>\n",
       "      <td>203.383133</td>\n",
       "      <td>0.976107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>0.645578</td>\n",
       "      <td>52.037102</td>\n",
       "      <td>49.502167</td>\n",
       "      <td>2.534935</td>\n",
       "      <td>[0.6782854304440461, 0.656536332774514, 0.6494...</td>\n",
       "      <td>0.627546</td>\n",
       "      <td>50.129713</td>\n",
       "      <td>4.871399</td>\n",
       "      <td>3.665440</td>\n",
       "      <td>1.475862</td>\n",
       "      <td>50.978029</td>\n",
       "      <td>2.035226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>6.540621</td>\n",
       "      <td>178.703690</td>\n",
       "      <td>179.721680</td>\n",
       "      <td>-1.017990</td>\n",
       "      <td>[3.3069675607945737, 7.530232964330011, 5.0975...</td>\n",
       "      <td>-0.584081</td>\n",
       "      <td>179.137599</td>\n",
       "      <td>0.569653</td>\n",
       "      <td>0.242809</td>\n",
       "      <td>-1.711013</td>\n",
       "      <td>178.010667</td>\n",
       "      <td>0.387805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1602 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      reconstruction_error_percentage      target  predictions      bias  \\\n",
       "563                          0.872467   30.429846    28.182663  2.247183   \n",
       "6789                         6.734052  307.836670   308.924255 -1.087585   \n",
       "3110                         1.485681   82.987839    82.817764  0.170074   \n",
       "7057                         2.748172  369.807526   371.961212 -2.153687   \n",
       "2614                         0.990742   67.861076    65.243706  2.617371   \n",
       "...                               ...         ...          ...       ...   \n",
       "2964                         1.913761   79.646431    76.761215  2.885216   \n",
       "1975                         1.169535   86.870735    88.112328 -1.241592   \n",
       "6054                         3.809520  205.387939   204.572617  0.815323   \n",
       "1088                         0.645578   52.037102    49.502167  2.534935   \n",
       "5560                         6.540621  178.703690   179.721680 -1.017990   \n",
       "\n",
       "                  reconstruction_error_percentage_array  \\\n",
       "563   [0.865808988877958, 0.8668465829545656, 0.8671...   \n",
       "6789  [10.185029277702128, 2.2025611446663187, 5.732...   \n",
       "3110  [0.7517255375362865, 0.6664800255598702, 0.686...   \n",
       "7057  [1.3816415978691088, 4.0989117322815245, 0.985...   \n",
       "2614  [1.2778963613711947, 0.7747250560136459, 1.508...   \n",
       "...                                                 ...   \n",
       "2964  [0.6348746987879306, 0.5525179639740401, 1.215...   \n",
       "1975  [1.2430595625005405, 1.1607447280814434, 1.275...   \n",
       "6054  [9.967309715008783, 4.466418914873627, 14.0424...   \n",
       "1088  [0.6782854304440461, 0.656536332774514, 0.6494...   \n",
       "5560  [3.3069675607945737, 7.530232964330011, 5.0975...   \n",
       "\n",
       "      predicted_bias_with_one_input  new_predictions_with_one_input  \\\n",
       "563                        0.334249                       28.516912   \n",
       "6789                      -0.591945                      308.332310   \n",
       "3110                      -0.038676                       82.779089   \n",
       "7057                       0.574940                      372.536152   \n",
       "2614                       0.140140                       65.383846   \n",
       "...                             ...                             ...   \n",
       "2964                      -0.037892                       76.723323   \n",
       "1975                      -0.027142                       88.085186   \n",
       "6054                       1.664630                      206.237247   \n",
       "1088                       0.627546                       50.129713   \n",
       "5560                      -0.584081                      179.137599   \n",
       "\n",
       "      original_error_percentage  new_error_percentage_with_one_input  \\\n",
       "563                    7.384799                             6.286375   \n",
       "6789                   0.353300                             0.161008   \n",
       "3110                   0.204939                             0.251543   \n",
       "7057                   0.582380                             0.737850   \n",
       "2614                   3.856954                             3.650444   \n",
       "...                         ...                                  ...   \n",
       "2964                   3.622530                             3.670105   \n",
       "1975                   1.429241                             1.397997   \n",
       "6054                   0.396967                             0.413514   \n",
       "1088                   4.871399                             3.665440   \n",
       "5560                   0.569653                             0.242809   \n",
       "\n",
       "      predicted_bias_with_array_input  new_predictions_with_array_input  \\\n",
       "563                          1.296565                         29.479228   \n",
       "6789                        -0.630219                        308.294036   \n",
       "3110                         1.086244                         83.904008   \n",
       "7057                        -1.152370                        370.808843   \n",
       "2614                         1.253283                         66.496989   \n",
       "...                               ...                               ...   \n",
       "2964                         0.222062                         76.983277   \n",
       "1975                         0.883410                         88.995737   \n",
       "6054                        -1.189483                        203.383133   \n",
       "1088                         1.475862                         50.978029   \n",
       "5560                        -1.711013                        178.010667   \n",
       "\n",
       "      new_error_percentage_with_array_input  \n",
       "563                                3.123966  \n",
       "6789                               0.148574  \n",
       "3110                               1.103980  \n",
       "7057                               0.270767  \n",
       "2614                               2.010118  \n",
       "...                                     ...  \n",
       "2964                               3.343721  \n",
       "1975                               2.446166  \n",
       "6054                               0.976107  \n",
       "1088                               2.035226  \n",
       "5560                               0.387805  \n",
       "\n",
       "[1602 rows x 12 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# bias_predictor = BiasPredictor().to(device)\n",
    "\n",
    "# X_reconstruction = np.stack(X_train['reconstruction_error_percentage_array'].values)\n",
    "# y = y_train\n",
    "\n",
    "# X_reconstruction_tensor = torch.tensor(X_reconstruction, dtype=torch.float32).to(device)\n",
    "# y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# def model_wrapper(X):\n",
    "#     X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         return bias_predictor(X_tensor).cpu().numpy()\n",
    "    \n",
    "    \n",
    "# bias_predictor.eval()\n",
    "\n",
    "# explainer = shap.KernelExplainer(model_wrapper, X_reconstruction_tensor[:750].cpu().numpy())\n",
    "\n",
    "# shap_values = explainer.shap_values(X_reconstruction_tensor[:750].cpu().numpy())\n",
    "\n",
    "# shap.summary_plot(shap_values, X_reconstruction_tensor[:750].cpu().numpy(), feature_names=[f\"Day -{i}\" for i in range(9, 0, -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap_values_array = np.array(shap_values)\n",
    "\n",
    "# # Reshape the 3D array to 2D\n",
    "# shap_values_2d = shap_values_array.reshape(-1, shap_values_array.shape[-1])\n",
    "\n",
    "# # Create the DataFrame\n",
    "# shap_values_df = pd.DataFrame(shap_values_2d, columns=[f\"Day -{i}\" for i in range(9, 0, -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_sums = shap_values_df.abs().sum(axis=0)\n",
    "# relative_weightage = column_sums / column_sums.sum()\n",
    "# print(\"Relative Weightage of Each Day:\", relative_weightage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print each relative weight separately\n",
    "# for i, weight in enumerate(relative_weightage):\n",
    "#     print(f\"Day -{i+1}:\", weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if weights is None:\n",
    "#     weights = []\n",
    "#     for weight in relative_weightage:\n",
    "#         weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.10619458980191179,\n",
    " 0.032590702943947676,\n",
    " 0.24322533687632794,\n",
    " 0.04211258336931392,\n",
    " 0.11436472610842367,\n",
    " 0.12440656192592138,\n",
    " 0.14868789216363856,\n",
    " 0.10144880432373195,\n",
    " 0.08696880248678329]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran 1\n",
      "ran 2\n",
      "ran 3\n"
     ]
    }
   ],
   "source": [
    "if 'reconstruction_error_percentage_array_weighted' not in X_train.columns:\n",
    "    X_train['reconstruction_error_percentage_array_weighted'] = X_train['reconstruction_error_percentage_array'].apply(lambda x: (np.array(x) * np.array(weights)).tolist())\n",
    "    print(\"ran 1\")\n",
    "\n",
    "if 'reconstruction_error_percentage_array_weighted' not in X_val.columns:\n",
    "    X_val['reconstruction_error_percentage_array_weighted'] = X_val['reconstruction_error_percentage_array'].apply(lambda x: (np.array(x) * np.array(weights)).tolist())\n",
    "    print(\"ran 2\")\n",
    "\n",
    "if 'reconstruction_error_percentage_array_weighted' not in X_test.columns:\n",
    "    X_test['reconstruction_error_percentage_array_weighted'] = X_test['reconstruction_error_percentage_array'].apply(lambda x: (np.array(x) * np.array(weights)).tolist())\n",
    "    print(\"ran 3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 16.8087, Val Loss: 17.0647\n",
      "Epoch 2/1000, Loss: 16.7836, Val Loss: 17.0608\n",
      "Epoch 3/1000, Loss: 16.7607, Val Loss: 17.0571\n",
      "Epoch 4/1000, Loss: 16.7434, Val Loss: 17.0533\n",
      "Epoch 5/1000, Loss: 16.7251, Val Loss: 17.0493\n",
      "Epoch 6/1000, Loss: 16.7113, Val Loss: 17.0451\n",
      "Epoch 7/1000, Loss: 16.6886, Val Loss: 17.0403\n",
      "Epoch 8/1000, Loss: 16.6679, Val Loss: 17.0349\n",
      "Epoch 9/1000, Loss: 16.6477, Val Loss: 17.0288\n",
      "Epoch 10/1000, Loss: 16.6257, Val Loss: 17.0211\n",
      "Epoch 11/1000, Loss: 16.5989, Val Loss: 17.0120\n",
      "Epoch 12/1000, Loss: 16.5521, Val Loss: 17.0017\n",
      "Epoch 13/1000, Loss: 16.5280, Val Loss: 16.9898\n",
      "Epoch 14/1000, Loss: 16.5119, Val Loss: 16.9759\n",
      "Epoch 15/1000, Loss: 16.4878, Val Loss: 16.9594\n",
      "Epoch 16/1000, Loss: 16.4436, Val Loss: 16.9410\n",
      "Epoch 17/1000, Loss: 16.4230, Val Loss: 16.9194\n",
      "Epoch 18/1000, Loss: 16.3870, Val Loss: 16.8972\n",
      "Epoch 19/1000, Loss: 16.3671, Val Loss: 16.8739\n",
      "Epoch 20/1000, Loss: 16.3416, Val Loss: 16.8517\n",
      "Epoch 21/1000, Loss: 16.2954, Val Loss: 16.8241\n",
      "Epoch 22/1000, Loss: 16.2648, Val Loss: 16.7944\n",
      "Epoch 23/1000, Loss: 16.2491, Val Loss: 16.7608\n",
      "Epoch 24/1000, Loss: 16.1725, Val Loss: 16.7223\n",
      "Epoch 25/1000, Loss: 16.1640, Val Loss: 16.6853\n",
      "Epoch 26/1000, Loss: 16.1200, Val Loss: 16.6470\n",
      "Epoch 27/1000, Loss: 16.0924, Val Loss: 16.6076\n",
      "Epoch 28/1000, Loss: 16.0626, Val Loss: 16.5722\n",
      "Epoch 29/1000, Loss: 16.0079, Val Loss: 16.5351\n",
      "Epoch 30/1000, Loss: 15.9782, Val Loss: 16.4981\n",
      "Epoch 31/1000, Loss: 15.9747, Val Loss: 16.4594\n",
      "Epoch 32/1000, Loss: 15.9212, Val Loss: 16.4196\n",
      "Epoch 33/1000, Loss: 15.9011, Val Loss: 16.3772\n",
      "Epoch 34/1000, Loss: 15.8199, Val Loss: 16.3293\n",
      "Epoch 35/1000, Loss: 15.8372, Val Loss: 16.2843\n",
      "Epoch 36/1000, Loss: 15.7597, Val Loss: 16.2484\n",
      "Epoch 37/1000, Loss: 15.7047, Val Loss: 16.2147\n",
      "Epoch 38/1000, Loss: 15.6744, Val Loss: 16.1800\n",
      "Epoch 39/1000, Loss: 15.6120, Val Loss: 16.1429\n",
      "Epoch 40/1000, Loss: 15.5762, Val Loss: 16.1017\n",
      "Epoch 41/1000, Loss: 15.5768, Val Loss: 16.0693\n",
      "Epoch 42/1000, Loss: 15.5467, Val Loss: 16.0247\n",
      "Epoch 43/1000, Loss: 15.4715, Val Loss: 15.9679\n",
      "Epoch 44/1000, Loss: 15.4288, Val Loss: 15.9103\n",
      "Epoch 45/1000, Loss: 15.3906, Val Loss: 15.8625\n",
      "Epoch 46/1000, Loss: 15.4036, Val Loss: 15.8249\n",
      "Epoch 47/1000, Loss: 15.2114, Val Loss: 15.7934\n",
      "Epoch 48/1000, Loss: 15.2768, Val Loss: 15.7691\n",
      "Epoch 49/1000, Loss: 15.2521, Val Loss: 15.7334\n",
      "Epoch 50/1000, Loss: 15.1958, Val Loss: 15.6826\n",
      "Epoch 51/1000, Loss: 15.0555, Val Loss: 15.6324\n",
      "Epoch 52/1000, Loss: 15.0842, Val Loss: 15.5967\n",
      "Epoch 53/1000, Loss: 15.0224, Val Loss: 15.5648\n",
      "Epoch 54/1000, Loss: 15.0638, Val Loss: 15.5400\n",
      "Epoch 55/1000, Loss: 14.9968, Val Loss: 15.5209\n",
      "Epoch 56/1000, Loss: 14.8706, Val Loss: 15.4939\n",
      "Epoch 57/1000, Loss: 15.0079, Val Loss: 15.4651\n",
      "Epoch 58/1000, Loss: 14.8853, Val Loss: 15.4387\n",
      "Epoch 59/1000, Loss: 14.7735, Val Loss: 15.4156\n",
      "Epoch 60/1000, Loss: 14.8825, Val Loss: 15.3947\n",
      "Epoch 61/1000, Loss: 14.8359, Val Loss: 15.3708\n",
      "Epoch 62/1000, Loss: 14.7550, Val Loss: 15.3440\n",
      "Epoch 63/1000, Loss: 14.8372, Val Loss: 15.3179\n",
      "Epoch 64/1000, Loss: 14.6961, Val Loss: 15.2899\n",
      "Epoch 65/1000, Loss: 14.7073, Val Loss: 15.2638\n",
      "Epoch 66/1000, Loss: 14.6292, Val Loss: 15.2436\n",
      "Epoch 67/1000, Loss: 14.6109, Val Loss: 15.2328\n",
      "Epoch 68/1000, Loss: 14.5915, Val Loss: 15.2322\n",
      "Epoch 69/1000, Loss: 14.5037, Val Loss: 15.2203\n",
      "Epoch 70/1000, Loss: 14.5120, Val Loss: 15.1927\n",
      "Epoch 71/1000, Loss: 14.3671, Val Loss: 15.1536\n",
      "Epoch 72/1000, Loss: 14.4564, Val Loss: 15.1159\n",
      "Epoch 73/1000, Loss: 14.4107, Val Loss: 15.0827\n",
      "Epoch 74/1000, Loss: 14.2928, Val Loss: 15.0532\n",
      "Epoch 75/1000, Loss: 14.2495, Val Loss: 15.0246\n",
      "Epoch 76/1000, Loss: 14.2568, Val Loss: 14.9944\n",
      "Epoch 77/1000, Loss: 14.1731, Val Loss: 14.9602\n",
      "Epoch 78/1000, Loss: 14.2108, Val Loss: 14.9278\n",
      "Epoch 79/1000, Loss: 14.2292, Val Loss: 14.8959\n",
      "Epoch 80/1000, Loss: 14.1143, Val Loss: 14.8557\n",
      "Epoch 81/1000, Loss: 14.1383, Val Loss: 14.8219\n",
      "Epoch 82/1000, Loss: 13.9727, Val Loss: 14.7924\n",
      "Epoch 83/1000, Loss: 13.9013, Val Loss: 14.7677\n",
      "Epoch 84/1000, Loss: 14.1444, Val Loss: 14.7387\n",
      "Epoch 85/1000, Loss: 13.9910, Val Loss: 14.7204\n",
      "Epoch 86/1000, Loss: 13.9115, Val Loss: 14.7027\n",
      "Epoch 87/1000, Loss: 13.9257, Val Loss: 14.6771\n",
      "Epoch 88/1000, Loss: 13.8182, Val Loss: 14.6347\n",
      "Epoch 89/1000, Loss: 13.7978, Val Loss: 14.5846\n",
      "Epoch 90/1000, Loss: 13.8755, Val Loss: 14.5455\n",
      "Epoch 91/1000, Loss: 13.6431, Val Loss: 14.5126\n",
      "Epoch 92/1000, Loss: 13.8550, Val Loss: 14.4822\n",
      "Epoch 93/1000, Loss: 13.7418, Val Loss: 14.4394\n",
      "Epoch 94/1000, Loss: 13.7017, Val Loss: 14.4024\n",
      "Epoch 95/1000, Loss: 13.6217, Val Loss: 14.3842\n",
      "Epoch 96/1000, Loss: 13.5108, Val Loss: 14.3648\n",
      "Epoch 97/1000, Loss: 13.5907, Val Loss: 14.3457\n",
      "Epoch 98/1000, Loss: 13.6143, Val Loss: 14.3292\n",
      "Epoch 99/1000, Loss: 13.4955, Val Loss: 14.3198\n",
      "Epoch 100/1000, Loss: 13.6702, Val Loss: 14.3146\n",
      "Epoch 101/1000, Loss: 13.5097, Val Loss: 14.3195\n",
      "Epoch 102/1000, Loss: 13.5639, Val Loss: 14.3321\n",
      "Epoch 103/1000, Loss: 13.6028, Val Loss: 14.3541\n",
      "Epoch 104/1000, Loss: 13.1414, Val Loss: 14.3777\n",
      "Epoch 105/1000, Loss: 13.4714, Val Loss: 14.3822\n",
      "Epoch 106/1000, Loss: 13.3598, Val Loss: 14.3475\n",
      "Epoch 107/1000, Loss: 13.3357, Val Loss: 14.3148\n",
      "Epoch 108/1000, Loss: 13.2062, Val Loss: 14.2953\n",
      "Epoch 109/1000, Loss: 13.1981, Val Loss: 14.2945\n",
      "Epoch 110/1000, Loss: 13.1637, Val Loss: 14.2973\n",
      "Epoch 111/1000, Loss: 13.1677, Val Loss: 14.3000\n",
      "Epoch 112/1000, Loss: 13.1492, Val Loss: 14.2806\n",
      "Epoch 113/1000, Loss: 13.6295, Val Loss: 14.2169\n",
      "Epoch 114/1000, Loss: 13.0403, Val Loss: 14.1746\n",
      "Epoch 115/1000, Loss: 13.2166, Val Loss: 14.1487\n",
      "Epoch 116/1000, Loss: 13.1371, Val Loss: 14.1367\n",
      "Epoch 117/1000, Loss: 12.9437, Val Loss: 14.1414\n",
      "Epoch 118/1000, Loss: 13.0374, Val Loss: 14.1543\n",
      "Epoch 119/1000, Loss: 12.9076, Val Loss: 14.1680\n",
      "Epoch 120/1000, Loss: 12.9311, Val Loss: 14.1733\n",
      "Epoch 121/1000, Loss: 13.0521, Val Loss: 14.1772\n",
      "Epoch 122/1000, Loss: 13.1681, Val Loss: 14.1620\n",
      "Epoch 123/1000, Loss: 12.8646, Val Loss: 14.1396\n",
      "Epoch 124/1000, Loss: 12.9329, Val Loss: 14.1119\n",
      "Epoch 125/1000, Loss: 12.7126, Val Loss: 14.1155\n",
      "Epoch 126/1000, Loss: 13.0099, Val Loss: 14.1448\n",
      "Epoch 127/1000, Loss: 12.9155, Val Loss: 14.1500\n",
      "Epoch 128/1000, Loss: 12.8326, Val Loss: 14.1446\n",
      "Epoch 129/1000, Loss: 12.9851, Val Loss: 14.1240\n",
      "Epoch 130/1000, Loss: 12.9289, Val Loss: 14.0922\n",
      "Epoch 131/1000, Loss: 12.7687, Val Loss: 14.0792\n",
      "Epoch 132/1000, Loss: 12.8950, Val Loss: 14.0752\n",
      "Epoch 133/1000, Loss: 12.7284, Val Loss: 14.0836\n",
      "Epoch 134/1000, Loss: 12.9504, Val Loss: 14.1031\n",
      "Epoch 135/1000, Loss: 12.7021, Val Loss: 14.1578\n",
      "Epoch 136/1000, Loss: 12.8476, Val Loss: 14.1673\n",
      "Epoch 137/1000, Loss: 13.0161, Val Loss: 14.1591\n",
      "Epoch 138/1000, Loss: 12.5254, Val Loss: 14.1606\n",
      "Epoch 139/1000, Loss: 12.6681, Val Loss: 14.1341\n",
      "Epoch 140/1000, Loss: 12.8056, Val Loss: 14.1087\n",
      "Epoch 141/1000, Loss: 12.7894, Val Loss: 14.1102\n",
      "Epoch 142/1000, Loss: 12.6565, Val Loss: 14.0932\n",
      "Epoch 143/1000, Loss: 12.4672, Val Loss: 14.1071\n",
      "Epoch 144/1000, Loss: 12.5851, Val Loss: 14.1453\n",
      "Epoch 145/1000, Loss: 12.6009, Val Loss: 14.1431\n",
      "Epoch 146/1000, Loss: 12.5787, Val Loss: 14.1084\n",
      "Epoch 147/1000, Loss: 12.4038, Val Loss: 14.1006\n",
      "Epoch 148/1000, Loss: 12.4548, Val Loss: 14.1252\n",
      "Epoch 149/1000, Loss: 12.6373, Val Loss: 14.1232\n",
      "Epoch 150/1000, Loss: 12.5341, Val Loss: 14.1084\n",
      "Epoch 151/1000, Loss: 12.2298, Val Loss: 14.1030\n",
      "Epoch 152/1000, Loss: 12.2316, Val Loss: 14.1011\n",
      "Epoch 153/1000, Loss: 12.4031, Val Loss: 14.0700\n",
      "Epoch 154/1000, Loss: 12.3463, Val Loss: 14.0661\n",
      "Epoch 155/1000, Loss: 12.3311, Val Loss: 14.0825\n",
      "Epoch 156/1000, Loss: 12.2311, Val Loss: 14.0921\n",
      "Epoch 157/1000, Loss: 12.3128, Val Loss: 14.0919\n",
      "Epoch 158/1000, Loss: 12.2816, Val Loss: 14.0944\n",
      "Epoch 159/1000, Loss: 12.1425, Val Loss: 14.1125\n",
      "Epoch 160/1000, Loss: 12.2039, Val Loss: 14.1256\n",
      "Epoch 161/1000, Loss: 12.2785, Val Loss: 14.1001\n",
      "Epoch 162/1000, Loss: 12.2233, Val Loss: 14.0887\n",
      "Epoch 163/1000, Loss: 12.0456, Val Loss: 14.0726\n",
      "Epoch 164/1000, Loss: 12.1963, Val Loss: 14.0542\n",
      "Epoch 165/1000, Loss: 12.2232, Val Loss: 14.0609\n",
      "Epoch 166/1000, Loss: 12.1399, Val Loss: 14.1152\n",
      "Epoch 167/1000, Loss: 12.1588, Val Loss: 14.2017\n",
      "Epoch 168/1000, Loss: 11.9948, Val Loss: 14.2611\n",
      "Epoch 169/1000, Loss: 11.9187, Val Loss: 14.3040\n",
      "Epoch 170/1000, Loss: 12.0574, Val Loss: 14.2814\n",
      "Epoch 171/1000, Loss: 12.0406, Val Loss: 14.2615\n",
      "Epoch 172/1000, Loss: 12.1795, Val Loss: 14.2281\n",
      "Epoch 173/1000, Loss: 11.8977, Val Loss: 14.2236\n",
      "Epoch 174/1000, Loss: 12.1399, Val Loss: 14.2088\n",
      "Epoch 175/1000, Loss: 11.8222, Val Loss: 14.1961\n",
      "Epoch 176/1000, Loss: 11.8563, Val Loss: 14.2110\n",
      "Epoch 177/1000, Loss: 11.9443, Val Loss: 14.2411\n",
      "Epoch 178/1000, Loss: 12.0302, Val Loss: 14.2669\n",
      "Epoch 179/1000, Loss: 11.9417, Val Loss: 14.3331\n",
      "Epoch 180/1000, Loss: 11.8279, Val Loss: 14.3799\n",
      "Epoch 181/1000, Loss: 11.9438, Val Loss: 14.3434\n",
      "Epoch 182/1000, Loss: 11.8121, Val Loss: 14.3115\n",
      "Epoch 183/1000, Loss: 11.6031, Val Loss: 14.2722\n",
      "Epoch 184/1000, Loss: 11.9455, Val Loss: 14.2351\n",
      "Epoch 185/1000, Loss: 11.6891, Val Loss: 14.2418\n",
      "Epoch 186/1000, Loss: 11.6808, Val Loss: 14.2872\n",
      "Epoch 187/1000, Loss: 11.7921, Val Loss: 14.3035\n",
      "Epoch 188/1000, Loss: 12.0293, Val Loss: 14.2714\n",
      "Epoch 189/1000, Loss: 11.7050, Val Loss: 14.2760\n",
      "Epoch 190/1000, Loss: 11.7461, Val Loss: 14.3053\n",
      "Epoch 191/1000, Loss: 11.8584, Val Loss: 14.3430\n",
      "Epoch 192/1000, Loss: 11.8205, Val Loss: 14.3723\n",
      "Epoch 193/1000, Loss: 11.7061, Val Loss: 14.3579\n",
      "Epoch 194/1000, Loss: 11.6497, Val Loss: 14.3280\n",
      "Epoch 195/1000, Loss: 11.5959, Val Loss: 14.3018\n",
      "Epoch 196/1000, Loss: 11.7702, Val Loss: 14.2672\n",
      "Epoch 197/1000, Loss: 11.7787, Val Loss: 14.2676\n",
      "Epoch 198/1000, Loss: 11.8103, Val Loss: 14.2922\n",
      "Epoch 199/1000, Loss: 11.6196, Val Loss: 14.3039\n",
      "Epoch 200/1000, Loss: 11.6201, Val Loss: 14.3331\n",
      "Epoch 201/1000, Loss: 11.6707, Val Loss: 14.3365\n",
      "Epoch 202/1000, Loss: 11.6763, Val Loss: 14.3274\n",
      "Epoch 203/1000, Loss: 11.6027, Val Loss: 14.3254\n",
      "Epoch 204/1000, Loss: 11.6027, Val Loss: 14.3448\n",
      "Early stopping triggered at epoch 204. Best Val Loss: 14.0542\n",
      "Epoch 1/1000, Loss: 16.9723, Val Loss: 17.2795\n",
      "Epoch 2/1000, Loss: 16.9574, Val Loss: 17.2761\n",
      "Epoch 3/1000, Loss: 16.9456, Val Loss: 17.2727\n",
      "Epoch 4/1000, Loss: 16.9366, Val Loss: 17.2695\n",
      "Epoch 5/1000, Loss: 16.9205, Val Loss: 17.2664\n",
      "Epoch 6/1000, Loss: 16.9132, Val Loss: 17.2632\n",
      "Epoch 7/1000, Loss: 16.9128, Val Loss: 17.2599\n",
      "Epoch 8/1000, Loss: 16.9039, Val Loss: 17.2565\n",
      "Epoch 9/1000, Loss: 16.8965, Val Loss: 17.2531\n",
      "Epoch 10/1000, Loss: 16.8888, Val Loss: 17.2496\n",
      "Epoch 11/1000, Loss: 16.8690, Val Loss: 17.2461\n",
      "Epoch 12/1000, Loss: 16.8694, Val Loss: 17.2424\n",
      "Epoch 13/1000, Loss: 16.8383, Val Loss: 17.2386\n",
      "Epoch 14/1000, Loss: 16.8413, Val Loss: 17.2346\n",
      "Epoch 15/1000, Loss: 16.8261, Val Loss: 17.2303\n",
      "Epoch 16/1000, Loss: 16.8163, Val Loss: 17.2257\n",
      "Epoch 17/1000, Loss: 16.8015, Val Loss: 17.2211\n",
      "Epoch 18/1000, Loss: 16.7722, Val Loss: 17.2158\n",
      "Epoch 19/1000, Loss: 16.7417, Val Loss: 17.2091\n",
      "Epoch 20/1000, Loss: 16.7243, Val Loss: 17.2014\n",
      "Epoch 21/1000, Loss: 16.7117, Val Loss: 17.1926\n",
      "Epoch 22/1000, Loss: 16.6688, Val Loss: 17.1821\n",
      "Epoch 23/1000, Loss: 16.6446, Val Loss: 17.1710\n",
      "Epoch 24/1000, Loss: 16.6018, Val Loss: 17.1583\n",
      "Epoch 25/1000, Loss: 16.6007, Val Loss: 17.1430\n",
      "Epoch 26/1000, Loss: 16.5837, Val Loss: 17.1237\n",
      "Epoch 27/1000, Loss: 16.5144, Val Loss: 17.1013\n",
      "Epoch 28/1000, Loss: 16.5036, Val Loss: 17.0779\n",
      "Epoch 29/1000, Loss: 16.4585, Val Loss: 17.0513\n",
      "Epoch 30/1000, Loss: 16.4316, Val Loss: 17.0233\n",
      "Epoch 31/1000, Loss: 16.3976, Val Loss: 16.9906\n",
      "Epoch 32/1000, Loss: 16.3910, Val Loss: 16.9594\n",
      "Epoch 33/1000, Loss: 16.3280, Val Loss: 16.9232\n",
      "Epoch 34/1000, Loss: 16.3001, Val Loss: 16.8880\n",
      "Epoch 35/1000, Loss: 16.2270, Val Loss: 16.8494\n",
      "Epoch 36/1000, Loss: 16.2163, Val Loss: 16.8119\n",
      "Epoch 37/1000, Loss: 16.1859, Val Loss: 16.7745\n",
      "Epoch 38/1000, Loss: 16.1281, Val Loss: 16.7366\n",
      "Epoch 39/1000, Loss: 16.1157, Val Loss: 16.6975\n",
      "Epoch 40/1000, Loss: 16.0847, Val Loss: 16.6637\n",
      "Epoch 41/1000, Loss: 16.0326, Val Loss: 16.6349\n",
      "Epoch 42/1000, Loss: 16.1024, Val Loss: 16.6159\n",
      "Epoch 43/1000, Loss: 15.9934, Val Loss: 16.5960\n",
      "Epoch 44/1000, Loss: 15.9380, Val Loss: 16.5726\n",
      "Epoch 45/1000, Loss: 15.9810, Val Loss: 16.5426\n",
      "Epoch 46/1000, Loss: 15.8950, Val Loss: 16.5037\n",
      "Epoch 47/1000, Loss: 15.8684, Val Loss: 16.4687\n",
      "Epoch 48/1000, Loss: 15.8119, Val Loss: 16.4448\n",
      "Epoch 49/1000, Loss: 15.7671, Val Loss: 16.4297\n",
      "Epoch 50/1000, Loss: 15.7389, Val Loss: 16.4148\n",
      "Epoch 51/1000, Loss: 15.7659, Val Loss: 16.3997\n",
      "Epoch 52/1000, Loss: 15.8220, Val Loss: 16.3864\n",
      "Epoch 53/1000, Loss: 15.7129, Val Loss: 16.3723\n",
      "Epoch 54/1000, Loss: 15.6722, Val Loss: 16.3543\n",
      "Epoch 55/1000, Loss: 15.7556, Val Loss: 16.3389\n",
      "Epoch 56/1000, Loss: 15.7490, Val Loss: 16.3274\n",
      "Epoch 57/1000, Loss: 15.5794, Val Loss: 16.3221\n",
      "Epoch 58/1000, Loss: 15.6722, Val Loss: 16.3162\n",
      "Epoch 59/1000, Loss: 15.7213, Val Loss: 16.3129\n",
      "Epoch 60/1000, Loss: 15.6208, Val Loss: 16.3062\n",
      "Epoch 61/1000, Loss: 15.5077, Val Loss: 16.2955\n",
      "Epoch 62/1000, Loss: 15.4841, Val Loss: 16.2925\n",
      "Epoch 63/1000, Loss: 15.5214, Val Loss: 16.2820\n",
      "Epoch 64/1000, Loss: 15.5714, Val Loss: 16.2600\n",
      "Epoch 65/1000, Loss: 15.5479, Val Loss: 16.2459\n",
      "Epoch 66/1000, Loss: 15.5521, Val Loss: 16.2374\n",
      "Epoch 67/1000, Loss: 15.4836, Val Loss: 16.2297\n",
      "Epoch 68/1000, Loss: 15.2968, Val Loss: 16.2247\n",
      "Epoch 69/1000, Loss: 15.4421, Val Loss: 16.2185\n",
      "Epoch 70/1000, Loss: 15.3562, Val Loss: 16.2108\n",
      "Epoch 71/1000, Loss: 15.3809, Val Loss: 16.2029\n",
      "Epoch 72/1000, Loss: 15.4250, Val Loss: 16.1788\n",
      "Epoch 73/1000, Loss: 15.2909, Val Loss: 16.1587\n",
      "Epoch 74/1000, Loss: 15.2977, Val Loss: 16.1394\n",
      "Epoch 75/1000, Loss: 15.2487, Val Loss: 16.1231\n",
      "Epoch 76/1000, Loss: 15.2348, Val Loss: 16.1128\n",
      "Epoch 77/1000, Loss: 15.3979, Val Loss: 16.1030\n",
      "Epoch 78/1000, Loss: 15.1549, Val Loss: 16.0825\n",
      "Epoch 79/1000, Loss: 15.0578, Val Loss: 16.0630\n",
      "Epoch 80/1000, Loss: 15.0192, Val Loss: 16.0461\n",
      "Epoch 81/1000, Loss: 15.1150, Val Loss: 16.0332\n",
      "Epoch 82/1000, Loss: 15.0528, Val Loss: 16.0232\n",
      "Epoch 83/1000, Loss: 15.1269, Val Loss: 16.0101\n",
      "Epoch 84/1000, Loss: 15.0451, Val Loss: 15.9736\n",
      "Epoch 85/1000, Loss: 15.2101, Val Loss: 15.9244\n",
      "Epoch 86/1000, Loss: 15.0074, Val Loss: 15.8890\n",
      "Epoch 87/1000, Loss: 14.9787, Val Loss: 15.8580\n",
      "Epoch 88/1000, Loss: 15.0356, Val Loss: 15.8306\n",
      "Epoch 89/1000, Loss: 14.9364, Val Loss: 15.8065\n",
      "Epoch 90/1000, Loss: 14.8581, Val Loss: 15.7973\n",
      "Epoch 91/1000, Loss: 14.8644, Val Loss: 15.7844\n",
      "Epoch 92/1000, Loss: 14.7499, Val Loss: 15.7665\n",
      "Epoch 93/1000, Loss: 14.7874, Val Loss: 15.7341\n",
      "Epoch 94/1000, Loss: 14.7769, Val Loss: 15.6859\n",
      "Epoch 95/1000, Loss: 14.7479, Val Loss: 15.6399\n",
      "Epoch 96/1000, Loss: 14.6445, Val Loss: 15.6015\n",
      "Epoch 97/1000, Loss: 14.6076, Val Loss: 15.5663\n",
      "Epoch 98/1000, Loss: 14.6032, Val Loss: 15.5301\n",
      "Epoch 99/1000, Loss: 14.7335, Val Loss: 15.4923\n",
      "Epoch 100/1000, Loss: 14.4747, Val Loss: 15.4750\n",
      "Epoch 101/1000, Loss: 14.4125, Val Loss: 15.4599\n",
      "Epoch 102/1000, Loss: 14.5191, Val Loss: 15.4247\n",
      "Epoch 103/1000, Loss: 14.3884, Val Loss: 15.3748\n",
      "Epoch 104/1000, Loss: 14.4773, Val Loss: 15.3281\n",
      "Epoch 105/1000, Loss: 14.6884, Val Loss: 15.3073\n",
      "Epoch 106/1000, Loss: 14.2129, Val Loss: 15.2979\n",
      "Epoch 107/1000, Loss: 14.4666, Val Loss: 15.2781\n",
      "Epoch 108/1000, Loss: 14.2207, Val Loss: 15.2612\n",
      "Epoch 109/1000, Loss: 14.2408, Val Loss: 15.2335\n",
      "Epoch 110/1000, Loss: 14.2885, Val Loss: 15.2137\n",
      "Epoch 111/1000, Loss: 13.9418, Val Loss: 15.1764\n",
      "Epoch 112/1000, Loss: 14.0346, Val Loss: 15.1246\n",
      "Epoch 113/1000, Loss: 14.0920, Val Loss: 15.0809\n",
      "Epoch 114/1000, Loss: 14.0777, Val Loss: 15.0493\n",
      "Epoch 115/1000, Loss: 13.8356, Val Loss: 15.0182\n",
      "Epoch 116/1000, Loss: 13.9196, Val Loss: 14.9871\n",
      "Epoch 117/1000, Loss: 13.7676, Val Loss: 14.9440\n",
      "Epoch 118/1000, Loss: 13.7884, Val Loss: 14.9005\n",
      "Epoch 119/1000, Loss: 13.7741, Val Loss: 14.8628\n",
      "Epoch 120/1000, Loss: 13.6380, Val Loss: 14.8280\n",
      "Epoch 121/1000, Loss: 13.5797, Val Loss: 14.7970\n",
      "Epoch 122/1000, Loss: 13.6324, Val Loss: 14.7536\n",
      "Epoch 123/1000, Loss: 13.5753, Val Loss: 14.6972\n",
      "Epoch 124/1000, Loss: 13.4286, Val Loss: 14.6508\n",
      "Epoch 125/1000, Loss: 13.4230, Val Loss: 14.6137\n",
      "Epoch 126/1000, Loss: 13.2925, Val Loss: 14.5861\n",
      "Epoch 127/1000, Loss: 13.3288, Val Loss: 14.5702\n",
      "Epoch 128/1000, Loss: 13.3941, Val Loss: 14.5811\n",
      "Epoch 129/1000, Loss: 13.4562, Val Loss: 14.6238\n",
      "Epoch 130/1000, Loss: 13.3639, Val Loss: 14.6559\n",
      "Epoch 131/1000, Loss: 13.3518, Val Loss: 14.6575\n",
      "Epoch 132/1000, Loss: 13.4475, Val Loss: 14.6020\n",
      "Epoch 133/1000, Loss: 13.1320, Val Loss: 14.5585\n",
      "Epoch 134/1000, Loss: 13.4168, Val Loss: 14.5199\n",
      "Epoch 135/1000, Loss: 13.1514, Val Loss: 14.4802\n",
      "Epoch 136/1000, Loss: 13.1357, Val Loss: 14.4504\n",
      "Epoch 137/1000, Loss: 13.2617, Val Loss: 14.4079\n",
      "Epoch 138/1000, Loss: 13.1730, Val Loss: 14.3765\n",
      "Epoch 139/1000, Loss: 13.0572, Val Loss: 14.3453\n",
      "Epoch 140/1000, Loss: 13.1937, Val Loss: 14.3217\n",
      "Epoch 141/1000, Loss: 13.0357, Val Loss: 14.3074\n",
      "Epoch 142/1000, Loss: 13.0234, Val Loss: 14.3246\n",
      "Epoch 143/1000, Loss: 12.9102, Val Loss: 14.3564\n",
      "Epoch 144/1000, Loss: 13.0101, Val Loss: 14.3666\n",
      "Epoch 145/1000, Loss: 12.9755, Val Loss: 14.3676\n",
      "Epoch 146/1000, Loss: 12.7792, Val Loss: 14.3501\n",
      "Epoch 147/1000, Loss: 12.9336, Val Loss: 14.3410\n",
      "Epoch 148/1000, Loss: 12.6498, Val Loss: 14.3242\n",
      "Epoch 149/1000, Loss: 12.8880, Val Loss: 14.3124\n",
      "Epoch 150/1000, Loss: 12.5410, Val Loss: 14.3342\n",
      "Epoch 151/1000, Loss: 12.8132, Val Loss: 14.3427\n",
      "Epoch 152/1000, Loss: 12.7003, Val Loss: 14.3137\n",
      "Epoch 153/1000, Loss: 12.7269, Val Loss: 14.2855\n",
      "Epoch 154/1000, Loss: 12.5469, Val Loss: 14.2416\n",
      "Epoch 155/1000, Loss: 12.8436, Val Loss: 14.2087\n",
      "Epoch 156/1000, Loss: 12.5951, Val Loss: 14.2424\n",
      "Epoch 157/1000, Loss: 12.7050, Val Loss: 14.3086\n",
      "Epoch 158/1000, Loss: 12.7317, Val Loss: 14.3641\n",
      "Epoch 159/1000, Loss: 12.6212, Val Loss: 14.3701\n",
      "Epoch 160/1000, Loss: 12.5400, Val Loss: 14.3309\n",
      "Epoch 161/1000, Loss: 12.5595, Val Loss: 14.2909\n",
      "Epoch 162/1000, Loss: 12.7506, Val Loss: 14.2821\n",
      "Epoch 163/1000, Loss: 12.6342, Val Loss: 14.2933\n",
      "Epoch 164/1000, Loss: 12.3440, Val Loss: 14.2979\n",
      "Epoch 165/1000, Loss: 12.2807, Val Loss: 14.2913\n",
      "Epoch 166/1000, Loss: 12.4802, Val Loss: 14.2634\n",
      "Epoch 167/1000, Loss: 12.3031, Val Loss: 14.2420\n",
      "Epoch 168/1000, Loss: 12.9086, Val Loss: 14.2089\n",
      "Epoch 169/1000, Loss: 12.3489, Val Loss: 14.1964\n",
      "Epoch 170/1000, Loss: 12.4495, Val Loss: 14.2046\n",
      "Epoch 171/1000, Loss: 12.4677, Val Loss: 14.2713\n",
      "Epoch 172/1000, Loss: 12.3558, Val Loss: 14.3302\n",
      "Epoch 173/1000, Loss: 12.3612, Val Loss: 14.3084\n",
      "Epoch 174/1000, Loss: 12.4558, Val Loss: 14.2702\n",
      "Epoch 175/1000, Loss: 12.3584, Val Loss: 14.2522\n",
      "Epoch 176/1000, Loss: 12.4048, Val Loss: 14.2215\n",
      "Epoch 177/1000, Loss: 12.4327, Val Loss: 14.2042\n",
      "Epoch 178/1000, Loss: 12.1829, Val Loss: 14.2154\n",
      "Epoch 179/1000, Loss: 12.2104, Val Loss: 14.2010\n",
      "Epoch 180/1000, Loss: 12.0459, Val Loss: 14.1695\n",
      "Epoch 181/1000, Loss: 12.2477, Val Loss: 14.1708\n",
      "Epoch 182/1000, Loss: 12.1705, Val Loss: 14.1850\n",
      "Epoch 183/1000, Loss: 12.2118, Val Loss: 14.2150\n",
      "Epoch 184/1000, Loss: 12.2257, Val Loss: 14.2622\n",
      "Epoch 185/1000, Loss: 12.1694, Val Loss: 14.2771\n",
      "Epoch 186/1000, Loss: 12.3077, Val Loss: 14.2950\n",
      "Epoch 187/1000, Loss: 12.2292, Val Loss: 14.3427\n",
      "Epoch 188/1000, Loss: 12.3882, Val Loss: 14.4104\n",
      "Epoch 189/1000, Loss: 12.1329, Val Loss: 14.4644\n",
      "Epoch 190/1000, Loss: 12.0513, Val Loss: 14.4892\n",
      "Epoch 191/1000, Loss: 11.9217, Val Loss: 14.4712\n",
      "Epoch 192/1000, Loss: 12.3098, Val Loss: 14.4037\n",
      "Epoch 193/1000, Loss: 12.3491, Val Loss: 14.3203\n",
      "Epoch 194/1000, Loss: 12.0693, Val Loss: 14.3633\n",
      "Epoch 195/1000, Loss: 12.1290, Val Loss: 14.3746\n",
      "Epoch 196/1000, Loss: 11.9855, Val Loss: 14.3839\n",
      "Epoch 197/1000, Loss: 11.9723, Val Loss: 14.4056\n",
      "Epoch 198/1000, Loss: 12.0028, Val Loss: 14.4390\n",
      "Epoch 199/1000, Loss: 11.8316, Val Loss: 14.4172\n",
      "Epoch 200/1000, Loss: 11.8628, Val Loss: 14.3431\n",
      "Epoch 201/1000, Loss: 11.9021, Val Loss: 14.2845\n",
      "Epoch 202/1000, Loss: 11.9133, Val Loss: 14.2774\n",
      "Epoch 203/1000, Loss: 11.7549, Val Loss: 14.2538\n",
      "Epoch 204/1000, Loss: 11.9824, Val Loss: 14.2650\n",
      "Epoch 205/1000, Loss: 11.8513, Val Loss: 14.3011\n",
      "Epoch 206/1000, Loss: 12.2064, Val Loss: 14.3230\n",
      "Epoch 207/1000, Loss: 11.7320, Val Loss: 14.2866\n",
      "Epoch 208/1000, Loss: 11.7092, Val Loss: 14.2454\n",
      "Epoch 209/1000, Loss: 11.6303, Val Loss: 14.2250\n",
      "Epoch 210/1000, Loss: 12.0007, Val Loss: 14.1829\n",
      "Epoch 211/1000, Loss: 11.9438, Val Loss: 14.1650\n",
      "Epoch 212/1000, Loss: 11.6283, Val Loss: 14.1837\n",
      "Epoch 213/1000, Loss: 11.8808, Val Loss: 14.2512\n",
      "Epoch 214/1000, Loss: 11.5073, Val Loss: 14.2978\n",
      "Epoch 215/1000, Loss: 11.4346, Val Loss: 14.3247\n",
      "Epoch 216/1000, Loss: 11.3827, Val Loss: 14.3320\n",
      "Epoch 217/1000, Loss: 11.6710, Val Loss: 14.3356\n",
      "Epoch 218/1000, Loss: 11.8131, Val Loss: 14.3394\n",
      "Epoch 219/1000, Loss: 11.9270, Val Loss: 14.3514\n",
      "Epoch 220/1000, Loss: 11.8108, Val Loss: 14.3843\n",
      "Epoch 221/1000, Loss: 11.6641, Val Loss: 14.3912\n",
      "Epoch 222/1000, Loss: 11.5371, Val Loss: 14.4003\n",
      "Epoch 223/1000, Loss: 11.6152, Val Loss: 14.4025\n",
      "Epoch 224/1000, Loss: 11.5204, Val Loss: 14.3969\n",
      "Epoch 225/1000, Loss: 11.4663, Val Loss: 14.3850\n",
      "Epoch 226/1000, Loss: 11.4043, Val Loss: 14.3710\n",
      "Epoch 227/1000, Loss: 11.5101, Val Loss: 14.3537\n",
      "Epoch 228/1000, Loss: 11.5340, Val Loss: 14.3431\n",
      "Epoch 229/1000, Loss: 11.5439, Val Loss: 14.3367\n",
      "Epoch 230/1000, Loss: 11.6253, Val Loss: 14.3395\n",
      "Epoch 231/1000, Loss: 11.4424, Val Loss: 14.3423\n",
      "Epoch 232/1000, Loss: 11.3855, Val Loss: 14.3548\n",
      "Epoch 233/1000, Loss: 11.5656, Val Loss: 14.3576\n",
      "Epoch 234/1000, Loss: 11.4255, Val Loss: 14.3421\n",
      "Epoch 235/1000, Loss: 11.4629, Val Loss: 14.3086\n",
      "Epoch 236/1000, Loss: 11.4376, Val Loss: 14.2919\n",
      "Epoch 237/1000, Loss: 11.3684, Val Loss: 14.2752\n",
      "Epoch 238/1000, Loss: 11.5110, Val Loss: 14.2518\n",
      "Epoch 239/1000, Loss: 11.5192, Val Loss: 14.2350\n",
      "Epoch 240/1000, Loss: 11.1992, Val Loss: 14.2271\n",
      "Epoch 241/1000, Loss: 11.4903, Val Loss: 14.2524\n",
      "Epoch 242/1000, Loss: 11.4680, Val Loss: 14.2288\n",
      "Epoch 243/1000, Loss: 11.2191, Val Loss: 14.1793\n",
      "Epoch 244/1000, Loss: 11.2902, Val Loss: 14.1472\n",
      "Epoch 245/1000, Loss: 11.3542, Val Loss: 14.1401\n",
      "Epoch 246/1000, Loss: 11.3524, Val Loss: 14.1738\n",
      "Epoch 247/1000, Loss: 11.5564, Val Loss: 14.2242\n",
      "Epoch 248/1000, Loss: 11.1011, Val Loss: 14.2372\n",
      "Epoch 249/1000, Loss: 11.2848, Val Loss: 14.2586\n",
      "Epoch 250/1000, Loss: 11.0865, Val Loss: 14.2680\n",
      "Epoch 251/1000, Loss: 11.1168, Val Loss: 14.2828\n",
      "Epoch 252/1000, Loss: 11.4011, Val Loss: 14.2051\n",
      "Epoch 253/1000, Loss: 11.3632, Val Loss: 14.1580\n",
      "Epoch 254/1000, Loss: 11.3456, Val Loss: 14.1837\n",
      "Epoch 255/1000, Loss: 10.9819, Val Loss: 14.2257\n",
      "Epoch 256/1000, Loss: 11.4641, Val Loss: 14.1823\n",
      "Epoch 257/1000, Loss: 11.2240, Val Loss: 14.1192\n",
      "Epoch 258/1000, Loss: 11.1001, Val Loss: 14.1080\n",
      "Epoch 259/1000, Loss: 11.2809, Val Loss: 14.1291\n",
      "Epoch 260/1000, Loss: 11.2022, Val Loss: 14.1810\n",
      "Epoch 261/1000, Loss: 11.3926, Val Loss: 14.2501\n",
      "Epoch 262/1000, Loss: 11.1163, Val Loss: 14.2851\n",
      "Epoch 263/1000, Loss: 11.0622, Val Loss: 14.3081\n",
      "Epoch 264/1000, Loss: 11.0567, Val Loss: 14.3300\n",
      "Epoch 265/1000, Loss: 11.1094, Val Loss: 14.3681\n",
      "Epoch 266/1000, Loss: 10.9961, Val Loss: 14.4403\n",
      "Epoch 267/1000, Loss: 11.1642, Val Loss: 14.5141\n",
      "Epoch 268/1000, Loss: 10.9928, Val Loss: 14.5383\n",
      "Epoch 269/1000, Loss: 10.9749, Val Loss: 14.5150\n",
      "Epoch 270/1000, Loss: 11.0453, Val Loss: 14.4614\n",
      "Epoch 271/1000, Loss: 11.3454, Val Loss: 14.4010\n",
      "Epoch 272/1000, Loss: 11.0245, Val Loss: 14.3745\n",
      "Epoch 273/1000, Loss: 11.2128, Val Loss: 14.3853\n",
      "Epoch 274/1000, Loss: 10.9753, Val Loss: 14.4598\n",
      "Epoch 275/1000, Loss: 10.8928, Val Loss: 14.5065\n",
      "Epoch 276/1000, Loss: 11.1119, Val Loss: 14.4777\n",
      "Epoch 277/1000, Loss: 11.0739, Val Loss: 14.4611\n",
      "Epoch 278/1000, Loss: 10.9638, Val Loss: 14.4851\n",
      "Epoch 279/1000, Loss: 10.9469, Val Loss: 14.5232\n",
      "Epoch 280/1000, Loss: 11.0393, Val Loss: 14.6069\n",
      "Epoch 281/1000, Loss: 10.9161, Val Loss: 14.6584\n",
      "Epoch 282/1000, Loss: 10.8054, Val Loss: 14.7222\n",
      "Epoch 283/1000, Loss: 11.0531, Val Loss: 14.6963\n",
      "Epoch 284/1000, Loss: 10.8155, Val Loss: 14.6264\n",
      "Epoch 285/1000, Loss: 11.0967, Val Loss: 14.5567\n",
      "Epoch 286/1000, Loss: 10.9737, Val Loss: 14.5268\n",
      "Epoch 287/1000, Loss: 11.0783, Val Loss: 14.5516\n",
      "Epoch 288/1000, Loss: 11.0469, Val Loss: 14.6226\n",
      "Epoch 289/1000, Loss: 10.9664, Val Loss: 14.6225\n",
      "Epoch 290/1000, Loss: 10.8368, Val Loss: 14.5229\n",
      "Epoch 291/1000, Loss: 11.0814, Val Loss: 14.4517\n",
      "Epoch 292/1000, Loss: 10.8598, Val Loss: 14.4143\n",
      "Epoch 293/1000, Loss: 10.8659, Val Loss: 14.4537\n",
      "Epoch 294/1000, Loss: 11.1118, Val Loss: 14.5259\n",
      "Epoch 295/1000, Loss: 10.9764, Val Loss: 14.6210\n",
      "Epoch 296/1000, Loss: 10.8871, Val Loss: 14.7208\n",
      "Epoch 297/1000, Loss: 10.7605, Val Loss: 14.7201\n",
      "Epoch 298/1000, Loss: 10.8679, Val Loss: 14.6677\n",
      "Early stopping triggered at epoch 298. Best Val Loss: 14.1080\n",
      "Epoch 1/1000, Loss: 17.0484, Val Loss: 17.3921\n",
      "Epoch 2/1000, Loss: 17.0322, Val Loss: 17.3881\n",
      "Epoch 3/1000, Loss: 17.0196, Val Loss: 17.3840\n",
      "Epoch 4/1000, Loss: 17.0119, Val Loss: 17.3798\n",
      "Epoch 5/1000, Loss: 17.0009, Val Loss: 17.3753\n",
      "Epoch 6/1000, Loss: 16.9858, Val Loss: 17.3702\n",
      "Epoch 7/1000, Loss: 16.9826, Val Loss: 17.3640\n",
      "Epoch 8/1000, Loss: 16.9647, Val Loss: 17.3571\n",
      "Epoch 9/1000, Loss: 16.9513, Val Loss: 17.3498\n",
      "Epoch 10/1000, Loss: 16.9395, Val Loss: 17.3426\n",
      "Epoch 11/1000, Loss: 16.9140, Val Loss: 17.3349\n",
      "Epoch 12/1000, Loss: 16.8953, Val Loss: 17.3255\n",
      "Epoch 13/1000, Loss: 16.8824, Val Loss: 17.3157\n",
      "Epoch 14/1000, Loss: 16.8624, Val Loss: 17.3051\n",
      "Epoch 15/1000, Loss: 16.8547, Val Loss: 17.2941\n",
      "Epoch 16/1000, Loss: 16.8288, Val Loss: 17.2822\n",
      "Epoch 17/1000, Loss: 16.8340, Val Loss: 17.2710\n",
      "Epoch 18/1000, Loss: 16.7883, Val Loss: 17.2588\n",
      "Epoch 19/1000, Loss: 16.7772, Val Loss: 17.2461\n",
      "Epoch 20/1000, Loss: 16.7643, Val Loss: 17.2372\n",
      "Epoch 21/1000, Loss: 16.7321, Val Loss: 17.2318\n",
      "Epoch 22/1000, Loss: 16.7034, Val Loss: 17.2254\n",
      "Epoch 23/1000, Loss: 16.6670, Val Loss: 17.2168\n",
      "Epoch 24/1000, Loss: 16.6402, Val Loss: 17.2048\n",
      "Epoch 25/1000, Loss: 16.6025, Val Loss: 17.1880\n",
      "Epoch 26/1000, Loss: 16.5506, Val Loss: 17.1673\n",
      "Epoch 27/1000, Loss: 16.5531, Val Loss: 17.1425\n",
      "Epoch 28/1000, Loss: 16.4934, Val Loss: 17.1134\n",
      "Epoch 29/1000, Loss: 16.4769, Val Loss: 17.0811\n",
      "Epoch 30/1000, Loss: 16.4696, Val Loss: 17.0459\n",
      "Epoch 31/1000, Loss: 16.4474, Val Loss: 17.0113\n",
      "Epoch 32/1000, Loss: 16.3917, Val Loss: 16.9814\n",
      "Epoch 33/1000, Loss: 16.3740, Val Loss: 16.9544\n",
      "Epoch 34/1000, Loss: 16.3927, Val Loss: 16.9320\n",
      "Epoch 35/1000, Loss: 16.3128, Val Loss: 16.9073\n",
      "Epoch 36/1000, Loss: 16.3144, Val Loss: 16.8818\n",
      "Epoch 37/1000, Loss: 16.2876, Val Loss: 16.8537\n",
      "Epoch 38/1000, Loss: 16.2324, Val Loss: 16.8245\n",
      "Epoch 39/1000, Loss: 16.2311, Val Loss: 16.7955\n",
      "Epoch 40/1000, Loss: 16.1670, Val Loss: 16.7674\n",
      "Epoch 41/1000, Loss: 16.1517, Val Loss: 16.7382\n",
      "Epoch 42/1000, Loss: 16.1239, Val Loss: 16.7148\n",
      "Epoch 43/1000, Loss: 16.1093, Val Loss: 16.6947\n",
      "Epoch 44/1000, Loss: 16.0915, Val Loss: 16.6742\n",
      "Epoch 45/1000, Loss: 16.0289, Val Loss: 16.6497\n",
      "Epoch 46/1000, Loss: 16.0409, Val Loss: 16.6306\n",
      "Epoch 47/1000, Loss: 15.9759, Val Loss: 16.6088\n",
      "Epoch 48/1000, Loss: 15.9819, Val Loss: 16.5863\n",
      "Epoch 49/1000, Loss: 15.9024, Val Loss: 16.5624\n",
      "Epoch 50/1000, Loss: 15.8980, Val Loss: 16.5384\n",
      "Epoch 51/1000, Loss: 15.8550, Val Loss: 16.5147\n",
      "Epoch 52/1000, Loss: 15.8378, Val Loss: 16.4948\n",
      "Epoch 53/1000, Loss: 15.8410, Val Loss: 16.4765\n",
      "Epoch 54/1000, Loss: 15.7707, Val Loss: 16.4576\n",
      "Epoch 55/1000, Loss: 15.7825, Val Loss: 16.4403\n",
      "Epoch 56/1000, Loss: 15.7426, Val Loss: 16.4236\n",
      "Epoch 57/1000, Loss: 15.7059, Val Loss: 16.4027\n",
      "Epoch 58/1000, Loss: 15.7054, Val Loss: 16.3845\n",
      "Epoch 59/1000, Loss: 15.8264, Val Loss: 16.3731\n",
      "Epoch 60/1000, Loss: 15.6238, Val Loss: 16.3605\n",
      "Epoch 61/1000, Loss: 15.7103, Val Loss: 16.3425\n",
      "Epoch 62/1000, Loss: 15.6129, Val Loss: 16.3230\n",
      "Epoch 63/1000, Loss: 15.5709, Val Loss: 16.3097\n",
      "Epoch 64/1000, Loss: 15.5843, Val Loss: 16.3016\n",
      "Epoch 65/1000, Loss: 15.5079, Val Loss: 16.2922\n",
      "Epoch 66/1000, Loss: 15.4937, Val Loss: 16.2808\n",
      "Epoch 67/1000, Loss: 15.5450, Val Loss: 16.2620\n",
      "Epoch 68/1000, Loss: 15.5244, Val Loss: 16.2428\n",
      "Epoch 69/1000, Loss: 15.4531, Val Loss: 16.2243\n",
      "Epoch 70/1000, Loss: 15.4317, Val Loss: 16.2051\n",
      "Epoch 71/1000, Loss: 15.4261, Val Loss: 16.1883\n",
      "Epoch 72/1000, Loss: 15.4537, Val Loss: 16.1711\n",
      "Epoch 73/1000, Loss: 15.3400, Val Loss: 16.1536\n",
      "Epoch 74/1000, Loss: 15.3697, Val Loss: 16.1339\n",
      "Epoch 75/1000, Loss: 15.3966, Val Loss: 16.1102\n",
      "Epoch 76/1000, Loss: 15.3218, Val Loss: 16.0809\n",
      "Epoch 77/1000, Loss: 15.2461, Val Loss: 16.0481\n",
      "Epoch 78/1000, Loss: 15.0886, Val Loss: 16.0201\n",
      "Epoch 79/1000, Loss: 15.2436, Val Loss: 15.9892\n",
      "Epoch 80/1000, Loss: 15.2106, Val Loss: 15.9555\n",
      "Epoch 81/1000, Loss: 15.0729, Val Loss: 15.9193\n",
      "Epoch 82/1000, Loss: 15.1879, Val Loss: 15.8825\n",
      "Epoch 83/1000, Loss: 15.0919, Val Loss: 15.8507\n",
      "Epoch 84/1000, Loss: 15.0344, Val Loss: 15.8248\n",
      "Epoch 85/1000, Loss: 15.0304, Val Loss: 15.8052\n",
      "Epoch 86/1000, Loss: 14.9219, Val Loss: 15.7840\n",
      "Epoch 87/1000, Loss: 14.9625, Val Loss: 15.7409\n",
      "Epoch 88/1000, Loss: 15.0872, Val Loss: 15.7004\n",
      "Epoch 89/1000, Loss: 14.8623, Val Loss: 15.6705\n",
      "Epoch 90/1000, Loss: 14.8237, Val Loss: 15.6446\n",
      "Epoch 91/1000, Loss: 14.7483, Val Loss: 15.6130\n",
      "Epoch 92/1000, Loss: 14.7261, Val Loss: 15.5917\n",
      "Epoch 93/1000, Loss: 14.6855, Val Loss: 15.5616\n",
      "Epoch 94/1000, Loss: 14.6460, Val Loss: 15.5197\n",
      "Epoch 95/1000, Loss: 14.5934, Val Loss: 15.4819\n",
      "Epoch 96/1000, Loss: 14.7249, Val Loss: 15.4359\n",
      "Epoch 97/1000, Loss: 14.7146, Val Loss: 15.3806\n",
      "Epoch 98/1000, Loss: 14.4559, Val Loss: 15.3293\n",
      "Epoch 99/1000, Loss: 14.6210, Val Loss: 15.2791\n",
      "Epoch 100/1000, Loss: 14.4878, Val Loss: 15.2405\n",
      "Epoch 101/1000, Loss: 14.4511, Val Loss: 15.1956\n",
      "Epoch 102/1000, Loss: 14.2886, Val Loss: 15.1645\n",
      "Epoch 103/1000, Loss: 14.3593, Val Loss: 15.1430\n",
      "Epoch 104/1000, Loss: 14.3240, Val Loss: 15.1153\n",
      "Epoch 105/1000, Loss: 14.0117, Val Loss: 15.0663\n",
      "Epoch 106/1000, Loss: 14.0222, Val Loss: 15.0290\n",
      "Epoch 107/1000, Loss: 14.0751, Val Loss: 14.9894\n",
      "Epoch 108/1000, Loss: 14.0261, Val Loss: 14.9495\n",
      "Epoch 109/1000, Loss: 14.0278, Val Loss: 14.9156\n",
      "Epoch 110/1000, Loss: 13.7930, Val Loss: 14.9007\n",
      "Epoch 111/1000, Loss: 13.9695, Val Loss: 14.9031\n",
      "Epoch 112/1000, Loss: 13.9383, Val Loss: 14.9164\n",
      "Epoch 113/1000, Loss: 13.9592, Val Loss: 14.8766\n",
      "Epoch 114/1000, Loss: 13.7824, Val Loss: 14.8214\n",
      "Epoch 115/1000, Loss: 13.7667, Val Loss: 14.7890\n",
      "Epoch 116/1000, Loss: 13.4669, Val Loss: 14.7421\n",
      "Epoch 117/1000, Loss: 13.7299, Val Loss: 14.6955\n",
      "Epoch 118/1000, Loss: 13.3297, Val Loss: 14.6474\n",
      "Epoch 119/1000, Loss: 13.4335, Val Loss: 14.6287\n",
      "Epoch 120/1000, Loss: 13.4664, Val Loss: 14.5878\n",
      "Epoch 121/1000, Loss: 13.5299, Val Loss: 14.5531\n",
      "Epoch 122/1000, Loss: 13.3914, Val Loss: 14.5212\n",
      "Epoch 123/1000, Loss: 13.3238, Val Loss: 14.5061\n",
      "Epoch 124/1000, Loss: 13.4090, Val Loss: 14.5091\n",
      "Epoch 125/1000, Loss: 13.2877, Val Loss: 14.5135\n",
      "Epoch 126/1000, Loss: 13.2042, Val Loss: 14.5294\n",
      "Epoch 127/1000, Loss: 13.1690, Val Loss: 14.5444\n",
      "Epoch 128/1000, Loss: 13.3057, Val Loss: 14.5660\n",
      "Epoch 129/1000, Loss: 13.2782, Val Loss: 14.6006\n",
      "Epoch 130/1000, Loss: 13.0751, Val Loss: 14.5968\n",
      "Epoch 131/1000, Loss: 13.1732, Val Loss: 14.5405\n",
      "Epoch 132/1000, Loss: 13.1609, Val Loss: 14.4910\n",
      "Epoch 133/1000, Loss: 13.2074, Val Loss: 14.4605\n",
      "Epoch 134/1000, Loss: 13.0099, Val Loss: 14.4375\n",
      "Epoch 135/1000, Loss: 12.8692, Val Loss: 14.4187\n",
      "Epoch 136/1000, Loss: 13.2444, Val Loss: 14.3780\n",
      "Epoch 137/1000, Loss: 13.0181, Val Loss: 14.3801\n",
      "Epoch 138/1000, Loss: 13.0364, Val Loss: 14.3683\n",
      "Epoch 139/1000, Loss: 13.1546, Val Loss: 14.3001\n",
      "Epoch 140/1000, Loss: 13.0644, Val Loss: 14.2575\n",
      "Epoch 141/1000, Loss: 13.1110, Val Loss: 14.2657\n",
      "Epoch 142/1000, Loss: 13.0478, Val Loss: 14.2973\n",
      "Epoch 143/1000, Loss: 12.9830, Val Loss: 14.3135\n",
      "Epoch 144/1000, Loss: 12.7387, Val Loss: 14.3131\n",
      "Epoch 145/1000, Loss: 12.7727, Val Loss: 14.3180\n",
      "Epoch 146/1000, Loss: 12.9682, Val Loss: 14.2768\n",
      "Epoch 147/1000, Loss: 12.7115, Val Loss: 14.2428\n",
      "Epoch 148/1000, Loss: 12.7239, Val Loss: 14.2505\n",
      "Epoch 149/1000, Loss: 12.6723, Val Loss: 14.2600\n",
      "Epoch 150/1000, Loss: 12.7582, Val Loss: 14.2369\n",
      "Epoch 151/1000, Loss: 12.5306, Val Loss: 14.2146\n",
      "Epoch 152/1000, Loss: 12.4214, Val Loss: 14.2064\n",
      "Epoch 153/1000, Loss: 12.7194, Val Loss: 14.2097\n",
      "Epoch 154/1000, Loss: 12.4614, Val Loss: 14.2194\n",
      "Epoch 155/1000, Loss: 12.7300, Val Loss: 14.2125\n",
      "Epoch 156/1000, Loss: 12.6489, Val Loss: 14.2139\n",
      "Epoch 157/1000, Loss: 12.6054, Val Loss: 14.1841\n",
      "Epoch 158/1000, Loss: 12.2698, Val Loss: 14.1612\n",
      "Epoch 159/1000, Loss: 12.5287, Val Loss: 14.1158\n",
      "Epoch 160/1000, Loss: 12.4301, Val Loss: 14.0699\n",
      "Epoch 161/1000, Loss: 12.2684, Val Loss: 14.0459\n",
      "Epoch 162/1000, Loss: 12.3888, Val Loss: 14.0368\n",
      "Epoch 163/1000, Loss: 12.4046, Val Loss: 14.0435\n",
      "Epoch 164/1000, Loss: 12.5086, Val Loss: 14.0360\n",
      "Epoch 165/1000, Loss: 12.3887, Val Loss: 14.0738\n",
      "Epoch 166/1000, Loss: 12.3541, Val Loss: 14.1016\n",
      "Epoch 167/1000, Loss: 12.5380, Val Loss: 14.0939\n",
      "Epoch 168/1000, Loss: 12.2176, Val Loss: 14.0754\n",
      "Epoch 169/1000, Loss: 12.2559, Val Loss: 14.0436\n",
      "Epoch 170/1000, Loss: 12.1076, Val Loss: 14.0129\n",
      "Epoch 171/1000, Loss: 12.3394, Val Loss: 13.9810\n",
      "Epoch 172/1000, Loss: 12.2201, Val Loss: 13.9822\n",
      "Epoch 173/1000, Loss: 12.4433, Val Loss: 13.9896\n",
      "Epoch 174/1000, Loss: 12.1775, Val Loss: 13.9768\n",
      "Epoch 175/1000, Loss: 12.2625, Val Loss: 13.9737\n",
      "Epoch 176/1000, Loss: 12.2918, Val Loss: 13.9978\n",
      "Epoch 177/1000, Loss: 12.1047, Val Loss: 14.0038\n",
      "Epoch 178/1000, Loss: 12.1007, Val Loss: 14.0090\n",
      "Epoch 179/1000, Loss: 12.1053, Val Loss: 14.0203\n",
      "Epoch 180/1000, Loss: 12.1872, Val Loss: 14.0500\n",
      "Epoch 181/1000, Loss: 12.1447, Val Loss: 14.0864\n",
      "Epoch 182/1000, Loss: 12.2920, Val Loss: 14.0996\n",
      "Epoch 183/1000, Loss: 12.0971, Val Loss: 14.1055\n",
      "Epoch 184/1000, Loss: 12.0343, Val Loss: 14.1670\n",
      "Epoch 185/1000, Loss: 11.9570, Val Loss: 14.2480\n",
      "Epoch 186/1000, Loss: 12.0481, Val Loss: 14.2578\n",
      "Epoch 187/1000, Loss: 12.0191, Val Loss: 14.2230\n",
      "Epoch 188/1000, Loss: 11.8972, Val Loss: 14.1896\n",
      "Epoch 189/1000, Loss: 11.9826, Val Loss: 14.1477\n",
      "Epoch 190/1000, Loss: 12.1873, Val Loss: 14.0926\n",
      "Epoch 191/1000, Loss: 11.8034, Val Loss: 14.0680\n",
      "Epoch 192/1000, Loss: 11.8722, Val Loss: 14.0903\n",
      "Epoch 193/1000, Loss: 11.8500, Val Loss: 14.1275\n",
      "Epoch 194/1000, Loss: 11.9253, Val Loss: 14.1653\n",
      "Epoch 195/1000, Loss: 11.6964, Val Loss: 14.2269\n",
      "Epoch 196/1000, Loss: 11.9208, Val Loss: 14.2608\n",
      "Epoch 197/1000, Loss: 11.8565, Val Loss: 14.2501\n",
      "Epoch 198/1000, Loss: 12.0948, Val Loss: 14.2358\n",
      "Epoch 199/1000, Loss: 11.8283, Val Loss: 14.2356\n",
      "Epoch 200/1000, Loss: 11.8000, Val Loss: 14.2399\n",
      "Epoch 201/1000, Loss: 11.8635, Val Loss: 14.2234\n",
      "Epoch 202/1000, Loss: 11.5876, Val Loss: 14.2351\n",
      "Epoch 203/1000, Loss: 11.7560, Val Loss: 14.2661\n",
      "Epoch 204/1000, Loss: 11.7003, Val Loss: 14.3039\n",
      "Epoch 205/1000, Loss: 11.5302, Val Loss: 14.2798\n",
      "Epoch 206/1000, Loss: 11.6465, Val Loss: 14.2545\n",
      "Epoch 207/1000, Loss: 11.5439, Val Loss: 14.2593\n",
      "Epoch 208/1000, Loss: 11.8248, Val Loss: 14.2944\n",
      "Epoch 209/1000, Loss: 11.5442, Val Loss: 14.3197\n",
      "Epoch 210/1000, Loss: 11.7021, Val Loss: 14.3174\n",
      "Epoch 211/1000, Loss: 11.6096, Val Loss: 14.2902\n",
      "Epoch 212/1000, Loss: 11.4978, Val Loss: 14.2703\n",
      "Epoch 213/1000, Loss: 11.6950, Val Loss: 14.2772\n",
      "Epoch 214/1000, Loss: 11.8104, Val Loss: 14.2910\n",
      "Epoch 215/1000, Loss: 11.6906, Val Loss: 14.3297\n",
      "Early stopping triggered at epoch 215. Best Val Loss: 13.9737\n",
      "Epoch 1/1000, Loss: 16.6716, Val Loss: 16.7952\n",
      "Epoch 2/1000, Loss: 16.6674, Val Loss: 16.7951\n",
      "Epoch 3/1000, Loss: 16.6561, Val Loss: 16.7951\n",
      "Epoch 4/1000, Loss: 16.6542, Val Loss: 16.7952\n",
      "Epoch 5/1000, Loss: 16.6356, Val Loss: 16.7946\n",
      "Epoch 6/1000, Loss: 16.6348, Val Loss: 16.7935\n",
      "Epoch 7/1000, Loss: 16.6252, Val Loss: 16.7912\n",
      "Epoch 8/1000, Loss: 16.6208, Val Loss: 16.7883\n",
      "Epoch 9/1000, Loss: 16.5992, Val Loss: 16.7852\n",
      "Epoch 10/1000, Loss: 16.5976, Val Loss: 16.7820\n",
      "Epoch 11/1000, Loss: 16.5846, Val Loss: 16.7789\n",
      "Epoch 12/1000, Loss: 16.5716, Val Loss: 16.7751\n",
      "Epoch 13/1000, Loss: 16.5473, Val Loss: 16.7703\n",
      "Epoch 14/1000, Loss: 16.5426, Val Loss: 16.7647\n",
      "Epoch 15/1000, Loss: 16.5210, Val Loss: 16.7589\n",
      "Epoch 16/1000, Loss: 16.5054, Val Loss: 16.7524\n",
      "Epoch 17/1000, Loss: 16.4776, Val Loss: 16.7461\n",
      "Epoch 18/1000, Loss: 16.4685, Val Loss: 16.7396\n",
      "Epoch 19/1000, Loss: 16.4466, Val Loss: 16.7331\n",
      "Epoch 20/1000, Loss: 16.4169, Val Loss: 16.7231\n",
      "Epoch 21/1000, Loss: 16.3925, Val Loss: 16.7112\n",
      "Epoch 22/1000, Loss: 16.3702, Val Loss: 16.6969\n",
      "Epoch 23/1000, Loss: 16.3315, Val Loss: 16.6793\n",
      "Epoch 24/1000, Loss: 16.3068, Val Loss: 16.6601\n",
      "Epoch 25/1000, Loss: 16.2974, Val Loss: 16.6371\n",
      "Epoch 26/1000, Loss: 16.2513, Val Loss: 16.6116\n",
      "Epoch 27/1000, Loss: 16.2300, Val Loss: 16.5843\n",
      "Epoch 28/1000, Loss: 16.2074, Val Loss: 16.5560\n",
      "Epoch 29/1000, Loss: 16.1835, Val Loss: 16.5277\n",
      "Epoch 30/1000, Loss: 16.1378, Val Loss: 16.4971\n",
      "Epoch 31/1000, Loss: 16.1114, Val Loss: 16.4640\n",
      "Epoch 32/1000, Loss: 16.0866, Val Loss: 16.4315\n",
      "Epoch 33/1000, Loss: 16.0697, Val Loss: 16.4095\n",
      "Epoch 34/1000, Loss: 16.0374, Val Loss: 16.3839\n",
      "Epoch 35/1000, Loss: 16.0209, Val Loss: 16.3628\n",
      "Epoch 36/1000, Loss: 15.9701, Val Loss: 16.3324\n",
      "Epoch 37/1000, Loss: 15.9437, Val Loss: 16.2960\n",
      "Epoch 38/1000, Loss: 15.9039, Val Loss: 16.2515\n",
      "Epoch 39/1000, Loss: 15.8829, Val Loss: 16.2124\n",
      "Epoch 40/1000, Loss: 15.8322, Val Loss: 16.1827\n",
      "Epoch 41/1000, Loss: 15.7920, Val Loss: 16.1669\n",
      "Epoch 42/1000, Loss: 15.7995, Val Loss: 16.1568\n",
      "Epoch 43/1000, Loss: 15.7343, Val Loss: 16.1477\n",
      "Epoch 44/1000, Loss: 15.6721, Val Loss: 16.1321\n",
      "Epoch 45/1000, Loss: 15.6749, Val Loss: 16.1012\n",
      "Epoch 46/1000, Loss: 15.6000, Val Loss: 16.0551\n",
      "Epoch 47/1000, Loss: 15.5804, Val Loss: 16.0068\n",
      "Epoch 48/1000, Loss: 15.5577, Val Loss: 15.9642\n",
      "Epoch 49/1000, Loss: 15.4899, Val Loss: 15.9240\n",
      "Epoch 50/1000, Loss: 15.5052, Val Loss: 15.8953\n",
      "Epoch 51/1000, Loss: 15.4030, Val Loss: 15.8672\n",
      "Epoch 52/1000, Loss: 15.3989, Val Loss: 15.8437\n",
      "Epoch 53/1000, Loss: 15.3353, Val Loss: 15.8095\n",
      "Epoch 54/1000, Loss: 15.2736, Val Loss: 15.7657\n",
      "Epoch 55/1000, Loss: 15.2565, Val Loss: 15.7220\n",
      "Epoch 56/1000, Loss: 15.2446, Val Loss: 15.6864\n",
      "Epoch 57/1000, Loss: 15.1745, Val Loss: 15.6625\n",
      "Epoch 58/1000, Loss: 15.1055, Val Loss: 15.6243\n",
      "Epoch 59/1000, Loss: 15.0824, Val Loss: 15.5815\n",
      "Epoch 60/1000, Loss: 15.0298, Val Loss: 15.5354\n",
      "Epoch 61/1000, Loss: 14.9582, Val Loss: 15.4851\n",
      "Epoch 62/1000, Loss: 15.0549, Val Loss: 15.4681\n",
      "Epoch 63/1000, Loss: 14.9394, Val Loss: 15.4596\n",
      "Epoch 64/1000, Loss: 14.9400, Val Loss: 15.4529\n",
      "Epoch 65/1000, Loss: 14.9050, Val Loss: 15.4356\n",
      "Epoch 66/1000, Loss: 14.8097, Val Loss: 15.4075\n",
      "Epoch 67/1000, Loss: 14.8594, Val Loss: 15.3792\n",
      "Epoch 68/1000, Loss: 14.6931, Val Loss: 15.3266\n",
      "Epoch 69/1000, Loss: 14.6663, Val Loss: 15.2826\n",
      "Epoch 70/1000, Loss: 14.6848, Val Loss: 15.2724\n",
      "Epoch 71/1000, Loss: 14.5992, Val Loss: 15.2661\n",
      "Epoch 72/1000, Loss: 14.6246, Val Loss: 15.2553\n",
      "Epoch 73/1000, Loss: 14.6373, Val Loss: 15.2304\n",
      "Epoch 74/1000, Loss: 14.5219, Val Loss: 15.1796\n",
      "Epoch 75/1000, Loss: 14.4876, Val Loss: 15.1463\n",
      "Epoch 76/1000, Loss: 14.3928, Val Loss: 15.1314\n",
      "Epoch 77/1000, Loss: 14.3231, Val Loss: 15.1253\n",
      "Epoch 78/1000, Loss: 14.4090, Val Loss: 15.1265\n",
      "Epoch 79/1000, Loss: 14.5084, Val Loss: 15.1282\n",
      "Epoch 80/1000, Loss: 14.2596, Val Loss: 15.1214\n",
      "Epoch 81/1000, Loss: 14.3168, Val Loss: 15.1031\n",
      "Epoch 82/1000, Loss: 14.3276, Val Loss: 15.0827\n",
      "Epoch 83/1000, Loss: 14.2630, Val Loss: 15.0652\n",
      "Epoch 84/1000, Loss: 14.3217, Val Loss: 15.0515\n",
      "Epoch 85/1000, Loss: 14.0231, Val Loss: 15.0305\n",
      "Epoch 86/1000, Loss: 14.1509, Val Loss: 15.0003\n",
      "Epoch 87/1000, Loss: 14.0065, Val Loss: 14.9663\n",
      "Epoch 88/1000, Loss: 14.0050, Val Loss: 14.9204\n",
      "Epoch 89/1000, Loss: 14.0629, Val Loss: 14.8901\n",
      "Epoch 90/1000, Loss: 14.1134, Val Loss: 14.8641\n",
      "Epoch 91/1000, Loss: 13.8591, Val Loss: 14.8472\n",
      "Epoch 92/1000, Loss: 13.9449, Val Loss: 14.8297\n",
      "Epoch 93/1000, Loss: 13.7695, Val Loss: 14.7877\n",
      "Epoch 94/1000, Loss: 13.8962, Val Loss: 14.7507\n",
      "Epoch 95/1000, Loss: 13.9122, Val Loss: 14.7200\n",
      "Epoch 96/1000, Loss: 13.8271, Val Loss: 14.6949\n",
      "Epoch 97/1000, Loss: 13.7794, Val Loss: 14.6797\n",
      "Epoch 98/1000, Loss: 13.6725, Val Loss: 14.6399\n",
      "Epoch 99/1000, Loss: 13.7065, Val Loss: 14.6202\n",
      "Epoch 100/1000, Loss: 13.6837, Val Loss: 14.6048\n",
      "Epoch 101/1000, Loss: 13.7866, Val Loss: 14.5995\n",
      "Epoch 102/1000, Loss: 13.5506, Val Loss: 14.6149\n",
      "Epoch 103/1000, Loss: 13.8215, Val Loss: 14.5874\n",
      "Epoch 104/1000, Loss: 13.7372, Val Loss: 14.5422\n",
      "Epoch 105/1000, Loss: 13.7349, Val Loss: 14.5266\n",
      "Epoch 106/1000, Loss: 13.5771, Val Loss: 14.5051\n",
      "Epoch 107/1000, Loss: 13.5090, Val Loss: 14.4752\n",
      "Epoch 108/1000, Loss: 13.4107, Val Loss: 14.4598\n",
      "Epoch 109/1000, Loss: 13.5305, Val Loss: 14.4401\n",
      "Epoch 110/1000, Loss: 13.4926, Val Loss: 14.4112\n",
      "Epoch 111/1000, Loss: 13.2994, Val Loss: 14.3818\n",
      "Epoch 112/1000, Loss: 13.2886, Val Loss: 14.3569\n",
      "Epoch 113/1000, Loss: 13.4145, Val Loss: 14.3373\n",
      "Epoch 114/1000, Loss: 13.4152, Val Loss: 14.3452\n",
      "Epoch 115/1000, Loss: 13.3969, Val Loss: 14.3425\n",
      "Epoch 116/1000, Loss: 13.3431, Val Loss: 14.3370\n",
      "Epoch 117/1000, Loss: 13.2152, Val Loss: 14.3229\n",
      "Epoch 118/1000, Loss: 13.0966, Val Loss: 14.3066\n",
      "Epoch 119/1000, Loss: 13.1696, Val Loss: 14.2896\n",
      "Epoch 120/1000, Loss: 13.2456, Val Loss: 14.2740\n",
      "Epoch 121/1000, Loss: 13.3825, Val Loss: 14.2984\n",
      "Epoch 122/1000, Loss: 13.2167, Val Loss: 14.3230\n",
      "Epoch 123/1000, Loss: 13.2137, Val Loss: 14.3192\n",
      "Epoch 124/1000, Loss: 13.2471, Val Loss: 14.2920\n",
      "Epoch 125/1000, Loss: 13.2679, Val Loss: 14.2641\n",
      "Epoch 126/1000, Loss: 13.2735, Val Loss: 14.2400\n",
      "Epoch 127/1000, Loss: 13.0456, Val Loss: 14.2202\n",
      "Epoch 128/1000, Loss: 13.2302, Val Loss: 14.2079\n",
      "Epoch 129/1000, Loss: 13.0523, Val Loss: 14.1874\n",
      "Epoch 130/1000, Loss: 13.1793, Val Loss: 14.1718\n",
      "Epoch 131/1000, Loss: 12.9346, Val Loss: 14.1739\n",
      "Epoch 132/1000, Loss: 13.1019, Val Loss: 14.1784\n",
      "Epoch 133/1000, Loss: 12.9588, Val Loss: 14.1796\n",
      "Epoch 134/1000, Loss: 13.1093, Val Loss: 14.1510\n",
      "Epoch 135/1000, Loss: 13.2793, Val Loss: 14.1283\n",
      "Epoch 136/1000, Loss: 12.8616, Val Loss: 14.1134\n",
      "Epoch 137/1000, Loss: 12.9432, Val Loss: 14.1158\n",
      "Epoch 138/1000, Loss: 13.0108, Val Loss: 14.1259\n",
      "Epoch 139/1000, Loss: 12.8556, Val Loss: 14.1501\n",
      "Epoch 140/1000, Loss: 12.7535, Val Loss: 14.1781\n",
      "Epoch 141/1000, Loss: 12.9510, Val Loss: 14.1999\n",
      "Epoch 142/1000, Loss: 12.8796, Val Loss: 14.2078\n",
      "Epoch 143/1000, Loss: 12.8268, Val Loss: 14.1915\n",
      "Epoch 144/1000, Loss: 12.7881, Val Loss: 14.1675\n",
      "Epoch 145/1000, Loss: 12.7209, Val Loss: 14.1498\n",
      "Epoch 146/1000, Loss: 12.7954, Val Loss: 14.1505\n",
      "Epoch 147/1000, Loss: 12.7665, Val Loss: 14.1569\n",
      "Epoch 148/1000, Loss: 12.8936, Val Loss: 14.1685\n",
      "Epoch 149/1000, Loss: 12.7055, Val Loss: 14.1963\n",
      "Epoch 150/1000, Loss: 12.4748, Val Loss: 14.2385\n",
      "Epoch 151/1000, Loss: 12.5745, Val Loss: 14.2748\n",
      "Epoch 152/1000, Loss: 12.5691, Val Loss: 14.2703\n",
      "Epoch 153/1000, Loss: 12.5142, Val Loss: 14.2719\n",
      "Epoch 154/1000, Loss: 12.5265, Val Loss: 14.2576\n",
      "Epoch 155/1000, Loss: 12.5593, Val Loss: 14.2226\n",
      "Epoch 156/1000, Loss: 12.5163, Val Loss: 14.2083\n",
      "Epoch 157/1000, Loss: 12.1758, Val Loss: 14.1765\n",
      "Epoch 158/1000, Loss: 12.4561, Val Loss: 14.1557\n",
      "Epoch 159/1000, Loss: 12.4883, Val Loss: 14.1678\n",
      "Epoch 160/1000, Loss: 12.4584, Val Loss: 14.1704\n",
      "Epoch 161/1000, Loss: 12.5494, Val Loss: 14.1667\n",
      "Epoch 162/1000, Loss: 12.4396, Val Loss: 14.1925\n",
      "Epoch 163/1000, Loss: 12.2415, Val Loss: 14.2174\n",
      "Epoch 164/1000, Loss: 12.5490, Val Loss: 14.2185\n",
      "Epoch 165/1000, Loss: 12.4969, Val Loss: 14.2265\n",
      "Epoch 166/1000, Loss: 12.1822, Val Loss: 14.2209\n",
      "Epoch 167/1000, Loss: 12.1559, Val Loss: 14.2004\n",
      "Epoch 168/1000, Loss: 12.0173, Val Loss: 14.1802\n",
      "Epoch 169/1000, Loss: 12.0383, Val Loss: 14.1764\n",
      "Epoch 170/1000, Loss: 12.4392, Val Loss: 14.1757\n",
      "Epoch 171/1000, Loss: 12.0959, Val Loss: 14.1861\n",
      "Epoch 172/1000, Loss: 12.1538, Val Loss: 14.1866\n",
      "Epoch 173/1000, Loss: 12.0148, Val Loss: 14.1984\n",
      "Epoch 174/1000, Loss: 12.0873, Val Loss: 14.2213\n",
      "Epoch 175/1000, Loss: 12.0549, Val Loss: 14.2345\n",
      "Epoch 176/1000, Loss: 11.9383, Val Loss: 14.2526\n",
      "Early stopping triggered at epoch 176. Best Val Loss: 14.1134\n",
      "Epoch 1/1000, Loss: 16.7235, Val Loss: 16.8539\n",
      "Epoch 2/1000, Loss: 16.7052, Val Loss: 16.8523\n",
      "Epoch 3/1000, Loss: 16.6921, Val Loss: 16.8506\n",
      "Epoch 4/1000, Loss: 16.6796, Val Loss: 16.8488\n",
      "Epoch 5/1000, Loss: 16.6705, Val Loss: 16.8469\n",
      "Epoch 6/1000, Loss: 16.6669, Val Loss: 16.8446\n",
      "Epoch 7/1000, Loss: 16.6576, Val Loss: 16.8419\n",
      "Epoch 8/1000, Loss: 16.6511, Val Loss: 16.8390\n",
      "Epoch 9/1000, Loss: 16.6450, Val Loss: 16.8357\n",
      "Epoch 10/1000, Loss: 16.6399, Val Loss: 16.8321\n",
      "Epoch 11/1000, Loss: 16.6344, Val Loss: 16.8285\n",
      "Epoch 12/1000, Loss: 16.6293, Val Loss: 16.8242\n",
      "Epoch 13/1000, Loss: 16.6187, Val Loss: 16.8181\n",
      "Epoch 14/1000, Loss: 16.6055, Val Loss: 16.8114\n",
      "Epoch 15/1000, Loss: 16.5981, Val Loss: 16.8045\n",
      "Epoch 16/1000, Loss: 16.5922, Val Loss: 16.7964\n",
      "Epoch 17/1000, Loss: 16.5727, Val Loss: 16.7866\n",
      "Epoch 18/1000, Loss: 16.5630, Val Loss: 16.7756\n",
      "Epoch 19/1000, Loss: 16.5502, Val Loss: 16.7629\n",
      "Epoch 20/1000, Loss: 16.5190, Val Loss: 16.7489\n",
      "Epoch 21/1000, Loss: 16.5068, Val Loss: 16.7340\n",
      "Epoch 22/1000, Loss: 16.4749, Val Loss: 16.7179\n",
      "Epoch 23/1000, Loss: 16.4589, Val Loss: 16.7013\n",
      "Epoch 24/1000, Loss: 16.4369, Val Loss: 16.6839\n",
      "Epoch 25/1000, Loss: 16.4181, Val Loss: 16.6668\n",
      "Epoch 26/1000, Loss: 16.3811, Val Loss: 16.6475\n",
      "Epoch 27/1000, Loss: 16.3706, Val Loss: 16.6263\n",
      "Epoch 28/1000, Loss: 16.3463, Val Loss: 16.6019\n",
      "Epoch 29/1000, Loss: 16.3160, Val Loss: 16.5769\n",
      "Epoch 30/1000, Loss: 16.3009, Val Loss: 16.5531\n",
      "Epoch 31/1000, Loss: 16.2734, Val Loss: 16.5264\n",
      "Epoch 32/1000, Loss: 16.2462, Val Loss: 16.5048\n",
      "Epoch 33/1000, Loss: 16.2394, Val Loss: 16.4798\n",
      "Epoch 34/1000, Loss: 16.2051, Val Loss: 16.4483\n",
      "Epoch 35/1000, Loss: 16.1851, Val Loss: 16.4178\n",
      "Epoch 36/1000, Loss: 16.1628, Val Loss: 16.3912\n",
      "Epoch 37/1000, Loss: 16.1504, Val Loss: 16.3732\n",
      "Epoch 38/1000, Loss: 16.1238, Val Loss: 16.3555\n",
      "Epoch 39/1000, Loss: 16.0765, Val Loss: 16.3339\n",
      "Epoch 40/1000, Loss: 16.0569, Val Loss: 16.3063\n",
      "Epoch 41/1000, Loss: 16.0389, Val Loss: 16.2722\n",
      "Epoch 42/1000, Loss: 16.0167, Val Loss: 16.2411\n",
      "Epoch 43/1000, Loss: 16.0044, Val Loss: 16.2172\n",
      "Epoch 44/1000, Loss: 15.9583, Val Loss: 16.1984\n",
      "Epoch 45/1000, Loss: 15.9332, Val Loss: 16.1841\n",
      "Epoch 46/1000, Loss: 15.8839, Val Loss: 16.1591\n",
      "Epoch 47/1000, Loss: 15.8715, Val Loss: 16.1295\n",
      "Epoch 48/1000, Loss: 15.8378, Val Loss: 16.0864\n",
      "Epoch 49/1000, Loss: 15.8035, Val Loss: 16.0347\n",
      "Epoch 50/1000, Loss: 15.7592, Val Loss: 16.0027\n",
      "Epoch 51/1000, Loss: 15.7143, Val Loss: 15.9765\n",
      "Epoch 52/1000, Loss: 15.6650, Val Loss: 15.9561\n",
      "Epoch 53/1000, Loss: 15.6026, Val Loss: 15.9376\n",
      "Epoch 54/1000, Loss: 15.5907, Val Loss: 15.8790\n",
      "Epoch 55/1000, Loss: 15.5221, Val Loss: 15.8227\n",
      "Epoch 56/1000, Loss: 15.4753, Val Loss: 15.7686\n",
      "Epoch 57/1000, Loss: 15.4529, Val Loss: 15.7145\n",
      "Epoch 58/1000, Loss: 15.3716, Val Loss: 15.6455\n",
      "Epoch 59/1000, Loss: 15.3786, Val Loss: 15.5668\n",
      "Epoch 60/1000, Loss: 15.3356, Val Loss: 15.4917\n",
      "Epoch 61/1000, Loss: 15.2175, Val Loss: 15.4313\n",
      "Epoch 62/1000, Loss: 15.2198, Val Loss: 15.3752\n",
      "Epoch 63/1000, Loss: 15.1908, Val Loss: 15.3067\n",
      "Epoch 64/1000, Loss: 15.0953, Val Loss: 15.2432\n",
      "Epoch 65/1000, Loss: 15.0346, Val Loss: 15.1866\n",
      "Epoch 66/1000, Loss: 15.0014, Val Loss: 15.1309\n",
      "Epoch 67/1000, Loss: 14.8544, Val Loss: 15.0705\n",
      "Epoch 68/1000, Loss: 14.9019, Val Loss: 15.0103\n",
      "Epoch 69/1000, Loss: 14.7757, Val Loss: 14.9530\n",
      "Epoch 70/1000, Loss: 14.7843, Val Loss: 14.8939\n",
      "Epoch 71/1000, Loss: 14.7585, Val Loss: 14.8417\n",
      "Epoch 72/1000, Loss: 14.6930, Val Loss: 14.8013\n",
      "Epoch 73/1000, Loss: 14.6169, Val Loss: 14.8077\n",
      "Epoch 74/1000, Loss: 14.5645, Val Loss: 14.7901\n",
      "Epoch 75/1000, Loss: 14.6573, Val Loss: 14.7303\n",
      "Epoch 76/1000, Loss: 14.4658, Val Loss: 14.6507\n",
      "Epoch 77/1000, Loss: 14.4414, Val Loss: 14.6198\n",
      "Epoch 78/1000, Loss: 14.5160, Val Loss: 14.6078\n",
      "Epoch 79/1000, Loss: 14.4694, Val Loss: 14.6303\n",
      "Epoch 80/1000, Loss: 14.4603, Val Loss: 14.5924\n",
      "Epoch 81/1000, Loss: 14.3716, Val Loss: 14.5237\n",
      "Epoch 82/1000, Loss: 14.2518, Val Loss: 14.4775\n",
      "Epoch 83/1000, Loss: 14.3749, Val Loss: 14.4581\n",
      "Epoch 84/1000, Loss: 14.3045, Val Loss: 14.4421\n",
      "Epoch 85/1000, Loss: 14.2031, Val Loss: 14.4278\n",
      "Epoch 86/1000, Loss: 14.2010, Val Loss: 14.4220\n",
      "Epoch 87/1000, Loss: 14.1871, Val Loss: 14.4016\n",
      "Epoch 88/1000, Loss: 14.0677, Val Loss: 14.3632\n",
      "Epoch 89/1000, Loss: 14.0471, Val Loss: 14.3393\n",
      "Epoch 90/1000, Loss: 14.0593, Val Loss: 14.3256\n",
      "Epoch 91/1000, Loss: 14.0676, Val Loss: 14.3099\n",
      "Epoch 92/1000, Loss: 13.9902, Val Loss: 14.3001\n",
      "Epoch 93/1000, Loss: 13.9955, Val Loss: 14.2868\n",
      "Epoch 94/1000, Loss: 13.9281, Val Loss: 14.2712\n",
      "Epoch 95/1000, Loss: 13.9282, Val Loss: 14.2425\n",
      "Epoch 96/1000, Loss: 13.8698, Val Loss: 14.2275\n",
      "Epoch 97/1000, Loss: 13.8067, Val Loss: 14.2354\n",
      "Epoch 98/1000, Loss: 13.8547, Val Loss: 14.2310\n",
      "Epoch 99/1000, Loss: 13.8532, Val Loss: 14.2171\n",
      "Epoch 100/1000, Loss: 13.7595, Val Loss: 14.2039\n",
      "Epoch 101/1000, Loss: 13.7587, Val Loss: 14.1943\n",
      "Epoch 102/1000, Loss: 13.8537, Val Loss: 14.1863\n",
      "Epoch 103/1000, Loss: 13.7109, Val Loss: 14.1807\n",
      "Epoch 104/1000, Loss: 13.6641, Val Loss: 14.1792\n",
      "Epoch 105/1000, Loss: 13.8053, Val Loss: 14.1901\n",
      "Epoch 106/1000, Loss: 13.7053, Val Loss: 14.1916\n",
      "Epoch 107/1000, Loss: 13.7310, Val Loss: 14.1903\n",
      "Epoch 108/1000, Loss: 13.6805, Val Loss: 14.1925\n",
      "Epoch 109/1000, Loss: 13.6233, Val Loss: 14.1930\n",
      "Epoch 110/1000, Loss: 13.5214, Val Loss: 14.2074\n",
      "Epoch 111/1000, Loss: 13.6030, Val Loss: 14.2299\n",
      "Epoch 112/1000, Loss: 13.6356, Val Loss: 14.2401\n",
      "Epoch 113/1000, Loss: 13.6130, Val Loss: 14.2294\n",
      "Epoch 114/1000, Loss: 13.6705, Val Loss: 14.2244\n",
      "Epoch 115/1000, Loss: 13.5807, Val Loss: 14.2076\n",
      "Epoch 116/1000, Loss: 13.5262, Val Loss: 14.2029\n",
      "Epoch 117/1000, Loss: 13.5634, Val Loss: 14.1950\n",
      "Epoch 118/1000, Loss: 13.5020, Val Loss: 14.1904\n",
      "Epoch 119/1000, Loss: 13.4053, Val Loss: 14.2139\n",
      "Epoch 120/1000, Loss: 13.5003, Val Loss: 14.2219\n",
      "Epoch 121/1000, Loss: 13.5169, Val Loss: 14.1994\n",
      "Epoch 122/1000, Loss: 13.6174, Val Loss: 14.1719\n",
      "Epoch 123/1000, Loss: 13.4323, Val Loss: 14.1623\n",
      "Epoch 124/1000, Loss: 13.3604, Val Loss: 14.1598\n",
      "Epoch 125/1000, Loss: 13.5455, Val Loss: 14.1722\n",
      "Epoch 126/1000, Loss: 13.3775, Val Loss: 14.2096\n",
      "Epoch 127/1000, Loss: 13.3282, Val Loss: 14.2053\n",
      "Epoch 128/1000, Loss: 13.5784, Val Loss: 14.1563\n",
      "Epoch 129/1000, Loss: 13.4100, Val Loss: 14.1345\n",
      "Epoch 130/1000, Loss: 13.4013, Val Loss: 14.1236\n",
      "Epoch 131/1000, Loss: 13.6191, Val Loss: 14.1134\n",
      "Epoch 132/1000, Loss: 13.4946, Val Loss: 14.1363\n",
      "Epoch 133/1000, Loss: 13.5980, Val Loss: 14.1469\n",
      "Epoch 134/1000, Loss: 13.3336, Val Loss: 14.1447\n",
      "Epoch 135/1000, Loss: 13.3213, Val Loss: 14.1338\n",
      "Epoch 136/1000, Loss: 13.3176, Val Loss: 14.1248\n",
      "Epoch 137/1000, Loss: 13.1924, Val Loss: 14.1191\n",
      "Epoch 138/1000, Loss: 13.2694, Val Loss: 14.1205\n",
      "Epoch 139/1000, Loss: 13.3249, Val Loss: 14.1100\n",
      "Epoch 140/1000, Loss: 13.2692, Val Loss: 14.1052\n",
      "Epoch 141/1000, Loss: 13.3048, Val Loss: 14.1061\n",
      "Epoch 142/1000, Loss: 13.1602, Val Loss: 14.1166\n",
      "Epoch 143/1000, Loss: 13.1816, Val Loss: 14.1282\n",
      "Epoch 144/1000, Loss: 13.1213, Val Loss: 14.1413\n",
      "Epoch 145/1000, Loss: 13.2036, Val Loss: 14.1377\n",
      "Epoch 146/1000, Loss: 13.2229, Val Loss: 14.1297\n",
      "Epoch 147/1000, Loss: 13.0889, Val Loss: 14.1203\n",
      "Epoch 148/1000, Loss: 13.0507, Val Loss: 14.1218\n",
      "Epoch 149/1000, Loss: 13.1124, Val Loss: 14.1355\n",
      "Epoch 150/1000, Loss: 12.9905, Val Loss: 14.1604\n",
      "Epoch 151/1000, Loss: 13.0960, Val Loss: 14.1821\n",
      "Epoch 152/1000, Loss: 13.0059, Val Loss: 14.1804\n",
      "Epoch 153/1000, Loss: 13.0208, Val Loss: 14.1788\n",
      "Epoch 154/1000, Loss: 13.0729, Val Loss: 14.1735\n",
      "Epoch 155/1000, Loss: 13.1182, Val Loss: 14.1532\n",
      "Epoch 156/1000, Loss: 12.8305, Val Loss: 14.1507\n",
      "Epoch 157/1000, Loss: 13.0213, Val Loss: 14.1451\n",
      "Epoch 158/1000, Loss: 13.0763, Val Loss: 14.1528\n",
      "Epoch 159/1000, Loss: 13.1116, Val Loss: 14.1742\n",
      "Epoch 160/1000, Loss: 12.7861, Val Loss: 14.2097\n",
      "Epoch 161/1000, Loss: 12.9538, Val Loss: 14.2275\n",
      "Epoch 162/1000, Loss: 12.8175, Val Loss: 14.2132\n",
      "Epoch 163/1000, Loss: 12.8407, Val Loss: 14.2024\n",
      "Epoch 164/1000, Loss: 12.8370, Val Loss: 14.1999\n",
      "Epoch 165/1000, Loss: 12.7676, Val Loss: 14.1991\n",
      "Epoch 166/1000, Loss: 12.8150, Val Loss: 14.1946\n",
      "Epoch 167/1000, Loss: 12.8993, Val Loss: 14.1607\n",
      "Epoch 168/1000, Loss: 12.7629, Val Loss: 14.1330\n",
      "Epoch 169/1000, Loss: 12.7410, Val Loss: 14.1292\n",
      "Epoch 170/1000, Loss: 12.7244, Val Loss: 14.1384\n",
      "Epoch 171/1000, Loss: 12.8552, Val Loss: 14.1283\n",
      "Epoch 172/1000, Loss: 12.7403, Val Loss: 14.1189\n",
      "Epoch 173/1000, Loss: 12.6920, Val Loss: 14.1140\n",
      "Epoch 174/1000, Loss: 12.8127, Val Loss: 14.1039\n",
      "Epoch 175/1000, Loss: 12.7063, Val Loss: 14.1141\n",
      "Epoch 176/1000, Loss: 12.5781, Val Loss: 14.1433\n",
      "Epoch 177/1000, Loss: 12.8916, Val Loss: 14.1314\n",
      "Epoch 178/1000, Loss: 12.8184, Val Loss: 14.1711\n",
      "Epoch 179/1000, Loss: 12.6970, Val Loss: 14.1948\n",
      "Epoch 180/1000, Loss: 12.8831, Val Loss: 14.1682\n",
      "Epoch 181/1000, Loss: 12.7242, Val Loss: 14.1449\n",
      "Epoch 182/1000, Loss: 12.8264, Val Loss: 14.1365\n",
      "Epoch 183/1000, Loss: 12.6630, Val Loss: 14.1436\n",
      "Epoch 184/1000, Loss: 12.7825, Val Loss: 14.1474\n",
      "Epoch 185/1000, Loss: 12.6977, Val Loss: 14.0916\n",
      "Epoch 186/1000, Loss: 12.5771, Val Loss: 14.0708\n",
      "Epoch 187/1000, Loss: 12.6800, Val Loss: 14.0695\n",
      "Epoch 188/1000, Loss: 12.6544, Val Loss: 14.0636\n",
      "Epoch 189/1000, Loss: 12.6542, Val Loss: 14.0482\n",
      "Epoch 190/1000, Loss: 12.5719, Val Loss: 14.0344\n",
      "Epoch 191/1000, Loss: 12.4569, Val Loss: 14.0387\n",
      "Epoch 192/1000, Loss: 12.3311, Val Loss: 14.0358\n",
      "Epoch 193/1000, Loss: 12.4804, Val Loss: 14.0344\n",
      "Epoch 194/1000, Loss: 12.4012, Val Loss: 14.0484\n",
      "Epoch 195/1000, Loss: 12.4683, Val Loss: 14.1042\n",
      "Epoch 196/1000, Loss: 12.4197, Val Loss: 14.1503\n",
      "Epoch 197/1000, Loss: 12.5644, Val Loss: 14.1726\n",
      "Epoch 198/1000, Loss: 12.5061, Val Loss: 14.1261\n",
      "Epoch 199/1000, Loss: 12.2394, Val Loss: 14.0990\n",
      "Epoch 200/1000, Loss: 12.4526, Val Loss: 14.1003\n",
      "Epoch 201/1000, Loss: 12.4457, Val Loss: 14.0898\n",
      "Epoch 202/1000, Loss: 12.3857, Val Loss: 14.0771\n",
      "Epoch 203/1000, Loss: 12.3438, Val Loss: 14.0885\n",
      "Epoch 204/1000, Loss: 12.2281, Val Loss: 14.1099\n",
      "Epoch 205/1000, Loss: 12.3534, Val Loss: 14.1207\n",
      "Epoch 206/1000, Loss: 12.1424, Val Loss: 14.1104\n",
      "Epoch 207/1000, Loss: 12.2478, Val Loss: 14.0877\n",
      "Epoch 208/1000, Loss: 12.1891, Val Loss: 14.0680\n",
      "Epoch 209/1000, Loss: 12.2245, Val Loss: 14.0676\n",
      "Epoch 210/1000, Loss: 12.1592, Val Loss: 14.0826\n",
      "Epoch 211/1000, Loss: 12.1959, Val Loss: 14.1050\n",
      "Epoch 212/1000, Loss: 12.0167, Val Loss: 14.1417\n",
      "Epoch 213/1000, Loss: 12.3900, Val Loss: 14.1519\n",
      "Epoch 214/1000, Loss: 12.0772, Val Loss: 14.1721\n",
      "Epoch 215/1000, Loss: 12.1790, Val Loss: 14.1718\n",
      "Epoch 216/1000, Loss: 12.3214, Val Loss: 14.1672\n",
      "Epoch 217/1000, Loss: 12.0650, Val Loss: 14.1271\n",
      "Epoch 218/1000, Loss: 11.9299, Val Loss: 14.1206\n",
      "Epoch 219/1000, Loss: 12.3794, Val Loss: 14.1108\n",
      "Epoch 220/1000, Loss: 12.1624, Val Loss: 14.0961\n",
      "Epoch 221/1000, Loss: 11.9411, Val Loss: 14.0709\n",
      "Epoch 222/1000, Loss: 11.9726, Val Loss: 14.0550\n",
      "Epoch 223/1000, Loss: 12.0296, Val Loss: 14.0676\n",
      "Epoch 224/1000, Loss: 12.1822, Val Loss: 14.0758\n",
      "Epoch 225/1000, Loss: 11.7099, Val Loss: 14.0903\n",
      "Epoch 226/1000, Loss: 11.9158, Val Loss: 14.1322\n",
      "Epoch 227/1000, Loss: 12.0251, Val Loss: 14.1510\n",
      "Epoch 228/1000, Loss: 11.9972, Val Loss: 14.1879\n",
      "Epoch 229/1000, Loss: 12.0167, Val Loss: 14.2183\n",
      "Epoch 230/1000, Loss: 12.0384, Val Loss: 14.1529\n",
      "Epoch 231/1000, Loss: 11.9551, Val Loss: 14.0637\n",
      "Epoch 232/1000, Loss: 11.7395, Val Loss: 13.9975\n",
      "Epoch 233/1000, Loss: 11.8265, Val Loss: 13.9776\n",
      "Epoch 234/1000, Loss: 11.9114, Val Loss: 13.9769\n",
      "Epoch 235/1000, Loss: 11.7192, Val Loss: 13.9745\n",
      "Epoch 236/1000, Loss: 11.6709, Val Loss: 13.9898\n",
      "Epoch 237/1000, Loss: 11.6529, Val Loss: 14.0138\n",
      "Epoch 238/1000, Loss: 11.6238, Val Loss: 14.0319\n",
      "Epoch 239/1000, Loss: 11.8473, Val Loss: 14.0550\n",
      "Epoch 240/1000, Loss: 11.4866, Val Loss: 14.0659\n",
      "Epoch 241/1000, Loss: 12.0582, Val Loss: 14.0579\n",
      "Epoch 242/1000, Loss: 11.5074, Val Loss: 14.0483\n",
      "Epoch 243/1000, Loss: 11.6707, Val Loss: 14.0118\n",
      "Epoch 244/1000, Loss: 11.6727, Val Loss: 13.9936\n",
      "Epoch 245/1000, Loss: 11.6872, Val Loss: 14.0039\n",
      "Epoch 246/1000, Loss: 11.4698, Val Loss: 13.9850\n",
      "Epoch 247/1000, Loss: 11.5498, Val Loss: 13.9747\n",
      "Epoch 248/1000, Loss: 11.4039, Val Loss: 13.9734\n",
      "Epoch 249/1000, Loss: 11.6704, Val Loss: 13.9986\n",
      "Epoch 250/1000, Loss: 11.3303, Val Loss: 14.0099\n",
      "Epoch 251/1000, Loss: 11.7665, Val Loss: 13.9907\n",
      "Epoch 252/1000, Loss: 11.3309, Val Loss: 14.0023\n",
      "Epoch 253/1000, Loss: 11.5808, Val Loss: 14.0282\n",
      "Epoch 254/1000, Loss: 11.5277, Val Loss: 14.0993\n",
      "Epoch 255/1000, Loss: 11.1947, Val Loss: 14.1474\n",
      "Epoch 256/1000, Loss: 11.5352, Val Loss: 14.1727\n",
      "Epoch 257/1000, Loss: 11.0940, Val Loss: 14.1980\n",
      "Epoch 258/1000, Loss: 11.3426, Val Loss: 14.2154\n",
      "Epoch 259/1000, Loss: 11.4599, Val Loss: 14.2147\n",
      "Epoch 260/1000, Loss: 11.5087, Val Loss: 14.2181\n",
      "Epoch 261/1000, Loss: 11.4529, Val Loss: 14.2391\n",
      "Epoch 262/1000, Loss: 11.8299, Val Loss: 14.2056\n",
      "Epoch 263/1000, Loss: 11.4424, Val Loss: 14.1816\n",
      "Epoch 264/1000, Loss: 11.5283, Val Loss: 14.1243\n",
      "Epoch 265/1000, Loss: 11.3029, Val Loss: 14.1099\n",
      "Epoch 266/1000, Loss: 11.6032, Val Loss: 14.1186\n",
      "Epoch 267/1000, Loss: 11.2568, Val Loss: 14.1616\n",
      "Epoch 268/1000, Loss: 11.4510, Val Loss: 14.1977\n",
      "Epoch 269/1000, Loss: 11.2956, Val Loss: 14.2334\n",
      "Epoch 270/1000, Loss: 11.3157, Val Loss: 14.2748\n",
      "Epoch 271/1000, Loss: 11.2552, Val Loss: 14.2843\n",
      "Epoch 272/1000, Loss: 11.1416, Val Loss: 14.2649\n",
      "Epoch 273/1000, Loss: 11.4156, Val Loss: 14.2643\n",
      "Epoch 274/1000, Loss: 11.4326, Val Loss: 14.2532\n",
      "Epoch 275/1000, Loss: 11.4488, Val Loss: 14.2326\n",
      "Epoch 276/1000, Loss: 11.3652, Val Loss: 14.1996\n",
      "Epoch 277/1000, Loss: 11.2052, Val Loss: 14.1914\n",
      "Epoch 278/1000, Loss: 11.2280, Val Loss: 14.1721\n",
      "Epoch 279/1000, Loss: 11.4863, Val Loss: 14.1630\n",
      "Epoch 280/1000, Loss: 11.0066, Val Loss: 14.2019\n",
      "Epoch 281/1000, Loss: 11.1304, Val Loss: 14.2525\n",
      "Epoch 282/1000, Loss: 11.4991, Val Loss: 14.3404\n",
      "Epoch 283/1000, Loss: 11.1701, Val Loss: 14.4412\n",
      "Epoch 284/1000, Loss: 10.9283, Val Loss: 14.5239\n",
      "Epoch 285/1000, Loss: 11.1968, Val Loss: 14.5707\n",
      "Epoch 286/1000, Loss: 11.2692, Val Loss: 14.5662\n",
      "Epoch 287/1000, Loss: 11.1986, Val Loss: 14.4998\n",
      "Epoch 288/1000, Loss: 10.9474, Val Loss: 14.4716\n",
      "Early stopping triggered at epoch 288. Best Val Loss: 13.9734\n",
      "Epoch 1/1000, Loss: 16.9934, Val Loss: 17.2886\n",
      "Epoch 2/1000, Loss: 16.9776, Val Loss: 17.2850\n",
      "Epoch 3/1000, Loss: 16.9665, Val Loss: 17.2812\n",
      "Epoch 4/1000, Loss: 16.9485, Val Loss: 17.2771\n",
      "Epoch 5/1000, Loss: 16.9428, Val Loss: 17.2726\n",
      "Epoch 6/1000, Loss: 16.9334, Val Loss: 17.2676\n",
      "Epoch 7/1000, Loss: 16.9219, Val Loss: 17.2622\n",
      "Epoch 8/1000, Loss: 16.9150, Val Loss: 17.2563\n",
      "Epoch 9/1000, Loss: 16.9033, Val Loss: 17.2496\n",
      "Epoch 10/1000, Loss: 16.8931, Val Loss: 17.2426\n",
      "Epoch 11/1000, Loss: 16.8788, Val Loss: 17.2349\n",
      "Epoch 12/1000, Loss: 16.8646, Val Loss: 17.2264\n",
      "Epoch 13/1000, Loss: 16.8528, Val Loss: 17.2169\n",
      "Epoch 14/1000, Loss: 16.8390, Val Loss: 17.2063\n",
      "Epoch 15/1000, Loss: 16.8190, Val Loss: 17.1940\n",
      "Epoch 16/1000, Loss: 16.8085, Val Loss: 17.1800\n",
      "Epoch 17/1000, Loss: 16.7849, Val Loss: 17.1642\n",
      "Epoch 18/1000, Loss: 16.7663, Val Loss: 17.1472\n",
      "Epoch 19/1000, Loss: 16.7480, Val Loss: 17.1292\n",
      "Epoch 20/1000, Loss: 16.7343, Val Loss: 17.1106\n",
      "Epoch 21/1000, Loss: 16.7200, Val Loss: 17.0912\n",
      "Epoch 22/1000, Loss: 16.7049, Val Loss: 17.0718\n",
      "Epoch 23/1000, Loss: 16.6825, Val Loss: 17.0515\n",
      "Epoch 24/1000, Loss: 16.6724, Val Loss: 17.0337\n",
      "Epoch 25/1000, Loss: 16.6539, Val Loss: 17.0172\n",
      "Epoch 26/1000, Loss: 16.6315, Val Loss: 17.0001\n",
      "Epoch 27/1000, Loss: 16.6265, Val Loss: 16.9818\n",
      "Epoch 28/1000, Loss: 16.6026, Val Loss: 16.9605\n",
      "Epoch 29/1000, Loss: 16.5839, Val Loss: 16.9364\n",
      "Epoch 30/1000, Loss: 16.5712, Val Loss: 16.9129\n",
      "Epoch 31/1000, Loss: 16.5425, Val Loss: 16.8885\n",
      "Epoch 32/1000, Loss: 16.5191, Val Loss: 16.8639\n",
      "Epoch 33/1000, Loss: 16.4925, Val Loss: 16.8392\n",
      "Epoch 34/1000, Loss: 16.4736, Val Loss: 16.8151\n",
      "Epoch 35/1000, Loss: 16.4569, Val Loss: 16.7909\n",
      "Epoch 36/1000, Loss: 16.4192, Val Loss: 16.7693\n",
      "Epoch 37/1000, Loss: 16.3991, Val Loss: 16.7470\n",
      "Epoch 38/1000, Loss: 16.3757, Val Loss: 16.7268\n",
      "Epoch 39/1000, Loss: 16.3468, Val Loss: 16.7060\n",
      "Epoch 40/1000, Loss: 16.3286, Val Loss: 16.6856\n",
      "Epoch 41/1000, Loss: 16.2901, Val Loss: 16.6622\n",
      "Epoch 42/1000, Loss: 16.2636, Val Loss: 16.6372\n",
      "Epoch 43/1000, Loss: 16.2327, Val Loss: 16.6103\n",
      "Epoch 44/1000, Loss: 16.2105, Val Loss: 16.5824\n",
      "Epoch 45/1000, Loss: 16.1692, Val Loss: 16.5542\n",
      "Epoch 46/1000, Loss: 16.1346, Val Loss: 16.5272\n",
      "Epoch 47/1000, Loss: 16.0690, Val Loss: 16.5020\n",
      "Epoch 48/1000, Loss: 16.0384, Val Loss: 16.4788\n",
      "Epoch 49/1000, Loss: 16.0373, Val Loss: 16.4503\n",
      "Epoch 50/1000, Loss: 15.9527, Val Loss: 16.4232\n",
      "Epoch 51/1000, Loss: 15.9182, Val Loss: 16.3960\n",
      "Epoch 52/1000, Loss: 15.8692, Val Loss: 16.3575\n",
      "Epoch 53/1000, Loss: 15.8321, Val Loss: 16.3080\n",
      "Epoch 54/1000, Loss: 15.7791, Val Loss: 16.2529\n",
      "Epoch 55/1000, Loss: 15.7686, Val Loss: 16.1853\n",
      "Epoch 56/1000, Loss: 15.7426, Val Loss: 16.1109\n",
      "Epoch 57/1000, Loss: 15.6435, Val Loss: 16.0412\n",
      "Epoch 58/1000, Loss: 15.6308, Val Loss: 15.9806\n",
      "Epoch 59/1000, Loss: 15.5696, Val Loss: 15.9290\n",
      "Epoch 60/1000, Loss: 15.5126, Val Loss: 15.8729\n",
      "Epoch 61/1000, Loss: 15.4712, Val Loss: 15.8057\n",
      "Epoch 62/1000, Loss: 15.4338, Val Loss: 15.7273\n",
      "Epoch 63/1000, Loss: 15.3466, Val Loss: 15.6544\n",
      "Epoch 64/1000, Loss: 15.3094, Val Loss: 15.5959\n",
      "Epoch 65/1000, Loss: 15.2960, Val Loss: 15.5451\n",
      "Epoch 66/1000, Loss: 15.2091, Val Loss: 15.4962\n",
      "Epoch 67/1000, Loss: 15.1696, Val Loss: 15.4507\n",
      "Epoch 68/1000, Loss: 15.1408, Val Loss: 15.3928\n",
      "Epoch 69/1000, Loss: 14.9911, Val Loss: 15.3047\n",
      "Epoch 70/1000, Loss: 14.9761, Val Loss: 15.2269\n",
      "Epoch 71/1000, Loss: 14.8842, Val Loss: 15.1669\n",
      "Epoch 72/1000, Loss: 14.8613, Val Loss: 15.1153\n",
      "Epoch 73/1000, Loss: 14.8117, Val Loss: 15.0646\n",
      "Epoch 74/1000, Loss: 14.7657, Val Loss: 15.0204\n",
      "Epoch 75/1000, Loss: 14.6719, Val Loss: 15.0022\n",
      "Epoch 76/1000, Loss: 14.6402, Val Loss: 14.9640\n",
      "Epoch 77/1000, Loss: 14.6361, Val Loss: 14.8908\n",
      "Epoch 78/1000, Loss: 14.5298, Val Loss: 14.8202\n",
      "Epoch 79/1000, Loss: 14.5568, Val Loss: 14.7916\n",
      "Epoch 80/1000, Loss: 14.4377, Val Loss: 14.8112\n",
      "Epoch 81/1000, Loss: 14.3621, Val Loss: 14.8383\n",
      "Epoch 82/1000, Loss: 14.3913, Val Loss: 14.7749\n",
      "Epoch 83/1000, Loss: 14.2643, Val Loss: 14.6827\n",
      "Epoch 84/1000, Loss: 14.2657, Val Loss: 14.6212\n",
      "Epoch 85/1000, Loss: 14.3041, Val Loss: 14.6133\n",
      "Epoch 86/1000, Loss: 14.1593, Val Loss: 14.6743\n",
      "Epoch 87/1000, Loss: 14.1457, Val Loss: 14.7241\n",
      "Epoch 88/1000, Loss: 14.1994, Val Loss: 14.6836\n",
      "Epoch 89/1000, Loss: 14.0848, Val Loss: 14.6064\n",
      "Epoch 90/1000, Loss: 14.0077, Val Loss: 14.5631\n",
      "Epoch 91/1000, Loss: 14.0038, Val Loss: 14.5450\n",
      "Epoch 92/1000, Loss: 13.9700, Val Loss: 14.5468\n",
      "Epoch 93/1000, Loss: 13.8543, Val Loss: 14.5686\n",
      "Epoch 94/1000, Loss: 13.8408, Val Loss: 14.5599\n",
      "Epoch 95/1000, Loss: 13.7759, Val Loss: 14.5400\n",
      "Epoch 96/1000, Loss: 13.7571, Val Loss: 14.4923\n",
      "Epoch 97/1000, Loss: 13.8365, Val Loss: 14.4605\n",
      "Epoch 98/1000, Loss: 13.6709, Val Loss: 14.4260\n",
      "Epoch 99/1000, Loss: 13.6697, Val Loss: 14.4093\n",
      "Epoch 100/1000, Loss: 13.7518, Val Loss: 14.3652\n",
      "Epoch 101/1000, Loss: 13.7125, Val Loss: 14.3784\n",
      "Epoch 102/1000, Loss: 13.5407, Val Loss: 14.3823\n",
      "Epoch 103/1000, Loss: 13.6914, Val Loss: 14.3577\n",
      "Epoch 104/1000, Loss: 13.5399, Val Loss: 14.3309\n",
      "Epoch 105/1000, Loss: 13.5436, Val Loss: 14.3329\n",
      "Epoch 106/1000, Loss: 13.4878, Val Loss: 14.3655\n",
      "Epoch 107/1000, Loss: 13.4827, Val Loss: 14.3625\n",
      "Epoch 108/1000, Loss: 13.4271, Val Loss: 14.3356\n",
      "Epoch 109/1000, Loss: 13.5460, Val Loss: 14.2919\n",
      "Epoch 110/1000, Loss: 13.3579, Val Loss: 14.2859\n",
      "Epoch 111/1000, Loss: 13.5899, Val Loss: 14.3282\n",
      "Epoch 112/1000, Loss: 13.3825, Val Loss: 14.3970\n",
      "Epoch 113/1000, Loss: 13.4739, Val Loss: 14.3950\n",
      "Epoch 114/1000, Loss: 13.4953, Val Loss: 14.3539\n",
      "Epoch 115/1000, Loss: 13.3841, Val Loss: 14.3016\n",
      "Epoch 116/1000, Loss: 13.5997, Val Loss: 14.2810\n",
      "Epoch 117/1000, Loss: 13.3347, Val Loss: 14.2589\n",
      "Epoch 118/1000, Loss: 13.2812, Val Loss: 14.2482\n",
      "Epoch 119/1000, Loss: 13.3413, Val Loss: 14.2423\n",
      "Epoch 120/1000, Loss: 13.2835, Val Loss: 14.2499\n",
      "Epoch 121/1000, Loss: 13.2747, Val Loss: 14.2594\n",
      "Epoch 122/1000, Loss: 13.3247, Val Loss: 14.2666\n",
      "Epoch 123/1000, Loss: 13.2992, Val Loss: 14.2628\n",
      "Epoch 124/1000, Loss: 13.2439, Val Loss: 14.2605\n",
      "Epoch 125/1000, Loss: 13.2096, Val Loss: 14.2605\n",
      "Epoch 126/1000, Loss: 13.0632, Val Loss: 14.2586\n",
      "Epoch 127/1000, Loss: 13.4012, Val Loss: 14.2514\n",
      "Epoch 128/1000, Loss: 13.1426, Val Loss: 14.2527\n",
      "Epoch 129/1000, Loss: 13.0882, Val Loss: 14.2453\n",
      "Epoch 130/1000, Loss: 12.9558, Val Loss: 14.2294\n",
      "Epoch 131/1000, Loss: 12.9658, Val Loss: 14.2204\n",
      "Epoch 132/1000, Loss: 13.0890, Val Loss: 14.2338\n",
      "Epoch 133/1000, Loss: 13.0580, Val Loss: 14.2440\n",
      "Epoch 134/1000, Loss: 13.1856, Val Loss: 14.2497\n",
      "Epoch 135/1000, Loss: 13.1167, Val Loss: 14.2391\n",
      "Epoch 136/1000, Loss: 12.8855, Val Loss: 14.2164\n",
      "Epoch 137/1000, Loss: 12.9726, Val Loss: 14.2011\n",
      "Epoch 138/1000, Loss: 13.0096, Val Loss: 14.1771\n",
      "Epoch 139/1000, Loss: 12.8024, Val Loss: 14.1507\n",
      "Epoch 140/1000, Loss: 12.6237, Val Loss: 14.1246\n",
      "Epoch 141/1000, Loss: 12.8028, Val Loss: 14.0822\n",
      "Epoch 142/1000, Loss: 13.2890, Val Loss: 14.0414\n",
      "Epoch 143/1000, Loss: 12.9850, Val Loss: 14.0394\n",
      "Epoch 144/1000, Loss: 12.8481, Val Loss: 14.0378\n",
      "Epoch 145/1000, Loss: 12.9511, Val Loss: 14.0033\n",
      "Epoch 146/1000, Loss: 12.8275, Val Loss: 13.9987\n",
      "Epoch 147/1000, Loss: 13.0026, Val Loss: 14.0153\n",
      "Epoch 148/1000, Loss: 13.0539, Val Loss: 14.0230\n",
      "Epoch 149/1000, Loss: 13.0727, Val Loss: 14.0089\n",
      "Epoch 150/1000, Loss: 12.9025, Val Loss: 14.0009\n",
      "Epoch 151/1000, Loss: 12.6481, Val Loss: 13.9866\n",
      "Epoch 152/1000, Loss: 12.6996, Val Loss: 13.9938\n",
      "Epoch 153/1000, Loss: 12.9286, Val Loss: 14.0118\n",
      "Epoch 154/1000, Loss: 12.8840, Val Loss: 14.0165\n",
      "Epoch 155/1000, Loss: 12.8112, Val Loss: 13.9998\n",
      "Epoch 156/1000, Loss: 12.5219, Val Loss: 13.9988\n",
      "Epoch 157/1000, Loss: 12.5635, Val Loss: 14.0110\n",
      "Epoch 158/1000, Loss: 12.7795, Val Loss: 14.0355\n",
      "Epoch 159/1000, Loss: 12.6641, Val Loss: 14.0296\n",
      "Epoch 160/1000, Loss: 12.5291, Val Loss: 14.0244\n",
      "Epoch 161/1000, Loss: 12.4485, Val Loss: 14.0247\n",
      "Epoch 162/1000, Loss: 12.5574, Val Loss: 14.0313\n",
      "Epoch 163/1000, Loss: 12.4592, Val Loss: 14.0452\n",
      "Epoch 164/1000, Loss: 12.5356, Val Loss: 14.0278\n",
      "Epoch 165/1000, Loss: 12.5895, Val Loss: 14.0164\n",
      "Epoch 166/1000, Loss: 12.4613, Val Loss: 14.0262\n",
      "Epoch 167/1000, Loss: 12.6861, Val Loss: 14.0704\n",
      "Epoch 168/1000, Loss: 12.4656, Val Loss: 14.1200\n",
      "Epoch 169/1000, Loss: 12.4682, Val Loss: 14.1542\n",
      "Epoch 170/1000, Loss: 12.3915, Val Loss: 14.1411\n",
      "Epoch 171/1000, Loss: 12.3530, Val Loss: 14.1067\n",
      "Epoch 172/1000, Loss: 12.4773, Val Loss: 14.0942\n",
      "Epoch 173/1000, Loss: 12.2809, Val Loss: 14.0988\n",
      "Epoch 174/1000, Loss: 12.3606, Val Loss: 14.0946\n",
      "Epoch 175/1000, Loss: 12.5086, Val Loss: 14.0765\n",
      "Epoch 176/1000, Loss: 12.1907, Val Loss: 14.0476\n",
      "Epoch 177/1000, Loss: 12.3386, Val Loss: 14.0266\n",
      "Epoch 178/1000, Loss: 12.2951, Val Loss: 14.0196\n",
      "Epoch 179/1000, Loss: 12.3432, Val Loss: 14.0272\n",
      "Epoch 180/1000, Loss: 12.3714, Val Loss: 14.0263\n",
      "Epoch 181/1000, Loss: 12.1863, Val Loss: 14.0239\n",
      "Epoch 182/1000, Loss: 12.2953, Val Loss: 14.0144\n",
      "Epoch 183/1000, Loss: 12.1334, Val Loss: 14.0271\n",
      "Epoch 184/1000, Loss: 12.2097, Val Loss: 14.0528\n",
      "Epoch 185/1000, Loss: 12.0311, Val Loss: 14.0808\n",
      "Epoch 186/1000, Loss: 12.2097, Val Loss: 14.0865\n",
      "Epoch 187/1000, Loss: 12.0574, Val Loss: 14.0700\n",
      "Epoch 188/1000, Loss: 12.1372, Val Loss: 14.0659\n",
      "Epoch 189/1000, Loss: 12.2991, Val Loss: 14.0893\n",
      "Epoch 190/1000, Loss: 11.9681, Val Loss: 14.1068\n",
      "Epoch 191/1000, Loss: 11.9811, Val Loss: 14.1231\n",
      "Early stopping triggered at epoch 191. Best Val Loss: 13.9866\n",
      "Epoch 1/1000, Loss: 16.8869, Val Loss: 17.1646\n",
      "Epoch 2/1000, Loss: 16.8601, Val Loss: 17.1588\n",
      "Epoch 3/1000, Loss: 16.8448, Val Loss: 17.1521\n",
      "Epoch 4/1000, Loss: 16.8167, Val Loss: 17.1441\n",
      "Epoch 5/1000, Loss: 16.7965, Val Loss: 17.1361\n",
      "Epoch 6/1000, Loss: 16.7894, Val Loss: 17.1273\n",
      "Epoch 7/1000, Loss: 16.7817, Val Loss: 17.1178\n",
      "Epoch 8/1000, Loss: 16.7558, Val Loss: 17.1084\n",
      "Epoch 9/1000, Loss: 16.7157, Val Loss: 17.0986\n",
      "Epoch 10/1000, Loss: 16.6990, Val Loss: 17.0870\n",
      "Epoch 11/1000, Loss: 16.6788, Val Loss: 17.0739\n",
      "Epoch 12/1000, Loss: 16.6481, Val Loss: 17.0593\n",
      "Epoch 13/1000, Loss: 16.6260, Val Loss: 17.0420\n",
      "Epoch 14/1000, Loss: 16.5693, Val Loss: 17.0208\n",
      "Epoch 15/1000, Loss: 16.5323, Val Loss: 16.9954\n",
      "Epoch 16/1000, Loss: 16.4879, Val Loss: 16.9674\n",
      "Epoch 17/1000, Loss: 16.4646, Val Loss: 16.9368\n",
      "Epoch 18/1000, Loss: 16.4267, Val Loss: 16.9037\n",
      "Epoch 19/1000, Loss: 16.3902, Val Loss: 16.8663\n",
      "Epoch 20/1000, Loss: 16.3398, Val Loss: 16.8273\n",
      "Epoch 21/1000, Loss: 16.3038, Val Loss: 16.7888\n",
      "Epoch 22/1000, Loss: 16.2667, Val Loss: 16.7508\n",
      "Epoch 23/1000, Loss: 16.2261, Val Loss: 16.7141\n",
      "Epoch 24/1000, Loss: 16.1794, Val Loss: 16.6771\n",
      "Epoch 25/1000, Loss: 16.1623, Val Loss: 16.6412\n",
      "Epoch 26/1000, Loss: 16.0876, Val Loss: 16.6022\n",
      "Epoch 27/1000, Loss: 16.0564, Val Loss: 16.5620\n",
      "Epoch 28/1000, Loss: 16.0380, Val Loss: 16.5216\n",
      "Epoch 29/1000, Loss: 15.9409, Val Loss: 16.4754\n",
      "Epoch 30/1000, Loss: 15.8950, Val Loss: 16.4241\n",
      "Epoch 31/1000, Loss: 15.8039, Val Loss: 16.3712\n",
      "Epoch 32/1000, Loss: 15.7994, Val Loss: 16.3201\n",
      "Epoch 33/1000, Loss: 15.7353, Val Loss: 16.2667\n",
      "Epoch 34/1000, Loss: 15.6948, Val Loss: 16.2142\n",
      "Epoch 35/1000, Loss: 15.6036, Val Loss: 16.1651\n",
      "Epoch 36/1000, Loss: 15.5611, Val Loss: 16.1147\n",
      "Epoch 37/1000, Loss: 15.5177, Val Loss: 16.0587\n",
      "Epoch 38/1000, Loss: 15.4273, Val Loss: 16.0009\n",
      "Epoch 39/1000, Loss: 15.3862, Val Loss: 15.9473\n",
      "Epoch 40/1000, Loss: 15.3401, Val Loss: 15.8948\n",
      "Epoch 41/1000, Loss: 15.2839, Val Loss: 15.8476\n",
      "Epoch 42/1000, Loss: 15.2207, Val Loss: 15.7973\n",
      "Epoch 43/1000, Loss: 15.1898, Val Loss: 15.7410\n",
      "Epoch 44/1000, Loss: 15.1154, Val Loss: 15.6810\n",
      "Epoch 45/1000, Loss: 15.0491, Val Loss: 15.6208\n",
      "Epoch 46/1000, Loss: 15.0212, Val Loss: 15.5747\n",
      "Epoch 47/1000, Loss: 14.9423, Val Loss: 15.5216\n",
      "Epoch 48/1000, Loss: 14.8882, Val Loss: 15.4780\n",
      "Epoch 49/1000, Loss: 14.7907, Val Loss: 15.4041\n",
      "Epoch 50/1000, Loss: 14.7427, Val Loss: 15.3256\n",
      "Epoch 51/1000, Loss: 14.6837, Val Loss: 15.2457\n",
      "Epoch 52/1000, Loss: 14.5873, Val Loss: 15.1803\n",
      "Epoch 53/1000, Loss: 14.5899, Val Loss: 15.1455\n",
      "Epoch 54/1000, Loss: 14.6388, Val Loss: 15.0554\n",
      "Epoch 55/1000, Loss: 14.5736, Val Loss: 14.9822\n",
      "Epoch 56/1000, Loss: 14.3688, Val Loss: 14.8996\n",
      "Epoch 57/1000, Loss: 14.4190, Val Loss: 14.8360\n",
      "Epoch 58/1000, Loss: 14.3019, Val Loss: 14.7725\n",
      "Epoch 59/1000, Loss: 14.2537, Val Loss: 14.7283\n",
      "Epoch 60/1000, Loss: 14.3046, Val Loss: 14.6886\n",
      "Epoch 61/1000, Loss: 14.3035, Val Loss: 14.6541\n",
      "Epoch 62/1000, Loss: 14.1850, Val Loss: 14.6427\n",
      "Epoch 63/1000, Loss: 14.1354, Val Loss: 14.5877\n",
      "Epoch 64/1000, Loss: 14.0882, Val Loss: 14.5329\n",
      "Epoch 65/1000, Loss: 14.2003, Val Loss: 14.5020\n",
      "Epoch 66/1000, Loss: 14.1049, Val Loss: 14.5027\n",
      "Epoch 67/1000, Loss: 14.0089, Val Loss: 14.5203\n",
      "Epoch 68/1000, Loss: 14.0190, Val Loss: 14.5192\n",
      "Epoch 69/1000, Loss: 14.0471, Val Loss: 14.5156\n",
      "Epoch 70/1000, Loss: 13.9489, Val Loss: 14.4691\n",
      "Epoch 71/1000, Loss: 13.8639, Val Loss: 14.4407\n",
      "Epoch 72/1000, Loss: 13.9404, Val Loss: 14.4332\n",
      "Epoch 73/1000, Loss: 13.8857, Val Loss: 14.3931\n",
      "Epoch 74/1000, Loss: 13.9292, Val Loss: 14.3845\n",
      "Epoch 75/1000, Loss: 13.9316, Val Loss: 14.4069\n",
      "Epoch 76/1000, Loss: 13.8414, Val Loss: 14.3953\n",
      "Epoch 77/1000, Loss: 13.8270, Val Loss: 14.3425\n",
      "Epoch 78/1000, Loss: 13.7845, Val Loss: 14.3084\n",
      "Epoch 79/1000, Loss: 13.7934, Val Loss: 14.2922\n",
      "Epoch 80/1000, Loss: 13.9866, Val Loss: 14.2809\n",
      "Epoch 81/1000, Loss: 13.7256, Val Loss: 14.2816\n",
      "Epoch 82/1000, Loss: 13.6352, Val Loss: 14.2673\n",
      "Epoch 83/1000, Loss: 13.7293, Val Loss: 14.2383\n",
      "Epoch 84/1000, Loss: 13.8198, Val Loss: 14.2232\n",
      "Epoch 85/1000, Loss: 13.6903, Val Loss: 14.2109\n",
      "Epoch 86/1000, Loss: 13.7201, Val Loss: 14.2064\n",
      "Epoch 87/1000, Loss: 13.6823, Val Loss: 14.2019\n",
      "Epoch 88/1000, Loss: 13.7556, Val Loss: 14.2012\n",
      "Epoch 89/1000, Loss: 13.6455, Val Loss: 14.2042\n",
      "Epoch 90/1000, Loss: 13.5964, Val Loss: 14.2033\n",
      "Epoch 91/1000, Loss: 13.5551, Val Loss: 14.1949\n",
      "Epoch 92/1000, Loss: 13.5981, Val Loss: 14.1823\n",
      "Epoch 93/1000, Loss: 13.5661, Val Loss: 14.1693\n",
      "Epoch 94/1000, Loss: 13.4785, Val Loss: 14.1623\n",
      "Epoch 95/1000, Loss: 13.5173, Val Loss: 14.1703\n",
      "Epoch 96/1000, Loss: 13.4491, Val Loss: 14.1830\n",
      "Epoch 97/1000, Loss: 13.4669, Val Loss: 14.1926\n",
      "Epoch 98/1000, Loss: 13.4824, Val Loss: 14.2030\n",
      "Epoch 99/1000, Loss: 13.3457, Val Loss: 14.2131\n",
      "Epoch 100/1000, Loss: 13.4411, Val Loss: 14.2332\n",
      "Epoch 101/1000, Loss: 13.3046, Val Loss: 14.2409\n",
      "Epoch 102/1000, Loss: 13.3070, Val Loss: 14.2340\n",
      "Epoch 103/1000, Loss: 13.4498, Val Loss: 14.2065\n",
      "Epoch 104/1000, Loss: 13.2709, Val Loss: 14.1913\n",
      "Epoch 105/1000, Loss: 13.4254, Val Loss: 14.1831\n",
      "Epoch 106/1000, Loss: 13.4155, Val Loss: 14.1786\n",
      "Epoch 107/1000, Loss: 13.3602, Val Loss: 14.1774\n",
      "Epoch 108/1000, Loss: 13.3170, Val Loss: 14.1680\n",
      "Epoch 109/1000, Loss: 13.3561, Val Loss: 14.1554\n",
      "Epoch 110/1000, Loss: 13.2374, Val Loss: 14.1541\n",
      "Epoch 111/1000, Loss: 13.2476, Val Loss: 14.1681\n",
      "Epoch 112/1000, Loss: 13.0878, Val Loss: 14.1790\n",
      "Epoch 113/1000, Loss: 13.1389, Val Loss: 14.1770\n",
      "Epoch 114/1000, Loss: 13.2486, Val Loss: 14.1684\n",
      "Epoch 115/1000, Loss: 13.1428, Val Loss: 14.1564\n",
      "Epoch 116/1000, Loss: 13.0749, Val Loss: 14.1331\n",
      "Epoch 117/1000, Loss: 13.2420, Val Loss: 14.1160\n",
      "Epoch 118/1000, Loss: 13.0535, Val Loss: 14.1110\n",
      "Epoch 119/1000, Loss: 13.1406, Val Loss: 14.1257\n",
      "Epoch 120/1000, Loss: 13.3292, Val Loss: 14.1348\n",
      "Epoch 121/1000, Loss: 13.1588, Val Loss: 14.1422\n",
      "Epoch 122/1000, Loss: 13.2132, Val Loss: 14.1426\n",
      "Epoch 123/1000, Loss: 13.0457, Val Loss: 14.1416\n",
      "Epoch 124/1000, Loss: 13.0761, Val Loss: 14.1375\n",
      "Epoch 125/1000, Loss: 13.0410, Val Loss: 14.1243\n",
      "Epoch 126/1000, Loss: 12.9318, Val Loss: 14.1099\n",
      "Epoch 127/1000, Loss: 13.0672, Val Loss: 14.0988\n",
      "Epoch 128/1000, Loss: 13.0465, Val Loss: 14.1074\n",
      "Epoch 129/1000, Loss: 12.8302, Val Loss: 14.1159\n",
      "Epoch 130/1000, Loss: 13.0341, Val Loss: 14.1163\n",
      "Epoch 131/1000, Loss: 13.2799, Val Loss: 14.1228\n",
      "Epoch 132/1000, Loss: 12.8908, Val Loss: 14.1451\n",
      "Epoch 133/1000, Loss: 13.1221, Val Loss: 14.1461\n",
      "Epoch 134/1000, Loss: 13.0181, Val Loss: 14.1228\n",
      "Epoch 135/1000, Loss: 12.8779, Val Loss: 14.1293\n",
      "Epoch 136/1000, Loss: 12.8148, Val Loss: 14.1453\n",
      "Epoch 137/1000, Loss: 12.9036, Val Loss: 14.1440\n",
      "Epoch 138/1000, Loss: 12.9283, Val Loss: 14.1266\n",
      "Epoch 139/1000, Loss: 12.7410, Val Loss: 14.1392\n",
      "Epoch 140/1000, Loss: 12.8900, Val Loss: 14.1442\n",
      "Epoch 141/1000, Loss: 12.8090, Val Loss: 14.1205\n",
      "Epoch 142/1000, Loss: 13.0571, Val Loss: 14.0831\n",
      "Epoch 143/1000, Loss: 12.9844, Val Loss: 14.0943\n",
      "Epoch 144/1000, Loss: 12.8766, Val Loss: 14.1292\n",
      "Epoch 145/1000, Loss: 12.8826, Val Loss: 14.1997\n",
      "Epoch 146/1000, Loss: 12.9666, Val Loss: 14.2469\n",
      "Epoch 147/1000, Loss: 12.8440, Val Loss: 14.2608\n",
      "Epoch 148/1000, Loss: 12.8008, Val Loss: 14.2610\n",
      "Epoch 149/1000, Loss: 12.8554, Val Loss: 14.2475\n",
      "Epoch 150/1000, Loss: 12.7249, Val Loss: 14.2262\n",
      "Epoch 151/1000, Loss: 12.8757, Val Loss: 14.1703\n",
      "Epoch 152/1000, Loss: 12.6420, Val Loss: 14.1293\n",
      "Epoch 153/1000, Loss: 12.5262, Val Loss: 14.0858\n",
      "Epoch 154/1000, Loss: 12.6193, Val Loss: 14.0716\n",
      "Epoch 155/1000, Loss: 12.7871, Val Loss: 14.0694\n",
      "Epoch 156/1000, Loss: 12.5685, Val Loss: 14.0750\n",
      "Epoch 157/1000, Loss: 12.6882, Val Loss: 14.0932\n",
      "Epoch 158/1000, Loss: 12.5932, Val Loss: 14.1206\n",
      "Epoch 159/1000, Loss: 12.7087, Val Loss: 14.1448\n",
      "Epoch 160/1000, Loss: 12.5809, Val Loss: 14.1585\n",
      "Epoch 161/1000, Loss: 12.6669, Val Loss: 14.1823\n",
      "Epoch 162/1000, Loss: 12.3883, Val Loss: 14.2234\n",
      "Epoch 163/1000, Loss: 12.3370, Val Loss: 14.2327\n",
      "Epoch 164/1000, Loss: 12.5447, Val Loss: 14.2300\n",
      "Epoch 165/1000, Loss: 12.5199, Val Loss: 14.2107\n",
      "Epoch 166/1000, Loss: 12.6327, Val Loss: 14.2116\n",
      "Epoch 167/1000, Loss: 12.6746, Val Loss: 14.2196\n",
      "Epoch 168/1000, Loss: 12.4318, Val Loss: 14.2342\n",
      "Epoch 169/1000, Loss: 12.5287, Val Loss: 14.1983\n",
      "Epoch 170/1000, Loss: 12.3518, Val Loss: 14.1392\n",
      "Epoch 171/1000, Loss: 12.3819, Val Loss: 14.0961\n",
      "Epoch 172/1000, Loss: 12.4172, Val Loss: 14.0670\n",
      "Epoch 173/1000, Loss: 12.4701, Val Loss: 14.0629\n",
      "Epoch 174/1000, Loss: 12.5835, Val Loss: 14.0493\n",
      "Epoch 175/1000, Loss: 12.3035, Val Loss: 14.0391\n",
      "Epoch 176/1000, Loss: 12.3062, Val Loss: 14.0363\n",
      "Epoch 177/1000, Loss: 12.2005, Val Loss: 14.0317\n",
      "Epoch 178/1000, Loss: 12.4867, Val Loss: 14.0068\n",
      "Epoch 179/1000, Loss: 12.2010, Val Loss: 13.9790\n",
      "Epoch 180/1000, Loss: 12.4243, Val Loss: 13.9654\n",
      "Epoch 181/1000, Loss: 12.3195, Val Loss: 13.9640\n",
      "Epoch 182/1000, Loss: 12.2941, Val Loss: 13.9840\n",
      "Epoch 183/1000, Loss: 12.3064, Val Loss: 14.0046\n",
      "Epoch 184/1000, Loss: 12.1487, Val Loss: 14.0155\n",
      "Epoch 185/1000, Loss: 12.1915, Val Loss: 14.0191\n",
      "Epoch 186/1000, Loss: 12.1966, Val Loss: 14.0374\n",
      "Epoch 187/1000, Loss: 12.0625, Val Loss: 14.0457\n",
      "Epoch 188/1000, Loss: 12.1672, Val Loss: 14.0341\n",
      "Epoch 189/1000, Loss: 12.1609, Val Loss: 14.0355\n",
      "Epoch 190/1000, Loss: 12.2295, Val Loss: 14.0508\n",
      "Epoch 191/1000, Loss: 12.0776, Val Loss: 14.0705\n",
      "Epoch 192/1000, Loss: 12.0696, Val Loss: 14.0705\n",
      "Epoch 193/1000, Loss: 11.9720, Val Loss: 14.0786\n",
      "Epoch 194/1000, Loss: 12.0517, Val Loss: 14.0950\n",
      "Epoch 195/1000, Loss: 12.0364, Val Loss: 14.1260\n",
      "Epoch 196/1000, Loss: 12.0364, Val Loss: 14.1371\n",
      "Epoch 197/1000, Loss: 11.9267, Val Loss: 14.1571\n",
      "Epoch 198/1000, Loss: 12.0893, Val Loss: 14.1456\n",
      "Epoch 199/1000, Loss: 11.9010, Val Loss: 14.0988\n",
      "Epoch 200/1000, Loss: 11.8491, Val Loss: 14.0765\n",
      "Epoch 201/1000, Loss: 11.9262, Val Loss: 14.0832\n",
      "Epoch 202/1000, Loss: 11.7913, Val Loss: 14.0981\n",
      "Epoch 203/1000, Loss: 11.9120, Val Loss: 14.0784\n",
      "Epoch 204/1000, Loss: 11.9689, Val Loss: 14.0838\n",
      "Epoch 205/1000, Loss: 11.9195, Val Loss: 14.1094\n",
      "Epoch 206/1000, Loss: 11.8596, Val Loss: 14.1355\n",
      "Epoch 207/1000, Loss: 11.6861, Val Loss: 14.1603\n",
      "Epoch 208/1000, Loss: 11.9794, Val Loss: 14.2133\n",
      "Epoch 209/1000, Loss: 11.5574, Val Loss: 14.2589\n",
      "Epoch 210/1000, Loss: 11.7993, Val Loss: 14.2733\n",
      "Epoch 211/1000, Loss: 11.7181, Val Loss: 14.2796\n",
      "Epoch 212/1000, Loss: 11.6622, Val Loss: 14.2604\n",
      "Epoch 213/1000, Loss: 11.7939, Val Loss: 14.2683\n",
      "Epoch 214/1000, Loss: 12.2431, Val Loss: 14.2249\n",
      "Epoch 215/1000, Loss: 11.8561, Val Loss: 14.2632\n",
      "Epoch 216/1000, Loss: 11.9570, Val Loss: 14.2917\n",
      "Epoch 217/1000, Loss: 11.9422, Val Loss: 14.2986\n",
      "Epoch 218/1000, Loss: 11.9426, Val Loss: 14.3092\n",
      "Epoch 219/1000, Loss: 11.6743, Val Loss: 14.3197\n",
      "Epoch 220/1000, Loss: 11.9003, Val Loss: 14.3215\n",
      "Epoch 221/1000, Loss: 11.8233, Val Loss: 14.3006\n",
      "Early stopping triggered at epoch 221. Best Val Loss: 13.9640\n",
      "Epoch 1/1000, Loss: 17.0427, Val Loss: 17.3454\n",
      "Epoch 2/1000, Loss: 17.0196, Val Loss: 17.3420\n",
      "Epoch 3/1000, Loss: 17.0125, Val Loss: 17.3387\n",
      "Epoch 4/1000, Loss: 17.0116, Val Loss: 17.3354\n",
      "Epoch 5/1000, Loss: 17.0057, Val Loss: 17.3321\n",
      "Epoch 6/1000, Loss: 17.0031, Val Loss: 17.3284\n",
      "Epoch 7/1000, Loss: 16.9969, Val Loss: 17.3245\n",
      "Epoch 8/1000, Loss: 16.9931, Val Loss: 17.3205\n",
      "Epoch 9/1000, Loss: 16.9881, Val Loss: 17.3164\n",
      "Epoch 10/1000, Loss: 16.9807, Val Loss: 17.3122\n",
      "Epoch 11/1000, Loss: 16.9767, Val Loss: 17.3078\n",
      "Epoch 12/1000, Loss: 16.9678, Val Loss: 17.3033\n",
      "Epoch 13/1000, Loss: 16.9614, Val Loss: 17.2987\n",
      "Epoch 14/1000, Loss: 16.9533, Val Loss: 17.2938\n",
      "Epoch 15/1000, Loss: 16.9529, Val Loss: 17.2885\n",
      "Epoch 16/1000, Loss: 16.9434, Val Loss: 17.2826\n",
      "Epoch 17/1000, Loss: 16.9314, Val Loss: 17.2764\n",
      "Epoch 18/1000, Loss: 16.9242, Val Loss: 17.2701\n",
      "Epoch 19/1000, Loss: 16.9179, Val Loss: 17.2637\n",
      "Epoch 20/1000, Loss: 16.9082, Val Loss: 17.2565\n",
      "Epoch 21/1000, Loss: 16.8930, Val Loss: 17.2487\n",
      "Epoch 22/1000, Loss: 16.8799, Val Loss: 17.2377\n",
      "Epoch 23/1000, Loss: 16.8677, Val Loss: 17.2183\n",
      "Epoch 24/1000, Loss: 16.8357, Val Loss: 17.1926\n",
      "Epoch 25/1000, Loss: 16.8146, Val Loss: 17.1625\n",
      "Epoch 26/1000, Loss: 16.7947, Val Loss: 17.1279\n",
      "Epoch 27/1000, Loss: 16.7704, Val Loss: 17.0887\n",
      "Epoch 28/1000, Loss: 16.7350, Val Loss: 17.0466\n",
      "Epoch 29/1000, Loss: 16.6981, Val Loss: 17.0014\n",
      "Epoch 30/1000, Loss: 16.6762, Val Loss: 16.9547\n",
      "Epoch 31/1000, Loss: 16.6487, Val Loss: 16.9083\n",
      "Epoch 32/1000, Loss: 16.6089, Val Loss: 16.8652\n",
      "Epoch 33/1000, Loss: 16.5792, Val Loss: 16.8262\n",
      "Epoch 34/1000, Loss: 16.5336, Val Loss: 16.7925\n",
      "Epoch 35/1000, Loss: 16.5244, Val Loss: 16.7636\n",
      "Epoch 36/1000, Loss: 16.4813, Val Loss: 16.7386\n",
      "Epoch 37/1000, Loss: 16.4404, Val Loss: 16.7152\n",
      "Epoch 38/1000, Loss: 16.4051, Val Loss: 16.6930\n",
      "Epoch 39/1000, Loss: 16.3518, Val Loss: 16.6748\n",
      "Epoch 40/1000, Loss: 16.3387, Val Loss: 16.6564\n",
      "Epoch 41/1000, Loss: 16.2885, Val Loss: 16.6348\n",
      "Epoch 42/1000, Loss: 16.2362, Val Loss: 16.6103\n",
      "Epoch 43/1000, Loss: 16.2082, Val Loss: 16.5847\n",
      "Epoch 44/1000, Loss: 16.1697, Val Loss: 16.5531\n",
      "Epoch 45/1000, Loss: 16.1400, Val Loss: 16.5171\n",
      "Epoch 46/1000, Loss: 16.0903, Val Loss: 16.4824\n",
      "Epoch 47/1000, Loss: 16.0192, Val Loss: 16.4461\n",
      "Epoch 48/1000, Loss: 15.9843, Val Loss: 16.4062\n",
      "Epoch 49/1000, Loss: 15.9320, Val Loss: 16.3642\n",
      "Epoch 50/1000, Loss: 15.8770, Val Loss: 16.3211\n",
      "Epoch 51/1000, Loss: 15.8216, Val Loss: 16.2767\n",
      "Epoch 52/1000, Loss: 15.7596, Val Loss: 16.2295\n",
      "Epoch 53/1000, Loss: 15.6939, Val Loss: 16.1712\n",
      "Epoch 54/1000, Loss: 15.6456, Val Loss: 16.1112\n",
      "Epoch 55/1000, Loss: 15.5282, Val Loss: 16.0525\n",
      "Epoch 56/1000, Loss: 15.5340, Val Loss: 15.9991\n",
      "Epoch 57/1000, Loss: 15.4813, Val Loss: 15.9431\n",
      "Epoch 58/1000, Loss: 15.4066, Val Loss: 15.8941\n",
      "Epoch 59/1000, Loss: 15.3269, Val Loss: 15.8391\n",
      "Epoch 60/1000, Loss: 15.2429, Val Loss: 15.7847\n",
      "Epoch 61/1000, Loss: 15.1741, Val Loss: 15.7228\n",
      "Epoch 62/1000, Loss: 15.2078, Val Loss: 15.6472\n",
      "Epoch 63/1000, Loss: 15.0686, Val Loss: 15.5615\n",
      "Epoch 64/1000, Loss: 14.9605, Val Loss: 15.4767\n",
      "Epoch 65/1000, Loss: 14.8960, Val Loss: 15.4081\n",
      "Epoch 66/1000, Loss: 14.8206, Val Loss: 15.3560\n",
      "Epoch 67/1000, Loss: 14.7476, Val Loss: 15.3062\n",
      "Epoch 68/1000, Loss: 14.7627, Val Loss: 15.2390\n",
      "Epoch 69/1000, Loss: 14.6767, Val Loss: 15.1577\n",
      "Epoch 70/1000, Loss: 14.5411, Val Loss: 15.0813\n",
      "Epoch 71/1000, Loss: 14.5251, Val Loss: 15.0163\n",
      "Epoch 72/1000, Loss: 14.4767, Val Loss: 14.9630\n",
      "Epoch 73/1000, Loss: 14.5479, Val Loss: 14.9164\n",
      "Epoch 74/1000, Loss: 14.4259, Val Loss: 14.8567\n",
      "Epoch 75/1000, Loss: 14.3890, Val Loss: 14.8068\n",
      "Epoch 76/1000, Loss: 14.3520, Val Loss: 14.7673\n",
      "Epoch 77/1000, Loss: 14.2562, Val Loss: 14.7299\n",
      "Epoch 78/1000, Loss: 14.2515, Val Loss: 14.7052\n",
      "Epoch 79/1000, Loss: 14.2724, Val Loss: 14.6763\n",
      "Epoch 80/1000, Loss: 14.0725, Val Loss: 14.6484\n",
      "Epoch 81/1000, Loss: 14.1173, Val Loss: 14.6172\n",
      "Epoch 82/1000, Loss: 13.9704, Val Loss: 14.5954\n",
      "Epoch 83/1000, Loss: 13.9686, Val Loss: 14.5840\n",
      "Epoch 84/1000, Loss: 14.0265, Val Loss: 14.5639\n",
      "Epoch 85/1000, Loss: 14.0948, Val Loss: 14.5449\n",
      "Epoch 86/1000, Loss: 13.9227, Val Loss: 14.5238\n",
      "Epoch 87/1000, Loss: 13.8655, Val Loss: 14.4947\n",
      "Epoch 88/1000, Loss: 13.9267, Val Loss: 14.4721\n",
      "Epoch 89/1000, Loss: 13.9238, Val Loss: 14.4545\n",
      "Epoch 90/1000, Loss: 13.9289, Val Loss: 14.4432\n",
      "Epoch 91/1000, Loss: 13.9373, Val Loss: 14.4262\n",
      "Epoch 92/1000, Loss: 13.8096, Val Loss: 14.4335\n",
      "Epoch 93/1000, Loss: 13.7871, Val Loss: 14.4362\n",
      "Epoch 94/1000, Loss: 13.9923, Val Loss: 14.4150\n",
      "Epoch 95/1000, Loss: 13.8151, Val Loss: 14.3959\n",
      "Epoch 96/1000, Loss: 13.7041, Val Loss: 14.3978\n",
      "Epoch 97/1000, Loss: 13.8578, Val Loss: 14.3959\n",
      "Epoch 98/1000, Loss: 13.6140, Val Loss: 14.4059\n",
      "Epoch 99/1000, Loss: 13.6778, Val Loss: 14.4110\n",
      "Epoch 100/1000, Loss: 13.6734, Val Loss: 14.4310\n",
      "Epoch 101/1000, Loss: 13.6171, Val Loss: 14.4456\n",
      "Epoch 102/1000, Loss: 13.6903, Val Loss: 14.4719\n",
      "Epoch 103/1000, Loss: 13.5509, Val Loss: 14.4763\n",
      "Epoch 104/1000, Loss: 13.7478, Val Loss: 14.4483\n",
      "Epoch 105/1000, Loss: 13.6000, Val Loss: 14.4424\n",
      "Epoch 106/1000, Loss: 13.5048, Val Loss: 14.4344\n",
      "Epoch 107/1000, Loss: 13.5741, Val Loss: 14.4327\n",
      "Epoch 108/1000, Loss: 13.7337, Val Loss: 14.4131\n",
      "Epoch 109/1000, Loss: 13.5101, Val Loss: 14.3879\n",
      "Epoch 110/1000, Loss: 13.5406, Val Loss: 14.3934\n",
      "Epoch 111/1000, Loss: 13.5271, Val Loss: 14.4280\n",
      "Epoch 112/1000, Loss: 13.6243, Val Loss: 14.4458\n",
      "Epoch 113/1000, Loss: 13.4731, Val Loss: 14.4533\n",
      "Epoch 114/1000, Loss: 13.4594, Val Loss: 14.4097\n",
      "Epoch 115/1000, Loss: 13.3893, Val Loss: 14.3427\n",
      "Epoch 116/1000, Loss: 13.4915, Val Loss: 14.3098\n",
      "Epoch 117/1000, Loss: 13.4075, Val Loss: 14.3093\n",
      "Epoch 118/1000, Loss: 13.3706, Val Loss: 14.3257\n",
      "Epoch 119/1000, Loss: 13.3912, Val Loss: 14.3294\n",
      "Epoch 120/1000, Loss: 13.4116, Val Loss: 14.2787\n",
      "Epoch 121/1000, Loss: 13.3117, Val Loss: 14.2617\n",
      "Epoch 122/1000, Loss: 13.3364, Val Loss: 14.2604\n",
      "Epoch 123/1000, Loss: 13.2175, Val Loss: 14.2704\n",
      "Epoch 124/1000, Loss: 13.2674, Val Loss: 14.2846\n",
      "Epoch 125/1000, Loss: 13.4416, Val Loss: 14.2831\n",
      "Epoch 126/1000, Loss: 13.2051, Val Loss: 14.2562\n",
      "Epoch 127/1000, Loss: 13.1357, Val Loss: 14.2249\n",
      "Epoch 128/1000, Loss: 13.2271, Val Loss: 14.1956\n",
      "Epoch 129/1000, Loss: 13.1984, Val Loss: 14.1790\n",
      "Epoch 130/1000, Loss: 13.3354, Val Loss: 14.1945\n",
      "Epoch 131/1000, Loss: 13.1698, Val Loss: 14.1987\n",
      "Epoch 132/1000, Loss: 13.0556, Val Loss: 14.1777\n",
      "Epoch 133/1000, Loss: 13.0166, Val Loss: 14.1640\n",
      "Epoch 134/1000, Loss: 13.1012, Val Loss: 14.1503\n",
      "Epoch 135/1000, Loss: 12.9799, Val Loss: 14.1452\n",
      "Epoch 136/1000, Loss: 12.9073, Val Loss: 14.1642\n",
      "Epoch 137/1000, Loss: 13.0939, Val Loss: 14.1762\n",
      "Epoch 138/1000, Loss: 12.8242, Val Loss: 14.1648\n",
      "Epoch 139/1000, Loss: 13.1150, Val Loss: 14.1404\n",
      "Epoch 140/1000, Loss: 13.0055, Val Loss: 14.1236\n",
      "Epoch 141/1000, Loss: 13.0807, Val Loss: 14.1170\n",
      "Epoch 142/1000, Loss: 12.8088, Val Loss: 14.1111\n",
      "Epoch 143/1000, Loss: 13.0732, Val Loss: 14.1013\n",
      "Epoch 144/1000, Loss: 13.0100, Val Loss: 14.0667\n",
      "Epoch 145/1000, Loss: 12.8958, Val Loss: 14.0374\n",
      "Epoch 146/1000, Loss: 13.1544, Val Loss: 13.9945\n",
      "Epoch 147/1000, Loss: 12.9058, Val Loss: 13.9688\n",
      "Epoch 148/1000, Loss: 12.9217, Val Loss: 13.9577\n",
      "Epoch 149/1000, Loss: 12.8330, Val Loss: 13.9511\n",
      "Epoch 150/1000, Loss: 12.8560, Val Loss: 13.9403\n",
      "Epoch 151/1000, Loss: 12.6472, Val Loss: 13.9378\n",
      "Epoch 152/1000, Loss: 12.7741, Val Loss: 13.9534\n",
      "Epoch 153/1000, Loss: 12.8079, Val Loss: 13.9938\n",
      "Epoch 154/1000, Loss: 12.7969, Val Loss: 14.0267\n",
      "Epoch 155/1000, Loss: 12.6683, Val Loss: 13.9907\n",
      "Epoch 156/1000, Loss: 12.9035, Val Loss: 13.9713\n",
      "Epoch 157/1000, Loss: 12.8421, Val Loss: 13.9849\n",
      "Epoch 158/1000, Loss: 12.7454, Val Loss: 13.9897\n",
      "Epoch 159/1000, Loss: 12.7323, Val Loss: 13.9727\n",
      "Epoch 160/1000, Loss: 12.5101, Val Loss: 13.9805\n",
      "Epoch 161/1000, Loss: 12.6202, Val Loss: 13.9989\n",
      "Epoch 162/1000, Loss: 12.8745, Val Loss: 14.0065\n",
      "Epoch 163/1000, Loss: 12.5195, Val Loss: 14.0079\n",
      "Epoch 164/1000, Loss: 12.8487, Val Loss: 14.0170\n",
      "Epoch 165/1000, Loss: 12.6494, Val Loss: 14.0090\n",
      "Epoch 166/1000, Loss: 12.5878, Val Loss: 13.9613\n",
      "Epoch 167/1000, Loss: 12.6007, Val Loss: 13.9445\n",
      "Epoch 168/1000, Loss: 12.4672, Val Loss: 13.9185\n",
      "Epoch 169/1000, Loss: 12.4329, Val Loss: 13.8952\n",
      "Epoch 170/1000, Loss: 12.3239, Val Loss: 13.8770\n",
      "Epoch 171/1000, Loss: 12.4088, Val Loss: 13.8762\n",
      "Epoch 172/1000, Loss: 12.2875, Val Loss: 13.8777\n",
      "Epoch 173/1000, Loss: 12.4657, Val Loss: 13.8967\n",
      "Epoch 174/1000, Loss: 12.4868, Val Loss: 13.9075\n",
      "Epoch 175/1000, Loss: 12.4101, Val Loss: 13.9238\n",
      "Epoch 176/1000, Loss: 12.3978, Val Loss: 13.9463\n",
      "Epoch 177/1000, Loss: 12.0737, Val Loss: 13.9593\n",
      "Epoch 178/1000, Loss: 12.2846, Val Loss: 13.9470\n",
      "Epoch 179/1000, Loss: 12.3687, Val Loss: 13.9052\n",
      "Epoch 180/1000, Loss: 12.5495, Val Loss: 13.8642\n",
      "Epoch 181/1000, Loss: 12.0575, Val Loss: 13.8310\n",
      "Epoch 182/1000, Loss: 12.3322, Val Loss: 13.8304\n",
      "Epoch 183/1000, Loss: 12.2443, Val Loss: 13.8563\n",
      "Epoch 184/1000, Loss: 12.4085, Val Loss: 13.8417\n",
      "Epoch 185/1000, Loss: 12.2243, Val Loss: 13.8614\n",
      "Epoch 186/1000, Loss: 12.3994, Val Loss: 13.9016\n",
      "Epoch 187/1000, Loss: 12.5036, Val Loss: 13.9672\n",
      "Epoch 188/1000, Loss: 12.2467, Val Loss: 14.0436\n",
      "Epoch 189/1000, Loss: 12.3735, Val Loss: 14.1202\n",
      "Epoch 190/1000, Loss: 12.2583, Val Loss: 14.1886\n",
      "Epoch 191/1000, Loss: 12.1824, Val Loss: 14.2022\n",
      "Epoch 192/1000, Loss: 12.2417, Val Loss: 14.1838\n",
      "Epoch 193/1000, Loss: 12.2548, Val Loss: 14.1508\n",
      "Epoch 194/1000, Loss: 12.1463, Val Loss: 14.0865\n",
      "Epoch 195/1000, Loss: 11.9358, Val Loss: 14.0477\n",
      "Epoch 196/1000, Loss: 12.1623, Val Loss: 14.0105\n",
      "Epoch 197/1000, Loss: 12.2797, Val Loss: 13.9943\n",
      "Epoch 198/1000, Loss: 11.8677, Val Loss: 14.0160\n",
      "Epoch 199/1000, Loss: 12.1096, Val Loss: 14.1113\n",
      "Epoch 200/1000, Loss: 12.1619, Val Loss: 14.1449\n",
      "Epoch 201/1000, Loss: 11.9615, Val Loss: 14.1313\n",
      "Epoch 202/1000, Loss: 11.9697, Val Loss: 14.1331\n",
      "Epoch 203/1000, Loss: 11.6006, Val Loss: 14.1512\n",
      "Epoch 204/1000, Loss: 12.1361, Val Loss: 14.1404\n",
      "Epoch 205/1000, Loss: 11.9691, Val Loss: 14.1458\n",
      "Epoch 206/1000, Loss: 11.9218, Val Loss: 14.1496\n",
      "Epoch 207/1000, Loss: 11.6826, Val Loss: 14.1212\n",
      "Epoch 208/1000, Loss: 11.9776, Val Loss: 14.0720\n",
      "Epoch 209/1000, Loss: 11.8537, Val Loss: 13.9955\n",
      "Epoch 210/1000, Loss: 11.7712, Val Loss: 13.9804\n",
      "Epoch 211/1000, Loss: 11.6928, Val Loss: 13.9935\n",
      "Epoch 212/1000, Loss: 11.8199, Val Loss: 14.0073\n",
      "Epoch 213/1000, Loss: 11.6842, Val Loss: 14.0203\n",
      "Epoch 214/1000, Loss: 12.0519, Val Loss: 14.0171\n",
      "Epoch 215/1000, Loss: 12.1294, Val Loss: 14.0061\n",
      "Epoch 216/1000, Loss: 11.6606, Val Loss: 13.9959\n",
      "Epoch 217/1000, Loss: 11.7337, Val Loss: 14.0215\n",
      "Epoch 218/1000, Loss: 11.5429, Val Loss: 14.0613\n",
      "Epoch 219/1000, Loss: 12.1266, Val Loss: 14.0653\n",
      "Epoch 220/1000, Loss: 11.6390, Val Loss: 14.0525\n",
      "Epoch 221/1000, Loss: 12.2753, Val Loss: 14.1253\n",
      "Epoch 222/1000, Loss: 11.7624, Val Loss: 14.1759\n",
      "Early stopping triggered at epoch 222. Best Val Loss: 13.8304\n",
      "Epoch 1/1000, Loss: 16.7351, Val Loss: 16.7932\n",
      "Epoch 2/1000, Loss: 16.6548, Val Loss: 16.7932\n",
      "Epoch 3/1000, Loss: 16.6195, Val Loss: 16.7931\n",
      "Epoch 4/1000, Loss: 16.5997, Val Loss: 16.7923\n",
      "Epoch 5/1000, Loss: 16.5768, Val Loss: 16.7903\n",
      "Epoch 6/1000, Loss: 16.5477, Val Loss: 16.7859\n",
      "Epoch 7/1000, Loss: 16.5025, Val Loss: 16.7795\n",
      "Epoch 8/1000, Loss: 16.4897, Val Loss: 16.7707\n",
      "Epoch 9/1000, Loss: 16.4740, Val Loss: 16.7589\n",
      "Epoch 10/1000, Loss: 16.4380, Val Loss: 16.7450\n",
      "Epoch 11/1000, Loss: 16.3992, Val Loss: 16.7263\n",
      "Epoch 12/1000, Loss: 16.3580, Val Loss: 16.7032\n",
      "Epoch 13/1000, Loss: 16.3238, Val Loss: 16.6776\n",
      "Epoch 14/1000, Loss: 16.2594, Val Loss: 16.6508\n",
      "Epoch 15/1000, Loss: 16.2280, Val Loss: 16.6231\n",
      "Epoch 16/1000, Loss: 16.1825, Val Loss: 16.5942\n",
      "Epoch 17/1000, Loss: 16.1350, Val Loss: 16.5618\n",
      "Epoch 18/1000, Loss: 16.1033, Val Loss: 16.5296\n",
      "Epoch 19/1000, Loss: 16.0502, Val Loss: 16.5001\n",
      "Epoch 20/1000, Loss: 15.9922, Val Loss: 16.4690\n",
      "Epoch 21/1000, Loss: 15.9907, Val Loss: 16.4388\n",
      "Epoch 22/1000, Loss: 15.9658, Val Loss: 16.4088\n",
      "Epoch 23/1000, Loss: 15.8960, Val Loss: 16.3738\n",
      "Epoch 24/1000, Loss: 15.8391, Val Loss: 16.3339\n",
      "Epoch 25/1000, Loss: 15.8100, Val Loss: 16.2894\n",
      "Epoch 26/1000, Loss: 15.7644, Val Loss: 16.2492\n",
      "Epoch 27/1000, Loss: 15.7293, Val Loss: 16.2140\n",
      "Epoch 28/1000, Loss: 15.6661, Val Loss: 16.1823\n",
      "Epoch 29/1000, Loss: 15.6131, Val Loss: 16.1429\n",
      "Epoch 30/1000, Loss: 15.5938, Val Loss: 16.1111\n",
      "Epoch 31/1000, Loss: 15.5581, Val Loss: 16.0638\n",
      "Epoch 32/1000, Loss: 15.5083, Val Loss: 16.0138\n",
      "Epoch 33/1000, Loss: 15.5141, Val Loss: 15.9677\n",
      "Epoch 34/1000, Loss: 15.3778, Val Loss: 15.9274\n",
      "Epoch 35/1000, Loss: 15.3915, Val Loss: 15.8942\n",
      "Epoch 36/1000, Loss: 15.2950, Val Loss: 15.8554\n",
      "Epoch 37/1000, Loss: 15.2927, Val Loss: 15.8111\n",
      "Epoch 38/1000, Loss: 15.2286, Val Loss: 15.7619\n",
      "Epoch 39/1000, Loss: 15.1817, Val Loss: 15.7150\n",
      "Epoch 40/1000, Loss: 15.0920, Val Loss: 15.6631\n",
      "Epoch 41/1000, Loss: 15.0849, Val Loss: 15.6104\n",
      "Epoch 42/1000, Loss: 15.0585, Val Loss: 15.5382\n",
      "Epoch 43/1000, Loss: 15.0036, Val Loss: 15.4764\n",
      "Epoch 44/1000, Loss: 14.9908, Val Loss: 15.4300\n",
      "Epoch 45/1000, Loss: 14.8480, Val Loss: 15.3782\n",
      "Epoch 46/1000, Loss: 14.8604, Val Loss: 15.3174\n",
      "Epoch 47/1000, Loss: 14.7956, Val Loss: 15.2571\n",
      "Epoch 48/1000, Loss: 14.7392, Val Loss: 15.2001\n",
      "Epoch 49/1000, Loss: 14.7288, Val Loss: 15.1451\n",
      "Epoch 50/1000, Loss: 14.7135, Val Loss: 15.0718\n",
      "Epoch 51/1000, Loss: 14.6128, Val Loss: 14.9983\n",
      "Epoch 52/1000, Loss: 14.6372, Val Loss: 14.9455\n",
      "Epoch 53/1000, Loss: 14.4095, Val Loss: 14.8918\n",
      "Epoch 54/1000, Loss: 14.5437, Val Loss: 14.8351\n",
      "Epoch 55/1000, Loss: 14.5304, Val Loss: 14.7757\n",
      "Epoch 56/1000, Loss: 14.3723, Val Loss: 14.7339\n",
      "Epoch 57/1000, Loss: 14.3170, Val Loss: 14.7142\n",
      "Epoch 58/1000, Loss: 14.3155, Val Loss: 14.6799\n",
      "Epoch 59/1000, Loss: 14.3047, Val Loss: 14.6319\n",
      "Epoch 60/1000, Loss: 14.2808, Val Loss: 14.6021\n",
      "Epoch 61/1000, Loss: 14.2497, Val Loss: 14.5932\n",
      "Epoch 62/1000, Loss: 14.2110, Val Loss: 14.6366\n",
      "Epoch 63/1000, Loss: 14.2708, Val Loss: 14.6427\n",
      "Epoch 64/1000, Loss: 14.2485, Val Loss: 14.5813\n",
      "Epoch 65/1000, Loss: 14.0867, Val Loss: 14.5687\n",
      "Epoch 66/1000, Loss: 14.2167, Val Loss: 14.5672\n",
      "Epoch 67/1000, Loss: 14.2062, Val Loss: 14.5722\n",
      "Epoch 68/1000, Loss: 14.1006, Val Loss: 14.5670\n",
      "Epoch 69/1000, Loss: 14.1066, Val Loss: 14.5479\n",
      "Epoch 70/1000, Loss: 14.0440, Val Loss: 14.5209\n",
      "Epoch 71/1000, Loss: 13.9317, Val Loss: 14.5050\n",
      "Epoch 72/1000, Loss: 14.0877, Val Loss: 14.5100\n",
      "Epoch 73/1000, Loss: 13.9975, Val Loss: 14.4831\n",
      "Epoch 74/1000, Loss: 14.1271, Val Loss: 14.4358\n",
      "Epoch 75/1000, Loss: 14.0243, Val Loss: 14.4047\n",
      "Epoch 76/1000, Loss: 14.0051, Val Loss: 14.3851\n",
      "Epoch 77/1000, Loss: 14.0453, Val Loss: 14.3649\n",
      "Epoch 78/1000, Loss: 13.9583, Val Loss: 14.3572\n",
      "Epoch 79/1000, Loss: 13.9325, Val Loss: 14.3505\n",
      "Epoch 80/1000, Loss: 14.0050, Val Loss: 14.3316\n",
      "Epoch 81/1000, Loss: 13.8656, Val Loss: 14.3135\n",
      "Epoch 82/1000, Loss: 13.8082, Val Loss: 14.3002\n",
      "Epoch 83/1000, Loss: 13.8448, Val Loss: 14.2886\n",
      "Epoch 84/1000, Loss: 13.8639, Val Loss: 14.2813\n",
      "Epoch 85/1000, Loss: 13.7376, Val Loss: 14.2767\n",
      "Epoch 86/1000, Loss: 13.8627, Val Loss: 14.2742\n",
      "Epoch 87/1000, Loss: 13.7329, Val Loss: 14.2740\n",
      "Epoch 88/1000, Loss: 13.8376, Val Loss: 14.2572\n",
      "Epoch 89/1000, Loss: 13.8063, Val Loss: 14.2386\n",
      "Epoch 90/1000, Loss: 13.8757, Val Loss: 14.2282\n",
      "Epoch 91/1000, Loss: 13.6600, Val Loss: 14.2180\n",
      "Epoch 92/1000, Loss: 13.6374, Val Loss: 14.2043\n",
      "Epoch 93/1000, Loss: 13.5235, Val Loss: 14.1957\n",
      "Epoch 94/1000, Loss: 13.6371, Val Loss: 14.1826\n",
      "Epoch 95/1000, Loss: 13.6795, Val Loss: 14.1816\n",
      "Epoch 96/1000, Loss: 13.7077, Val Loss: 14.1796\n",
      "Epoch 97/1000, Loss: 13.5339, Val Loss: 14.1797\n",
      "Epoch 98/1000, Loss: 13.5173, Val Loss: 14.1777\n",
      "Epoch 99/1000, Loss: 13.6645, Val Loss: 14.1812\n",
      "Epoch 100/1000, Loss: 13.6135, Val Loss: 14.1886\n",
      "Epoch 101/1000, Loss: 13.5624, Val Loss: 14.1847\n",
      "Epoch 102/1000, Loss: 13.4945, Val Loss: 14.1774\n",
      "Epoch 103/1000, Loss: 13.3975, Val Loss: 14.1765\n",
      "Epoch 104/1000, Loss: 13.4823, Val Loss: 14.1844\n",
      "Epoch 105/1000, Loss: 13.4314, Val Loss: 14.1876\n",
      "Epoch 106/1000, Loss: 13.5257, Val Loss: 14.1858\n",
      "Epoch 107/1000, Loss: 13.4505, Val Loss: 14.1873\n",
      "Epoch 108/1000, Loss: 13.4780, Val Loss: 14.1909\n",
      "Epoch 109/1000, Loss: 13.5058, Val Loss: 14.1871\n",
      "Epoch 110/1000, Loss: 13.4854, Val Loss: 14.1893\n",
      "Epoch 111/1000, Loss: 13.3348, Val Loss: 14.1955\n",
      "Epoch 112/1000, Loss: 13.5630, Val Loss: 14.1843\n",
      "Epoch 113/1000, Loss: 13.3256, Val Loss: 14.1782\n",
      "Epoch 114/1000, Loss: 13.3002, Val Loss: 14.1855\n",
      "Epoch 115/1000, Loss: 13.3674, Val Loss: 14.1956\n",
      "Epoch 116/1000, Loss: 13.2601, Val Loss: 14.1835\n",
      "Epoch 117/1000, Loss: 13.2548, Val Loss: 14.1669\n",
      "Epoch 118/1000, Loss: 13.3325, Val Loss: 14.1629\n",
      "Epoch 119/1000, Loss: 13.2720, Val Loss: 14.1769\n",
      "Epoch 120/1000, Loss: 13.2920, Val Loss: 14.1957\n",
      "Epoch 121/1000, Loss: 13.2488, Val Loss: 14.2004\n",
      "Epoch 122/1000, Loss: 13.3264, Val Loss: 14.1855\n",
      "Epoch 123/1000, Loss: 13.2677, Val Loss: 14.1841\n",
      "Epoch 124/1000, Loss: 13.2058, Val Loss: 14.1886\n",
      "Epoch 125/1000, Loss: 13.0475, Val Loss: 14.2121\n",
      "Epoch 126/1000, Loss: 13.3149, Val Loss: 14.2146\n",
      "Epoch 127/1000, Loss: 13.3240, Val Loss: 14.1812\n",
      "Epoch 128/1000, Loss: 13.1472, Val Loss: 14.1602\n",
      "Epoch 129/1000, Loss: 13.4191, Val Loss: 14.1616\n",
      "Epoch 130/1000, Loss: 13.1579, Val Loss: 14.1938\n",
      "Epoch 131/1000, Loss: 13.1704, Val Loss: 14.2123\n",
      "Epoch 132/1000, Loss: 13.2801, Val Loss: 14.2071\n",
      "Epoch 133/1000, Loss: 13.2628, Val Loss: 14.1714\n",
      "Epoch 134/1000, Loss: 13.1054, Val Loss: 14.1465\n",
      "Epoch 135/1000, Loss: 13.1188, Val Loss: 14.1394\n",
      "Epoch 136/1000, Loss: 13.0172, Val Loss: 14.1524\n",
      "Epoch 137/1000, Loss: 13.0478, Val Loss: 14.1746\n",
      "Epoch 138/1000, Loss: 13.1280, Val Loss: 14.2005\n",
      "Epoch 139/1000, Loss: 13.1182, Val Loss: 14.2164\n",
      "Epoch 140/1000, Loss: 12.9775, Val Loss: 14.2172\n",
      "Epoch 141/1000, Loss: 13.0464, Val Loss: 14.1924\n",
      "Epoch 142/1000, Loss: 13.0523, Val Loss: 14.1679\n",
      "Epoch 143/1000, Loss: 13.0795, Val Loss: 14.1534\n",
      "Epoch 144/1000, Loss: 12.9846, Val Loss: 14.1407\n",
      "Epoch 145/1000, Loss: 13.0038, Val Loss: 14.1343\n",
      "Epoch 146/1000, Loss: 12.9613, Val Loss: 14.1293\n",
      "Epoch 147/1000, Loss: 13.0863, Val Loss: 14.1312\n",
      "Epoch 148/1000, Loss: 12.9867, Val Loss: 14.1364\n",
      "Epoch 149/1000, Loss: 12.9424, Val Loss: 14.1511\n",
      "Epoch 150/1000, Loss: 12.8752, Val Loss: 14.1592\n",
      "Epoch 151/1000, Loss: 12.9468, Val Loss: 14.1512\n",
      "Epoch 152/1000, Loss: 12.8625, Val Loss: 14.1470\n",
      "Epoch 153/1000, Loss: 12.8880, Val Loss: 14.1472\n",
      "Epoch 154/1000, Loss: 12.7618, Val Loss: 14.1008\n",
      "Epoch 155/1000, Loss: 12.9100, Val Loss: 14.0895\n",
      "Epoch 156/1000, Loss: 12.8056, Val Loss: 14.1127\n",
      "Epoch 157/1000, Loss: 12.7309, Val Loss: 14.1176\n",
      "Epoch 158/1000, Loss: 12.8827, Val Loss: 14.1007\n",
      "Epoch 159/1000, Loss: 12.8642, Val Loss: 14.0898\n",
      "Epoch 160/1000, Loss: 12.9236, Val Loss: 14.0823\n",
      "Epoch 161/1000, Loss: 12.6901, Val Loss: 14.0732\n",
      "Epoch 162/1000, Loss: 12.8020, Val Loss: 14.0835\n",
      "Epoch 163/1000, Loss: 12.6915, Val Loss: 14.0983\n",
      "Epoch 164/1000, Loss: 12.7727, Val Loss: 14.1178\n",
      "Epoch 165/1000, Loss: 12.6956, Val Loss: 14.1322\n",
      "Epoch 166/1000, Loss: 12.7980, Val Loss: 14.1522\n",
      "Epoch 167/1000, Loss: 12.5530, Val Loss: 14.1744\n",
      "Epoch 168/1000, Loss: 12.7359, Val Loss: 14.1809\n",
      "Epoch 169/1000, Loss: 12.6073, Val Loss: 14.2003\n",
      "Epoch 170/1000, Loss: 12.9647, Val Loss: 14.1945\n",
      "Epoch 171/1000, Loss: 12.5993, Val Loss: 14.1629\n",
      "Epoch 172/1000, Loss: 12.6547, Val Loss: 14.1491\n",
      "Epoch 173/1000, Loss: 12.4883, Val Loss: 14.1395\n",
      "Epoch 174/1000, Loss: 12.5259, Val Loss: 14.1236\n",
      "Epoch 175/1000, Loss: 12.5060, Val Loss: 14.1278\n",
      "Epoch 176/1000, Loss: 12.3624, Val Loss: 14.1434\n",
      "Epoch 177/1000, Loss: 12.4551, Val Loss: 14.1821\n",
      "Epoch 178/1000, Loss: 12.4850, Val Loss: 14.2016\n",
      "Epoch 179/1000, Loss: 12.4418, Val Loss: 14.1932\n",
      "Epoch 180/1000, Loss: 12.3449, Val Loss: 14.1736\n",
      "Epoch 181/1000, Loss: 12.5685, Val Loss: 14.1489\n",
      "Epoch 182/1000, Loss: 12.5823, Val Loss: 14.1171\n",
      "Epoch 183/1000, Loss: 12.4003, Val Loss: 14.0986\n",
      "Epoch 184/1000, Loss: 12.3505, Val Loss: 14.0599\n",
      "Epoch 185/1000, Loss: 12.6288, Val Loss: 14.0519\n",
      "Epoch 186/1000, Loss: 12.4588, Val Loss: 14.0482\n",
      "Epoch 187/1000, Loss: 12.3293, Val Loss: 14.0691\n",
      "Epoch 188/1000, Loss: 12.5162, Val Loss: 14.1084\n",
      "Epoch 189/1000, Loss: 12.4415, Val Loss: 14.0968\n",
      "Epoch 190/1000, Loss: 12.3832, Val Loss: 14.1036\n",
      "Epoch 191/1000, Loss: 12.1535, Val Loss: 14.1338\n",
      "Epoch 192/1000, Loss: 12.1770, Val Loss: 14.1535\n",
      "Epoch 193/1000, Loss: 12.2591, Val Loss: 14.1287\n",
      "Epoch 194/1000, Loss: 12.1641, Val Loss: 14.0996\n",
      "Epoch 195/1000, Loss: 12.1067, Val Loss: 14.0611\n",
      "Epoch 196/1000, Loss: 12.2256, Val Loss: 14.0311\n",
      "Epoch 197/1000, Loss: 12.3691, Val Loss: 14.0128\n",
      "Epoch 198/1000, Loss: 12.2743, Val Loss: 14.0217\n",
      "Epoch 199/1000, Loss: 12.0324, Val Loss: 14.0295\n",
      "Epoch 200/1000, Loss: 12.1253, Val Loss: 14.0558\n",
      "Epoch 201/1000, Loss: 12.2293, Val Loss: 14.0960\n",
      "Epoch 202/1000, Loss: 12.2885, Val Loss: 14.1310\n",
      "Epoch 203/1000, Loss: 12.2016, Val Loss: 14.1577\n",
      "Epoch 204/1000, Loss: 12.0119, Val Loss: 14.1735\n",
      "Epoch 205/1000, Loss: 12.1112, Val Loss: 14.1788\n",
      "Epoch 206/1000, Loss: 12.4981, Val Loss: 14.1499\n",
      "Epoch 207/1000, Loss: 12.1937, Val Loss: 14.1773\n",
      "Epoch 208/1000, Loss: 12.0459, Val Loss: 14.1781\n",
      "Epoch 209/1000, Loss: 12.1379, Val Loss: 14.1511\n",
      "Epoch 210/1000, Loss: 12.2231, Val Loss: 14.1303\n",
      "Epoch 211/1000, Loss: 12.1762, Val Loss: 14.1236\n",
      "Epoch 212/1000, Loss: 12.1207, Val Loss: 14.1177\n",
      "Epoch 213/1000, Loss: 12.0926, Val Loss: 14.1209\n",
      "Epoch 214/1000, Loss: 12.2340, Val Loss: 14.1523\n",
      "Epoch 215/1000, Loss: 12.1006, Val Loss: 14.2439\n",
      "Epoch 216/1000, Loss: 12.1312, Val Loss: 14.2743\n",
      "Epoch 217/1000, Loss: 12.0124, Val Loss: 14.2605\n",
      "Epoch 218/1000, Loss: 12.1598, Val Loss: 14.2429\n",
      "Epoch 219/1000, Loss: 12.0859, Val Loss: 14.2640\n",
      "Epoch 220/1000, Loss: 11.9719, Val Loss: 14.2392\n",
      "Epoch 221/1000, Loss: 12.0108, Val Loss: 14.2121\n",
      "Epoch 222/1000, Loss: 11.9256, Val Loss: 14.2002\n",
      "Epoch 223/1000, Loss: 12.2502, Val Loss: 14.2110\n",
      "Epoch 224/1000, Loss: 11.9356, Val Loss: 14.2148\n",
      "Epoch 225/1000, Loss: 12.2143, Val Loss: 14.2089\n",
      "Epoch 226/1000, Loss: 11.8637, Val Loss: 14.1973\n",
      "Epoch 227/1000, Loss: 12.1396, Val Loss: 14.1809\n",
      "Epoch 228/1000, Loss: 11.8593, Val Loss: 14.1808\n",
      "Epoch 229/1000, Loss: 12.0020, Val Loss: 14.2019\n",
      "Epoch 230/1000, Loss: 11.8462, Val Loss: 14.2309\n",
      "Epoch 231/1000, Loss: 11.7005, Val Loss: 14.2554\n",
      "Epoch 232/1000, Loss: 11.8129, Val Loss: 14.2595\n",
      "Epoch 233/1000, Loss: 11.7168, Val Loss: 14.2670\n",
      "Epoch 234/1000, Loss: 11.9984, Val Loss: 14.2664\n",
      "Epoch 235/1000, Loss: 11.8358, Val Loss: 14.2639\n",
      "Epoch 236/1000, Loss: 11.8348, Val Loss: 14.2600\n",
      "Epoch 237/1000, Loss: 11.7449, Val Loss: 14.2731\n",
      "Early stopping triggered at epoch 237. Best Val Loss: 14.0128\n",
      "Epoch 1/1000, Loss: 16.7968, Val Loss: 16.9614\n",
      "Epoch 2/1000, Loss: 16.7336, Val Loss: 16.9590\n",
      "Epoch 3/1000, Loss: 16.7166, Val Loss: 16.9555\n",
      "Epoch 4/1000, Loss: 16.6950, Val Loss: 16.9514\n",
      "Epoch 5/1000, Loss: 16.6914, Val Loss: 16.9464\n",
      "Epoch 6/1000, Loss: 16.6768, Val Loss: 16.9402\n",
      "Epoch 7/1000, Loss: 16.6671, Val Loss: 16.9327\n",
      "Epoch 8/1000, Loss: 16.6535, Val Loss: 16.9246\n",
      "Epoch 9/1000, Loss: 16.6429, Val Loss: 16.9155\n",
      "Epoch 10/1000, Loss: 16.6243, Val Loss: 16.9050\n",
      "Epoch 11/1000, Loss: 16.6167, Val Loss: 16.8952\n",
      "Epoch 12/1000, Loss: 16.5941, Val Loss: 16.8855\n",
      "Epoch 13/1000, Loss: 16.5647, Val Loss: 16.8751\n",
      "Epoch 14/1000, Loss: 16.5636, Val Loss: 16.8639\n",
      "Epoch 15/1000, Loss: 16.5418, Val Loss: 16.8510\n",
      "Epoch 16/1000, Loss: 16.5206, Val Loss: 16.8367\n",
      "Epoch 17/1000, Loss: 16.5216, Val Loss: 16.8211\n",
      "Epoch 18/1000, Loss: 16.4646, Val Loss: 16.8028\n",
      "Epoch 19/1000, Loss: 16.4644, Val Loss: 16.7829\n",
      "Epoch 20/1000, Loss: 16.4175, Val Loss: 16.7614\n",
      "Epoch 21/1000, Loss: 16.4001, Val Loss: 16.7414\n",
      "Epoch 22/1000, Loss: 16.3635, Val Loss: 16.7211\n",
      "Epoch 23/1000, Loss: 16.3256, Val Loss: 16.6989\n",
      "Epoch 24/1000, Loss: 16.2826, Val Loss: 16.6734\n",
      "Epoch 25/1000, Loss: 16.2488, Val Loss: 16.6466\n",
      "Epoch 26/1000, Loss: 16.2129, Val Loss: 16.6180\n",
      "Epoch 27/1000, Loss: 16.1883, Val Loss: 16.5871\n",
      "Epoch 28/1000, Loss: 16.1364, Val Loss: 16.5552\n",
      "Epoch 29/1000, Loss: 16.0961, Val Loss: 16.5213\n",
      "Epoch 30/1000, Loss: 16.0448, Val Loss: 16.4861\n",
      "Epoch 31/1000, Loss: 16.0091, Val Loss: 16.4506\n",
      "Epoch 32/1000, Loss: 15.9496, Val Loss: 16.4134\n",
      "Epoch 33/1000, Loss: 15.8903, Val Loss: 16.3762\n",
      "Epoch 34/1000, Loss: 15.8569, Val Loss: 16.3374\n",
      "Epoch 35/1000, Loss: 15.8444, Val Loss: 16.2965\n",
      "Epoch 36/1000, Loss: 15.7824, Val Loss: 16.2523\n",
      "Epoch 37/1000, Loss: 15.7302, Val Loss: 16.2029\n",
      "Epoch 38/1000, Loss: 15.6924, Val Loss: 16.1524\n",
      "Epoch 39/1000, Loss: 15.6194, Val Loss: 16.1017\n",
      "Epoch 40/1000, Loss: 15.5877, Val Loss: 16.0512\n",
      "Epoch 41/1000, Loss: 15.5157, Val Loss: 16.0102\n",
      "Epoch 42/1000, Loss: 15.4719, Val Loss: 15.9661\n",
      "Epoch 43/1000, Loss: 15.4468, Val Loss: 15.9197\n",
      "Epoch 44/1000, Loss: 15.4060, Val Loss: 15.8681\n",
      "Epoch 45/1000, Loss: 15.3513, Val Loss: 15.8177\n",
      "Epoch 46/1000, Loss: 15.2640, Val Loss: 15.7711\n",
      "Epoch 47/1000, Loss: 15.1923, Val Loss: 15.7247\n",
      "Epoch 48/1000, Loss: 15.1481, Val Loss: 15.6534\n",
      "Epoch 49/1000, Loss: 15.1084, Val Loss: 15.5965\n",
      "Epoch 50/1000, Loss: 15.0690, Val Loss: 15.5434\n",
      "Epoch 51/1000, Loss: 14.9370, Val Loss: 15.4893\n",
      "Epoch 52/1000, Loss: 14.9103, Val Loss: 15.4254\n",
      "Epoch 53/1000, Loss: 14.8156, Val Loss: 15.3513\n",
      "Epoch 54/1000, Loss: 14.7485, Val Loss: 15.2623\n",
      "Epoch 55/1000, Loss: 14.7780, Val Loss: 15.1791\n",
      "Epoch 56/1000, Loss: 14.6849, Val Loss: 15.1240\n",
      "Epoch 57/1000, Loss: 14.6873, Val Loss: 15.0643\n",
      "Epoch 58/1000, Loss: 14.6206, Val Loss: 15.0067\n",
      "Epoch 59/1000, Loss: 14.5350, Val Loss: 14.9574\n",
      "Epoch 60/1000, Loss: 14.4860, Val Loss: 14.9002\n",
      "Epoch 61/1000, Loss: 14.4200, Val Loss: 14.8253\n",
      "Epoch 62/1000, Loss: 14.3784, Val Loss: 14.7596\n",
      "Epoch 63/1000, Loss: 14.2904, Val Loss: 14.6910\n",
      "Epoch 64/1000, Loss: 14.3125, Val Loss: 14.6427\n",
      "Epoch 65/1000, Loss: 14.2817, Val Loss: 14.6155\n",
      "Epoch 66/1000, Loss: 14.3085, Val Loss: 14.5883\n",
      "Epoch 67/1000, Loss: 14.2291, Val Loss: 14.5459\n",
      "Epoch 68/1000, Loss: 14.2109, Val Loss: 14.4989\n",
      "Epoch 69/1000, Loss: 14.1687, Val Loss: 14.4729\n",
      "Epoch 70/1000, Loss: 14.0800, Val Loss: 14.4298\n",
      "Epoch 71/1000, Loss: 14.1776, Val Loss: 14.3865\n",
      "Epoch 72/1000, Loss: 14.0158, Val Loss: 14.3684\n",
      "Epoch 73/1000, Loss: 13.9780, Val Loss: 14.3621\n",
      "Epoch 74/1000, Loss: 14.0817, Val Loss: 14.3550\n",
      "Epoch 75/1000, Loss: 14.1038, Val Loss: 14.3357\n",
      "Epoch 76/1000, Loss: 13.9162, Val Loss: 14.3175\n",
      "Epoch 77/1000, Loss: 13.8528, Val Loss: 14.3110\n",
      "Epoch 78/1000, Loss: 13.9169, Val Loss: 14.3058\n",
      "Epoch 79/1000, Loss: 13.8563, Val Loss: 14.3039\n",
      "Epoch 80/1000, Loss: 13.8429, Val Loss: 14.2974\n",
      "Epoch 81/1000, Loss: 13.9179, Val Loss: 14.3086\n",
      "Epoch 82/1000, Loss: 13.8436, Val Loss: 14.3236\n",
      "Epoch 83/1000, Loss: 13.9235, Val Loss: 14.3206\n",
      "Epoch 84/1000, Loss: 13.8005, Val Loss: 14.2895\n",
      "Epoch 85/1000, Loss: 13.8742, Val Loss: 14.2772\n",
      "Epoch 86/1000, Loss: 13.7767, Val Loss: 14.2709\n",
      "Epoch 87/1000, Loss: 13.8938, Val Loss: 14.2662\n",
      "Epoch 88/1000, Loss: 13.6905, Val Loss: 14.2513\n",
      "Epoch 89/1000, Loss: 13.8355, Val Loss: 14.2212\n",
      "Epoch 90/1000, Loss: 13.7325, Val Loss: 14.2058\n",
      "Epoch 91/1000, Loss: 13.6099, Val Loss: 14.1848\n",
      "Epoch 92/1000, Loss: 13.7109, Val Loss: 14.1676\n",
      "Epoch 93/1000, Loss: 13.5440, Val Loss: 14.1552\n",
      "Epoch 94/1000, Loss: 13.7227, Val Loss: 14.1473\n",
      "Epoch 95/1000, Loss: 13.6308, Val Loss: 14.1371\n",
      "Epoch 96/1000, Loss: 13.5385, Val Loss: 14.1385\n",
      "Epoch 97/1000, Loss: 13.5118, Val Loss: 14.1428\n",
      "Epoch 98/1000, Loss: 13.7039, Val Loss: 14.1436\n",
      "Epoch 99/1000, Loss: 13.5282, Val Loss: 14.1424\n",
      "Epoch 100/1000, Loss: 13.6258, Val Loss: 14.1425\n",
      "Epoch 101/1000, Loss: 13.4684, Val Loss: 14.1357\n",
      "Epoch 102/1000, Loss: 13.3543, Val Loss: 14.1275\n",
      "Epoch 103/1000, Loss: 13.5453, Val Loss: 14.1320\n",
      "Epoch 104/1000, Loss: 13.3598, Val Loss: 14.1628\n",
      "Epoch 105/1000, Loss: 13.4021, Val Loss: 14.1534\n",
      "Epoch 106/1000, Loss: 13.6242, Val Loss: 14.1100\n",
      "Epoch 107/1000, Loss: 13.3512, Val Loss: 14.1068\n",
      "Epoch 108/1000, Loss: 13.4045, Val Loss: 14.1237\n",
      "Epoch 109/1000, Loss: 13.4378, Val Loss: 14.1552\n",
      "Epoch 110/1000, Loss: 13.3463, Val Loss: 14.2006\n",
      "Epoch 111/1000, Loss: 13.5427, Val Loss: 14.2195\n",
      "Epoch 112/1000, Loss: 13.2638, Val Loss: 14.2016\n",
      "Epoch 113/1000, Loss: 13.3568, Val Loss: 14.1747\n",
      "Epoch 114/1000, Loss: 13.2773, Val Loss: 14.1634\n",
      "Epoch 115/1000, Loss: 13.5272, Val Loss: 14.1634\n",
      "Epoch 116/1000, Loss: 13.3625, Val Loss: 14.1648\n",
      "Epoch 117/1000, Loss: 13.2057, Val Loss: 14.1531\n",
      "Epoch 118/1000, Loss: 13.3646, Val Loss: 14.1330\n",
      "Epoch 119/1000, Loss: 13.4655, Val Loss: 14.1223\n",
      "Epoch 120/1000, Loss: 13.2573, Val Loss: 14.1149\n",
      "Epoch 121/1000, Loss: 13.1433, Val Loss: 14.1039\n",
      "Epoch 122/1000, Loss: 13.2008, Val Loss: 14.0920\n",
      "Epoch 123/1000, Loss: 13.3208, Val Loss: 14.0667\n",
      "Epoch 124/1000, Loss: 13.1209, Val Loss: 14.0480\n",
      "Epoch 125/1000, Loss: 13.0602, Val Loss: 14.0351\n",
      "Epoch 126/1000, Loss: 13.1562, Val Loss: 14.0245\n",
      "Epoch 127/1000, Loss: 13.1419, Val Loss: 14.0373\n",
      "Epoch 128/1000, Loss: 13.3257, Val Loss: 14.0665\n",
      "Epoch 129/1000, Loss: 13.1356, Val Loss: 14.0769\n",
      "Epoch 130/1000, Loss: 13.0156, Val Loss: 14.0896\n",
      "Epoch 131/1000, Loss: 13.0993, Val Loss: 14.1023\n",
      "Epoch 132/1000, Loss: 13.1210, Val Loss: 14.0944\n",
      "Epoch 133/1000, Loss: 13.1112, Val Loss: 14.0832\n",
      "Epoch 134/1000, Loss: 13.0354, Val Loss: 14.0752\n",
      "Epoch 135/1000, Loss: 12.9867, Val Loss: 14.0636\n",
      "Epoch 136/1000, Loss: 13.1649, Val Loss: 14.0498\n",
      "Epoch 137/1000, Loss: 12.8542, Val Loss: 14.0483\n",
      "Epoch 138/1000, Loss: 12.9335, Val Loss: 14.0466\n",
      "Epoch 139/1000, Loss: 12.8423, Val Loss: 14.0595\n",
      "Epoch 140/1000, Loss: 13.0686, Val Loss: 14.0547\n",
      "Epoch 141/1000, Loss: 12.8313, Val Loss: 14.0693\n",
      "Epoch 142/1000, Loss: 12.7150, Val Loss: 14.0791\n",
      "Epoch 143/1000, Loss: 12.7253, Val Loss: 14.0801\n",
      "Epoch 144/1000, Loss: 12.8731, Val Loss: 14.0783\n",
      "Epoch 145/1000, Loss: 12.6940, Val Loss: 14.0707\n",
      "Epoch 146/1000, Loss: 12.6476, Val Loss: 14.0537\n",
      "Epoch 147/1000, Loss: 12.8163, Val Loss: 14.0328\n",
      "Epoch 148/1000, Loss: 12.7850, Val Loss: 14.0427\n",
      "Epoch 149/1000, Loss: 12.8152, Val Loss: 14.0582\n",
      "Epoch 150/1000, Loss: 12.8726, Val Loss: 14.0502\n",
      "Epoch 151/1000, Loss: 12.8519, Val Loss: 14.0287\n",
      "Epoch 152/1000, Loss: 12.6973, Val Loss: 14.0105\n",
      "Epoch 153/1000, Loss: 12.7978, Val Loss: 14.0003\n",
      "Epoch 154/1000, Loss: 12.8274, Val Loss: 13.9930\n",
      "Epoch 155/1000, Loss: 12.7737, Val Loss: 13.9950\n",
      "Epoch 156/1000, Loss: 12.8025, Val Loss: 13.9876\n",
      "Epoch 157/1000, Loss: 12.4754, Val Loss: 13.9611\n",
      "Epoch 158/1000, Loss: 12.5350, Val Loss: 13.9475\n",
      "Epoch 159/1000, Loss: 12.5136, Val Loss: 13.9329\n",
      "Epoch 160/1000, Loss: 12.5763, Val Loss: 13.9218\n",
      "Epoch 161/1000, Loss: 12.5046, Val Loss: 13.9115\n",
      "Epoch 162/1000, Loss: 12.6575, Val Loss: 13.9116\n",
      "Epoch 163/1000, Loss: 12.6453, Val Loss: 13.9257\n",
      "Epoch 164/1000, Loss: 12.4951, Val Loss: 13.9681\n",
      "Epoch 165/1000, Loss: 12.4417, Val Loss: 13.9857\n",
      "Epoch 166/1000, Loss: 12.4203, Val Loss: 14.0281\n",
      "Epoch 167/1000, Loss: 12.3817, Val Loss: 14.0614\n",
      "Epoch 168/1000, Loss: 12.4130, Val Loss: 14.0566\n",
      "Epoch 169/1000, Loss: 12.4449, Val Loss: 14.0345\n",
      "Epoch 170/1000, Loss: 12.6201, Val Loss: 14.0197\n",
      "Epoch 171/1000, Loss: 12.7527, Val Loss: 13.9974\n",
      "Epoch 172/1000, Loss: 12.2305, Val Loss: 13.9490\n",
      "Epoch 173/1000, Loss: 12.1742, Val Loss: 13.9052\n",
      "Epoch 174/1000, Loss: 12.6510, Val Loss: 13.8961\n",
      "Epoch 175/1000, Loss: 12.1424, Val Loss: 13.9120\n",
      "Epoch 176/1000, Loss: 12.3511, Val Loss: 13.8956\n",
      "Epoch 177/1000, Loss: 12.4848, Val Loss: 13.9337\n",
      "Epoch 178/1000, Loss: 12.4051, Val Loss: 13.9855\n",
      "Epoch 179/1000, Loss: 12.3864, Val Loss: 14.0007\n",
      "Epoch 180/1000, Loss: 12.2808, Val Loss: 13.9638\n",
      "Epoch 181/1000, Loss: 12.1312, Val Loss: 13.9649\n",
      "Epoch 182/1000, Loss: 12.3807, Val Loss: 13.9571\n",
      "Epoch 183/1000, Loss: 12.1233, Val Loss: 13.9417\n",
      "Epoch 184/1000, Loss: 12.3015, Val Loss: 13.9220\n",
      "Epoch 185/1000, Loss: 12.1737, Val Loss: 13.8892\n",
      "Epoch 186/1000, Loss: 12.1393, Val Loss: 13.8824\n",
      "Epoch 187/1000, Loss: 12.0208, Val Loss: 13.8792\n",
      "Epoch 188/1000, Loss: 12.2472, Val Loss: 13.8700\n",
      "Epoch 189/1000, Loss: 12.2137, Val Loss: 13.8882\n",
      "Epoch 190/1000, Loss: 12.2917, Val Loss: 13.9072\n",
      "Epoch 191/1000, Loss: 12.0588, Val Loss: 13.9302\n",
      "Epoch 192/1000, Loss: 12.1791, Val Loss: 13.9612\n",
      "Epoch 193/1000, Loss: 12.0745, Val Loss: 14.0013\n",
      "Epoch 194/1000, Loss: 12.2757, Val Loss: 14.0519\n",
      "Epoch 195/1000, Loss: 12.0068, Val Loss: 14.0696\n",
      "Epoch 196/1000, Loss: 11.9925, Val Loss: 14.0805\n",
      "Epoch 197/1000, Loss: 11.8462, Val Loss: 14.0637\n",
      "Epoch 198/1000, Loss: 12.1830, Val Loss: 14.0702\n",
      "Epoch 199/1000, Loss: 11.9727, Val Loss: 14.0600\n",
      "Epoch 200/1000, Loss: 11.9604, Val Loss: 14.0588\n",
      "Epoch 201/1000, Loss: 11.8318, Val Loss: 14.0588\n",
      "Epoch 202/1000, Loss: 12.1113, Val Loss: 14.0431\n",
      "Epoch 203/1000, Loss: 11.9743, Val Loss: 14.0430\n",
      "Epoch 204/1000, Loss: 12.1489, Val Loss: 14.0299\n",
      "Epoch 205/1000, Loss: 11.8745, Val Loss: 14.0166\n",
      "Epoch 206/1000, Loss: 11.8788, Val Loss: 14.0230\n",
      "Epoch 207/1000, Loss: 11.7349, Val Loss: 14.0191\n",
      "Epoch 208/1000, Loss: 11.9154, Val Loss: 14.0257\n",
      "Epoch 209/1000, Loss: 11.7826, Val Loss: 14.0317\n",
      "Epoch 210/1000, Loss: 11.6795, Val Loss: 14.0541\n",
      "Epoch 211/1000, Loss: 11.7965, Val Loss: 14.0703\n",
      "Epoch 212/1000, Loss: 11.8370, Val Loss: 14.1008\n",
      "Epoch 213/1000, Loss: 11.7418, Val Loss: 14.1103\n",
      "Epoch 214/1000, Loss: 12.9301, Val Loss: 14.1493\n",
      "Epoch 215/1000, Loss: 12.0037, Val Loss: 14.1648\n",
      "Epoch 216/1000, Loss: 11.9164, Val Loss: 14.1199\n",
      "Epoch 217/1000, Loss: 12.0934, Val Loss: 14.0634\n",
      "Epoch 218/1000, Loss: 12.1455, Val Loss: 14.0255\n",
      "Epoch 219/1000, Loss: 11.9602, Val Loss: 13.9958\n",
      "Epoch 220/1000, Loss: 11.8849, Val Loss: 13.9908\n",
      "Epoch 221/1000, Loss: 11.8985, Val Loss: 13.9827\n",
      "Epoch 222/1000, Loss: 12.1847, Val Loss: 13.9733\n",
      "Epoch 223/1000, Loss: 11.7720, Val Loss: 14.0128\n",
      "Epoch 224/1000, Loss: 11.7215, Val Loss: 14.0765\n",
      "Epoch 225/1000, Loss: 11.7562, Val Loss: 14.1369\n",
      "Epoch 226/1000, Loss: 11.8414, Val Loss: 14.1623\n",
      "Epoch 227/1000, Loss: 11.7809, Val Loss: 14.1503\n",
      "Epoch 228/1000, Loss: 11.8162, Val Loss: 14.1503\n",
      "Early stopping triggered at epoch 228. Best Val Loss: 13.8700\n"
     ]
    }
   ],
   "source": [
    "error_arr_input_weighted = []\n",
    "std_arr_input_weighted = []\n",
    "max_error_arr_input_weighted = []\n",
    "\n",
    "for i in range(10):\n",
    "    bias_predictor = train_bias_predictor(\n",
    "        torch.tensor(\n",
    "            np.vstack(X_train['reconstruction_error_percentage_array_weighted'].values),  # Stack arrays into a 2D array\n",
    "            dtype=torch.float32\n",
    "        ).to(device),\n",
    "        torch.tensor(y_train, dtype=torch.float32).to(device),\n",
    "        torch.tensor(\n",
    "            np.vstack(X_val['reconstruction_error_percentage_array_weighted'].values),  # Stack arrays into a 2D array\n",
    "            dtype=torch.float32\n",
    "        ).to(device),\n",
    "        torch.tensor(y_val, dtype=torch.float32).to(device),\n",
    "        epochs=1000,\n",
    "        learning_rate=0.001,\n",
    "        patience=40\n",
    "    )\n",
    "    features = torch.tensor(np.vstack(X_test['reconstruction_error_percentage_array_weighted'].values), dtype=torch.float32).to(device)\n",
    "    predictions = bias_predictor(features).squeeze().cpu().detach().numpy()\n",
    "\n",
    "    X_test['predicted_bias_with_array_input_weighted'] = np.nan\n",
    "    X_test['predicted_bias_with_array_input_weighted'] = predictions\n",
    "\n",
    "    X_test['new_predictions_with_array_input_weighted'] = X_test['predictions'] + X_test['predicted_bias_with_array_input_weighted']\n",
    "    X_test['new_error_percentage_with_array_input_weighted'] = (abs(X_test['target'] - X_test['new_predictions_with_array_input_weighted']) / X_test['target']) * 100\n",
    "    avg_error = X_test['new_error_percentage_with_array_input_weighted'].mean()\n",
    "    std_error = X_test['new_error_percentage_with_array_input_weighted'].std()\n",
    "    max_error = X_test['new_error_percentage_with_array_input_weighted'].max()\n",
    "    error_arr_input_weighted.append(avg_error)\n",
    "    std_arr_input_weighted.append(std_error)\n",
    "    max_error_arr_input_weighted.append(max_error)\n",
    "    \n",
    "\n",
    "combined_df = pd.DataFrame({'error_one_input': error_one_input, 'std_one_input': std_one_input, 'error_arr_input': error_arr_input, 'std_arr_input': std_arr_input, 'error_arr_input_weighted': error_arr_input_weighted, 'std_arr_input_weighted': std_arr_input_weighted})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_one_input</th>\n",
       "      <th>std_one_input</th>\n",
       "      <th>error_arr_input</th>\n",
       "      <th>std_arr_input</th>\n",
       "      <th>error_arr_input_weighted</th>\n",
       "      <th>std_arr_input_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.199795</td>\n",
       "      <td>1.988540</td>\n",
       "      <td>1.789094</td>\n",
       "      <td>1.628445</td>\n",
       "      <td>1.767203</td>\n",
       "      <td>1.617722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.034791</td>\n",
       "      <td>0.038615</td>\n",
       "      <td>0.029418</td>\n",
       "      <td>0.016707</td>\n",
       "      <td>0.031655</td>\n",
       "      <td>0.026536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.141825</td>\n",
       "      <td>1.931153</td>\n",
       "      <td>1.749680</td>\n",
       "      <td>1.603995</td>\n",
       "      <td>1.720592</td>\n",
       "      <td>1.580207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.180631</td>\n",
       "      <td>1.966133</td>\n",
       "      <td>1.768953</td>\n",
       "      <td>1.617650</td>\n",
       "      <td>1.748274</td>\n",
       "      <td>1.594662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.197196</td>\n",
       "      <td>1.980797</td>\n",
       "      <td>1.786308</td>\n",
       "      <td>1.624155</td>\n",
       "      <td>1.769321</td>\n",
       "      <td>1.622635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.225092</td>\n",
       "      <td>2.020721</td>\n",
       "      <td>1.805428</td>\n",
       "      <td>1.637376</td>\n",
       "      <td>1.784392</td>\n",
       "      <td>1.630596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.247472</td>\n",
       "      <td>2.049757</td>\n",
       "      <td>1.844594</td>\n",
       "      <td>1.658586</td>\n",
       "      <td>1.825230</td>\n",
       "      <td>1.659043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       error_one_input  std_one_input  error_arr_input  std_arr_input  \\\n",
       "count        10.000000      10.000000        10.000000      10.000000   \n",
       "mean          2.199795       1.988540         1.789094       1.628445   \n",
       "std           0.034791       0.038615         0.029418       0.016707   \n",
       "min           2.141825       1.931153         1.749680       1.603995   \n",
       "25%           2.180631       1.966133         1.768953       1.617650   \n",
       "50%           2.197196       1.980797         1.786308       1.624155   \n",
       "75%           2.225092       2.020721         1.805428       1.637376   \n",
       "max           2.247472       2.049757         1.844594       1.658586   \n",
       "\n",
       "       error_arr_input_weighted  std_arr_input_weighted  \n",
       "count                 10.000000               10.000000  \n",
       "mean                   1.767203                1.617722  \n",
       "std                    0.031655                0.026536  \n",
       "min                    1.720592                1.580207  \n",
       "25%                    1.748274                1.594662  \n",
       "50%                    1.769321                1.622635  \n",
       "75%                    1.784392                1.630596  \n",
       "max                    1.825230                1.659043  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_error_one_input</th>\n",
       "      <th>max_error_arr_input</th>\n",
       "      <th>max_error_arr_input_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.670249</td>\n",
       "      <td>13.467119</td>\n",
       "      <td>13.401450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.188573</td>\n",
       "      <td>0.632626</td>\n",
       "      <td>0.298046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.435174</td>\n",
       "      <td>12.884786</td>\n",
       "      <td>13.038387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.553108</td>\n",
       "      <td>13.219586</td>\n",
       "      <td>13.151782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.626296</td>\n",
       "      <td>13.280163</td>\n",
       "      <td>13.334987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.787869</td>\n",
       "      <td>13.570499</td>\n",
       "      <td>13.628608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18.043797</td>\n",
       "      <td>15.097001</td>\n",
       "      <td>13.827979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       max_error_one_input  max_error_arr_input  max_error_arr_input_weighted\n",
       "count            10.000000            10.000000                     10.000000\n",
       "mean             17.670249            13.467119                     13.401450\n",
       "std               0.188573             0.632626                      0.298046\n",
       "min              17.435174            12.884786                     13.038387\n",
       "25%              17.553108            13.219586                     13.151782\n",
       "50%              17.626296            13.280163                     13.334987\n",
       "75%              17.787869            13.570499                     13.628608\n",
       "max              18.043797            15.097001                     13.827979"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_df = pd.DataFrame({'max_error_one_input': max_error_one_input, 'max_error_arr_input': max_error_arr_input, 'max_error_arr_input_weighted': max_error_arr_input_weighted})\n",
    "max_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
