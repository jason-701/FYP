{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Day', 'Weekday',\n",
      "       'Week', 'Month', 'Year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('SPY.csv')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]  \n",
    "dates = df[\"Date\"]\n",
    "# if \"Open\" and \"High\" and \"Low\" and \"Close\" and \"Volume\" in df.columns:\n",
    "#     df = df[feature_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X[feature_columns]), columns=feature_columns, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test-validation split\n",
    "X_train_val, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_val = X_val.values\n",
    "X_test = X_test.values\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, X_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, X_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, X_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files exist. Loading the models...\n",
      "Mean Squared Error (MSE): 0.00010549359785044233\n",
      "Mean Absolute Error (MAE): 0.00756581290235826\n",
      "Reconstruction Error Percentage: 0.010549359785044232\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and print the device being used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),  # Reduced number of neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),  # Reduced dropout rate\n",
    "            nn.Linear(64, encoding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Define the Decoder model with reduced complexity and dropout\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoding_dim, input_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),  # Reduced number of neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),  # Reduced dropout rate\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "# Define input dimensions\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Model file paths\n",
    "encoder_model_file = './models/encoder.pth'\n",
    "decoder_model_file = './models/decoder.pth'\n",
    "\n",
    "# Hyperparameters\n",
    "encoding_dim = 8\n",
    "learning_rate = 0.001\n",
    "epochs = 50  # Reduced epochs for quick testing\n",
    "batch_size = 16  # Reduced batch size to reduce memory usage\n",
    "accumulation_steps = 2  # Gradient accumulation steps\n",
    "patience = 5  # Early stopping patience\n",
    "\n",
    "# Check if the model files exist\n",
    "if os.path.exists(encoder_model_file) and os.path.exists(decoder_model_file):\n",
    "    print(\"Model files exist. Loading the models...\")\n",
    "    encoder = Encoder(input_dim, encoding_dim).to(device)\n",
    "    decoder = Decoder(encoding_dim, input_dim).to(device)\n",
    "    encoder.load_state_dict(torch.load(encoder_model_file))\n",
    "    decoder.load_state_dict(torch.load(decoder_model_file))\n",
    "else:\n",
    "    print(\"Model files do not exist. Training new models...\")\n",
    "\n",
    "    # Initialize the encoder and decoder models\n",
    "    encoder = Encoder(input_dim, encoding_dim).to(device)\n",
    "    decoder = Decoder(encoding_dim, input_dim).to(device)\n",
    "    optimizer_encoder = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    optimizer_decoder = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Track training loss\n",
    "    encoder_losses = []\n",
    "    decoder_losses = []\n",
    "\n",
    "    # Train the Encoder separately\n",
    "    encoder.train()\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        dataloader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device), torch.tensor(X_train, dtype=torch.float32).to(device)), batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        for i, (batch_X, _) in enumerate(dataloader):\n",
    "            batch_X = batch_X.to(device)\n",
    "            optimizer_encoder.zero_grad()\n",
    "            encoded = encoder(batch_X)\n",
    "            # Pass the encoded output through the decoder\n",
    "            decoded = decoder(encoded)\n",
    "            # Calculate loss between decoder output and original input\n",
    "            loss = criterion(decoded, batch_X)\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer_encoder.step()\n",
    "                optimizer_encoder.zero_grad()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        encoder_losses.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Encoder Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation loss\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            val_encoded = encoder(torch.tensor(X_val, dtype=torch.float32).to(device))\n",
    "            val_decoded = decoder(val_encoded)\n",
    "            val_loss = criterion(val_decoded, torch.tensor(X_val, dtype=torch.float32).to(device)).item()\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Train the Decoder separately\n",
    "    encoded_train = encoder(torch.tensor(X_train, dtype=torch.float32).to(device)).detach()\n",
    "    decoder.train()\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        dataloader = DataLoader(TensorDataset(encoded_train, torch.tensor(X_train, dtype=torch.float32).to(device)), batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "        for i, (batch_X, batch_y) in enumerate(dataloader):\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer_decoder.zero_grad()\n",
    "            outputs = decoder(batch_X)\n",
    "            # Calculate loss between decoder output and original input\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer_decoder.step()\n",
    "                optimizer_decoder.zero_grad()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        decoder_losses.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Decoder Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation loss\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = decoder(encoded_train)\n",
    "            val_loss = criterion(val_outputs, torch.tensor(X_train, dtype=torch.float32).to(device)).item()\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(\"Early stopping counter:\", patience_counter)\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Save the trained models\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    torch.save(encoder.state_dict(), encoder_model_file)\n",
    "    torch.save(decoder.state_dict(), decoder_model_file)\n",
    "    print(\"Models trained and saved.\")\n",
    "\n",
    "# Combine Encoder and Decoder to form the Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "autoencoder = Autoencoder(encoder, decoder).to(device)\n",
    "autoencoder.eval()\n",
    "\n",
    "# Calculate reconstruction error on the test set\n",
    "with torch.no_grad():\n",
    "    reconstructed_test = autoencoder(torch.tensor(X_test, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = np.mean((X_test - reconstructed_test) ** 2)\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = np.mean(np.abs(X_test - reconstructed_test))\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "\n",
    "# Calculate Reconstruction Error Percentage\n",
    "reconstruction_error = np.mean(np.square(X_test - reconstructed_test), axis=1)\n",
    "reconstruction_error_percentage = np.mean(reconstruction_error) * 100\n",
    "print(f'Reconstruction Error Percentage: {reconstruction_error_percentage}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
